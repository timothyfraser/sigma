[["introduction.html", "System Reliability and Six Sigma in R Introduction", " System Reliability and Six Sigma in R Timothy Fraser, PhD 2023-07-18 Introduction Your online textbook for learning reliability and six sigma techniques in R! These coding workshops were made for Cornell University Course SYSEN 5300. Follow along with RStudio.Cloud to learn to apply six sigma techniques in R! Figure 0.1: Photo by Naser Tamimi on Unsplash "],["workshop-coding-in-r.html", "1 Workshop: Coding in R Getting Started 1.1 Introduction to R 1.2 Basic Calculations in R LC 1 1.3 Types of Values in R 1.4 Types of Data in R LC 2 1.5 Common Functions in R 1.6 6. Missing Data LC 3 1.7 Packages 1.8 Visualizing Data with Histograms LC 4 1.9 Wrap-up", " 1 Workshop: Coding in R Welcome to RStudio Cloud! You made it! This document will introduce you to how to start coding in R, using RStudio Cloud. We will use the R statistical coding language frequently in class to conduct analyses and visualization. Hello world! We are coding in R! Getting Started Making an RStudio.Cloud account We’ll be using RStudio.Cloud, a virtual version of R you can access from any computer with an internet browser (PC, Mac, Chromebook, anything). To get set up, please follow the steps in this short Video playlist! Accessing your First RStudio.Cloud Tutorial Today’s tutorial is at the following link: https://timothyfraser.github.io/sysen/workshop_1 Please copy and paste it into your browser url bar! For a quick visual orientation, take a peek at the image below. Read and follow along with the instructions on the webpage! Read the tutorial code (below), and then type it in and run it in your R session! Figure 1.1: Visual Intro to Using RStudio.Cloud 1.1 Introduction to R The document in your RStudio Cloud project document is an ‘R script.’ (its name ends in .R). It contains two kinds of text: ‘code’ - instructions to our statistical calculator ‘comments’ - any text that immediately follows a ‘#’ sign. # For example, # Comments are ignored by the calculator, so we can write ourselves notes. Notice: 4 windows in R. Window 1 (upper left): Scripts! Window 2 (bottom left): Console (this shows the output for our calculator) Window 3 (upper right): Environment (this shows any data the computer is holding onto for us) Window 4 (bottom right): Files (this shows our working project folder, our scripts, and any data files.) A few tips: To change the background theme (and save your eyes), go to Tools &gt;&gt; Global Options &gt;&gt; Appearance &gt;&gt; Editor Theme &gt;&gt; Dracula To increase the font size, go to Tools &gt;&gt; Global Options &gt;&gt; Appearance &gt;&gt; Editor Font Size To make a script, go to File &gt;&gt; New File &gt;&gt; R Script, then save it and name it. Figure 1.2: Open New Script Figure 1.3: Save New Script! Let’s learn to use R! 1.2 Basic Calculations in R Try highlighting the following with your cursor, and then press CTRL and ENTER simultaneously, or the ‘Run’ button above. Addition: 1 + 5 ## [1] 6 Subtraction: 5 - 2 ## [1] 3 Multiplication: 2 * 3 ## [1] 6 Division: 15 / 5 ## [1] 3 Exponents: 2^2 ## [1] 4 Square-Roots: sqrt(4) ## [1] 2 Order of Operations: Still applies! Like in math normally, R calculations are evaluated from left to right, prioritizing parentheses, then multiplication and division, then addition and subtraction. 2 * 2 - 5 ## [1] -1 Use Parentheses! 2 * (2 - 5) ## [1] -6 LC 1 ‘LC’ stands for Learning Check! Our tutorials have questions throughout to give you some short coding challenges to try and work through. Below is the question tab. Read the question, and try to answer it on your own! Then, click the answer button to see the answer. (Note: There are often many different ways to code the same thing!) Feeling stumped? You can check the answer, but be sure to code it yourself afterwards! Question Try calculating something wild in R! Solve for x below using the commands you just learned in R! \\(x = \\sqrt{ (\\frac{2 - 5 }{5})^{4} }\\) \\(x = (1 - 7)^{2} \\times 5 - \\sqrt{49}\\) \\(x = 2^{2} + 2^{2} \\times 2^{2} - 2^{2} \\div 2^{2}\\) Answer Here’s how we coded it! How does yours compare? If your result is different, compare code. What’s different? Be sure to go back and adjust your code so you understand the answer! \\(x = \\sqrt{ (\\frac{2 - 5 }{5})^{4} }\\) sqrt( ((2 - 5) / 5)^4 ) ## [1] 0.36 \\(x = (1 - 7)^{2} \\times 5 - \\sqrt{49}\\) (1 - 7)^2 * 5 - sqrt(49) ## [1] 173 \\(x = 2^{2} + 2^{2} \\times 2^{2} - 2^{2} \\div 2^{2}\\) 2^2 + 2^2 * 2^2 - 2^2 / 2^2 ## [1] 19 1.3 Types of Values in R R accepts 2 type of data: # Numeric Values 15000 ## [1] 15000 0.0005 ## [1] 5e-04 -8222 # notice no commas allowed ## [1] -8222 and # Character Strings &quot;Coding!&quot; # Uses quotation marks ## [1] &quot;Coding!&quot; &quot;Corgis!&quot; # Can contain anything - numbers, characters, etc. ## [1] &quot;Corgis!&quot; &quot;Coffee!&quot; ## [1] &quot;Coffee!&quot; (Note: R also uses something called factors, which are characters, but have a specific order. We’ll learn them later.) 1.4 Types of Data in R 1.4.1 Values First, R uses values - which are single numbers or characters. 2 # this is a value ## [1] 2 &quot;x&quot; # this is also a value ## [1] &quot;x&quot; You can save a value as a named object in the R Environment. That means, we tell R to remember that whenever you use a certain name, it means that value. To name something as an object, use an arrow! myvalue &lt;- 2 Now let’s highlight and press CTRL ENTER on myvalue (or the Mac Equivalent). myvalue ## [1] 2 Notice how it’s listed in the R Environment (upper right), and how it outputs as 2 in the console? We can do operations too! secondvalue &lt;- myvalue + 2 # add 2 to myvalue secondvalue # check new value - oooh, it&#39;s 4! ## [1] 4 We can also overwrite old objects with new objects. myvalue &lt;- &quot;I overwrote it!&quot; myvalue ## [1] &quot;I overwrote it!&quot; And we can also remove objects from the Environment, with remove(). remove(myvalue, secondvalue) 1.4.2 Vectors Second, R contains values in vectors, which are sets of values. # This is a numeric vector c(1, 4, 8) # is the same as 1, 4, 8 ## [1] 1 4 8 and… # This is a character vector c(&quot;Boston&quot;, &quot;New York&quot;, &quot;Los Angeles&quot;) ## [1] &quot;Boston&quot; &quot;New York&quot; &quot;Los Angeles&quot; But if you combine numeric and character values in one vector… # This doesn&#39;t work - R immediately makes it into a character vector c(1, &quot;Boston&quot;, 2) ## [1] &quot;1&quot; &quot;Boston&quot; &quot;2&quot; Why do we use vectors? Because you can do mathematical operations on entire vectors of values, all at once! c(1,2,3,4) * 2 # this multiplies each value by 2! ## [1] 2 4 6 8 c(1,2,3,4) + 2 # this adds 2 to each value! ## [1] 3 4 5 6 We can save vectors as objects too! # Here&#39;s a vector of (hypothetical) seawall heights in 10 towns. myheights &lt;- c(4, 4.5, 5, 5, 5, 5.5, 5.5, 6, 6.5, 6.5) # And here&#39;s a list of hypothetical names for those towns mytowns &lt;- c(&quot;Gloucester&quot;, &quot;Newburyport&quot;, &quot;Provincetown&quot;, &quot;Plymouth&quot;, &quot;Marblehead&quot;, &quot;Chatham&quot;, &quot;Salem&quot;, &quot;Ipswich&quot;, &quot;Falmouth&quot;, &quot;Boston&quot;) # And here&#39;s a list of years when those seawalls were each built. myyears &lt;- c(1990, 1980, 1970, 1930, 1975, 1975, 1980, 1920, 1995, 2000) Plus, we can still do operations on entire vectors! myyears + 1 ## [1] 1991 1981 1971 1931 1976 1976 1981 1921 1996 2001 1.4.3 Dataframes Third, R bundles vectors into data.frames. # Using the data.frame command, we make a data.frame, data.frame( height = myheights, # length 10 town = mytowns, # length 10 year = myyears) # length 10 ## height town year ## 1 4.0 Gloucester 1990 ## 2 4.5 Newburyport 1980 ## 3 5.0 Provincetown 1970 ## 4 5.0 Plymouth 1930 ## 5 5.0 Marblehead 1975 ## 6 5.5 Chatham 1975 ## 7 5.5 Salem 1980 ## 8 6.0 Ipswich 1920 ## 9 6.5 Falmouth 1995 ## 10 6.5 Boston 2000 And inside, we put a bunch of vectors of EQUAL LENGTHS, giving each vector a name. And when it outputs in the console, it looks like a spreadsheet! BECAUSE ALL SPREADSHEETS ARE DATAFRAMES! AND ALL COLUMNS ARE VECTORS! AND ALL CELLS ARE VALUES! Actually, we can make data.frames into objects too! # Let&#39;s name our data.frame about seawalls &#39;sw&#39; sw &lt;- data.frame( height = myheights, town = mytowns, year = myyears) # Notice this last parenthesis; very important # Check the contents of sw! sw ## height town year ## 1 4.0 Gloucester 1990 ## 2 4.5 Newburyport 1980 ## 3 5.0 Provincetown 1970 ## 4 5.0 Plymouth 1930 ## 5 5.0 Marblehead 1975 ## 6 5.5 Chatham 1975 ## 7 5.5 Salem 1980 ## 8 6.0 Ipswich 1920 ## 9 6.5 Falmouth 1995 ## 10 6.5 Boston 2000 Although, we could do this too, and it would be equivalent: sw &lt;- data.frame( # It&#39;s okay to split code across multiple lines. # It keeps things readable. height = c(4, 4.5, 5, 5, 5, 5.5, 5.5, 6, 6.5, 6.5), town = c(&quot;Gloucester&quot;, &quot;Newburyport&quot;, &quot;Provincetown&quot;, &quot;Plymouth&quot;, &quot;Marblehead&quot;, &quot;Chatham&quot;, &quot;Salem&quot;, &quot;Ipswich&quot;, &quot;Falmouth&quot;, &quot;Boston&quot;), year = c(1990, 1980, 1970, 1930, 1975, 1975, 1980, 1920, 1995, 2000)) # Let&#39;s check out our dataframe! sw ## height town year ## 1 4.0 Gloucester 1990 ## 2 4.5 Newburyport 1980 ## 3 5.0 Provincetown 1970 ## 4 5.0 Plymouth 1930 ## 5 5.0 Marblehead 1975 ## 6 5.5 Chatham 1975 ## 7 5.5 Salem 1980 ## 8 6.0 Ipswich 1920 ## 9 6.5 Falmouth 1995 ## 10 6.5 Boston 2000 But what if we want to work with the vectors again? We can use the ‘$’ sign to say, ‘grab the following vector from inside this data.frame.’ sw$height ## [1] 4.0 4.5 5.0 5.0 5.0 5.5 5.5 6.0 6.5 6.5 We can also do operations on that vector from within the dataframe. sw$height + 1 ## [1] 5.0 5.5 6.0 6.0 6.0 6.5 6.5 7.0 7.5 7.5 We can also update values, like the following: # sw$height &lt;- sw$height + 1 # I&#39;ve put this in comments, since I don&#39;t actually want to do it (it&#39;ll change our data) # but good to know, right? LC 2 Question How would you make your own data.frame? Please make up a data.frame of with 3 vectors and 4 values each. Make 1 vector numeric and 2 vectors character data. How many rows are in that data.frame? Answer Here’s my example! # Make a data.frame called &#39;mayhem&#39; mayhem &lt;- data.frame( # make a character vector of 4 dog by their names dogs = c(&quot;Mocha&quot;, &quot;Domino&quot;, &quot;Latte&quot;, &quot;Dot&quot;), # Classify the type of dog as a character vector types = c(&quot;corgi&quot;, &quot;dalmatian&quot;, &quot;corgi&quot;, &quot;dalmatian&quot;), # Record the number of treats eaten per year per dog treats_per_year = c(5000, 3000, 2000, 10000)) # View the resulting &#39;mayhem&#39;! mayhem ## dogs types treats_per_year ## 1 Mocha corgi 5000 ## 2 Domino dalmatian 3000 ## 3 Latte corgi 2000 ## 4 Dot dalmatian 10000 1.5 Common Functions in R We can also run functions that come pre-installed to analyze vectors. These include: mean(), median(), sum(), min(), max(), range(), quantile(), sd(), var(), and length(). Figure 1.4: Figure 3: Descriptive Stats function Cheatsheet! 1.5.1 Measures of Central Tendency mean(sw$height) # the mean seawall height among these towns ## [1] 5.35 median(sw$height) # the median seawall height ## [1] 5.25 sum(sw$height) # total meters of seawall height! (weird number, but okay) ## [1] 53.5 1.5.2 Measures of Dispersion min(sw$height) # smallest seawall height ## [1] 4 max(sw$height) # tallest seawall height ## [1] 6.5 range(sw$height) # range of seawalls (min &amp; max) ## [1] 4.0 6.5 quantile(sw$height, probs = 0.25) # 25th percentile ## 25% ## 5 quantile(sw$height, probs = 0.75) # 75th percentile ## 75% ## 5.875 sd(sw$height) # the standard deviation of seawall heights ## [1] 0.8181958 var(sw$height) # the variance of seawall heights (= standard deviation squared) ## [1] 0.6694444 1.5.3 Other Good Functions length(sw$height) # the number of values in this vector ## [1] 10 length(sw) # the number of vectors in this data.frame ## [1] 3 That’s really fast! We’ll learn more about these descriptive statistics in later lessons! 1.6 6. Missing Data Sometimes, data.frames include missing data for a case/observation. For example, let’s say there is an 11th town, where the seawall height is unknown. # We would write: mysw &lt;- c(4, 4.5, 5, 5, 5, 5.5, 5.5, 6, 6.5, 6.5, NA) # see the &#39;NA&#39; for non-applicable If you run mean(mysw) now, R doesn’t know how to add 6.5 + NA. The output will become NA instead of 5.35. mean(mysw) ## [1] NA To fix this, we can add an ‘argument’ to the function, telling it to omit NAs from the calculation. mean(mysw, na.rm = TRUE) # short for, &#39;remove NAs&#39; ## [1] 5.35 Pretty cool, no? Each function is unique, often made by different people, so only these functions have na.rm as an argument. LC 3 Question Jun Kanda (2015) measured max seawall heights (seawall_m) in 13 Japanese towns (town) after the 2011 tsunami in Tohoku, Japan, compared against the height of the tsunami wave (wave_m). Using this table, please code and answer the questions below. town seawall_m wave_m Kuji South 12.0 14.5 Fudai 15.5 18.4 Taro 13.7 16.3 Miyako 8.5 11.8 Yamada 6.6 10.9 Ohtsuchi 6.4 15.1 Tohni 11.8 21.0 Yoshihama 14.3 17.2 Hirota 6.5 18.3 Karakuwa East 6.1 14.4 Onagawa 5.8 18.0 Souma 6.2 14.5 Nakoso 6.2 7.7 Reproduce this table as a data.frame in R, and save it as an object named jp. How much greater was the mean height of the tsunami than the mean height of seawalls? Evaluate how much these heights varied on average among towns. Did seawall height vary more than tsunami height? How much more/less? Answer Reproduce this table as a data.frame in R, and save it as an object named jp. # Make a dataframe named jp, jp &lt;- data.frame( # containing a character vector of 13 town names, town = c(&quot;Kuji South&quot;, &quot;Fudai&quot;, &quot;Taro&quot;, &quot;Miyako&quot;, &quot;Yamada&quot;, &quot;Ohtsuchi&quot;, &quot;Tohni&quot;, &quot;Yoshihama&quot;, &quot;Hirota&quot;, &quot;Karakuwa East&quot;, &quot;Onagawa&quot;, &quot;Souma&quot;, &quot;Nakoso&quot;), # and a numeric vector of 13 max seawall heights in meters seawall_m = c(12.0, 15.5, 13.7, 8.5, 6.6, 6.4, 11.8, 14.3, 6.5, 6.1, 5.8, 6.2, 6.2), # and a numeric vector of 13 max tsunami heights in meters wave_m = c(14.5, 18.4, 16.3, 11.8, 10.9, 15.1, 21.0, 17.2, 18.3, 14.4, 18.0, 14.5, 7.7) ) # View contents! jp ## town seawall_m wave_m ## 1 Kuji South 12.0 14.5 ## 2 Fudai 15.5 18.4 ## 3 Taro 13.7 16.3 ## 4 Miyako 8.5 11.8 ## 5 Yamada 6.6 10.9 ## 6 Ohtsuchi 6.4 15.1 ## 7 Tohni 11.8 21.0 ## 8 Yoshihama 14.3 17.2 ## 9 Hirota 6.5 18.3 ## 10 Karakuwa East 6.1 14.4 ## 11 Onagawa 5.8 18.0 ## 12 Souma 6.2 14.5 ## 13 Nakoso 6.2 7.7 How much greater was the mean height of the tsunami than the mean height of seawalls? # Get mean of wave height mean(jp$wave_m) ## [1] 15.23846 The average wave was 15.24 meters tall. # Get mean of seawall height mean(jp$seawall_m) ## [1] 9.2 The average seawall was 9.2 meters tall. # Get difference in mean seawall height mean(jp$wave_m) - mean(jp$seawall_m) ## [1] 6.038462 The average wave was 6.04 meters taller than the average seawall. Evaluate how much these heights varied on average among towns. Did seawall height vary more than tsunami height? How much more/less? # Get standard deviation of wave height sd(jp$wave_m) ## [1] 3.587603 On average, wave height varied by 3.59 meters. # Get standard deviation of seawall height sd(jp$seawall_m) ## [1] 3.675368 On average, seawall height varied by 3.68 meters. # Get difference sd(jp$wave_m) - sd(jp$seawall_m) ## [1] -0.08776516 That means wave height varied by -0.09 meters less than seawall height. 1.7 Packages 1.7.1 Using Packages Some functions come pre-built into R, but lots of people have come together to build ‘packages’ of functions that help R users all over the world do more, cool things, so we don’t each have to ‘reinvent the wheel.’ ggplot2, which we use below, is one of these! 1.7.2 Installing Packages We can use the library() function to load a package (like fipping an ‘on’ switch for the package). After loading it, R will recognize that package’s functions when you run them! But if you try to load a package that has never been installed on your computer, you might get this error: library(ggplot2) Error in library(ggplot2) : there is no package called ‘ggplot2’ In this case, we need to install those packages (only necessary once), using install.packages(). (If a message pops up, just accept ‘Yes’.) install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;)) After a successful install, you’ll get a message like this: ================================================== downloaded 1.9 MB * installing *binary* package ‘ggplot2’ ... * DONE (ggplot2) * installing *binary* package ‘dplyr’ ... * DONE (dplyr) The downloaded source packages are in ‘/tmp/RtmpefCnYe/downloaded_packages’ 1.7.3 Loading Packages Finally, we can load our packages with library(). library(ggplot2) library(dplyr) Tada! You have turned on your packages! 1.8 Visualizing Data with Histograms The power of R is that you can process data, calculate statistics, and visualize it all together, very quickly. We can do this using hist() and geom_histogram(), among other functions. 1.8.1 hist() For example, let’s imagine that we had seawall height data from cities in several states. We might want to compare those states. # Create 30 cities, ten per state (MA, RI, ME) allsw &lt;- data.frame( height = c(4, 4.5, 5, 5, 5.5, 5.5, 5.5, 6, 6, 6.5, 4, 4,4, 4, 4.5, 4.5, 4.5, 5, 5, 6, 5.5, 6, 6.5, 6.5, 7, 7, 7, 7.5, 7.5, 8), states = c(&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;, &quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;, &quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;)) # Take a peek! allsw ## height states ## 1 4.0 MA ## 2 4.5 MA ## 3 5.0 MA ## 4 5.0 MA ## 5 5.5 MA ## 6 5.5 MA ## 7 5.5 MA ## 8 6.0 MA ## 9 6.0 MA ## 10 6.5 MA ## 11 4.0 RI ## 12 4.0 RI ## 13 4.0 RI ## 14 4.0 RI ## 15 4.5 RI ## 16 4.5 RI ## 17 4.5 RI ## 18 5.0 RI ## 19 5.0 RI ## 20 6.0 RI ## 21 5.5 ME ## 22 6.0 ME ## 23 6.5 ME ## 24 6.5 ME ## 25 7.0 ME ## 26 7.0 ME ## 27 7.0 ME ## 28 7.5 ME ## 29 7.5 ME ## 30 8.0 ME Every vector is a distribution - a range of low to high values. We can use the hist() function to quickly visualize a vector’s distribution. hist(allsw$height) 1.8.2 geom_histogram() in ggplot2 hist() is great for a quick check, but for anything more complex, we’re going to use ggplot2, the most popular visualization package in R. # Load ggplot2 package library(ggplot2) # Tell the ggplot function to... ggplot( # draw data from the &#39;allsw&#39; data.frame data = allsw, # and &#39;map&#39; the vector &#39;height&#39; to be an &#39;aes&#39;thetic on the &#39;x&#39;-axis. mapping = aes(x = height)) + # make histograms of distribution, geom_histogram( # With white outlines color = &quot;white&quot;, # With blue inside fill fill = &quot;steelblue&quot;, # where every half meter gets a bin (binwidth = 0.5) binwidth = 0.5) + # add labels labs(x = &quot;Seawall Height&quot;, y = &quot;Frequency (# of cities)&quot;) Looks much nicer, right? Lots more code, but lots more options for customizing. We’ll learn ggplot2 more over this term, and it will become second nature in time! (Just takes practice!) The value of ggplot2 really comes alive when we make complex visuals. For example, our data allsw$height essentially contains 3 vectors, one per state; one for MA, one for RI, one for ME. Can we visualize each of these vectors’ distributions using separate histograms? # Repeat code from before... ggplot(data = allsw, mapping = aes(x = height)) + geom_histogram(color = &quot;white&quot;, fill = &quot;steelblue&quot;, binwidth = 0.5) + labs(x = &quot;Seawall Height&quot;, y = &quot;Frequency (# of cities)&quot;) + # don&#39;t forget the &#39;+&#39;! # But also ## Split into panels by state! facet_wrap(~states) We can now see, according to our hypothetical example, that states host different distributions of seawall heights. Massachusetts (MA) has lower seawalls, evenly distributed around 5.5 m. Maine (ME) has higher seawalls, skewed towards 7 m. Rhode Island (RI) has lower seawalls, skewed towards 4 m. LC 4 Question Challenge: Please make a histogram of Jun Kanda’s sample of seawall heights (seawall_m) in the jp object from LC 3. First, make a histogram using the hist() function. Then, try and use the geom_histogram() function from ggplot2! Answer First, make a histogram using the hist() function. # Tell R to make a histogram from the &#39;seawall_m&#39; vector inside &#39;jp&#39;! hist(jp$seawall_m) Then, try and use the geom_histogram() function from ggplot2! # Tell ggplot to grab the &#39;seawall_m&#39; vector from the &#39;jp&#39; data.frame, # and make a histogram! ggplot(data = jp, mapping = aes(x = seawall_m)) + geom_histogram() Looks pretty weird, huh? hist() automatically chooses the binwidth, but ggplot() gives us more control over the whole plot. We’ll learn more about this soon! 1.9 Wrap-up 1.9.1 Next Steps Throughout the rest of the course, we’re going to advance each of these skills: working with types of data in R calculating meaningful statistics in R visualizing meaningful trends in R 1.9.2 Advice Be sure to clear your environment often. That means, using remove() or the broom tool in the upper right hand corner. remove(allsw, mysw, sw, myheights, mytowns, myyears) You can clean your console too, using broom in console’s upper right corner. Save often. (Control + Save usually works on PC.) You can download files using more / export, or upload them. You’ll be a rockstar at using R in no time! Stay tuned for our next Workshop! 1.9.3 Troubleshooting: If your session freezes, go to ‘Session’ &gt;&gt; ‘Restart R.’ If that doesn’t work, go to ‘Session’ &gt;&gt; ‘Terminate’. If that doesn’t work, click on the elipsis (…) in the white banner at the top, and select Relaunch Project. If that doesn’t work, let me know! Having problems? There are three causes of most all problems in R. there’s a missing parenthesis or missing quotation mark in one’s code. You’re using a function from a package that needs to be loaded (we’ll talk about this in later workshops). Too much data in your environment is causing R to crash. Clear the environment. "],["skill-visualization-with-ggplot-in-r.html", "Skill: Visualization with ggplot in R Getting Started Gapminder data Your first scatterplot LC 1 LC 2 LC 3 Improving our Visualizations LC 4 LC 5 Visualizing diamonds data LC 6 LC 7 LC 8 Visualizing Distributions LC 9 Breaking Up a Visual LC 10", " Skill: Visualization with ggplot in R Visualization is a key part of statistical analyses, especially in systems engineering! Visuals themselves are often the analysis themselves! In this tutorial, we’re going to learn how to visualize data in the ggplot2 package. Please follow along using the code below! Getting Started Loading Packages Let’s load our packages with library(). # Data viz and data manipulation packages library(ggplot2) library(dplyr) # Data sources library(gapminder) Notes: SAVE YOUR SCRIPT. Always comment your code (what I’m doing now), use lots of spaces, and keep it clean. Gapminder data Economist Hans Rosling made a dataset that examines change in life expectancy over time for most countries in the world. It is contained in the gapminder package! # Let&#39;s view it. (see console below) gapminder ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # ℹ 1,694 more rows Each row is a country-year, marking the life expectancy, population, and gross domestic product (GDP) per capita. On your end, you can only can see some of it, right? Let’s check out what vectors are in this dataframe, using the glimpse function from the dplyr package. # (Remember, a vector is a column in a spreadsheet; # a data.frame is a spreadsheet.) glimpse(gapminder) ## Rows: 1,704 ## Columns: 6 ## $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Af… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, … ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, … ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, … ## $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, … ## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811… # Nice, we can see things more concisely. Our data has six variables. Great! Your first scatterplot Using the gapminder data, let’s map a series of vectors to become aesthetic features in the visualization (point, colors, fills, etc.). ggplot(data = gapminder, mapping = aes( # Let&#39;s make the x-axis gross-domestic product per capita (wealth per person) x = year, # Let&#39;s make the y-axis country life expectancy y = lifeExp)) Huh! We made an empty graph. Cool. That’s because ggplot needs helper functions to add aesthetic features to the graph. For example, adding + geom_point() will overlay a scatterplot. # Make a scatterplot ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + # same as above, except &quot;+&quot; geom_point() LC 1 Question What kind of relationship does this graph show? Why might it matter to policymakers? Answer The graph above shows that as average wealth (GDP per capita) in a country increases, those countries’ life expectancy increases swiftly, but then tapers off. This highlights that there is a strong relationship between wealth and health globally. LC 2 Question What happens when you add the alpha, changing its values in the 3 visuals below? # Run the following code: ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.2) ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.5) ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 1) Answer Alpha ranges from 0 to 1 and describes feature transparency. Increasing alpha to 1 makes points fully opaque! Decreasing alpha to 0 makes points fully transparent! LC 3 Question We can make it more visually appealing. What happens when we do each of the following? If you want to make it a single color, where do you need to write color = ...? If you want to make it multiple colors according to a vector, where do you need to write color =? # Run the following code: # Version 1 ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.5, color = &quot;steelblue&quot;) # Version 2 ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(alpha = 0.5) Answer To assign a single color, you need to put color outside the aes() phrase, and write the name of the color. To assign multiple colors, you need to put the color inside the aes(...) phrase, and write the name of the vector in the data that it corresponds to (eg. continent). Improving our Visualizations We can (and should!) make our visualizations much more readable by adding appropriate labels. ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(alpha = 0.5) + # Add labels! labs(x = &quot;GDP per capita (USD)&quot;, # label for x-values y = &quot;Life Expectancy (years)&quot;, # label for y-values color = &quot;Continent&quot;, # label for colors title = &quot;Does Wealth affect Health?&quot;, # overall title subtitle = &quot;Global Health Trends by Continent&quot;, # subtitle! caption = &quot;Points display individual country-year observations.&quot;) # caption We can actually save visualizations as objects too, which can make things faster. Let’s save our visual as myviz myviz &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(alpha = 0.5) + labs(x = &quot;GDP per capita (USD)&quot;, y = &quot;Life Expectancy (years)&quot;, color = &quot;Continent&quot;, title = &quot;Does Wealth affect Health?&quot;, # overall title subtitle = &quot;Global Health Trends by Continent&quot;, # subtitle! caption = &quot;Points display individual country-year observations.&quot;) # caption Next, let’s try a few more learning check that will ask you to try our ways to improve the quality and readability of your visuals! LC 4 Question Now run myviz - what happens? myviz Answer When you save a ggplot to an object, eg. naming it myviz, you can call up the visual again as many times as you want by just running the myviz object, just like any other object. LC 5 Question We can do better, adding things onto our myviz object! Try changing themes. What happens below? # Version theme_bw myviz + # How about this theme? theme_bw() # Version theme_dark myviz + # How about this theme? theme_dark() # Version theme_classic myviz + # How about this theme? theme_classic() Answer theme_bw() makes a nice black-and-white graph; theme_dark() makes a funky graph with a dark grey background; theme_classic() makes a very simple graph, with fewer distractions. I personally really like the default theme or theme_bw(). Sometimes theme_classic() can be really helpful if you have a particularly busy visual. Visualizing diamonds data Next, let’s use the diamonds dataset, which comes with the ggplot2 package This is a dataset of over 50,000 diamond sales. # Check out first 3 rows... diamonds %&gt;% head(3) ## # A tibble: 3 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 We can use this visualization to check whether the cut of diamonds really has any relationship with price. glimpse(diamonds) ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Ve… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I,… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1… ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, … ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.… Looks like cut is an ordinal variable (fair, good, ideal, etc.), while price is numeric (eg. dollars). A boxplot might be helpful! ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut)) + # notice how we added group = cut, to tell it to use 5 different boxes, one per cut? geom_boxplot() Huh. How odd. Looks like the cut of diamonds has very little impact on what price they are sold at! We can see lots of outliers at the top - really expensive diamonds for that cut. LC 6 Question Let’s make this visualization more visually appealing. What changed in the code to make these two different visual effects? Why? (Hint: fill.) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut)) + geom_boxplot(fill = &quot;steelblue&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() Answer In the first visual, we assigned all the boxplots to have the same fill (fill = \"steelblue\"), but in the second visual, we assigned the boxplot fill to be shaded based on the cut of diamond. This adds a cool color range! LC 7 Question Sometimes, the names of categories won’t fit well. We can try the following. What did we do? ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() Answer We used coord_flip() to flip the coordinates of the x and y axis, which gives our cut labels more room! LC 8 Question Sometimes, the legend doesn’t fit well. We can try this: What happens when you change legend.position from \"right\" to \"bottom\" to \"left\" to \"top\"? ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;bottom&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;right&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;left&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;top&quot;) Answer These four values for legend.position will relocate the fill legend (and any other legends) to be at the top, bottom, left, or right of the visual! Visualizing Distributions Different geom_ functions use colors in different ways, but this is a good example. For example, below is a histogram. It visualizes the approximate distribution of a set of values. We can see how frequently diamonds are sold for certain prices versus others. ggplot(data = diamonds, mapping = aes(x = price, group = cut, fill = cut)) + geom_histogram(color = &quot;white&quot;) + # notice new function here labs(x = &quot;Price (USD)&quot;, y = &quot;Frequency of Price (Count)&quot;, title = &quot;US Diamond Sales&quot;) LC 9 Question Are most diamonds cheap or expensive? What type of distribution would you call this? Normal? Uniform? Left Skewed? Right Skewed? Answer This is strongly right-skewed distribution, because the majority of the distribution leans to the left (the clump of the data), while it has a long tail that skews to the right. The median is less than the mean in a right skewed distribution. Breaking Up a Visual Finally, we might want to break up our visual into multiple parts. We can use facet_wrap to do this, but how exactly does it work? Let’s test it out. LC 10 Question What changed in the code below, and what did it result in? ggplot(data = diamonds, mapping = aes(x = price, fill = cut)) + geom_histogram(color = &quot;white&quot;) + facet_wrap(~cut) + # must be categorical variable labs(x = &quot;Price (USD)&quot;, y = &quot;Frequency of Price (Count)&quot;, title = &quot;US Diamond Sales&quot;) Answer This visual split up our histograms into separate panels (making it much more readable), and easier to compare distributions. We write facet_wrap(~ before the variable name (eg. cut) to specify that we want to split up the data by the values of cut. This sorts our rows of data into 5 different piles (since there are 5 different categories in cut) and makes a panel out of each. You made it! You have now tried out a series of visuals in ggplot. We will use ggplot a lot in this course, so please be sure to reach out when you have questions, talk with others in your group, and work together to build great visualization skills! (Plus, it’s super applicable professionally!) "],["skill-failure-modes-and-effects-analysis-in-r.html", "Skill: Failure Modes and Effects Analysis in R Getting Started Example: Ben and Jerry’s Ice Cream Calculating Criticality LC 1", " Skill: Failure Modes and Effects Analysis in R This tutorial will introduce you to Failure Modes and Effects Analysis (FMEA) in R! Getting Started Please open up your project on RStudio.Cloud, for our Github class repository. Start a new R script (File &gt;&gt; New &gt;&gt; R Script). Save the R script as lesson_3.R. And let’s get started! Load Packages # Load tidyverse, which contains dplyr and most data wrangling functions library(tidyverse) # Load DiagrammeR, which we&#39;ll use to make diagrams today! library(DiagrammeR) Example: Ben and Jerry’s Ice Cream Ben and Jerry’s main headquarters is in Waterbury, VT, just outside of Burlington, where it makes a lot of ice cream. (It’s also fun to visit.) Their staff likely has to take considerable care to make sure that all that ice cream stays refrigerated! Suppose Ben and Jerry’s has decided to build a new ice cream production plant in Ithaca, NY. For the sake of Ben and Jerry’s (nay, the world!) let’s use Failure Modes and Effects Analysis (FMEA) to identify any hypothetical vulnerability that might occur at this new ice cream business! Scope &amp; Resolution As our scope, we’re going to just focus on melting. What are all the possible ways that ice cream could melt during this process? Melting would have several negative impacts, such as getting exposed to heat, bacteria, and, worst of all, melting the ice cream! This example is primarily people-centric, because it’s important to remember that people are part of our technological systems! Before proceeding, you can optionally check out our tutorial on making mermaid charts! (Estimated time: 10 minutes.) You’ll learn how to make the chart used below! Measuring Criticality FMEA includes uses 3 measures to calculate a criticality index, meaning the overall risk of each combination of severity and underlying conditions. $ severity occurence detection = criticality $ Each gets classified on a scale from 1-10: severity: 1 = none, 10 = hazardous/catastrophic occurrence: 1 = almost impossible, 10 = almost certain detection: 1 = almost certain, 10 = almost impossible These will produce a criticality index from 1 to 1000. Suppose we want to be 99% sure that our technology won’t fail and negatively impact society. We would need a criticality index (also known as RPN) of 990 points or less! (Because 1000 - 10 = 990).) So, let’s analyze them! Block Diagram Below, we’ve visualized what the process of shipping out ice cream looks like once it has been made, using mermaid. This involves the following several steps: Worker 1 puts ice cream in Freezer Worker 2 loads ice cream into Truck Worker 3 transports ice cream to Store Also, Worker 2 takes the ice cream from the Freezer for loading And Worker 3 drives the ice cream from loading dock to Store Plus, several possible failure modes are involved, as discussed below. Figure 1.5: Ben &amp; Jerry’s Ice Cream Block Diagram Failure Modes We’ll make a tidy data.frame() of each of the ways our block diagram above could fail, which were contained above in failures. We’ll call this data.frame f. f &lt;- tibble( # Make a vector of routes to failure failure_mode = c( &quot;freezer --&gt; fail_break&quot;, &quot;loading --&gt; fail_time&quot;, &quot;loading --&gt; fail_eat&quot;, &quot;transport --&gt; fail_time&quot;, &quot;transport --&gt; fail_eat&quot;) ) # Worker 2 could leave ice cream out while loading # Worker 2 could eat the ice cream while loading # Worker 3 could leave the ice cream out in transit # Worker 3 could eat the ice cream in transit Calculating Criticality Next, we’re going to make a few judgement calls, to calculate the overall risk for this FMEA. Estimate Severity What’s the severity of the effects of these failures, on a scale from 1 (low) to 10 (high)? We’ll mutate() the data.frame to include a new column severity, and save it as a new data.frame f1. fail_break: It’s pretty bad it the freezer breaks; that could ruin days worth of product. Let’s call that an 8. Not catastrophic, but not good for the company! fail_time: It’s not great it a single shipment gets left out and melts while waiting for pickup. But it’s just one shipment. Let’s call that a 5. fail_eat: How much ice cream could one worker really eat? That’s probably a 1. f1 &lt;- f %&gt;% mutate(severity = c(8, 5, 1, 5, 1)) # Check out the contents! f1 ## # A tibble: 5 × 2 ## failure_mode severity ## &lt;chr&gt; &lt;dbl&gt; ## 1 freezer --&gt; fail_break 8 ## 2 loading --&gt; fail_time 5 ## 3 loading --&gt; fail_eat 1 ## 4 transport --&gt; fail_time 5 ## 5 transport --&gt; fail_eat 1 Estimate Occurrence How often does this occur, from 1 (almost never) to 10 (almost always)? Let’s rank occurrence as follows: fail_break: It’s pretty rare that the freezer would break (eg. 2). fail_time: It’s probably somewhat rare that shipments melt (eg. 5). fail_eat: If I were a worker, I would eat that all the time (eg. 8). f2 &lt;- f1 %&gt;% mutate(occurrence = c(2, 5, 8, 5, 8)) Estimate Detection Finally, how likely is it that we would detect the occurrence? If very likely, that’s a 1. If very unlikely, that’s a 10. fail_break: Workers would very quickly detect if the freezer were broken. (eg. 1). fail_time: You might not know it had melted until the product gets to the store. (eg. 8) fail_eat: Might get caught. Low chance. (eg. 3). f3 &lt;- f2 %&gt;% mutate(detection = c(1, 8, 3, 8, 3)) Estimate Criticality (RPN) Using our data in f3, let’s estimate criticality (aka RPN, the risk priority number). f4 &lt;- f3 %&gt;% mutate(criticality = severity * occurrence * detection) We can add up the criticality/RPN to estimate the total risk priority, out of 1000, which is the max_criticality possible. We can divide these two to get the probability of system failure. Is that risk greater than 0.010, aka 0.1%? If so, bad news! f4 %&gt;% summarize( total_criticality = sum(criticality), max_criticality = 10*10*10, probability = total_criticality / max_criticality) ## # A tibble: 1 × 3 ## total_criticality max_criticality probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 464 1000 0.464 Well, that’s not good! Looks like the new factory will need to figure out a way to keep their product from melting! (In reality, I’m sure Ben and Jerry’s has strict quality control!) LC 1 Question What other ways could failure occur here? Add three more kinds of failure to your tibble f, then estimate their severity, occurence, detection, and criticality, and recalculate the total probability of failure at this ice cream plant. Answer fprime &lt;- tibble( # Make a vector of routes to failure failure_mode = c( &quot;freezer --&gt; fail_break&quot;, &quot;loading --&gt; fail_time&quot;, &quot;loading --&gt; fail_eat&quot;, &quot;transport --&gt; fail_time&quot;, &quot;transport --&gt; fail_eat&quot;, # Technically, worker 1 could eat it before taking it to freezer &quot;w1 --&gt; fail_eat&quot;, # A fourth worker at the store could eat it before delivering it &quot;w4 --&gt; fail_eat&quot;, # The fourth worker could also leave it out! &quot;w4 --&gt; fail_time&quot;) ) %&gt;% # Estimate quantities of interest mutate(severity = c(8, 5, 1, 5, 1, 1, 1, 5), occurrence = c(2, 5, 8, 5, 8, 8, 8, 5), detection = c(1, 8, 3, 8, 3, 3, 3, 8)) %&gt;% # Calculate criticality mutate(criticality = severity * occurrence * detection) # Let&#39;s check it out! fprime ## # A tibble: 8 × 5 ## failure_mode severity occurrence detection criticality ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 freezer --&gt; fail_break 8 2 1 16 ## 2 loading --&gt; fail_time 5 5 8 200 ## 3 loading --&gt; fail_eat 1 8 3 24 ## 4 transport --&gt; fail_time 5 5 8 200 ## 5 transport --&gt; fail_eat 1 8 3 24 ## 6 w1 --&gt; fail_eat 1 8 3 24 ## 7 w4 --&gt; fail_eat 1 8 3 24 ## 8 w4 --&gt; fail_time 5 5 8 200 # Let&#39;s calculate the total risk! fprime %&gt;% summarize( total_criticality = sum(criticality), max_criticality = 10*10*10, probability = total_criticality / max_criticality) ## # A tibble: 1 × 3 ## total_criticality max_criticality probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 712 1000 0.712 Ooph! Not good! Great work! All done! See you in class! "],["workshop-distributions-and-descriptive-statistics.html", "2 Workshop: Distributions and Descriptive Statistics Getting Started 2.1 Distributions LC 1 2.2 2. Descriptive Statistics LC 2 2.3 Describing Shape LC 3 2.4 Simulating Distributions 2.5 Comparing Distributions LC 4 Conclusion", " 2 Workshop: Distributions and Descriptive Statistics Figure 2.1: Yay Distributions! This tutorial will introduce you to how to code and analyses distributions in R, using descriptive statistics and visualization! Getting Started Please open up last week’s RStudio.Cloud project. Start a new R script (File &gt;&gt; New &gt;&gt; R Script). Save the R script as workshop_2.R. And let’s get started! Load Packages We’re going to use extra functions from 3 packages today, including ggplot2, dplyr (pronounced DIP-LER), and MASS. [Note: Please be sure to load them first, otherwise your functions will not work.] library(ggplot2) # for visualization library(dplyr) # for pipelines! library(MASS) # for fitting distributions The Pipeline We’ll also be using a new coding symbol today, %&gt;%, called a pipeline. Pipelines let us connect data to functions, with fewer parentheses! Figure 2.2: Old-School Pipeline For example: # let&#39;s make a vector ```x``` and do some operations on it. x &lt;- c(1,2,3) # These are the same! mean(x) ## [1] 2 x %&gt;% mean() ## [1] 2 Using pipelines keeps our code neat and tidy. It lets us run long sequences of code without saving it bit by bit as objects. For example, we can take them mean()`` ofx*and* then get thelength()``` of the resulting vector, all in one sequence. Without a pipeline, you end up in parenthesis hell very quickly. # without pipe length(mean(x)) ## [1] 1 # with pipe x %&gt;% mean() %&gt;% length() ## [1] 1 Handy, right? To simplify things, there’s a special ‘hotkey’ shortcut for making pipelines too. In Windows and Linux, use Ctrl Shift M. In Mac, use Cmd Shift M. 2.1 Distributions Any vector can be expressed as a distribution (especially numeric vectors). A distribution stacks the values in a vector in order from lowest to highest to show the frequency of values. There are several ways to visualize distributions, including histograms, density plots, violin plots, jitter plots, ribbon plots, and more; the most common are histograms and density plots, which we will learn today. For example, Figure 2 shows our seawall vector from Workshop 1 in part A (left). In part B (right), that vector is shown as a distribution: its blocks are stacked to make a histogram (bars), while the distribution itself (line) is approximated by a curve, known as a density function. Figure 2.3: Figure 2: Seawall Vector as a Distribution Any distribution can be described with 4 traits, shown above in part C. These include: Size (how many values are in it), Location (eg. where is it clumped), Spread (how much do values vary?), and Shape (eg. bell curve). LC 1 Question Using the hist() function we learned before, draw the histogram of this vector of seawalls, naming the vector sw! Answer Using the hist() function we learned before, draw the histogram of this vector of seawalls, naming the vector sw! # Many options! # You could code it as a vector, save it as an object, then use your functions! sw &lt;- c(4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4) sw %&gt;% hist() # or you could do it like this! # hist(sw) 2.2 2. Descriptive Statistics What’s a statistic? A statistic is a single number that summarizes something about a sample. That’s it! No magic! Statistics is the process of making statistics (eg. many single numbers) so we can understand samples of data! They help people make decisions when faced with uncertainty. We’ll learn several functions to make statistics that describe our distributions. Trait Meaning Type Functions Size How many values? statistics length() Location Where is it clumped? statistics mean(), median() Spread How much do values vary? statistics sd(), var(), range(), quantile() Shape What shape does it resemble? distributions rnorm(), rbinom(), rpois(),[skewness &amp; kurtosis - no functions] Click on the tabs below to review our functions for Size, Location, and Spread from Workshop 1. (We’ll get to shape in a minute.) 2.2.1 Size How big is our sample? Use length() on a vector to find the number of values in the vector sw we made in LC1. length(sw) ## [1] 10 2.2.2 Location Where is our sample clumped? Figure 2.4: Figure 3: Statistics for Location Use mean() and median() to find the most central values. sw %&gt;% mean() ## [1] 5.35 sw %&gt;% median() ## [1] 5.25 Fun fact: mode() doesn’t work in R; it’s huge pain. You have to use this code instead. sw %&gt;% table() %&gt;% sort(decreasing = TRUE) ## . ## 5 5.5 6.5 4 4.5 6 ## 3 2 2 1 1 1 2.2.3 Spread (1) How much does our sample vary? Figure 2.5: Figure 4: Statistics for Spread Use quantile() to check for any percentile in a vector, from 0 (min) to 0.5 (median) to 1 (max). If you have quantile(), you don’t need to remember min(), max(), range(), or even median(). sw %&gt;% quantile(probs = 0) # min ## 0% ## 4 sw %&gt;% quantile(probs = 1) # max ## 100% ## 6.5 2.2.4 Spread (2) 2.2.4.1 Standard Deviation But we can also evaluate how much our values vary from the mean on average - the standard deviation, often abbreviated as \\(\\sigma\\) (sigma). This is written as: $ = $ Figure 2.6: Figure 5: Standard Deviation, the ultimate Statistic for Spread We can calculate this ‘by hand’, or use the sd() function. # Calculating in R still faster than on your own! sqrt( sum((sw - mean(sw))^2) / (length(sw) - 1) ) ## [1] 0.8181958 # Get the standard deviation by code! sw %&gt;% sd() ## [1] 0.8181958 2.2.4.2 Variance Sometimes, we might want the variance, which is the standard deviation squared. This accentuates large deviations in a sample. # Get the variance! sw %&gt;% var() ## [1] 0.6694444 sd(sw)^2 ## [1] 0.6694444 # See? var = sd^2! 2.2.4.3 Coefficient of Variation (CV) We could also calculate the coefficient of variation (CV), meaning how great a share of the mean does that average variation constitute? (Also put, how many times does the mean fit into the standard deviation.) sd(sw) / mean(sw) ## [1] 0.1529338 The standard deviation constitutes 15% of the size of the mean seawall height. 2.2.4.4 Standard Error (SE) But these numbers don’t have much meaning to us, unless we know seawalls really well. Wouldn’t it be nice if we had a kind of uniform measure, that told us how big is the variation in the data, given how big the data is itself? Good news! We do! We can calculate the sample size-adjusted variance like so: var(sw) / length(sw) ## [1] 0.06694444 # or sd(sw)^2 / length(sw) ## [1] 0.06694444 This means we could take this set of seawalls and compare it against samples of coastal infrastructure in Louisiana, in Japan, in Australia, and make meaningful comparisons, having adjusted for sample size. However, sample-size adjusted variance is a little bit of a funky concept, and so it’s much more common for us to use the sample-size adjusted standard deviation, more commonly known as the standard error, or se. $ SE = = = $ # Calculated as: se &lt;- sd(sw) / sqrt(length(sw)) # Or as: se &lt;- sqrt( sd(sw)^2 / length(sw) ) # Or as: se &lt;- sqrt( var(sw) / length(sw)) # See standard error se ## [1] 0.2587362 LC 2 Question Suppose we collected data on 10 randomly selected chunks of cheese from a production line! We measured their moisture in grams (g) in each product We want to make sure we’re making some quality cheesy goodness, so let’s find out how much those moisture (cheesiness) levels vary! The moisture in our cheese weighed 5.52 g, 5.71 g, 5.06 g, 5.10 g, 4.98 g, 5.50 g, 4.81 g, 5.55 g, 4.74 g, &amp; 5.39 g. Please convert the following values into a vector named cheese! How much did moisture levels vary, on average? We need to compare these levels with cheese produced in Vermont, France, and elsewhere. What’s the coefficient of variance and standard error for these moisture levels? Answer Please convert the following values into a vector named cheese! cheese &lt;- c(5.52, 5.71, 5.06, 5.10, 4.98, 5.50, 4.81, 5.55, 4.74, 5.39) How much did moisture levels vary, on average? We need to compare these levels with cheese produced in Vermont, France, and elsewhere. What’s the coefficient of variance and standard error for these moisture levels? # Coefficient of variation cv &lt;- sd(cheese) / mean(cheese) # Check it! cv ## [1] 0.06491759 # Standard Error se &lt;- sd(cheese) / sqrt(length(cheese)) # Check it se ## [1] 0.1074885 # When you&#39;re finished, remove extra data. remove(cheese, se, cv) 2.3 Describing Shape How then do we describe the shape of a distribution? We can use skewness and kurtosis for this. There’s no direct function for skewness or kurtosis in R, but as you’ll see below, we can quickly calculate it using the functions we already know. 2.3.1 Skewness Skewness describes whether the bulk of the distribution sits to the left or right of the center, and its formula are written out below. It is commonly estimated using the formula on the left, while the formula on the right closely approximates it. (We’re going to use the right-hand formula below, since it’s a little cleaner.) \\[ Skewness = \\frac{ \\sum^{N}_{i=1}{(x - \\bar{x})^{3} / n } }{ [\\sum^{N}_{i=1}{ (x - \\bar{x})^{2} / n }]^{3/2} } \\approx \\frac{ \\sum^{N}_{i=1}{ (x - \\bar{x})^{3} } }{ (n - 1) \\times \\sigma^{3} } \\] When people say that a certain person’s perspective is skewed, they mean, it’s very far from the mean. In this case, we want to know, how skewed are the heights of seawalls overall compared to the mean? To figure this out, we’ll need 4 ingredients: \\(x_{i \\to N}\\): our vector of values (seawall heights! sw) \\(N\\): the length of our vector (how many seawalls? length(sw)) \\(\\bar{x}\\): our mean value: (the mean seawall height? mean(sw)) \\(\\sigma\\): the standard deviation of our vector (how much do the seawall heights vary on average? sd(sw)) Yeah! You just used them a bunch! So let’s calculate skewness! First, we measure diff, how far is each value from the mean? diff &lt;- sw - mean(sw) # Check it out! diff ## [1] -0.85 -0.35 0.15 -0.35 0.15 1.15 1.15 0.65 -0.35 -1.35 diff measures how far / how skewed each of these values (\\(x\\)) are from the mean \\(\\bar{x}\\)). See the visual below! Next, we’re going to cube diff, to emphasize extreme differences from the mean Squaring would turn everything positive, but we care whether those differences are positive or negative, so we cube it instead. diff^3 ## [1] -0.614125 -0.042875 0.003375 -0.042875 0.003375 1.520875 ## [7] 1.520875 0.274625 -0.042875 -2.460375 Then, we’re going to get a few helper values, like: # Get the sample-size # To be conservative, we&#39;ll subtract 1; this happens often in stats n &lt;- length(sw) - 1 # Get the standard deviation sigma &lt;- sw %&gt;% sd() Now, we can calculate, on average, how big are these cubed differences? sum(diff^3) / n ## [1] 0.01333333 Well, that’s nifty, how do we compare this funky number to other samples? We’re going to need to put it in terms of a common unit, a “standard” unit - like the standard deviation! Plus, we’ll have to cube the standard deviation, so that it’s in the same terms as our numerator \\(diff^{3}\\). skew &lt;- sum(diff^3) / ( n * sigma^3) # Check it! skew ## [1] 0.0243426 Voila! A standardized measure you can use to compare the skew of our sample of seawalls to any other sample! For comparison, here are a few other values of skew we might possibly get. 2.3.2 Kurtosis Kurtosis describes how tightly bound the distribution is around the mean. Is it extremely pointy, with a narrow distribution (high kurtosis), or does it span wide (low kurtosis)? We can estimate it using the formula on the left, and the formula on the right is approximately the same. \\[ Kurtosis = \\frac{ \\sum^{N}_{i=1}{(x - \\bar{x})^{4} / n } }{ [\\sum^{N}_{i=1}{ (x - \\bar{x})^{2} / n }]^2 } \\approx \\frac{ \\sum^{N}_{i=1}{ (x - \\bar{x})^{4} } }{ (n - 1) \\times \\sigma^{4} } \\] Like skew, we calculate how far each value is from the mean, but we take those differences to the 4th power (\\((x - \\bar{x})^{4}\\)), which hyper-accentuates any extreme deviations and returns only positive values. Then, we calculate the sample-size adjusted average of those differences. Finally, to measure it in a consistent unit comparable across distributions, we divide by the standard deviation taken to the 4th power; the powers in the numerator and denominator then more-or-less cancel each other out. moments::skewness(sw) ## [1] 0.02565935 # 0.2565 x &lt;- sw sum( (x - mean(x))^3 ) / ((length(x) - 1) *sd(x)^3) ## [1] 0.0243426 a &lt;- sum( (x - mean(x))^3 ) / length(x) b &lt;- (sum( (x - mean(x))^2 ) / length(x))^(3/2) a/b ## [1] 0.02565935 # 0.256 # Get the differences again diff &lt;- sw - mean(sw) # And take them to the fourth power diff^4 ## [1] 0.52200625 0.01500625 0.00050625 0.01500625 0.00050625 ## [6] 1.74900625 1.74900625 0.17850625 0.01500625 3.32150625 They’re all positive! Next, same as above, we’ll get the conservative estimate of the sample size (n - 1) and the standard deviation. # Get the sample-size # To be conservative, we&#39;ll subtract 1; this happens often in stats n &lt;- length(sw) - 1 # Get the standard deviation sigma &lt;- sw %&gt;% sd() So when we put it all together… kurt &lt;- sum(diff^4) / ( n * sigma^4) # Check it! kurt ## [1] 1.875851 We can measure kurtosis! A pretty normal bell curve has a kurtosis of about 3, so our data doesn’t demonstrate much kurtosis. Kurtosis ranges from 0 to infinity (it is always positive), and the higher it goes, the pointier the distribution! Finally, just a heads up: As mentioned above, there are a few different formulas floating around there for skewness and kurtosis, so don’t be too surprised if your numbers vary when calculating it in one package versus another versus by hand. (But, if the numbers are extremely different, that’s probably a sign something is up.) LC 3 Question A contractor is concerned that the majority of seawalls in her region might skew lower than their region’s vulnerability to storms requires. Assume (hypothetically) that our sample’s seawalls are the appropriate height for our level of vulnerability, and that both regions share the same level of vulnerability. The mean seawall in her region is about the same height as in our sample (~5.35), but how do the skewness and kurtosis of her region’s seawalls compare to our sample? Her region has 12 seawalls! Their height (in meters) are 4.15, 4.35, 4.47, 4.74, 4.92, 5.19, 5.23, 5.35, 5.55, 5.70, 5.78, &amp; 7.16. Calculate these statistics and interpret your results in a sentence or two. Answer # Make a vector of these 12 seawalls x &lt;- c(4.15, 4.35, 4.47, 4.74, 4.92, 5.19, 5.23, 5.35, 5.55, 5.70, 5.78, 7.16) # Calculate skewness skewness &lt;- sum( (x - mean(x))^3) / ((length(x) - 1) * sd(x)^3) # Calculate Kurtosis kurtosis &lt;- sum( (x - mean(x))^4) / ((length(x) - 1) * sd(x)^4) # View them! c(skewness, kurtosis) ## [1] 0.9016585 3.5201492 Her region’s seawalls are somewhat positively, right skewed, with a skewness of about +0.90. This is much more skewed than our hypothetical area’s seawalls, which are skewed at just +0.02. But, her region’s seawalls’ traits are much more closely clustered around the mean than ours, with a kurtosis of 3.52 compared to our 1.88. Since both hypothetical regions have comparable levels of vulnerability to storm surges, her region’s seawalls do appear to skew low. 2.4 Simulating Distributions Finally, to describe shape, we need some shapes to compare our distributions to. Fortunately, the rnorm(), rbinom(), rpois(), and rgamma() functions allow us to draw the shapes of several common distributions. Table 2 shows the shape of these distributions, and their ranges. Table 2.1: Table 2: Example Distributions Distributions Span Function Parameters Example Resources Normal -Inf to +Inf rnorm() mean, sd .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Poisson 0, 1, 2, 3… rpois() lambda (mean) .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Gamma 0.1, 2.5, 5.5, +Inf rgamma() shape, rate .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Exponential same rexp() rate .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Weibull same rweibull() shape, scale .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Binomial 0 vs. 1 rbinom() probability .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Uniform min to max runif() min, max .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Note: Wikipedia is actually a pretty fantastic source on distributions. To determine what kind of distribution our vector has, we can visually compare it using simulation. We can compare our real observed distribution against random distributions to determine whether our data matches the shape of a normal vs. poisson distribution, for example. To do so, let’s get some statistics from our data to help us visualize what a distribution with those traits would look like. As our raw data, let’s use our vector of seawall heights sw. # Let&#39;s remake again our vector of seawall heights sw &lt;- c(4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4) To simulate, you feed your simulator function (1) n values to draw and (2) any required statistics necessary for computing draws from that distribution. (For example, rnorm() requires the mean and standard deviation.) Fortunately, statisticians have figured out for us 2 ways to figure out what statistics to provide. First, there are a few equations called method of moments estimators that do a great job of estimating those statistics. Alternatively, we can ask R to compute the values of those statistics using the MASS package’s fitdistr(). We’ll show both below. 2.4.1 Normal Distribution rnorm() randomly generates for us any number of values randomly sampled from a normal distribution. We just need to supply: (1) n values to draw, (2) the mean of that distribution, and (3) the sd of that distribution. # For example mymean &lt;- sw %&gt;% mean() mysd &lt;- sw %&gt;% sd() # simulate! mynorm &lt;- rnorm(n = 1000, mean = mymean, sd = mysd) # Visualize! mynorm %&gt;% hist() How do mymean and mysd compare to the fitdistr() estimates? mymean ## [1] 5.35 mysd ## [1] 0.8181958 Let’s compare with fitdistr()! sw %&gt;% fitdistr(densfun = &quot;normal&quot;) ## mean sd ## 5.3500000 0.7762087 ## (0.2454588) (0.1735655) Pretty darn close! Great! 2.4.2 Poisson Distribution rpois() randomly samples integers (eg. 0, 1, 2, 3) from a poisson distribution, based on lambda, the average rate of occurrence. We can approximate that by taking the mean of sw. mypois &lt;- rpois(1000, lambda = mymean) mypois %&gt;% hist() Results in a somewhat skewed distribution, bounded at zero. So how well did mymean match up with the ‘best’ estimate of lambda from fitdistr()? mymean ## [1] 5.35 sw %&gt;% fitdistr(densfun = &quot;Poisson&quot;) ## lambda ## 5.3500000 ## (0.7314369) Excellent! 2.4.3 Gamma Distribution rgamma() randomly samples positive real numbers greater than zero from a gamma distribution. It’s like the continuous version of rpois(). It requires 2 paramters, shape and rate. You can estimate these a couple of different ways, but a simple one is to use the method of moments, which says that: shape \\(\\approx \\frac{mean^{2}}{ variance}\\). scale \\(\\approx \\frac{variance}{ mean }\\). # For shape, we want the rate of how much greater the mean-squared is than the variance. myshape &lt;- mean(sw)^2 / var(sw) # For rate, we like to get the inverse of the variance divided by the mean. myrate &lt;- 1 / (var(sw) / mean(sw) ) # Simulate it! mygamma &lt;- rgamma(1000, shape = myshape, rate = myrate) ## View it! mygamma %&gt;% hist() So how well did myshape and myrate match up with the ‘best-fit’ estimates from fitdistr()? myshape ## [1] 42.7556 myrate ## [1] 7.991701 sw %&gt;% fitdistr(densfun = &quot;gamma&quot;) ## shape rate ## 46.711924 8.731202 ## (20.815512) ( 3.911664) Pretty good! In the same ballpark. 2.4.4 Exponential Distribution rexp() randomly simulates positive real numbers over zero from an exponential distribution. Here, the method of moments says: rate \\(\\approx \\frac{ 1 }{ mean })\\) # We&#39;ll name this myrate2! myrate_e &lt;- 1 / mean(sw) # Simulate it! myexp &lt;- rexp(n = 1000, rate = myrate_e) # Visualize it! myexp %&gt;% hist() How then did myrate_e match up with the estimates from fitdistr()? myrate_e ## [1] 0.1869159 sw %&gt;% fitdistr(densfun = &quot;exponential&quot;) ## rate ## 0.18691589 ## (0.05910799) Pretty solid! 2.4.5 Weibull Distribution rweibull() randomly samples positive real numbers over zero too, but from a Weibull distribution. It requires a shape and scale parameter, but its method of moments equation is pretty complex. Once we get into Weibull distribution territory, it’s better to just use fitdistr(). mystats &lt;- sw %&gt;% fitdistr(densfun = &quot;weibull&quot;) # Here, we&#39;re going to extract the estimate for shape myshape_w &lt;- mystats$estimate[1] # and the estimate for scale myscale_w &lt;- mystats$estimate[2] # simulate! myweibull &lt;- rweibull(n = 1000, shape = myshape_w, scale = myscale_w) # View it! myweibull %&gt;% hist() 2.4.6 Binomial Distribution Next, the binomial distribution is a bit of a special case, in that it’s mostly only helpful for binary variables (with values 0 and 1). But let’s try an example anyways. rbinom() randomly draws n simulated values from a set of provided values at a given probability (prob). It’s usually used for drawing binary variables (0 and 1); a coin flip would have prob = 0.5, or a 50-50 chance. (This time we can’t use fitdistr().) rbinom(n = 10, size = 1, prob = 0.5) ## [1] 0 1 0 1 0 0 1 0 0 1 To get a meaningful simulation, maybe we calculate the proportion of values that are greater than the mean. # In how many cases was the observed value greater than the mean? myprob &lt;- sum(sw &gt; mymean) / length(sw) # Sample from binomial distribution with that probability mybinom &lt;- rbinom(1000, size = 1, prob = myprob) # View histogram! mybinom %&gt;% hist() 2.4.7 Uniform Distribution Finally, the uniform distribution is also a special case. The frequency of values in a uniform distribution is more-or-less uniform. It also only spans the length of a specified interval \\(a \\to b\\). A common range is a = 0 to b = 1. So, the frequency of 1.5 in that interval would be… zero. # Simulate a uniform distribution ranging from 0 to 1 myunif &lt;- runif(n = 1000, min = 0, max = 1) # View histogram! myunif %&gt;% hist(xlim = c(-0.5,1.5)) 2.5 Comparing Distributions Finally, we’re going to want to outfit those vectors in nice data.frames (skipping rbinom() and runif()), and stack them into 1 data.frame to visualize. We can do this using the bind_rows() function from the dplyr package. # Using bind_rows(), mysim &lt;- bind_rows( # Make a bunch of data.frames, all with the same variable names, data.frame(x = sw, type = &quot;Observed&quot;), # and stack them! data.frame(x = mynorm, type = &quot;Normal&quot;), # and stack it! data.frame(x = mypois, type = &quot;Poisson&quot;), # Stack, stack, stack stack it! data.frame(x = mygamma, type = &quot;Gamma&quot;), # so many stacks! data.frame(x = myexp, type = &quot;Exponential&quot;), # so much data!!!!!! data.frame(x = myweibull, type = &quot;Weibull&quot;) ) Next, we can visualize those distributions using geom_density() in ggplot (or geom_histogram(), really, if that floats your boat). # Let&#39;s write the initial graph and save it as an object g1 &lt;- ggplot(data = mysim, mapping = aes(x = x, fill = type)) + geom_density(alpha = 0.5) + labs(x = &quot;Seawall Height (m)&quot;, y = &quot;Density (Frequency)&quot;, subtitle = &quot;Which distribution fits best?&quot;, fill = &quot;Type&quot;) # Then view it! g1 Personally, I can’t read much out of that, so it would be helpful to narrow in the x-axis a bit. We can do that with xlim(), narrowing to just between values 0 and 10. g1 + xlim(0,10) Beautiful! Wow! It looks like the Normal, Gamma, and Weibull distributions all do a pretty excellent job of matching the observed distribution. &lt;br. LC 4 Question You’ve been recruited to evaluate the frequency of Corgi sightings in the Ithaca Downtown. A sample of 10 students each reported the number of corgis they saw last Tuesday in town. Using the method of moments (or fitdistr() for Weibull) and ggplot(), find out which type of distribution best matches the observed corgi distribution! Beth saw 5, Javier saw 1, June saw 10(!), Tim saw 3, Melanie saw 4, Mohammad saw 3, Jenny say 6, Yosuke saw 4, Jimena saw 5, and David saw 2. Answer First, let’s get the stats. # Make distribution of Corgis corgi &lt;- c(5, 1, 10, 3, 4, 3, 6, 4, 5, 2) # Compute statistics for distributions corgi_mean &lt;- mean(corgi) corgi_sd &lt;- sd(corgi) corgi_shape &lt;- mean(corgi)^2 / var(corgi) corgi_rate &lt;- 1 / (var(corgi) / mean(corgi) ) corgi_rate_e &lt;- 1 / mean(corgi) # For Weibull, use fitdistr() from MASS package corgi_stats &lt;- corgi %&gt;% fitdistr(densfun = &quot;weibull&quot;) corgi_shape_w &lt;- corgi_stats$estimate[1] corgi_scale_w &lt;- corgi_stats$estimate[2] Next, let’s bind them together. corgisim &lt;- bind_rows( # Get observed vector data.frame(x = corgi, type = &quot;Observed&quot;), # Get normal dist data.frame(x = rnorm(1000, mean = corgi_mean, sd = corgi_sd), type = &quot;Normal&quot;), # Get poisson data.frame(x = rpois(1000, lambda = corgi_mean), type = &quot;Poisson&quot;), # Get gamma data.frame(x = rgamma(1000, shape = corgi_shape, rate = corgi_rate), type = &quot;Gamma&quot;), # Get exponential data.frame(x = rexp(1000, rate = corgi_rate_e), type = &quot;Exponential&quot;), # Get weibull data.frame(x = rweibull(1000, shape = corgi_shape_w, scale = corgi_scale_w), type = &quot;Weibull&quot;) ) Finally, let’s visualize it! # Visualize! ggplot(data = corgisim, mapping = aes(x = x, fill = type)) + geom_density(alpha = 0.5) + # Narrow it to 0 to 15 to suit plot xlim(0,15) + labs(x = &quot;Corgi Sightings!&quot;, y = &quot;Density (Frequency)&quot;) Neat - looks like the Poisson, Gamma, and Weibull function match well, although the Poisson looks pretty odd! Conclusion So now, you know how to use descriptive statistics in R, how to visualize and evaluate a distribution, and how to simulate several different types of distributions! You’re well on your way to some serious stats for systems engineering! "],["workshop-probability.html", "3 Workshop: Probability Getting Started 3.1 Probability LC 1 3.2 Probability Functions LC 2 Hypothetical Probability Functions LC 3 3.3 ", " 3 Workshop: Probability This tutorial will introduce you to probability and how to code and visualize probabilistic analyses in R! Getting Started Please open up your RStudio.Cloud project. Start a new R script (File &gt;&gt; New &gt;&gt; R Script). Save the R script as workshop_3.R. And let’s get started! Load Packages In this tutorial, we’re going to use more of the dplyr and ggplot2 packages, plus the broom package and mosaicCalc package. library(dplyr) library(ggplot2) library(broom) library(mosaicCalc) Key Functions We’re going to use 3 functions a lot below. This includes bind_rows(), mutate(), and summarize(). So what are they? bind_rows(): stacks 2 or more data.frames on top of each other, matching columns by name. mutate(): creates or edits variables in a data.frame. summarize(): consolidates many rows of data into a single summary statistic (or a set of them.) Examples How might we use bind_rows()? mycoffee &lt;- bind_rows( # Make first data.frame data.frame( # Containing these vectors style and price style = c(&quot;latte&quot;, &quot;cappuccino&quot;, &quot;americano&quot;), price = c(5, 4, 3)), # Make second data.frame data.frame( # Containing these vectors style, price, and shop style = c(&quot;coffee&quot;, &quot;hot cocoa&quot;), price = c(3, 2), shop = c(&quot;Gimme Coffee&quot;, &quot;Starbucks&quot;)) ) # Notice how they stack, # but in first data.frame values, # shop gets filled with NA, # since it wasn&#39;t in first dataframe mycoffee ## style price shop ## 1 latte 5 &lt;NA&gt; ## 2 cappuccino 4 &lt;NA&gt; ## 3 americano 3 &lt;NA&gt; ## 4 coffee 3 Gimme Coffee ## 5 hot cocoa 2 Starbucks How might we use mutate()? mycoffee &lt;- mycoffee %&gt;% # Add a new vector (must be of same length as data.frame) # vector is number of those drinks purchased mutate(purchased = c(5, 4, 10, 2, 1)) How might we use summarize()? mycoffee %&gt;% # Summarize data.frame into one row summarize( # Calculate mean price of drinks mean_price = mean(price), # Calculate total drinks purchased total_purchased = sum(purchased)) ## mean_price total_purchased ## 1 3.4 22 Great! Now let’s apply these to probability! 3.1 Probability Probability refers to how often a specify event is expected to occur, given a sufficient number of times. We’re going to learn (and compute!) several common probability formula in R. 3.1.1 Conditional Probability Conditional Probability: probability of two events happening together reflects the probability of the first happening, times the probability of the second happening given that the first has already occurred. \\[ P(AB) = P(A) \\times P(B|A)\\] In other words, if two events are interdependent, you multiply. 3.1.2 Example: Twizzlers (#fig:img_twizzlers)Twizzlers vs. Red Vines You’ve been hired by the Hershey’s Chocolate Company to investigate quality control on their Twizzlers sweets packaging line. At the start of an assembly line, you mixed in 8,000 Red Vines with a sample of 10,000 Twizzlers. What’s the probability of a packer picking up a Red Vine on the assembly line? sweets &lt;- data.frame( # We know there are 10,000 twizzlers twizzlers = 10000, # and 8,000 redvines redvines = 8000) %&gt;% # So together, there are 18,000 sweets available # So there&#39;s a 8000-in-18,000 chance of picking a redvine mutate(prob_1 = redvines / (twizzlers + redvines)) # Check it! sweets ## twizzlers redvines prob_1 ## 1 10000 8000 0.4444444 But then, what’s the probability of a packer picking up 2 Red Vines in a row on the assembly line? sweets %&gt;% # After picking 1 Red Vine, # there&#39;s now 1 less Red Vine in circulation mutate( # Subtract 1 redvine redvines = redvines - 1, # Recalculate total total = twizzlers + redvines) %&gt;% # calculate probability of picking a second red vine now that 1 is gone mutate(prob_2 = redvines / total) %&gt;% # Finally, multiply the first and second probability together # When it&#39;s this AND that, you multiply mutate(prob = prob_1 * prob_2) ## twizzlers redvines prob_1 total prob_2 prob ## 1 10000 7999 0.4444444 17999 0.4444136 0.1975171 Alternatively, if two events are independent (mutually exclusive), meaning they do not affect each other, you add those probabilities together. You dump in a 1000 pieces of Black Licorice. If a packer picks up 2 sweets, what’s the probability it’s a piece of Black Licorice or Red Vines? sweets %&gt;% # Add a column for black licorice mutate(black_licorice = 1000) %&gt;% # Get total mutate(total = twizzlers + redvines + black_licorice) %&gt;% # Recompute probabilities mutate(prob_1 = redvines / total, prob_2 = black_licorice / total) %&gt;% # When it&#39;s this OR that, you add probabilities mutate(prob_3 = prob_1 + prob_2) ## twizzlers redvines prob_1 black_licorice total prob_2 ## 1 10000 8000 0.4210526 1000 19000 0.05263158 ## prob_3 ## 1 0.4736842 remove(mycoffee, sweets) Total Probabilities We can also examine total probabilities. Any event \\(A\\) that is mutually exclusive from event \\(E\\) (can’t happen at the same time) has the following probability… \\[ P(A) = \\sum_{i=1}^{n}{P(A|E_{i}) \\times P(E_{i}) } \\] Example: Marbles! (#fig:img_marbles)So many marbles! You’ve got 3 bags (\\(E_{1 \\to 3}\\)), each containing 3 marbles, each with a different split of red vs. blue marbles. If we choose a bag at random and sample a marble at random (2 mutually exclusive events), what’s the probability that marble will be red (\\(P(A)\\))? I like to map these out, so I understand visually what all the possible pathways are. Here’s a chart I made using mermaid, where I’ve diagrammed each possible set of actions, like choosing Bag 1 then Marble a (1 pathway), choosing Bag 1 then Marble b (a second pathway), etc. If we look at the ties to the marbles, you’ll see I labeled each tie to a red marble as 1 and each tie to a blue marble as 0. If we add these pathways up, we’ll get the total probability: 0.67 (aka 2/3). (#fig:img_mermaid)Drawing Probability Diagrams But what if we can’t diagram it out? Perhaps we’re choosing from 100s of marbles, or we’re limited on time! How would we solve this problem mathematically? The key here is knowing that: the blue marbles don’t really matter we need the probability of choosing a bag we need the probability of choosing a red marble in each bag. Here’s what we know: There’s an equal chance of choosing any bag of the 3 bags (because random). (If 1 bag were on a really high shelf, then maybe the probabilities would be different, i.e. not random, but let’s assume they’re random this time.) # there are 3 bags n_bags &lt;- 3 # So.... # In this case, P(Bag1) = P(Bag2) = P(Bag3) # and P(Bag1) + P(Bag2) + P(Bag3) = 100% = 1.0 # 1/3 chance of picking Bag 1 # written P(Bag1) pbag1 &lt;- 1 / n_bags # 1/3 chance of picking Bag 2 # written P(Bag2) pbag2 &lt;- 1 / n_bags # 1/3 chance of picking Bag 3 # written P(Bag3) pbag3 &lt;- 1 / n_bags # Check it! c(pbag1, pbag2, pbag3) ## [1] 0.3333333 0.3333333 0.3333333 There are 3 marbles in each bag. # Total marbles in Bag 1 m_bag1 &lt;- 3 # Total marbles in Bag 2 m_bag2 &lt;- 3 # Total marbles in Bag 3 m_bag3 &lt;- 3 There are 3 red marbles in bag 1, 1 red marbles in bag 2, and 2 red marbles in bag 3. # So, we can calculate the percentages in each bag. # percentage of red marbles in Bag 1 # written P(Red|Bag1) pm_bag1 &lt;- 3 / m_bag1 # percentage of red marbles in Bag 2 # written P(Red|Bag2) pm_bag2 &lt;- 1 / m_bag2 # percentage of red marbles in Bag 3 # written P(Red|Bag3) pm_bag3 &lt;- 2 / m_bag3 # Check it! c(pm_bag1, pm_bag2, pm_bag3) ## [1] 1.0000000 0.3333333 0.6666667 Selecting Bag 1 and then selecting a Red Marble are interdependent events, so we multiply them. # For example # P(Bag1 &amp; Red) = P(Red|Bag1) * P(Bag1) pm_bag1 * pbag1 ## [1] 0.3333333 But each pathway (eg. Bag 1 x Marble A) is distinct and independent of the other pathways, so we can add them together. # P(Bag1 &amp; Red) = P(Red|Bag1) * P(Bag1) pm_bag1 * pbag1 + # P(Bag2 &amp; Red) = P(Red|Bag2) * P(Bag2) pm_bag2 * pbag2 + # P(Bag3 &amp; Red) = P(Red|Bag3) * P(Bag3) pm_bag3 * pbag3 ## [1] 0.6666667 And that gives us the same answer: 0.67 or 2/3. However, that required making a lot of objects in R. Can we do this more succinctly using vectors and data.frames? We could compute a bag-wise data.frame, where each row represents a choice (bag) from event1. bags &lt;- data.frame( bag_id = 1:3, # For each bag, how many do you get to choose? bags = c(1, 1, 1), # For each bag, how many marbles do you get to choose? marbles = c(3, 3, 3), # For each bag, how many marbles are red? red = c(3, 1, 2)) %&gt;% # Then, we can calculate the probability of... mutate( # choosing that bag out of all bags prob_bag = bags / sum(bags), # choosing red out of all marbles in that bag prob_red = red / marbles, # choosing BOTH that bag AND a red marble in that bag prob_bagred = prob_red * prob_bag) Finally, we could just sum the joint probabilities all together. bags %&gt;% summarize(prob_bagred = sum(prob_bagred)) ## prob_bagred ## 1 0.6666667 Much faster! # Let&#39;s remove this now unnecessary data remove(bags, n_bags, m_bag1, m_bag2, m_bag3, pbag1, pbag2, pbag3, pm_bag1, pm_bag2, pm_bag3) Bayes Rule A cool trick, called Bayes’ Rule, reveals that we can figure out a probability of interest that depends on other probabilities. Let’s say, we want to know, what’s the probability of OUTCOME given CONDITION. Bayes’ Rule states that the probability of the OUTCOME occurring given CONDITION is equal to the joint probability of the Outcome and Condition both occurring, divided by the probability of the condition occurring. \\[ P(Outcome = 1| Condition = 1) = \\frac{ P(Outcome = 1\\ \\&amp; \\ Condition = 1) }{ P(Condition = 1)} \\] Thanks to Conditional Probability and Total Probability tricks, we can break that down into quantities we can calculate. \\[ P(Outcome=1 | Condition=0) = \\frac{ P(Condition=1|Outcome=1) \\times P(Outcome=1) }{ \\sum{ P(Condition | Outcome) } \\times P(Outcome)} \\] \\[ = \\frac{ P(Condition=1|Outcome=1) \\times P(Outcome=1) }{ P(Condition=1|Outcome=1) \\times P(Outcome=1) + P(Condition=1|Outcome=0) \\times P(Outcome=0)} \\] Let’s define some terms: posterior: Posterior probability is the probability that the outcome occurs given that the condition occurs. prior: the probability that the outcome occurs, independent of anything else. likelihood: the probability that the condition occurs, given that the outcome occurs. evidence: the total probability that the condition does or does not occur. Example: Coffee Shop (Incomplete Information) (#fig:img_coffee)Yay Coffee! A local coffee chain needs your help to analyze their supply chain issues. They know that their scones help them sell coffee, but does their coffee help them sell scones? Over the last week, when 7 customers bought scones, 3 went on to buy coffee. When 3 customers didn’t buy scones, just 2 bought coffee. In general, 7 out of 10 of customers ever bought scones. What’s the probability that a customer will buy a scone, given that they just bought coffee? # We want to know this p_scone_coffee &lt;- NULL # But we know this! p_coffee_scone &lt;- 3/7 p_coffee_no_scone &lt;- 2/3 p_scone &lt;- 7/10 # AND # If 7 out of 10 customers ever bought scones, # then 3 out of 10 NEVER bought scones p_no_scone &lt;- 3 / 10 Using these 3~4 probabilities, we can deduce the total probability of coffee (the denominator), meaning whether you got coffee OR whether you didn’t get coffee. # Total Prob of Coffee = Getting Cofee + Not getting coffee p_coffee &lt;- p_coffee_scone * p_scone + p_coffee_no_scone * p_no_scone # Check it! p_coffee ## [1] 0.5 So let’s use p_coffee to get the probability of getting a scone given that you got coffee! p_scone_coffee &lt;- p_coffee_scone * p_scone / p_coffee It’s magic! Bayes’ Rule is helpful when we don’t have complete information, and just have some raw percentages or probabilities. Example: Coffee Shop (Complete Information) But, if we do have complete information, then we can actually prove Bayes’ Rule quite quickly. For example, say those percentages the shop owner gave us were actually meticulously tabulated by a barista. We talk to the barista, and she explains that she can tell us right away the proportion of folks who got a scone given that they got coffee. She shows us her spreadsheet of orders, listing for each customer, whether they got coffee and whether they got a scone. orders &lt;- tibble( coffee = c(&quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;), scone = c(&quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;)) We can tabulate these quickly using table(), tallying up how many folks did this. orders %&gt;% table() ## scone ## coffee no yes ## no 1 4 ## yes 2 3 # Let&#39;s skip to the end and just calculate the proportion directly! # Out of all people who got coffee, how many got scones? orders %&gt;% summarize(p_scone_coffee = sum(scone == &quot;yes&quot; &amp; coffee == &quot;yes&quot;) / sum(coffee == &quot;yes&quot;) ) ## # A tibble: 1 × 1 ## p_scone_coffee ## &lt;dbl&gt; ## 1 0.6 # The end! # Now that we know this, let&#39;s prove that Bayes works. orders %&gt;% summarize( # The goal (posterior) p_scone_coffee = sum(scone == &quot;yes&quot; &amp; coffee == &quot;yes&quot;) / sum(coffee == &quot;yes&quot;), # The data p_coffee_scone = sum(coffee == &quot;yes&quot; &amp; scone == &quot;yes&quot;) / sum(scone == &quot;yes&quot;), p_coffee_no_scone = sum(coffee == &quot;yes&quot; &amp; scone == &quot;no&quot;) / sum(scone == &quot;no&quot;), p_scone = sum(scone == &quot;yes&quot;) / sum(coffee == &quot;yes&quot; | coffee == &quot;no&quot;), p_no_scone = sum(scone == &quot;no&quot;) / sum(coffee == &quot;yes&quot; | coffee == &quot;no&quot;), # Now recalculate the goal, using the data we have collected. # Does &#39;bayes&#39; equal &#39;p_scone_coffee&#39;? bayes = p_coffee_scone * p_scone / (p_coffee_scone * p_scone + p_coffee_no_scone * p_no_scone)) ## # A tibble: 1 × 6 ## p_scone_coffee p_coffee_scone p_coffee_no_scone p_scone p_no_scone ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.6 0.429 0.667 0.7 0.3 ## # ℹ 1 more variable: bayes &lt;dbl&gt; # It should! And it does! Tada! LC 1 Question (#fig:img_bank)Figure 1. Bank of Evil, from Despicable Me A local business is trying to open a new branch and needs a short-term loan. They meet with a bank, but are surprised by the interest rates, so they do a little research. Over the last 5 years: 50 businesses applied to this bank for loans. 29 receive loans. 18 went bankrupt. 13 of those that went bankrupt got loans. Our local business wants to know: what’s the probability that a firms goes bankrupt, given that they received a loan? Assume that a bankruptcy rate above 10% indicates bad lending practices. Should you stay away from that bank? Answer We know the following: # The probability of getting a loan (13) given that you went bankrupt (18) p_loan_bankrupt &lt;- 13 / 18 # Probability of going bankrupt (18), whether or not you got a loan (50) p_bankrupt &lt;- 18 / 50 # Probability of NOT going bankrupt (50 - 18 = 32), whether or not you got a loan (50) p_no_bankrupt &lt;- 32 / 50 # Probability of getting a loan (29), whether or not you go bankrupt (50) p_loan &lt;- 29 / 50 # Probability you get a loan (26), given that you don&#39;t go bankrupt (32) p_loan_no_bankrupt &lt;- 26 / 32 # Apply Bayes Rule p_bankrupt_loan &lt;- p_loan_bankrupt * p_bankrupt / (p_loan_bankrupt * p_bankrupt + p_loan_no_bankrupt * p_no_bankrupt) # Check! p_bankrupt_loan ## [1] 0.3333333 Looks like there is a 33% chance that you will go bankrupt if you get a loan. That’s much higher than 10%. Maybe be a little wary of that bank? # Clear excess data remove(firms, p_loan_bankrupt, p_bankrupt, p_no_bankrupt, p_loan, p_loan_no_bankrupt, p_bankrupt_loan) 3.2 Probability Functions How then do we use probability in our statistical analyses of risk, performance, and other systems engineering concepts? Probability allows us to measure for any statistic (or parameter) mu, how extreme is that statistic? This is called type II error, measured by a p-value, the probability that a more extreme value occurred than our statistic. It’s an extremely helpful benchmark. In order to evaluate how extreme it is, we need values to compare it to. We can do this using (1) an observed distribution, (2) making a probability function curve of the observed distribution, or (3) assuming the probability function of a hypothetical distribution. 3.2.1 Example: Observed Distributions (#fig:img_er)Figure 2. Your Local ER For example, a local hospital wants to make their health care services more affordable, given the surge in inflation. They measured n = 15 patients who stayed 1 night over the last 7 days, how much were they charged (before insurance)? Let’s call this vector obs (for ‘observed data’). A 16th patient received a bill of $3000 (above the national mean of ~$2500). We’ll record this as stat below. # Let&#39;s record our vector of 15 patients obs &lt;- c(1126, 1493, 1528, 1713, 1912, 2060, 2541, 2612, 2888, 2915, 3166, 3552, 3692, 3695, 4248) # And let&#39;s get our new patient data point to compare against stat &lt;- 3000 Here, we know the full observed distribution of values (cost), so we can directly compute the p_value from them, using the logical operator &gt;=. # Which values of in vector obs were greater than or equal to stat? obs &gt;= stat ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [11] TRUE TRUE TRUE TRUE TRUE R interprets TRUE == 1 &amp; FALSE == 0, so we can take the mean() to get the percentage of values in obs greater than or equal to stat. # Get p-value, the probability of getting a value greater than or equal to stat mean(obs &gt;= stat) ## [1] 0.3333333 # This means Total Probability, where probability of each cost is 1/n sum( (obs &gt;= stat) / length(obs) ) ## [1] 0.3333333 Unfortunately, this only takes into account the exact values we observed (eg. $1493), but it can’t tell us anything about values we didn’t observe (eg. $1500). But logically, we know that the probability of getting a bill of $1500 should be pretty similar to a bill of $1493. So how do we fill in the gaps? 3.2.2 Observed Probability Density Functions Above, we calculated the probability of getting a more extreme hospital bill based on a limited sample of points, but for more precise probabilities, we need to fill in the gaps between our observed data points. For a vector x, the probability density function is a curve providing the probability (y) of each value across the range of x. It shows the relative frequency (probability) of each possible value in the range. We can ask R to estimate the probability density function for any observed vector using density(). This returns the density (y) of a bunch of hypothetical values (x) matching our distribution’s curve. We can access those results using the broom package, by tidy()-ing it into a data.frame. obs %&gt;% density() %&gt;% tidy() %&gt;% tail(3) ## # A tibble: 3 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5715. 0.000000725 ## 2 5727. 0.000000675 ## 3 5739. 0.000000628 But that’s data, not a function, right? Functions are equations, machines you can pump an input into to get a specific output. Given a data.frame of 2 vectors, R can actually approximate the function (equation) connecting vector 1 to vector 2 using approxfun(), creating your own function! So cool! # Let&#39;s make dobs(), the probability density function for our observed data. dobs &lt;- obs %&gt;% density() %&gt;% tidy() %&gt;% approxfun() # Now let&#39;s get a sequence (seq()) of costs from 1000 to 3000, in units of 1000.... seq(1000, 3000, by = 1000) ## [1] 1000 2000 3000 # and let&#39;s feed it a range of data to get the frequencies at those costs! seq(1000, 3000, by = 1000) %&gt;% dobs() ## [1] 0.0001505136 0.0003081445 0.0003186926 For now, let’s get the densities for costs ranging from the min to the max observed cost. mypd &lt;- tibble( # Get sequence from min to max, in units of $10 cost = seq(min(obs), max(obs), by = 10), # Get probability densities prob_cost_i = dobs(cost)) %&gt;% # Classify each row as TRUE (1) if cost greater than or equal to stat, or FALSE (0) if not. # This is the probability that each row is extreme (1 or 0) mutate(prob_extreme_i = cost &gt;= stat) # Check it out! mypd %&gt;% head(3) ## # A tibble: 3 × 3 ## cost prob_cost_i prob_extreme_i ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 1126 0.000186 FALSE ## 2 1136 0.000189 FALSE ## 3 1146 0.000191 FALSE We’ll save it to mypd, naming the x-axis cost and the y-axis prob_cost_i, to show the probability of each cost in row i (eg. $1126, $1136, $1146, … n). We’ll also calculate prob_extreme_i, the probability that each ith cost is extreme (greater than or equal to our 16th patient’s bill). Either it is extreme (TRUE == 100% = 1) or it isn’t (FALSE == 0% == 0). Our density function dobs() estimated prob_cost_i (y), the probability/relative frequency of cost (x) occurring, where x represents every possible value of cost. We can visualize mypd using geom_area() or geom_line() in ggplot2! We can add geom_vline() to draw a vertical line at the location of stat on the xintercept. mypd %&gt;% ggplot(mapping = aes(x = cost, y = prob_cost_i, fill = prob_extreme_i)) + # Fill in the area from 0 to y along x geom_area() + # Or just draw curve with line geom_line() + # add vertical line geom_vline(xintercept = stat, color = &quot;red&quot;, size = 3) + # Add theme and labels theme_classic() + labs(x = &quot;Range of Patient Costs (n = 15)&quot;, y = &quot;Probability&quot;, subtitle = &quot;Probability Density Function of Hospital Stay Costs&quot;) + # And let&#39;s add a quick label annotate(&quot;text&quot;, x = 3500, y = 1.5e-4, label = &quot;(%) Area\\nunder\\ncurve??&quot;, size = 5) (#fig:plot_pdf)Figure 3. Visualizing a Probability Density Function! 3.2.3 Applying Probability Density Functions Great! We can view the probability density function now above. But how do we translate that into a single probability that measures how extreme Patient 16’s bill is? We have the probability prob_cost_i at points cost estimated by the probability density function saved in mypd. We can calculate the total probability or p_value that a value of cost will be greater than our statistic stat, using our total probability formula. We can even restate it, so that it looks a little more like the weighted average it truly is. \\[ P_{\\ Extreme} = \\sum_{i = 1}^{n}{ P (Cost | Extreme_{\\ i}) \\times P (Cost_{\\ i}) } = \\frac{ \\sum_{i = 1}^{n}{ P (Cost_{i}) \\times P(Extreme)_{\\ i} } }{ \\sum_{i = 1}^{n}{ P(Cost_{i}) } } \\] p &lt;- mypd %&gt;% # Calculate the conditional probability of each cost occurring given that condition mutate(prob_cost_extreme_i = prob_cost_i * prob_extreme_i) %&gt;% # Next, let&#39;s summarize these probabilities summarize( # Add up all probabilities of each cost given its condition in row i prob_cost_extreme = sum(prob_cost_extreme_i), # Add up all probabilities of each cost in any row i prob_cost = sum(prob_cost_i), # Calculate the weighted average, or total probability of getting an extreme cost # by dividing these two sums! prob_extreme = prob_cost_extreme / prob_cost) # Check it out! p ## # A tibble: 1 × 3 ## prob_cost_extreme prob_cost prob_extreme ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0317 0.0868 0.365 Very cool! Visually, what’s happening here? ggplot() + geom_area(data = mypd, mapping = aes(x = cost, y = prob_cost_i, fill = prob_extreme_i)) + geom_vline(xintercept = stat, color = &quot;red&quot;, size = 3) + theme_classic() + labs(x = &quot;Range of Patient Costs (n = 15)&quot;, y = &quot;Probability&quot;, subtitle = &quot;Probability Density Function of Hospital Stay Costs&quot;) + annotate(&quot;text&quot;, x = 3500, y = 1.5e-4, label = paste(&quot;P(Extreme)&quot;, &quot;\\n&quot;, &quot; = &quot;, p$prob_extreme %&gt;% round(2), sep = &quot;&quot;), size = 5) (#fig:plot_pdf_area)Figure 4. PDF with Area Under Curve! 3.2.4 Observed Cumulative Distribution Functions Alternatively, we can calculate that p-value for prob_extreme a different way, by looking at the cumulative probability. To add a values/probabilities in a vector together sequentially, we can use cumsum() (short for cumulative sum). For example: # Normally c(1:5) ## [1] 1 2 3 4 5 # Cumulatively summed c(1:5) %&gt;% cumsum() ## [1] 1 3 6 10 15 # Same as c(1, 2+1, 3+2+1, 4+3+2+1, 5+4+3+2+1) ## [1] 1 3 6 10 15 Every probability density function (PDF) can also be represented as a cumulative distribution function (CDF). Here, we calculate the cumulative total probability of receiving each cost, applying cumsum() to the probability (prob_cost) of each value (cost). In this case, we’re basically saying, we’re interested in all the costs, so don’t discount any. \\[ P_{\\ Extreme} = \\sum_{i = 1}^{n}{ P (Cost | Extreme_{\\ i} = 1) \\times P (Cost_{\\ i}) } = \\frac{ \\sum_{i = 1}^{n}{ P (Cost_{i}) \\times 1 } }{ \\sum_{i = 1}^{n}{ P(Cost_{i}) } } \\] mypd %&gt;% # For instance, we can do the first step here, # taking the cumulative probability of costs i through j.... mutate(prob_cost_cumulative = cumsum(prob_cost_i)) %&gt;% head(3) ## # A tibble: 3 × 4 ## cost prob_cost_i prob_extreme_i prob_cost_cumulative ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 1126 0.000186 FALSE 0.000186 ## 2 1136 0.000189 FALSE 0.000375 ## 3 1146 0.000191 FALSE 0.000566 Our prob_cost_cumulative in row 3 above shows the total probability of n = 3 patients receiving a cost of 1126 OR 1136 OR 1146. But, we want an average estimate for 1 patient. So, like in a weighted average, we can divide by the total probability of all (n) hypothetical patients in the probability density function receiving any of these costs. This gives us our revised prob_cost_cumulative, which ranges from 0 to 1! mycd &lt;- mypd %&gt;% # For instance, we can do the first step here, # taking the cumulative probability of costs i through j.... mutate(prob_cost_cumulative = cumsum(prob_cost_i) / sum(prob_cost_i)) %&gt;% # We can also then identify the segment that is extreme! mutate(prob_extreme = prob_cost_cumulative * prob_extreme_i) # Take a peek at the tail! mycd %&gt;% tail(3) ## # A tibble: 3 × 5 ## cost prob_cost_i prob_extreme_i prob_cost_cumulative prob_extreme ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4226 0.000144 TRUE 0.997 0.997 ## 2 4236 0.000142 TRUE 0.998 0.998 ## 3 4246 0.000140 TRUE 1 1 Let’s visualize mycd, our cumulative probabilities! viz_cd &lt;- ggplot() + # Show the cumulative probability of each cost, # shaded by whether it is &quot;extreme&quot; (cost &gt;= stat) or not geom_area(data = mycd, mapping = aes(x = cost, y = prob_cost_cumulative, fill = prob_extreme_i)) + # Show cumulative probability of getting an extreme cost geom_area(data = mycd, mapping = aes(x = cost, y = prob_extreme, fill = prob_extreme_i)) + # Show the 16th patient&#39;s cost geom_vline(xintercept = stat, color = &quot;red&quot;, size = 3) + # Add formatting theme_classic() + labs(x = &quot;Cost of Hospital Stays (n = 15)&quot;, y = &quot;Cumulative Probability of Cost&quot;, fill = &quot;P(Extreme i)&quot;, title = &quot;Cumulative Distribution Function for Cost of Hospital Stays&quot;, subtitle = &quot;Probability of Cost more Extreme than $3000 = 0.36&quot;) # View it! viz_cd # (Note: I&#39;ve added some more annotation to mine # than your image will have - don&#39;t worry!) (#fig:plot_cdf)Figure 5. Visualizing a Cumulative Distribution Function! But wouldn’t it be handy if we could just make a literal cumulative distribution function, just like we did for the probability density function dobs()? pobs &lt;- obs %&gt;% density() %&gt;% tidy() %&gt;% # Sort from smallest to largest arrange(x) %&gt;% # take cumulative sum, divided by total probability mutate(y = cumsum(y) / sum(y)) %&gt;% # Make cumulative distribution function pobs()! approxfun() # We&#39;ll test it out! 1 - pobs(3000) ## [1] 0.3726192 # Pretty close to our probability we calculated before! # Clear unnecessary data. remove(stat, mycd, p, viz_cd) 3.2.5 Using Calculus! Above we took a computational-approach to the CDF, using R to number-crunch the CDF. To summarize: We took a vector of empirical data obs, We estimated the probability density function (PDF) using density() We calculated the cumulative probability distribution ourselves We connected-the-dots of our CDF into a function with approxfun(). We did this because we started with empirical data, where where the density function is unknown! But sometimes, we do know the density function, perhaps because systems engineers have modeled it for decades! In these cases, we could alternatively use calculus in R to obtain the CDF and make probabilistic assessments. Here’s how! For example, this equation does a pretty okay job of approximating the shape of our distribution in obs. \\[ f(x) = \\frac{-2}{10^7} + \\frac{25x}{10^8} - \\frac{45x^2}{10^{12}} \\] We can write that up in a function, which we will call pdf. For every x value we supply, it will compute that equation to predict that value’s relative refequency/probability. # Write out our nice polynomial function pdf = function(x){ -2/10^7 + 25/10^8*x + -45/10^12*x^2 } # Check it! c(2000, 3000) %&gt;% pdf() ## [1] 0.0003198 0.0003448 The figure below demonstrates that it approximates the true density relatively closely. # We&#39;re going to add another column to our mypd dataset, mypd &lt;- mypd %&gt;% # approximating the probability of each cost with our new pdf() mutate(prob_cost_i2 = pdf(cost)) ggplot() + geom_line(data = mypd, mapping = aes(x = cost, y = prob_cost_i, color = &quot;from raw data&quot;)) + geom_line(data = mypd, mapping = aes(x = cost, y = prob_cost_i2, color = &quot;from function&quot;)) + theme_classic() + labs(x = &quot;Cost&quot;, y = &quot;Probability&quot;, color = &quot;Type&quot;) So how do we generate the cumulative density function? The mosaicCalc package can help us with its functions D() and antiD(). D() computes the derivative of a function (Eg. CDF -&gt; PDF) antiD() computes its integral (Eg. PDF -&gt; CDF) # Compute the anti-derivative (integral) of the function pdf(x), solving for x. cdf &lt;- antiD(tilde = pdf(x) ~ x) # It works just the same as our other functions obs %&gt;% head() %&gt;% cdf() ## [1] 0.1368449 0.2284130 0.2380292 0.2910549 0.3517389 0.3989108 # (Note: Our function is not a perfect fit for the data, so probabilities exceed 1!) # Let&#39;s compare our cdf() function made with calculus with pobs(), our computationally-generated CDF function. obs %&gt;% head() %&gt;% pobs() ## [1] 0.07774089 0.16376985 0.17348497 0.22750868 0.28831038 ## [6] 0.33387171 # Pretty similar results. The differences are due the fact that our original function is just an approximation, rather than dobs(), which is a perfect fit for our densities. And we can also take the derivative of our cdf() function with D() to get back our pdf(), which we’ll call pdf2(). pdf2 &lt;- D(tilde = cdf(x) ~ x) # Let&#39;s compare results.... # Our original pdf... obs %&gt;% head() %&gt;% pdf() ## [1] 0.0002242456 0.0002727428 0.0002767347 0.0002960034 0.0003132915 ## [6] 0.0003238380 # Our pdf dervied from cdf... obs %&gt;% head() %&gt;% pdf2() ## [1] 0.0002242456 0.0002727428 0.0002767347 0.0002960034 0.0003132915 ## [6] 0.0003238380 # They&#39;re the same! Tada! You can do calculus in R! remove(mypd, pdf, pdf2, cdf, obs) LC 2 Question A month has gone by, and our hospital has now billed 30 patients. You’ve heard that hospital bills at or above $3000 a day may somewhat deter people from seeking future medical care, while bills at or above $4000 may greatly deter people from seeking future care. (These numbers are hypothetical.) Using the vectors below, please calculate the following, using a PDF or CDF. What’s the probability that a bill might somewhat deter a patient from going to the hospital? What’s the probability that a bill might greatly deter a patient from going to the hospital? What’s the probability that a patient might be somewhat deterred but not greatly deterred from going to the hospital? Note: Assume that the PDF matches the range of observed patients. # Let&#39;s record our vector of 30 patients patients &lt;- c(1126, 1493, 1528, 1713, 1912, 2060, 2541, 2612, 2888, 2915, 3166, 3552, 3692, 3695, 4248, 3000, 3104, 3071, 2953, 2934, 2976, 2902, 2998, 3040, 3030, 3021, 3028, 2952, 3013, 3047) # And let&#39;s get our statistics to compare against somewhat &lt;- 3000 greatly &lt;- 4000 Answer # Get the probability density function for our new data dobs2 &lt;- patients %&gt;% density() %&gt;% tidy() %&gt;% approxfun() # Get the probability densities mypd2 &lt;- tibble( cost = seq(min(patients), max(patients), by = 10), prob_cost_i = cost %&gt;% dobs2()) %&gt;% # Calculate probability of being somewhat deterred mutate( prob_somewhat_i = cost &gt;= somewhat, prob_greatly_i = cost &gt;= greatly, prob_somewhat_not_greatly_i = cost &gt;= somewhat &amp; cost &lt; greatly) To calculate these probabilities straight from the probability densities, do like so: mypd2 %&gt;% summarize( # Calculate total probability of a cost somewhat deterring medical care prob_somewhat = sum(prob_cost_i * prob_somewhat_i) / sum(prob_cost_i), # Calculate total probability of a cost greatly deterring medical care prob_greatly = sum(prob_cost_i * prob_greatly_i) / sum(prob_cost_i), # Calculate total probability of a cost somewhat-but-not-greatly deterring medical care prob_somewhat_not_greatly = sum(prob_cost_i * prob_somewhat_not_greatly_i) / sum(prob_cost_i)) ## # A tibble: 1 × 3 ## prob_somewhat prob_greatly prob_somewhat_not_greatly ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.431 0.0172 0.414 To calculate these probabilities from the cumulative distribution functions, we can do the following: # Get cumulative probabilities of each mycd2 &lt;- mypd2 %&gt;% mutate( # Calculate total probability of a cost somewhat deterring medical care prob_somewhat = cumsum(prob_cost_i) * prob_somewhat_i / sum(prob_cost_i), # Calculate total probability of a cost greatly deterring medical care prob_greatly = cumsum(prob_cost_i) * prob_greatly_i / sum(prob_cost_i), # Calculate total probability of a cost somewhat-but-not-greatly deterring medical care prob_somewhat_not_greatly = cumsum(prob_cost_i) * prob_somewhat_not_greatly_i / sum(prob_cost_i)) # Check it! mycd2 %&gt;% tail(3) ## # A tibble: 3 × 8 ## cost prob_cost_i prob_somewhat_i prob_greatly_i ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 4226 0.000106 TRUE TRUE ## 2 4236 0.000107 TRUE TRUE ## 3 4246 0.000107 TRUE TRUE ## # ℹ 4 more variables: prob_somewhat_not_greatly_i &lt;lgl&gt;, ## # prob_somewhat &lt;dbl&gt;, prob_greatly &lt;dbl&gt;, ## # prob_somewhat_not_greatly &lt;dbl&gt; Finally, if you want to visualize them, here’s how you would do it! ggplot() + # Get cumulative probability generally geom_area(data = mycd2, mapping = aes(x = cost, y = cumsum(prob_cost_i) / sum(prob_cost_i), fill = &quot;Very Little&quot;)) + # Get cumulative probability if somewhat but not greatly geom_area(data = mycd2, mapping = aes(x = cost, y = prob_somewhat_not_greatly, fill = &quot;Somewhat-not-Greatly&quot;)) + # Get cumulative probability if greatly geom_area(data = mycd2, mapping = aes(x = cost, y = prob_greatly, fill = &quot;Greatly&quot;)) + theme_classic() + labs(x = &quot;Cost of Hospital Stay (n = 30)&quot;, y = &quot;Cumulative Probability&quot;, subtitle = &quot;Probability of Hospital Bill Deterring Future Hospital Visits&quot;, fill = &quot;Deter Future Visits&quot;) Hypothetical Probability Functions Often, though, our sample of data is just one of the many samples we could have possibly gotten; had we looked at a different hospital (by chance), or a different sample of patients come in by chance, we might have gotten slightly different hospital bills. The problem is, we almost never can see the distribution of the true population of all observations (eg. all hospital bills). But, if we can approximately guess what type of distribution that population has, we can very easily compute the probability density functions and cumulative distribution functions of several of the most well known distributions in R (eg. Normal, Poisson, Gamma, etc.) Example: Farmers Market The Ithaca Farmers Market is a vendor-owned cooperative that runs a massive Saturday-and-Sunday morning market for local produce, street food, and hand-made goods, on the waterfront of Cayuga Lake. In markets and street fairs, some stalls’ brands are often better known than others, so businesses new to the market might worry that people won’t come to their stalls without specific prompting. This past Saturday, a volunteer tracked 500 customers and recorded how many stalls each customers visited during their stay. They calculated the following statistics. The average customer visited a mean of 5.5 stalls and a median of 5 stalls, with a standard deviation of 2.5 stalls. Figure 3.1: Ithaca Farmers Market! Market operators wants to know: What’s the probability that customers will stop by 5 stalls? What’s the probability that customers will stop by at max 5 stalls? What’s the probability that customers will stop by over 5 stalls? How many visits did people usually make? Estimate the interquartile range (25th-75th percentiles). Unfortunately, the wind blew the raw data away into Cayuga Lake before they could finish their analysis. How can we approximate the unobserved distribution of visits and compute these probabilities? Below, we will (1) use these statistics to guess which of several possible archetypal hypothetical distributions it most resembles, and then (2) compute probabilities based off of the shape of that hypothetical distribution. Guessing the Shape of an Unobserved Distribution We don’t have the actual data, but we know several basic features of our distribution! Our variable is visits, a count ranging from 0 to infinity. (Can’t have -5 visits, can’t have 1.3 visits.) The median (5) is less than the mean (5.5), so our distribution is right-skewed. This sounds like a classic Poisson distribution! Let’s simulate some poisson-distributed data to demonstrate. # Randomly sample 500 visits from a poisson distribution with a mean of 5.5 visits &lt;- rpois(n = 500, lambda = 5.5) # Check out the distribution! visits %&gt;% hist() Compute Probabilities using Hypothetical Distributions Much like rpois() randomly generates poisson distributed values, dpois() generates the density of any value on a poisson distribution centered on a given mean, while ppois() returns for any percentile in the distribution the cumulative probability (percentage of the area under the density curve) up until that point. area under the curve (probability) of. See the Table below for several examples. Table 3.1: Table 1: Probability Functions (r, d, p, and q) Meaning Purpose Main Input Normal Poisson Gamma Exponential Random Draws from Distribution Simulate a distribution n = # of simulations rnorm() rpois() rgamma() rexp() Probability Density Function Get Probability of Value in Distribution x = value in distribution dnorm() dpois() dgamma() dexp() Cumulative Distribution Function Get % of Distribution LESS than Value q = a cumulative probability pnorm() ppois() pgamma() pexp() Quantiles Function Get Value of any Percentile in Distribution p = percentile qnorm() qpois() qgamma() qexp() Density So, what percentage of customers stopped by 1 stall? Below, dpois() tells us the density() or frequency of your value, given a distribution where the mean = 5.5. # Get the frequency for 5 visits in the distribution pd5 &lt;- dpois(5, lambda = 5.5) # Check it! pd5 ## [1] 0.1714007 Looks like 17.1% of customers stopped by 5 stalls. We can validate this using our simulated visits from above. We can calculate the density() function, extract it using approxfun(), and then assign it to dsim(), our own exact probability density function for our data. It works just like dpois(), but you don’t need to specify lambda, because it only works for this exact distribution! # Approximate the PDF of our simulated visits dsim &lt;- visits %&gt;% density() %&gt;% approxfun() # Try our density function for our simulated data! dsim(5) ## [1] 0.1852041 # Pretty close to our results from dpois()! Cumulative Probabilities What percentage of customers stopped by at max 5 stalls? # Get the cumulative frequency for a value (5) in the distribution cd5 &lt;- ppois(q = 5, lambda = 5.5) # Check it! cd5 ## [1] 0.5289187 Looks like just 52.9% of customers stopped by 1 stall or fewer. What percentage of customers stopped by over 5 stalls? # Get the probability they will NOT stop at 5 or fewer stalls 1 - cd5 ## [1] 0.4710813 We can validate our results against our simulated distribution. psim &lt;- visits %&gt;% density() %&gt;% tidy() %&gt;% # Get cumulative probability distribution arrange(x) %&gt;% # Get cumulative probabilities mutate(y = cumsum(y) / sum(y)) %&gt;% # Turn it into a literal function! approxfun() # Check it! psim(5) ## [1] 0.4579077 # Pretty close to cdf5! Quantiles How many visits did people usually make? Estimate the interquartile range (25th-75th percentiles) of the unobserved distribution. q5 &lt;- qpois(p = c(.25, .75), lambda = 5.5) # Check it! q5 ## [1] 4 7 Looks like 50% of folks visited between 4 and 7 stalls We can compare against our simulated data using quantile(). # Approximate the quantile function of this distribution qsim &lt;- tibble( # Get a vector of percentiles from 0 to 1, in units of 0.001 x = seq(0, 1, by = 0.001), # Using our simulated distribution, # get the quantiles (values) at those points from this distribution y = visits %&gt;% quantile(probs = x)) %&gt;% # Approximate function! approxfun() # Check it! qsim(c(.25, .75)) ## [1] 4 7 rm(visits, pd5, cd5, q5, dsim, psim, qsim) LC 3 Question What if we are not certain whether our unobserved vector of visits has a Poisson distribution or not? To give you more practice, please calculate the probability that customers will stop at more than 5 stalls, using appropriate functions for the (1) Normal, (2) Gamma, and (3) Exponential distribution! (See our table above for the list of function names.) 3.2.6 Answer We know there were n = 500 customers, with a mean of 5.5 visits, a median of 5 visits, and a standard deviation of 2.5 visits. For a Normal Distribution: We learned in Workshop 2 that rnorm() requires a mean and sd (standard deviation); we conveniently have both! 1 - pnorm(5, mean = 5.5, sd = 2.5) ## [1] 0.5792597 For a Gamma Distribution: We learned in Workshop 2 that rgamma() requires a shape and scale (or rate); we can calculate these from the mean and sd (standard deviation). # shape = mean^2 / variance = mean^2 / sd^2 shape &lt;- 5.5^2 / 2.5^2 # scale = variance / mean scale &lt;- 2.5^2 / 5.5 # AND # rate = 1 / scale rate &lt;- 1 / scale # So... # Get 1 - cumulative probability up to 5 1 - pgamma(5, shape = shape, scale = scale) ## [1] 0.5211581 # OR (same) 1 - pgamma(5, shape = shape, rate = rate) ## [1] 0.5211581 For an Exponential Distribution: We learned in Workshop 2 that rexp() requires a rate; we can calculate this from the mean. # For exponential distribution, # rate = 1 / mean rate &lt;- 1 / 5.5 # So... # Get 1 - cumulative probability up to 5 1 - pexp(5, rate = rate) ## [1] 0.4028903 3.3 And that’s a wrap! Nice work! You can now figure out a lot of things about the world if you (a) can guess their distribution and (b) have one or two statistics about that distribution. Here we go! "],["workshop-system-reliability.html", "4 Workshop: System Reliability Getting Started 4.1 Concepts LC 1 4.2 Joint Probabilities 4.3 Types of Failure Rates 4.4 Units LC 2 4.5 System Reliability 4.6 Renewal Rates via Bayes Rule", " 4 Workshop: System Reliability In this tutorial, we’re going to learn how to conduct Reliability Analysis (also known as Survival Analysis) in R. (#fig:img_epic_fail)Figure 1. Why We Do Reliability Analysis Getting Started Packages library(tidyverse) library(broom) library(DiagrammeR) 4.1 Concepts In Reliability/Survival Analysis, our quantity of interest is the amount of time it takes to reach a particular outcome (eg. time to failure, time to death, time to market saturation, etc.) Let’s learn how to approximate them in R! 4.1.1 Life Distributions All technologies, operations, etc. have a ‘lifetime distribution’. If you took a sample of, say, cars in New York, you could measure how long each car functioned properly (its life-span), and build a Lifetime Distribution from that vector. # Let&#39;s imagine a normally distributed lifespan for these cars... lifespan &lt;- rnorm(100, mean = 5, sd = 1) The lifetime distribution is the probability density function telling us how frequently each potential lifespan is expected to occur. # We can build ourself the PDF of our lifetime distribution here dlife &lt;- lifespan %&gt;% density() %&gt;% tidy() %&gt;% approxfun() In contrast, the Cumulative Distribution Function (CDF) for a lifetime distribution tells us, for any time \\(t\\), the probability that a car will fail by time \\(t\\). # And we can build the CDF here plife &lt;- lifespan %&gt;% density() %&gt;% tidy() %&gt;% mutate(y = cumsum(y) / sum(y)) %&gt;% approxfun() Having built these functions for our cars, we can generate the probability (PDF) and cumulative probability (CDF) of failure across our observed vector of car lifespans, from ~2.83 to ~8.05. Reliability or Survival Analysis is concerned with the probability that a unit (our car) will still be operating by a specific time \\(t\\), representing the percentage of all cars that will survive to that point in time. So let’s also calculate 1 - cumulative probability of failure. mycars &lt;- tibble( time = seq(min(lifespan), max(lifespan), by = 0.1), # Get probability of failing at time time prob = time %&gt;% dlife(), # Get probability of failing at or before time t prob_cumulative = time %&gt;% plife(), # Get probability of surving past time t # (NOT failing at or before time t) prob_survival = 1 - time %&gt;% plife()) Let’s plot our three curves! ggplot() + # Make one area plot for Cumulative Probability (CDF) geom_area(data = mycars, mapping = aes(x = time, y = prob_cumulative, fill = &quot;Cumulative Probability&quot;), alpha = 0.5) + # Make one area plot for Relibability geom_area(data = mycars, mapping = aes(x = time, y = prob_survival, fill = &quot;Reliability (Survival)&quot;), alpha = 0.5) + # Make one area plot for Probability (PDF) geom_area(data = mycars, mapping = aes(x = time, y = prob, fill = &quot;Probability&quot;), alpha = 0.5) + theme_classic() + theme(legend.position = &quot;bottom&quot;) + labs(x = &quot;Lifespan of Car&quot;, y = &quot;Probability&quot;, subtitle = &quot;Example Life Distributions&quot;) Figure 4.1: Figure 1. Life Distributions of a Fleet of Cars This new reliability function allows us to calculate 2 quantities of interest: expected (average) number of cars that fail up to time \\(t\\). total cars expected to still operate after time \\(t\\). 4.1.2 Example: Airplane Propeller Failure! Suppose Lockheed Martin purchases 800 new airplane propellers. When asked about the failure rate, the seller reports that every 1500 days, 2 of these propellers are expected to break. Using this, we can calculate \\(m\\), the mean time to fail! \\[ \\lambda \\ t = t_{days} \\times \\frac{2 \\ units}{1500 \\ days} \\ \\ \\ and \\ \\ \\ m = \\frac{1500}{2} = 750 \\ days \\] This lets us generate the failure rate \\(F(t)\\), also known as the Cumulative Distribution Function, and we can write it up like this. \\[ CDF(t) = F(t) = 1 - e^{-(2t/1500)} = 1 - e^{-t/750} \\] What’s pretty cool is, we can tell R to make a matching function fplane(), using the function() command. # For any value `t` we supply, do the following to that `t` value. fplane = function(t){ 1 - exp( -1*(t / 750)) } Let’s use our function fplane() to answer our Lockheed engineers’ questions. What’s the probability a propeller will fail by t = 600 days? By t = 5000 days? fplane(t = c(600, 5000)) ## [1] 0.5506710 0.9987274 Looks like 55% will fail by 600 days, and 99% fail by 5000 days. What’s the probability a propeller will fail between 600 and 5000 days? fplane(t = 5000) - fplane(t = 600) ## [1] 0.4480563 ~45% more will fail between this period. What percentage of new propellers will work more than 6000 days? 1 - fplane(t = 6000) ## [1] 0.0003354626 0.03% will survive past 6000 days. If Lockheed uses 300 propellers, how many will fail in 1 year? In 3 years? # Given a sample of 300 propellers, n &lt;- 300 # We project n * fplane(t = 362.25) will fail in 1 year (365.25 days) # that&#39;s ~115 propellers. n*fplane(t = 365.25) ## [1] 115.6599 # We also prject that n * fplane(t = 3 * 362.25) will fail in 3 years # that&#39;s ~230 propellers! n*fplane(t = 3*365.25) ## [1] 230.3988 Pretty powerful! LC 1 Question (#fig:img_phone)Figure 3. Exploding Phones! (True Story) Hypothetical: Samsung is releasing a new Galaxy phone. But after the 2016 debacle of exploding phones, they want to estimate how likely it is a phone will explode (versus stay intact). Their pre-sale trials suggest that every 500 days, 5 phones are expected to explode. What percentage of phones are expected to work after more than 6 months? 1 year? Answer Using the information above, we can calculate the mean time to fail m, the rate of how many days it takes for an average unit to fail. days &lt;- 500 units &lt;- 5 m &lt;- days / units # Check it! m ## [1] 100 We can use m to make our explosion function fexplode(), which in this case, is our (catastrophic) failure function \\(f(t)\\)! \\[ CDF(days) = Explode(days) = 1 - e^{-(days \\times \\frac{1 \\ unit}{100 \\ days})} = 1 - e^{-0.01 \\times days} \\] fexplode = function(days){ 1 - exp(-1*days*0.01) } Then, we can calculate \\(r(t)\\), the cumulative probability that a phone will not explode after \\(t\\) days. Let’s answer our questions! What percentage of phone are expected to survive 6 months? # What percent 1 - fexplode(365.25 / 2) ## [1] 0.1610162 What percentage of phone are expected to survive 1 year? 1 - fexplode(365.25) ## [1] 0.02592623 4.2 Joint Probabilities Two extra rules of probability can help us understand system reliability. 4.2.1 Multiplication Rule probability that \\(n\\) units with a reliability function \\(R(t)\\) survive past time \\(t\\) is multiplied, because of conditional probability, to equal \\(R(t)^{n}\\).. Figure 4.2: Figure 2. Oops. For example, there’s a 50% chance that 1 coffee cup breaks at local coffeeshop Coffee Please! every 6 months (180 days). Thus, the mean number of days to cup failure is \\(m = \\frac{180 \\ days}{ 1 \\ cup \\times 0.50 \\ chance} = 360 \\ days\\), while the relative frequency (probability) that a cup will break is $ = $. fcup = function(days){ 1 - exp( -1*(days/360))} # So the probability that 1 breaks within 100 days is XX percent fcup(100) ## [1] 0.2425349 And let’s write out a reliability function too, based on our function for the failure function. # Notice how we can reference an earlier function fcup in our later function? Always have to define functions in order. rcup = function(days){ 1 - fcup(days) } # So the probability that 1 *doesn&#39;t* break within 100 days is XX perecent rcup(100) ## [1] 0.7574651 But the probability that two break within 100 days is… fcup(100) * fcup(100) ## [1] 0.05882316 And the probability that 5 break within 100 days is… very small! fcup(100)^5 ## [1] 0.0008392106 4.2.2 Compliment Rule The probability that at least 1 of \\(n\\) units fails by time \\(t\\) is \\(1 - R(t)^{n}\\). So, if Coffee Please! buys 2 new cups for their store, the probability that at least 1 unit breaks within a year is… 1 - rcup(days = 365.25)^2 ## [1] 0.868555 While if they buy 5 new cups for their store, the chance at least 1 cup breaks within a year is… 1 - rcup(days = 365.25)^5 ## [1] 0.9937359 4.3 Types of Failure Rates 4.3.1 Hazard Rate Function But if a unit has survived up until now, shouldn’t its odds of failing change? We can express this as: \\[ P(Fail \\ Tomorrow | Survive \\ until \\ Today) = \\frac{ F(days + \\Delta days) - F(days) }{ \\Delta days \\times R(days)} = \\frac{ F(days + 1 ) - F(days) }{ 1 \\times R(days)} \\] Local coffeeshop Coffee Please! also has a lucky mug, which has stayed intact for 5 years, despite being dropped numerous times by patrons. Coffee Please!’s failure rate suggests they had a 99.3% chance of it breaking to date. # we call this the Hazard Rate Z zcup = function(days, plus = 1){ ( fcup(days + plus) - fcup(days) ) / (plus * rcup(days) ) } 4.3.2 Accumulative Hazard Function \\(H(t)\\): total accumulated risk of experiencing the event of interest that has been gained by progressing from 0 to time \\(t\\). the (instantaneous) hazard rate \\(h(t)\\) can grow or shrink over time, but the cumulative hazard rate only increases or stays constant. hcup = function(days){ -1*log( rcup(days) ) } # This captures the accumulative probability of a hazard (failure) occurring given the number of days past. hcup(100) ## [1] 0.2777778 4.3.3 Average Failure Rate The hazard rate \\(z(t)\\) varies over time, so let’s generate a single statistic to summarize the distribution of hazard rates that \\(z(t)\\) can provide us between times \\(t_{a} \\to t_{z}\\). We’ll call this the Average Failure Rate \\(AFR(T)\\). afrcup = function(t1,t2){ (hcup(t2) - hcup(t1) ) / (t2 - t1)} afrcup(0, 5) ## [1] 0.002777778 # Or write it as.... afrcup = function(t1,t2){ (log(rcup(t1)) - log(rcup(t2)) ) / (t2 - t1)} afrcup(0, 5) ## [1] 0.002777778 # And if we&#39;re going from 0 to time t, # it simplifies to... afrcup = function(days){ hcup(days) / days } afrcup(5) ## [1] 0.002777778 # or to this afrcup = function(days){ -1*log(rcup(days)) / days } afrcup(5) ## [1] 0.002777778 When the probability for a time \\(t\\) is less than 0.10, \\(AFR = F(t) / T\\). This means that \\(F(t) = 1 - e^{-T \\times AFR(T)} \\approx T \\times AFR(T) \\ \\ when \\ F(T) &lt; 0.10\\). afrcup = function(days){ fcup(days) / days } afrcup(5) ## [1] 0.002758577 # and this is approximately.... 4.3.4 Table of Functions (#tab:tab_functions)Table 1. Failure and Reliability Functions Function Name Formula Equivalency Meaning \\(F(t)\\) Failure Function \\(1 - e^{-\\lambda t}\\) \\(1 - e^{-H(t)}\\) Cumulative Distribution Function (CDF) of Lifespans \\(R(t)\\) Reliability Distribution \\(e^{-\\lambda t}\\) \\(e^{-H(t)}\\) Remainder of CDF for Lifespans \\(f(t)\\) Change in Failure Function \\(\\frac{F(t + \\Delta t) - F(t)}{\\Delta t}\\) \\(-R&#39;(t)\\) Change in CDF at time \\(t_{2}\\) \\(-\\) at \\(t_{1}\\), per extra timestep \\(z(t)\\) Failure Rate (Hazard Rate) \\(\\frac{f(t)}{R(t)}\\) \\(\\lambda = \\frac{1}{m}\\) Mean Failure Rate \\(\\lambda\\); Inverse of Mean Time to Fail \\(m\\) \\(H(t)\\) Accumulative Hazard Rate \\(-log( R(t) )\\) \\(\\lambda t\\) Total Risk of Failure gained from time 0 to \\(t\\) \\(AFR(t_1,t_2)\\) Average Failure Rate \\(\\frac{H(t_{2}) - H(t_{1})}{t_{2} - t_{1}}\\) \\(\\frac{log(R(t_{1})) -log(R(t_{2})}{t_{2} - t_{1}}\\) Average failures per timestep between times \\(t_{1}\\) and \\(t_{2}\\) 4.4 Units Units can be tough with failure rates, because they get tiny really quickly. Here are some common units: Percent per thousand hours, where \\(\\% / K = 10^5 \\times z(t)\\) Failure in Time (FIT) per thousand hours, also known as Parts per Million per Thousand Hours, written \\(PPM/K = 10^9 \\times z(t)\\). This equals \\(10^4 \\times Failure \\ Rate \\ in \\ \\% / K\\). For this lifetime function \\(F(t) = 1 - e^{-(t/2000)^{0.5}}\\), what’s the failure rate at t = 10, t = 1000, and t = 10,000 hours? Convert them into \\(\\%/K\\) and \\(PPM/K\\). 4.4.1 Failure Functions First, let’s write the failure function f(t). # Write failure rate f = function(t){ 1 - exp(-(t/2000)^0.5) } Second, let’s write the hazard rate z(t), for a 1 unit change in t. # Write hazard rate z = function(t, change = 1){ # Often I like to break up my functions into multiple lines; # it makes it much clearer to me. # To help, we can make &#39;temporary&#39; objects; # they only exist within the function. # Get change in failure function deltaf &lt;- (f(t+change) - f(t) ) / change # Get reliability function r &lt;- 1 - f(t) # Get hazard rate deltaf / r } Third, let’s write the average hazard rate afr(t1,t2). afr = function(t1,t2){ # Let&#39;s get the survival rate r(t) r1 = 1 - f(t1) r2 = 1 - f(t2) # Let&#39;s get the accumulative hazard rate h(t) h1 = -log(r1) h2 = -log(r2) # And let&#39;s calculate the averge failure rate! afr = (h2 - h1) / (t2 - t1) # And return it! afr } 4.4.2 Conversion Functions Fourth, let’s write some functions to convert our results into %/K and PPM/K, so we can be lazy! We’ll call our functions pk() and ppmk(). # % per 1000 hours pk = function(rate){ rate * 100 * 10^3 } # PPM/1000 hours ppmk = function(rate){ rate * 10^9 } 4.4.3 Converting Estimates Let’s compare our hazard rates when t = 10, per hour, in % per 1000 hours, and in PPM per 1000 hours. # Per hour... Ew. Not very readable. z(t = 10) ## [1] 0.003445358 # % per 1000 hours.... Wheee! Much more legible z(t = 10) %&gt;% pk() ## [1] 344.5358 # PPM per 1000 hours.... Who! Big numbers! z(t = 10) %&gt;% ppmk() ## [1] 3445358 Finally, let’s calculate the Average Failure Rate between 1000 and 10000 hours, in %/K. # Tada! Average Failure Rate from 1000 to 10000 hours, in % of units per 1000 hours afr(1000, 10000) %&gt;% pk() ## [1] 16.98846 # And in ppmk! afr(1000, 10000) %&gt;% ppmk() ## [1] 169884.6 LC 2 Question (#fig:img_ramen)Figure 5. Does Instant Ramen ever go bad? A food safety inspector is investigating the average shelf life of instant ramen noodles. A company estimates the average shelf life of a package of ramen noodles at ~240 days per package. In a moment of poor judgement, she hires a team of hungry college students to taste-test old packages of that company’s ramen noodles, randomly sampled from a warehouse. When a student comes down with food poisoning, she records that product as having gone bad after XX days. She treats the record of ramen food poisonings as a sample of the lifespan of ramen products. ramen &lt;- c(163, 309, 215, 211, 246, 198, 281, 180, 317, 291, 238, 281, 215, 208, 212, 300, 231, 240, 285, 232, 252, 261, 310, 226, 282, 140, 208, 280, 237, 270, 185, 409, 293, 164, 231, 237, 269, 233, 246, 287, 187, 232, 180, 227, 215, 260, 236, 229, 263, 220) Using this data, please calculate… What’s the cumulative probability of a pack of ramen going bad within 8 months (240 days)? Are the company’s predictions accurate? What’s the average failure rate (\\(\\lambda\\)) for the period between 8 months (240 days) to 1 year? What’s the mean time to fail (\\(m\\)) for the period between 8 months to 1 year? Answer First, we take her ramen lifespan data, estimate the PDF with density(), and make the failure function (CDF), which I’ve called framen() below. # Get failure function f(t) = CDF of ramen failure framen &lt;- ramen %&gt;% density() %&gt;% tidy() %&gt;% # Now compute CDF mutate(y = cumsum(y) / sum(y)) %&gt;% approxfun() Second, we calculate the reliability function rramen(). # Get survival function r(t) = 1 - f(t) rramen &lt;- function(days){ 1 - framen(days) } Third, we can shortcut to the average failure rate, called afrramen() below, by using the reliability function rramen() to make our hazard rates at time 1 (h1) and time 2 (h2). # Get average failure rate from time 1 to time 2 afrramen &lt;- function(days1, days2){ h1 &lt;- -1*log(rramen(days1)) h2 &lt;- -1*log(rramen(days2)) (h2 - h1) / (days2 - days1) } What’s the cumulative probability of a pack of ramen going bad within 8 months (240 days)? Are the company’s predictions accurate? framen(240) ## [1] 0.5082465 Yes! ~50% of packages will go bad within 8 months. Pretty accurate! What’s the average failure rate (\\(\\lambda\\)) for the period between 8 months (240 days) to 1 year? lambda &lt;- afrramen(240, 365) # Check it! lambda ## [1] 0.0256399 On average, between 8 months to 1 year, ramen packages go bad at a rate of ~0.026 units per day. What’s the mean time to fail (\\(m\\)) for the period between 8 months to 1 year? # Calculate the inverse of lambda! m &lt;- 1 / lambda # check it! m ## [1] 39.00172 39 days per package. In other words, during this period post-expiration, this data suggests 1 package will go bad every 39 days. 4.5 System Reliability Reliability rates become extremely useful when we look at an entire system! This is where system reliability analysis can really improve the lives of ordinary people, decision-makers, and day-to-day users, because we can give them the knowledge they need to make decisions. So what knowledge do users usually need? How likely is the system as a whole to survive (or fail) over time? 4.5.1 Series Systems (#fig:fig_series)Dominos: A Series System In a series system, we have a set of \\(n\\) components (sometimes called nodes in a network), which get utilized sequentially. A domino train, for example, is a series system. It only takes 1 component to fail to stop the entire system (causing system failure). The overall reliability of a series system is defined as the success of every individual component (A AND B AND C). We write it using the formula below. \\[ Series \\ System \\ Reliability = R_{S} = \\prod^{n}_{i = 1}R_{i} = R_1 \\times R_2 \\times ... \\times R_n \\] We can also visualize it below, where each labelled node is a component. (#fig:mermaid_series)Figure 6. Example Series System 4.5.2 Parallel Systems (#fig:fig_parallel)Spoons: A Parallel System In a parallel system (a.k.a. redundant system), we have a set of \\(n\\) components, but only 1 component needs to function in order for the system to function. The overall reliability of a series system is defined as the success of any individual component (A OR B OR [A AND B]). A silverware drawer is an simple example of a parallel system. You probably just need 1 spoon for yourself at any time, but you have a stock of several spoons available in case you need them. We write it using the formula below, where each component \\(i\\) has a reliability rate of \\(R_{i}\\) and a failure rate of \\(F_{i}\\). \\[ Parallel \\ System \\ Reliability = R_{P} = 1 - \\prod^{n}_{i = 1}(1 - R_{i}) = 1 - \\prod^{n}_{i = 1}F_{i} = 1 - (F_1 \\times F_2 \\times ... \\times F_n) \\] Or we can represent it visually, where each labelled node is a component. (Unlabelled nodes are just junctions for relationships, so they don’t get a probability. Some diagrams will not have these; you need them in mermaid.) (#fig:mermaid_parallel)Figure 7. Example Parallel System 4.5.3 Combined Systems Most systems actually involve combining the probabilities of several subsystems. When combining configurations, we calculate the probabilities of each subsystem, then then calculate the overall probability of the final system. Series System with Nested Parallel System: Figure 4.3: Figure 8. Series System with Nested Parallel System In the Figure above, we calculate the reliability rate for the parallel system, then calculate the reliability rate for the whole series; the parallel system’s rate becomes just one probability in the overall series system: \\(0.80 \\times (1 - (1 - 0.98) \\times (1 - 0.99) \\times (1 - 0.90)) \\times 0.95 \\approx 0.76\\). Parallel System with Nested Series Systems (Right Diagram): Figure 4.4: Figure 9. Parallel System with Nested Series Systems In the figure above, we calculate the reliability rate for each series system first, then calculate the reliability rate for the whole parallel system; each series’ rate becomes one probability in the overall parallel system. \\(1 - ((1 - 0.80 \\times 0.90) \\times (1 - 0.95 \\times 0.99)) \\approx 0.98\\) The key is identifying exactly which system is nested in which system! 4.5.4 Example: Business Reliability Local businesses deal heavily with series system reliability, even if they don’t regularly model it. You’ve been hired to analyze the system reliability of our local coffee shop Coffee Please! Our coffee shop’s ability to serve cold brew coffee relies on 5 components, each with its own constant chance of failure. Water: Access to clean tap water. (Water outages occur ~ 3 days a year.) Coffee Grounds: Sufficient coffee grounds supply. (Ran out of stock 5 days in the last year). Refrigerator: Refrigerate coffee for 12 hours. (1% fail in a year.) Dishwasher: Run dishwasher on used cups (failed 2 times in last 60 days). Register: Use Cash Register to process transaction and give change (Failed 5 times in the 3 months). We can represent this in a system diagram below. (#fig:coffee_series)Figure 10. Example Series System in a Coffeeshop We can extract the average daily failure rate \\(lambda\\) for each of these components. # Water outage occrred 3 days in last 365 days lambda_water &lt;- 3 / 365 # Ran out of stock 5 days in last 365 days lambda_grounds &lt;- 5 / 365 # 1% of refrigerators fail within a 365 days lambda_refrigerator &lt;- 0.01 / 365 # Failed 2 times in last 60 days lambda_dishwasher &lt;- 2 / 60 # Failed 5 times in last 90 days lambda_cash &lt;- 5 / 90 Assuming a constant chance of failure, we can write ourselves a quick failure function f and reliability function r for an exponential distribution. # Write a reliability function r = function(t, lambda){ exp(-1*t*lambda) } And we can calculate the overall reliability of this coffeeshop’s series system in 1 day by multiplying these reliability rates together. r(1, lambda_water) * r(1, lambda_grounds) * r(1, lambda_refrigerator) * r(1, lambda_dishwasher) * r(1, lambda_cash) ## [1] 0.8950872 This means there’s an 89.5% chance of this system fully functioning on a given day! 4.6 Renewal Rates via Bayes Rule We would hope that failed parts often get replaced, so we might want to adjust our functions accordingly. Renewal Rate: \\(r(t)\\) reflects the mean number of failures per unit at time \\(t\\). Example: Let’s say that… For 10 units, we calculated how many days post production they lasted till failure (failure) as well as how many days post production till they were replaced (replace). Using this, we can calculate the lag-time, or the time taken for renewal. units &lt;- tibble( id = 1:15, failure = c(10, 200, 250, 300, 350, 375, 525, 525, 525, 525, 600, 650, 650, 675, 725), replace = c(100, 250, 350, 440, 550, 390, 600, 625, 660, 605, 700, 700, 700, 725, 750), renewal = replace - failure ) # Let&#39;s get the failure funciont... # by calculating the CDF of failure # first, we calculate the PDF of failure f &lt;- units$failure %&gt;% density() %&gt;% tidy() %&gt;% # Get CDF mutate(y = cumsum(y) / sum(y)) %&gt;% # turn into function approxfun() # Let&#39;s also calculate the CDF of replacement fr &lt;- units$replace %&gt;% density() %&gt;% tidy() %&gt;% # Get CDF mutate(y = cumsum(y) / sum(y)) %&gt;% # Get function approxfun() Above, we made the function fr(), which represents the cumulative probability of replacement, unrelated to failure. But what we really want to know is a conditional probability, specifically: how likely is a unit to get replaced at time \\(b\\), given that it failed at time \\(a\\)? Fortunately, we can use Bayes’ Rule to deduce this. First, let’s make a function f_fr(), meaning the cumulative probability of failure given replacement. This should (probably) be the same probability of failure as usual, but we need to restart the clock after replacement, so we’ll set the time as \\(time_{failure} - time_{replacement}\\). # Probability of failure given replacement f_fr = function(time, time_replacement){ f(time - time_replacement) } Next, we’ll use Bayes Rule to get the cumulative probability of replacement given failure, estimated in a function fr_f(). # Probability of replacement given failure fr_f = function(time, time_replacement){ # Estimate conditional probability of Failure given Replacement times Replacement top &lt;- f_fr(time, time_replacement) * fr(time_replacement) # Estimate total probability of Failure bottom &lt;- f_fr(time, time_replacement) * fr(time_replacement) + (1 - f_fr(time, time_replacement)) * (1 - fr(time_replacement)) # Divide them, and voila! top/bottom } Finally, what do these functions actually look like? Let’s simulate failure and replacement over time, in a dataset of fakeunits. fakeunits &lt;- tibble( # As time increases from 0 to 1100, time = seq(0, 1100, by = 1), # Let&#39;s get the probability of failure at that time, prob_f = time %&gt;% f(), # Let&#39;s get the probability of replacement at time t + 10 given failure at time t prob_fr_f_10 = fr_f(time = time, time_replacement = time + 10), # How about t + 50? prob_fr_f_50 = fr_f(time = time, time_replacement = time + 50) ) # Check it! fakeunits %&gt;% head(3) ## # A tibble: 3 × 4 ## time prob_f prob_fr_f_10 prob_fr_f_50 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.0341 0.000436 0.000484 ## 2 1 0.0344 0.000442 0.000490 ## 3 2 0.0348 0.000449 0.000496 And let’s visualize it! (#fig:img_cumulative)Figure 11. Renewal Rates From this point on, the math for \\(z(t)\\), \\(H(t)\\), and \\(AFR(t)\\) for renewal rates gets a little tricky, so we’ll save that for another day. But you’re well on your way to working with life distributions! "],["workshop-useful-life-distributions-exponential.html", "5 Workshop: Useful Life Distributions (Exponential) 5.1 Getting Started 5.2 Quantities of Interest LC 1 LC 2 5.3 Quantities of Interest (continued) LC 3 5.4 System Failure with Independent Failure Rates 5.5 Statistical Techniques for Exponential Distributions LC 4 5.6 Conclusion", " 5 Workshop: Useful Life Distributions (Exponential) In this workshop, we’re going to learn some R functions for working with common life distributions, namely the Exponential distributions. 5.1 Getting Started 5.1.1 Load Packages Let’s start by loading the tidyverse package, which will let us mutate(), filter(), and summarize() data quickly. We’ll also load mosaicCalc, for taking derivatives and integrals (eg. D() and antiD()). # Load packages library(tidyverse) library(mosaicCalc) 5.1.2 Key Concepts In this lesson, we’ll be building on several key concepts from prior lessons. I’ve defined them below as a helpful review. life distribution: the distribution of a vector of \\(n\\) products, whose values recording the amount of time it took for each product to fail. In other words, its lifespan. probability density function (PDF): the function describing the probability (relative frequency) of any value in a distribution. cumulative distribution function (CDF): the function describing the cumulative probability of each successive value in a distribution. Spans from 0 to 1. Have questions? I strongly recommend you review Workshops 2, 3, and 4 before this one! It will help it all fit together! 5.1.3 Our Data In this workshop, we’re going to work with a data.frame called masks. An extremely annoying moment in the COVID-era is when a part of your mask snaps, requiring you to get a fresh mask. Let’s examine a (hypothetical) sample of n = 50 masks produced by Company X to explore how often this happens! Please import the masks.csv data.frame below. Each row is a mask, with its own unique id. Columns describe how many hours it took for the left_earloop to snap, the right_earloop to snap, the nose wire to snap, and the fabric of the mask to tear. masks &lt;- read_csv(&quot;workshops/masks.csv&quot;) # Let&#39;s glimpse() its contents! masks %&gt;% glimpse() ## Rows: 50 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1… ## $ left_earloop &lt;dbl&gt; 6, 16, 46, 4, 1, 5, 32, 35, 27, 3, 4, 7, 1, … ## $ right_earloop &lt;dbl&gt; 12, 1, 17, 14, 19, 8, 18, 14, 5, 8, 8, 2, 12… ## $ wire &lt;dbl&gt; 4, 1, 8, 29, 23, 8, 10, 38, 11, 31, 7, 4, 3,… ## $ fabric &lt;dbl&gt; 177, 462, 65, 405, 2483, 1064, 287, 2819, 10… 5.2 Quantities of Interest When we work with life distributions, we often want to find several useful quantities of interest (a.k.a. parameters) about them. Let’s find out how to do that with an exponential distribution! 5.2.1 Lack of Memory Exponential distributions are famous for a key trait. The failure rate $ $ remains constant in an exponential distribution. The probability that a product fails in the next hour of use is the same at t = 0, t = 100, or t = infinity! It doesn’t worsen with time. (It’s the literal meaning of the saying, “if it is not broke, don’t fix it!”) 5.2.2 Mean Time to Fail The Mean Time to Fail describes the mean of a lifespan distribution. For example, let’s calculate the mean time to fail (in hours) for a mask’s left_earloop in our sample. stat &lt;- masks %&gt;% summarize( # We can take the mean of this vector of time to fail in hours mttf = mean(left_earloop), # Lambda is the reciprocal of the MTTF lambda = 1 / mttf) # Check out the contents! stat ## # A tibble: 1 × 2 ## mttf lambda ## &lt;dbl&gt; &lt;dbl&gt; ## 1 13.4 0.0749 In an exponential distribution, the MTTF always has a cumulative probability of 1 - 1 / e = 0.632. (This can be coded in R like so:) 1 - 1 / exp(1) ## [1] 0.6321206 Let’s assume our sample’s left earloops have an exponential lifespan distribution, and use pexp() to calculate the cumulative probability of getting an MTTF of 13.36. We’ll need to supply pexp() the benchmark in the distribution in question (mttf), plus the the rate parameter \\(\\lambda\\), which we always need when simulating an exponential distribution. prob &lt;- pexp(stat$mttf, rate = stat$lambda) Indeed, the figure below compares the observed PDF function (made using density()) to the assumed exponential PDF (made using dexp()), and we can see that that 63% of the distribution has failed by the Mean Time to Fail. 5.2.3 Mean Time to Fail via Integration Mean Time to Fail (MTTF) can be number-crunched empirically as the mean of observed lifespans, assuming an exponential distribution. But it is also equal to the integral of the reliability function: $MTTF = _{0}^{}{R(t)dx} $. So, let’s make ourselves a nice reliability function to help us calculate the MTTF. We know the reliability function can be stated as \\(R(t) = 1 - F(t)\\). Assuming an exponential distribution, the failure rate \\(F(t) = 1 - e^{-\\lambda t}\\). We know \\(\\lambda = \\frac{1}{MTTF}\\), and above, we found that lambda = 0.0748502994011976 for a left-earloop. # Reliability Function for exponential distribution r = function(t, lambda){ exp(-1*t*lambda)} We can calculate it below… # Use mosaicCalc&#39;s antiD function # To get integral of r(t, lambda) as x goes from 0 to infinity mttf = antiD(tilde = r(t, lambda) ~ t) Great! We have developed our own mttf function for an exponential distribution! If we feed t a suitably large value, like 1000 (approaching infinity), we will reach the original observed/estimated mttf. mttf(t = 1000, lambda = stat$lambda) ## [1] 13.36 mttf(t = Inf, lambda = stat$lambda) ## [1] 13.36 This only helps you if you know lambda, the inverse of the MTTF, or have the reliability function but not the MTTF. 5.2.4 Median Time to Fail \\(T_{50}\\) We might also want to know the median time to failure (\\(T_{50}\\)), the value on the x axis that splits the area under the curve in half at 50% and 50%. We can calculate this as… \\(F(T_{50}) = 50\\% = 0.5 = 1 - e^{-\\lambda T_{50}}\\) where: \\(T_{50} = \\frac{log(2)}{ \\lambda } = \\frac{0.693}{\\lambda}\\) # Let&#39;s update stat to include the observed &#39;median&#39; # and &#39;t50&#39;, the median assuming an exponential distribution stat &lt;- masks %&gt;% summarize( # The literal mean time to fail # in our observed distribution is this mttf = mean(left_earloop), # And lambda is this... lambda = 1 / mttf, # The observed median is this.... median = median(left_earloop), # But if we assume it&#39;s an exponential distribution # and calculate the median from lambda, # we get t50, which is very close. t50 = log(2) / lambda) # Check it out! stat ## # A tibble: 1 × 4 ## mttf lambda median t50 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 13.4 0.0749 8 9.26 5.2.5 Modal Time to Fail Finally, the modal time to fail is pretty easy to calculate. Its the most common time to fail, also known as the max probability in a PDF. masks %&gt;% summarize( # Let&#39;s get lambda, the reciprocal of the MTTF lambda = 1 / mean(left_earloop), # And let&#39;s estimate the PDF... t = 1:max(left_earloop), prob = dexp(t, rate = lambda)) %&gt;% # And let&#39;s sort the data.frame from highest to lowest arrange(desc(prob)) %&gt;% # Grab first 3 rows, for brevity head(3) ## # A tibble: 3 × 3 ## lambda t prob ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.0749 1 0.0695 ## 2 0.0749 2 0.0644 ## 3 0.0749 3 0.0598 # This reveals that t = 1 is our mode LC 1 Question A competing mask manufacturer made a mask whose earloops fail at a constant failure rate of 0.08. What is the probability that 1 fails before 20 hours of use? What is the probability that 2 fail before 20 hours of use? After how long should we expect 1% failures? Answer What is the probability that 1 fails before 20 hours of use? # Let&#39;s generate an expondential failure function, # because **constant** rate of failure f = function(t, lambda){1 - exp(-1*t*lambda)} # Use failure function (CDF) to get area under curve BEFORE 20 hours. f(t = 20, lambda = 0.08) ## [1] 0.7981035 # There&#39;s a 79% chance 1 fails within 20 hours What is the probability that 2 fail before 20 hours of use? # For n failures, take F(t) to the nth power f(t = 20, lambda = 0.08)^2 ## [1] 0.6369692 # There&#39;s a 63% chance 2 fail within 20 hours. After how long should we expect 1% failures? # We can solve this using the failure function # f(t) = 1 - e^{-t*lambda} # But we need to invert it, # to solve for t # f(t) = 1 - e^{-t*lambda} # e^{-t*lambda} = 1 - f(t) # log(1 - f(t)) = -t*lambda # -log(1 - f(t)) / lambda = t # So if we set f(t) = 1%, and lambda = 0.08, # this will tell us at what time F(t) will equal 1% -log(1 - 0.01) / 0.08 ## [1] 0.1256292 # Looks like that time of interest is t = 0.1256 hours. LC 2 Question Above, we examined a sample of surgical masks, checking how often their left_earloop snapped. How does that compare with the right_earloop? Calculate the mean time to fail for the right earloop, and \\(\\lamba\\), the mean failure rate. Is the right earloop more or less reliable than the left earloop? Answer MTTF and Lambda compare &lt;- masks %&gt;% summarize(mttf_right = mean(right_earloop), mttf_left = mean(left_earloop), lambda_right = 1 / mttf_right, lambda_left = 1 / mttf_left) # Check it compare ## # A tibble: 1 × 4 ## mttf_right mttf_left lambda_right lambda_left ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.6 13.4 0.0943 0.0749 # Looks like the left earloop fails less often. 5.3 Quantities of Interest (continued) 5.3.1 Conditional Reliability (Survival) Function We may also want to know, after age t, what’s the probability that a product survives an additional x years to age t + x? We can restate this in terms of \\(T_{Fail}\\), the time at which the product finally fails. We want to know, what’s the probability that \\(T_{Fail}\\) is greater than \\(t + x\\), given that we already know \\(T_{Fail}\\) must be greater than \\(t\\) (since it hasn’t failed yet as of time \\(t\\))? Fortunately, this can be simplified in terms of the reliability functions. As long as we can calculate \\(R(x + t)\\) and \\(R(t)\\), we can find \\(R(x|t)\\), the conditional survival function. \\[ R(x | t) = \\frac{ R(x + t) }{ R(t) } = \\frac{ P(T_{Fail} &gt; x + t) }{P(T_{Fail} &gt; t)} \\] So, let’s use our nice reliability function from before to help us calculate the conditional reliability function. # Reliability Function for exponential distribution r = function(t, lambda){ exp(-1*t*lambda)} So, what’s the probability that a left-earloop that has lasted 10 hours will last another 5 hours? r(t = 10 + 5, lambda = stat$lambda) / r(t = 10, lambda = stat$lambda) ## [1] 0.6878039 Looks like there’s a 69% chance it will last another 5 hours, given that it has already lasted 10 hours. Let’s finish up by building ourselves a nice Conditional Reliability function cr, which which calculates the conditional probability of any item surviving x more hours given that it survived t hours and a mean failure rate of lambda. cr = function(t, x, lambda){ # We can actually nest functions inside each other, # to make them easier to write r = function(t, lambda){ exp(-1*t*lambda)} # Calculate R(x + t) / R(t) output &lt;- r(t = t + x, lambda) / r(t = t, lambda) # and return the result! return(output) } # Let&#39;s compare our result to above! It&#39;s the same! cr(t = 10, x = 5, lambda = stat$lambda) ## [1] 0.6878039 If we were to visualize our conditional reliability function cr below as x ranges from 1 to 50, it would produce the following curve. 5.3.2 \\(\\mu(t)\\): Mean Residual Life The Conditional Reliability Function \\(R(x|t)\\) also allows us to calculate the Mean Residual Life (MRL, a.k.a. \\(\\mu\\)) at time \\(t\\), in this case referring to the average \\(x\\) more years the product is expected to survive after time \\(t\\). We can calculate it using the distribution below. \\[MRL(t) = \\mu(t) = \\int_{0}^{\\infty}{ R(x|t)dx} = \\frac{1}{R(t)} \\int_{0}^{\\infty}{ R(x) dx} \\] Technically, the Mean Residual Life at time \\(t\\) represents how many times greater is the MTTF expected than the probability of survival at time t (proportion of cases not yet failed). It shows the mean expected remaining life years, for however \\(x\\) many more time steps come after \\(t\\). # Calculate the probability of survival R(t = 10) prob_survival_t = r(t = 10, lambda = stat$lambda) # Calculate the MTTF for +x more years mttf_inf = mttf(t = Inf, lambda = stat$lambda) # calculate the Mean Residual Life for t=10 mttf_inf / prob_survival_t ## [1] 28.24081 We can formalize this as function mu(t, lambda). (Since the Greek letter \\(\\mu\\) is pronounced mu.) # Calculate Mean Residual Life mu = function(t, lambda){ # Get the Reliability Function for exponential distribution r = function(t, lambda){ exp(-1*t*lambda)} # Get the MTTF (integral of reliability function) mttf = antiD(tilde = r(t = t, lambda = lambda) ~ t) # Now calculate mu(), the Mean Residual Life function at time t output &lt;- mttf(t = Inf, lambda = lambda) / r(t = t, lambda = lambda) return(output) } Let’s test it out and compare to our previous version. mu(t = 10, lambda = stat$lambda) ## [1] Inf Works perfectly! 5.3.3 \\(g(t)\\): MRL as % of MTTF We can also estimate \\(g(t)\\), which describes, how large is the MRL \\(\\mu(t)\\), relative to the MTTF? When \\(t = 0\\), \\(\\mu(t) = \\mu = MTTF\\), so we can write \\(g(t)\\) as follows: \\[g(t) = \\frac{MRL(t)}{MTTF} = \\frac{\\mu(t)}{\\mu(t = 0)}\\] So, for the mask component we’ve been working with, left_earloop, we can calculate the MRL-to-MTTF ratio g(t) after t = 10 hours as follows: mu(t = 10, lambda = stat$lambda) / mu(t = 0, lambda = stat$lambda) ## [1] NaN LC 3 Question A competing firm produced a mask with a wire that fails at a constant rate of 1 failure per 240 hours. The probability that the wire survives 1 week (t = 168 hours) in continuous use is… ( You buy the mask, and it works without failure for 2 weeks (t = 336 hours) The probability the wire will snap during the next week (t = 504 hours) is… Answer The probability that the wire survives 1 week (t = 168 hours) in continuous use is… ( # Write the reliability function r = function(t, lambda){ exp(-1*t*lambda)} # Probability it survives 1 month (760 hours) is... r(t = 168, lambda = 1 / 240) ## [1] 0.4965853 # ~ 50% You buy the mask, and it works without failure for 2 weeks (t = 336 hours) The probability the wire will snap during the next week (t = 504 hours) is… # We can calculate it 2 ways. # First, we can take # R(t + x) / R(t) r(t = 504, lambda = 1 / 240) / r(t = 336, lambda = 1 / 240) ## [1] 0.4965853 # Or, we can calculate the failure function f = function(t, lambda){1 - exp(-1*t*lambda)} # And just calculate the rate of F(t = x) # Because lambda is a constant failure rate f(t = 168, lambda = 1 / 240) ## [1] 0.5034147 5.4 System Failure with Independent Failure Rates Consider a mask with a left and right earloop, which have independent failure rates \\(\\lambda_{left}\\) and \\(\\lambda_{right}\\). What’s the probability that the left loop fails before the right loop? We can express this as: \\[ P(j \\ fails \\ first) = \\frac{ \\lambda_{j}}{ \\sum_{i=1}^{n}{ \\lambda{i}} }\\] In other words, the probability that component \\(j\\) fails first reflects how big \\(\\lambda_{j}\\) is relative to all the other failure rates in total. Let’s test this out with our masks dataset. masks %&gt;% summarize( # Calculate failure rates of left and right loops lambda_left = 1 / mean(left_earloop), lambda_right = 1 / mean(right_earloop), # Calculate the total probability of either loop failing lambda_sum = lambda_left + lambda_right, # Calculate probability the left loop fails first prob_left_first = lambda_left / lambda_sum) ## # A tibble: 1 × 4 ## lambda_left lambda_right lambda_sum prob_left_first ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0749 0.0943 0.169 0.442 # Looks like a probability of about 44% that left loop fails first. 5.4.1 Reliability Functions with Multiple Inputs Suppose a fraction of our masks are shipped in from manufacturer 1, while some are shipped from manufacturer 2. A fraction \\(p\\) is coming from manufacturer 1, while \\(1-p\\) is coming from manufacturer 2. We can use the rules of total probability to calculate the reliability function and other quantities for this mask: \\[R(t) = \\sum_{i=1}^{n}{ ( \\ p_{i} \\times R_i(t) \\ ) } = \\sum_{i=1}^{n}{ ( \\ p_{i} \\times e^{-\\lambda_{i} t} ) }\\] \\[MTTF = \\sum_{i=1}^{n}{ \\frac{p_i}{\\lambda_{i}}}\\] \\[f(t) = \\frac{-d}{dt}R(t) = \\sum_{i=1}^{n}{p\\lambda_{i}e^{-\\lambda_{i}t}}\\] \\[z(t) = \\frac{ f(t) }{ R(t)} = \\frac{ \\sum_{i=1}^{n}{p\\lambda_{i}e^{-\\lambda_{i}t}} }{ \\sum_{i=1}^{n}{ ( \\ p_{i} \\times R_i(t) \\ ) } } \\] It’s pretty messy, but powerful! Suppose we order 75% of our stock from a manufacturer with a failure rate of 1 fabric tear per 50 hours, but 25% from our stock from a manufacturer with a failure rate of 1 tear per 100 hours. What is the (1) overall mean time to failure and (2) overall failure rate at 100 hours for any random mask in your supply? We can tally this up in a stock data.frame. stock &lt;- data.frame( prob = c(0.75, 0.25), lambda = c(1 / 50, 1 / 100)) To calculate the MTTF, we just take the sum of the fraction of each proportion and each failure rate lambda. stock %&gt;% summarize(mttf = sum(prob / lambda)) ## mttf ## 1 62.5 To calculate the overall failure rate, we will generate the reliability function, take its derivative to get the PDF. # Let&#39;s write an exponential reliability function r = function(t, lambda){ exp(-1*t*lambda) } # Let&#39;s derive an exponential PDF f(t), using mosaicCalc&#39;s D() f = D(-1*r(t, lambda) ~ t) Then, we use the PDFs \\(f_i(t)\\) and the Reliability functions \\(R_i(t)\\) to get calculate the overall failure rate \\(z(t)\\). stock %&gt;% summarize( total_f = sum(prob * f(t = 100, lambda = lambda)), total_r = sum(prob * r(t = 100, lambda = lambda)), z = total_f / total_r) ## total_f total_r z ## 1 0.002949728 0.1934713 0.01524633 We could even write it as a function, where p and lambda are equal length vectors for plant 1, plant 2, plant 3, … plant \\(n\\). z = function(t){ # Set the input percentages of products from plants 1 and 2 p = c(0.75, 0.25) # Set the failure rates for each lambda = c(0.02, 0.01) # Let&#39;s write an exponential reliability function r = function(t, lambda){ exp(-1*t*lambda) } # Let&#39;s derive an exponential PDF f(t), f = D(-1*r(t, lambda) ~ t) # Calculate total probability total_f = sum( p * f(t, lambda) ) # Calculate total reliability total_r = sum( p * r(t, lambda) ) # Calculate overall failure rate z = total_f / total_r return(z) } # Try it out! z(t = 1) ## [1] 0.0174812 z(t = 10) ## [1] 0.01730786 z(t = 100) ## [1] 0.01524633 5.4.2 Phase-Type Distributions One way to more accurately model the lifespan of a product is to accept that its failure rate may remain constant, but it might change between failure rates as it passes through phases. Let’s model that! We can write the time \\(T\\) to critical failure (via either overstress OR degraded failure) as \\(T = min(T_c, T_d+T_{dc})\\). The total probability of a product being in any phase \\(a_{i \\to n}\\) equals 1. \\(a_c = 25\\%\\) \\(a_d = 40\\%\\) \\(a_dc = 10\\%\\) We can represent it in a dataframe, like so: myphase &lt;- data.frame( id = 1:4, phase = c(&quot;c&quot;, &quot;d&quot;, &quot;c&quot;, &quot;dc&quot;), alpha = c(0.25, 0.40, 0.25, 0.10), lambda = c(0.50, 1, 0.50, 3) ) And, using the same tricks from the preceding section, we can calculate z(t), the overall hazard rate of a phase-type exponential distribution, by taking the sum of the weighted probabilities. r = function(t, lambda){ exp(-1*t*lambda) } # Let&#39;s derive an exponential PDF f(t), f = D(-1*r(t, lambda) ~ t) z = function(t, data){ output &lt;- data %&gt;% summarize( prob_f = sum(alpha * f(t = t, lambda)), prob_r = sum(alpha * r(t = t, lambda)), ratio_z = prob_f / prob_r) output$ratio_z %&gt;% return() } # Take a peek! z(t = 1, data = myphase) ## [1] 0.6888965 z(t = 2, data = myphase) ## [1] 0.6161738 z(t = 5, data = myphase) ## [1] 0.5308124 z(t = 10, data = myphase) ## [1] 0.5026807 5.5 Statistical Techniques for Exponential Distributions Finally, we’re going to examine several key tools that will help you (1) cross-tabulate failure data over time, (2) use statistics to determine whether an archetypal distribution (eg. exponential) fits your data sufficiently, (3) how to estimate the failure rate \\(\\lambda\\) from tabulated data, and (4) how to plan experiments for product testing. Here we go! 5.5.1 Factors in R For the next section, you’ll need to understand factors. Factors are ordered vectors. They are helpful in ggplot and elsewhere for telling R what order to interpret things in. You can make your own vector into a factor using factor(vector, levels = c(\"first\", \"second\", \"etc\")). # Make character vector myvector &lt;- c(&quot;surgical&quot;, &quot;KN-95&quot;, &quot;KN-95&quot;, &quot;N-95&quot;, &quot;surgical&quot;) # Turn it into a factor myfactor &lt;- factor(myvector, levels = c(&quot;N-95&quot;, &quot;KN-95&quot;, &quot;surgical&quot;)) # check it myfactor ## [1] surgical KN-95 KN-95 N-95 surgical ## Levels: N-95 KN-95 surgical factors can be reduced to numeric vectors using as.numeric(). This returns the level for each value in the factor. # Turn the factor numeric mynum &lt;- myfactor %&gt;% as.numeric() # Compare data.frame(myfactor, mynum) ## myfactor mynum ## 1 surgical 3 ## 2 KN-95 2 ## 3 KN-95 2 ## 4 N-95 1 ## 5 surgical 3 # for example, N-95, which was ranked first in the factor, receives a 1 anytime the value N-95 appears 5.5.2 Crosstabulation Sometimes, we might want to tabulate failures in terms of meaningful units of time, counting the total failures every 5 hours, every 24 hours, etc. Let’s learn how! d1 &lt;- data.frame(t = masks$left_earloop) %&gt;% # classify each value into 5-point width bins (0 to 5, 6-10, 11-15, etc.) # then convert it to a numeric ranking of categories from 1 to n bins mutate(label = cut_interval(t, length = 5)) # Let&#39;s take a peek d1 %&gt;% glimpse() ## Rows: 50 ## Columns: 2 ## $ t &lt;dbl&gt; 6, 16, 46, 4, 1, 5, 32, 35, 27, 3, 4, 7, 1, 20, 22, … ## $ label &lt;fct&gt; &quot;(5,10]&quot;, &quot;(15,20]&quot;, &quot;(45,50]&quot;, &quot;[0,5]&quot;, &quot;[0,5]&quot;, &quot;[… Step 2: Tabulate Observations per Bin. d2 &lt;- d1 %&gt;% # For each bin label group_by(label, .drop = FALSE) %&gt;% # Get total observed rows in each bin # .drop = FALSE records factor levels in label that have 0 cases summarize(count = n()) d2 %&gt;% glimpse() ## Rows: 11 ## Columns: 2 ## $ label &lt;fct&gt; &quot;[0,5]&quot;, &quot;(5,10]&quot;, &quot;(10,15]&quot;, &quot;(15,20]&quot;, &quot;(20,25]&quot;, … ## $ count &lt;int&gt; 17, 13, 3, 7, 2, 2, 3, 0, 0, 1, 2 Step 3: Get bounds and midpoint of Bins Last, we might need the bounds (upper and lower value), or the midpoint. Here’s how! d3 &lt;- d2 %&gt;% # Get bin ranking, lower and upper bounds, and midpoint mutate( bin = as.numeric(label), lower = (bin - 1) * 5, upper = bin * 5, midpoint = (lower + upper) / 2) # Check it! d3 ## # A tibble: 11 × 6 ## label count bin lower upper midpoint ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [0,5] 17 1 0 5 2.5 ## 2 (5,10] 13 2 5 10 7.5 ## 3 (10,15] 3 3 10 15 12.5 ## 4 (15,20] 7 4 15 20 17.5 ## 5 (20,25] 2 5 20 25 22.5 ## 6 (25,30] 2 6 25 30 27.5 ## 7 (30,35] 3 7 30 35 32.5 ## 8 (35,40] 0 8 35 40 37.5 ## 9 (40,45] 0 9 40 45 42.5 ## 10 (45,50] 1 10 45 50 47.5 ## 11 (50,55] 2 11 50 55 52.5 5.5.3 Chi-squared Chi-squared (\\(\\chi^{2}\\)) is a special statistic used frequently to evaluate the relationship between two categorical variables. In this case, the first variable is the type of data (observed vs. model), while the second variable is bin. Back in Workshop 2, we visually compared several distributions to an observed distribution, to determine which fits best. But can we do that statistically? One way to do this is to chop our observed data into bins of length equal width using cut_interval() from ggplot2. We must specify: Step 1 Crosstabulate Observed Values into Bins. # Let&#39;s repeat our process from before! c1 &lt;- data.frame(t = masks$left_earloop) %&gt;% # Part 1: Split into bins mutate(label = cut_interval(t, length = 5)) %&gt;% # Part 2: Tally up by bin group_by(label, .drop = FALSE) %&gt;% summarize(count = n()) %&gt;% mutate( bin = as.numeric(label), lower = (bin - 1) * 5, upper = bin * 5, midpoint = (lower + upper) / 2) Step 3: Calculate Observed and Expected Values per Bin. Sometimes you might only receive tabulated data, meaning a table of bins, not the original vector. In that case, start from Step 3! # Get any parameters you need (might need to be provided if only tabulated data) mystat = masks %&gt;% summarize(lambda = 1 / mean(left_earloop)) # Get your &#39;model&#39; function f = function(t, lambda){ 1 - exp(-1*lambda*t) } # Now calculate cumulatives c2 &lt;- c1 %&gt;% mutate( # Get total units in table total = sum(count), # Get cumulative OBSERVED in that bin o = cumsum(count), # Get cumulative EXPECTED in that bin due to model e = f(t = midpoint, lambda = mystat$lambda) * total) # Note: pexp(t) is equivalent to f(t) for exponential distribution # e = pexp(midpoint, rate = mystat$lambda) * total # Check it! c2 ## # A tibble: 11 × 9 ## label count bin lower upper midpoint total o e ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 [0,5] 17 1 0 5 2.5 50 17 8.53 ## 2 (5,10] 13 2 5 10 7.5 50 30 21.5 ## 3 (10,15] 3 3 10 15 12.5 50 33 30.4 ## 4 (15,20] 7 4 15 20 17.5 50 40 36.5 ## 5 (20,25] 2 5 20 25 22.5 50 42 40.7 ## 6 (25,30] 2 6 25 30 27.5 50 44 43.6 ## 7 (30,35] 3 7 30 35 32.5 50 47 45.6 ## 8 (35,40] 0 8 35 40 37.5 50 47 47.0 ## 9 (40,45] 0 9 40 45 42.5 50 47 47.9 ## 10 (45,50] 1 10 45 50 47.5 50 48 48.6 ## 11 (50,55] 2 11 50 55 52.5 50 50 49.0 Step 4: Calculate Chi-squared Statistic Chi-squared here represents the sum of ratios for each bin. Each ratio is the (1) squared difference between the observed and expected value over (2) the expected value. Ranges from 0 to infinity. The bigger (more positive) the statistic, the greater difference between the observed and expected data. Degrees of Freedom (df) is used as the standard deviation in the chi-squared distribution, to help evaluate how extreme is our chi-squared statistic? Chi-squared Distribution is a distribution of squared deviations from a normal distribution centered at 0. This means it only has positive values. c3 &lt;- c2 %&gt;% summarize( # Calculate Chi-squared statistic chisq = sum((o - e)^2 / e), # Calculate number of bins (rows) nbin = n(), # Record number of parameters used (jjust lambda) np = 1, # Calculate degree of freedom df = nbin - np - 1) # Check it! c3 ## # A tibble: 1 × 4 ## chisq nbin np df ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.5 11 1 9 Step 5: Calculate p-values and confidence intervals Last, let’s use the pchisq() function to evaluate the CDF of the Chi-squared distribution. We’ll find out: p_value: what’s the probability of getting a value greater than or equal to (more extreme than) our observed chi-squared statistic? “statistically significant”: if our observed statistic is more extreme than most possible chi-squared statistics (eg. &gt;95% of the distribution), it’s probably not due to chance! We call it ‘statistically significant.’ # Calculate area remaining under the curve c4 &lt;- c3 %&gt;% mutate(p_value = 1 - pchisq(q = chisq, df = df)) # Check c4 ## # A tibble: 1 × 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.5 11 1 9 0.188 For a visual representation: Step 6: Do it all in one step! Let’s try this one more time, all in one code chunk: # Get any parameters you need # Right now, we&#39;ll sub in our observed lambda value, mystat = masks %&gt;% summarize(lambda = 1 / mean(left_earloop)) # Note: if your data is crosstabulated and you DON&#39;T have the observed lambda value, # you might need to use the methods described below in &#39;Estimating Lambda&#39; # Get your &#39;model&#39; function f = function(t, lambda){ 1 - exp(-1*lambda*t) } data.frame(t = masks$left_earloop) %&gt;% # Step 1: Split into bins mutate(label = cut_interval(t, length = 5)) %&gt;% # Step 2: Tally up by bin group_by(label, .drop = FALSE) %&gt;% summarize(count = n()) %&gt;% mutate( bin = as.numeric(label), lower = (bin - 1) * 5, upper = bin * 5, midpoint = (lower + upper) / 2) %&gt;% # Step 3: Calculate Observed vs. Expected mutate( total = sum(count), o = cumsum(count), e = f(t = midpoint, lambda = mystat$lambda) * total) %&gt;% # Calculate Chi-squared statistic and p-value summarize( chisq = sum((o - e)^2 / e), nbin = n(), np = 1, df = nbin - np - 1, p_value = 1 - pchisq(q = chisq, df = df)) ## # A tibble: 1 × 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.5 11 1 9 0.188 # And if that&#39;s a lot to type every time, you could always code a function for it :) 5.5.4 Estimating Lambda Depending on whether we have complete data or not, we may need to estimate \\(\\lambda\\), rather than compute it directly. If we have a complete sample of data (eg. not censored), then we can just calculate: $ = $ # Eg. 1 / mean(masks$left_earloop) ## [1] 0.0748503 If our data is censored or pre-tabulated into groups, we may need to use the midpoint and count of failures in each bin to calculate \\(\\lambda\\). Tabulated: tallied up in equally sized bins. Tabulated, Type I Censoring: experiment stops when time \\(t\\) reaches \\(limit\\). Tabulated, Type II Censoring: experiment stops when number of units failed \\(n_{failed}\\) reaches \\(limit\\). For example, if we receive just the d3 data.frame we made above, how do we estimate \\(\\lambda\\) from it? We will need: \\(r\\): total failures (sum(count &gt; 0)) \\(n\\): total observations (failed or not) (provided; otherwise, \\(n = r\\)) \\(\\sum_{i=1}^{r}{t_i}\\): total unit-hours (sum(midpoint*count)) \\(T\\): timestep of (1) last failure observed (sometimes written \\(t_r\\)) or (2) last timestep recorded; usually obtained by max(midpoint). Unless some cases do not fail, \\((n - r)t_z\\) will cancel out as 0. \\[ \\hat{\\lambda} = \\frac{ r }{ \\sum_{i=1}^{r}{t_i} + (n - r)t_z} \\] # Let&#39;s calculate it! d4 &lt;- d3 %&gt;% summarize( r = sum(count &gt; 0), # total failures hours = sum(midpoint*count), # total failure-hours n = r, # in this case, total failures = total obs tz = max(midpoint), # end of study period # Calculate lambda hat! lambda_hat = r / (hours + (n - r)*tz)) 1 / mean(masks$left_earloop) ## [1] 0.0748503 Also, our estimate of lambda might be slightly off due to random sampling error. (Every time you take a random sample, there’s a little change, right?) So, let’s build a confidence interval around our estimate. We can do so by weighting \\(\\hat{\\lambda}\\) by a factor \\(k\\). This factor \\(k\\) may be greater depending on (1) the total number of failures \\(r\\) and (2) our level of acceptable error (\\(alpha\\)). Like any statistic, our estimate \\(\\hat{\\lambda}\\) has a full sampling distribution of \\(\\hat{\\lambda}\\) values we could get due to random sampling error, with some of them occurring more frequently than others. We want to find the upper and lower bounds around the 90%, 95%, or perhaps 99% most common (middle-most) values in that sampling distribution. So, if \\(alpha = 0.10\\), we’re going to get a 90% confidence interval (dubbed \\(interval\\)) spanning from the 5%~95% of that sampling distribution. # To calculate a &#39;one-tailed&#39; 90% CI (eg. we only care if above 90%) alpha = 0.10 ci = 1 - alpha # To adjust this to be &#39;two-tailed&#39; 90% CI (eg. we care if below 5% or above 95%)... 0.5 + (ci / 2) ## [1] 0.95 0.5 - ci / 2 ## [1] 0.05 No Censoring: If we have complete data (all observations failed!), using this formula to calculate factor \\(k\\): # For example, let&#39;s say r = 50 r = 50 k = qchisq(ci, df = 2*r) / (2*r) Time-Censoring: If we only record up to a specific time (eg. planning an experiment), use this formula to calculate factor \\(k\\), setting as the degrees of freedom \\(df = 2(r+1)\\). # For example, let&#39;s say r = 50 r = 50 k = qchisq(ci, df = 2*(r+1)) / (2*r) # Clear these remove(r, k) So, since we do not have time censoring in our d4 dataset, we can go ahead an calculate the df = 2*r and compute the 90% lower and upper confidence intervals. d4 %&gt;% summarize( lambda_hat = lambda_hat, r = r, k_upper = qchisq(0.95, df = 2*r) / (2*r), k_lower = qchisq(0.05, df = 2*r) / (2*r), lower = lambda_hat * k_lower, upper = lambda_hat * k_upper) ## # A tibble: 1 × 6 ## lambda_hat r k_upper k_lower lower upper ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0138 9 1.60 0.522 0.00722 0.0222 5.5.5 Planning Experiments When planning an experiment, we can use the following formula to determine the necessary values of \\(r\\), \\(t\\), \\(lambda\\), \\(k\\), or \\(n\\) for our experiment to function: \\[ \\frac{r}{n \\ t} \\times k_{r, 1 - \\alpha} = \\hat{\\lambda} \\] Note: Experiment planning by definition makes time-censored datasets, so you’ll need your adjusted \\(k\\) formula so that \\(df = 2(r+1)\\). Let’s try an example. Imagine a company wants to test a new line of masks. Your company can budget 500 hours to test these masks’ elastic bands, and they accept that up to 10 of these masks may break in the process. Company policy anticipates a failure rate of just 0.00002 masks per hour, and we want to be 90% confidence that the failure rate will not exceed this level. (But it’s fine if it goes below that level.) If we aim to fit within these constraints, how many masks need to be tested in this product trial? # Max failures r = 10 # Number of hours t = 500 # Max acceptable failure rate lambda = 0.00002 # Calcuate a one-tailed confidence interval alpha = 0.10 ci = 1 - alpha # Use these ingredients to calculate the factor k, using time-censored rule df = 2*(r+1) k = qchisq(ci, df = 2*(r+1)) / (2*r) # Reformat # r / (t*n) * k = lambda n = k * r / (lambda * t) # Check it! n ## [1] 1540.664 Looks like you’ll need a sample of about 1541 masks to be able to fit those constraints. LC 4 Question A small start-up is product testing a new super-effective mask. They product tested 25 masks over 60 days. They contract you to analyze the masks’ lifespan data, recorded below as the number of days to failure. # Lifespan in days supermasks &lt;- c(1, 2, 2, 2, 3, 3, 4, 4, 5, 9, 13, 15, 17, 19, 20, 21, 23, 24, 24, 24, 32, 33, 33, 34, 54) Cross-tabulate the lifespan distribution in intervals of 7 days. Estimate \\(\\hat{\\lambda}\\) from the cross-tabulated data. Estimate a 95% confidence interval for \\(\\hat{\\lambda}\\). Using the cross-tabulated data, do these masks’ lifespan distribution fit an exponential distribution, or does their distribution differ to a statistically significant degree from the exponential? How much? (eg. statistic and p-value). Answer Cross-tabulate the lifespan distribution in intervals of 7 days. ## # A tibble: 8 × 6 ## label count bin lower upper midpoint ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [0,7] 9 1 0 7 3.5 ## 2 (7,14] 2 2 7 14 10.5 ## 3 (14,21] 5 3 14 21 17.5 ## 4 (21,28] 4 4 21 28 24.5 ## 5 (28,35] 4 5 28 35 31.5 ## 6 (35,42] 0 6 35 42 38.5 ## 7 (42,49] 0 7 42 49 45.5 ## 8 (49,56] 1 8 49 56 52.5 Estimate \\(\\hat{\\lambda}\\) from the cross-tabulated data. # Let&#39;s estimate lambda! b &lt;- a %&gt;% summarize( r = sum(count &gt; 0), # total failures days = sum(midpoint*count), # total failure-days n = r, # in this case, total failures = total obs tz = max(midpoint), # end of study period # Calculate lambda hat! lambda_hat = r / (days + (n - r)*tz)) # Check it b ## # A tibble: 1 × 5 ## r days n tz lambda_hat ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 416. 6 52.5 0.0144 Estimate a 95% confidence interval for \\(\\hat{\\lambda}\\). c &lt;- b %&gt;% summarize( lambda_hat = lambda_hat, r = r, k_upper = qchisq(0.975, df = 2*r) / (2*r), k_lower = qchisq(0.025, df = 2*r) / (2*r), lower = lambda_hat * k_lower, upper = lambda_hat * k_upper) # Check it c ## # A tibble: 1 × 6 ## lambda_hat r k_upper k_lower lower upper ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0144 6 1.94 0.367 0.00529 0.0280 Using the cross-tabulated data, do these masks’ lifespan distribution fit an exponential distribution, or does their distribution differ to a statistically significant degree from the exponential? How much? (eg. statistic and p-value). # Get your &#39;model&#39; function f = function(t, lambda){ 1 - exp(-1*lambda*t) } # Get lambda-hat from b$lambda_hat # Step 3: Calculate Observed vs. Expected d &lt;- a %&gt;% mutate( total = sum(count), o = cumsum(count), e = f(t = midpoint, lambda = b$lambda_hat) * total) %&gt;% # Calculate Chi-squared statistic and p-value summarize( chisq = sum((o - e)^2 / e), nbin = n(), np = 1, df = nbin - np - 1, p_value = 1 - pchisq(q = chisq, df = df)) # Check it! d ## # A tibble: 1 × 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 169. 8 1 6 0 5.6 Conclusion Congrats! You made it! You’ve just picked up some of the key techniques for evaluating product lifespans in R! "],["workshop-useful-life-distributions-weibull-gamma-lognormal.html", "6 Workshop: Useful Life Distributions (Weibull, Gamma, &amp; Lognormal) Getting Started 6.1 Maximum Likelihood Estimation (MLE) LC 1 6.2 Gamma Distribution LC 2 6.3 Weibull Distribution LC 3 LC 4 6.4 Lognormal Distribution LC 5 LC 6 6.5 Conclusion", " 6 Workshop: Useful Life Distributions (Weibull, Gamma, &amp; Lognormal) In this workshop, we’re going to continue learning some R functions for working with common life distributions, specifically the Weibull, Gamma, and Log-Normal distributions. Getting Started Load Packages Let’s start by loading the tidyverse package. We’ll also load mosaicCalc, for taking derivatives and integrals (eg. D() and antiD()). # Load packages library(tidyverse) library(mosaicCalc) Load Data In this workshop, we’re going to use a dataset of crop lifetimes! (Agriculture needs statistics too!) For each crop (id), we measured its lifespan in days from the day its seed is planted to the time it is ripe (usually about 90 days). # Import crop lifespan data! crops &lt;- read_csv(&quot;workshops/crops.csv&quot;) 6.1 Maximum Likelihood Estimation (MLE) In our last few workshops, we learned that the exponential distribution does a pretty good job of approximating many life distributions, and that we can evaluate the fit of a distribution using a chi-squared test. In this workshop, we’ll learn several alternative, related distributions that might fit your data even better depending on the scenario. But modeling any data using a distribution requires that we know the parameters that best match the observed data. This can be really hard, as we found out with \\(\\hat{\\lambda}\\) in the exponential distribution. Below, we’ll learn a computational approach called Maximum Likelihood Estimation (MLE), that will help us find the true (most likely) values for any parameter in any dataset. It’s pretty robust if the number of failures is ‘large’ enough, where ‘large’ counts as &gt;10 (which is really small!). 6.1.1 Likelihood Function MLE involves 3 ingredients: sample: a vector of raw empirical data probability density function (PDF): tells us probability (relative frequency) of each value in a sample occurring. likelihood function: tells us probability of getting this EXACT sample, given the PDF values for each observed data point. We can get the “probability” of any sample by multiplying the density function f(t) at each data point \\(t_{i}\\), for r data points. Multiplication indicates joint probability, so this product really means how likely is is to get this specific sample. We call it the likelihood of that sample. \\[ LIK = \\prod_{i = 1}^{r}{ f(t_i) } = f(t_1) \\times f(t_2) \\times ... f(t_n) \\] # Let&#39;s write a basic pdf function d = function(t, lambda){lambda * exp(-t*lambda) } # Let&#39;s imagine we already knew lambda... mylambda = 0.01 crops %&gt;% # Calculate probability density function for observed data mutate(prob = d(days, lambda = mylambda)) %&gt;% summarize( # Let&#39;s calculate the likelihood likelihood = prob %&gt;% prod(), # Since that&#39;s a really tiny number, # let&#39;s take its log to get log-Likelihood loglik = prob %&gt;% prod() %&gt;% log()) ## # A tibble: 1 × 2 ## likelihood loglik ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7.55e-116 -265. Technically, our likelihood above shows the joint probability of each of these observed values occurring together supposing that: they have an exponential probability density function (PDF), and… that the parameter lambda in that PDF equals 0.01. So we can get different (higher/lower) likelihoods of these values occuring together if we supply (1) different parameter values and therefore (2) a different hypothesized PDF. The most accurate parameters will be the ones that maximize the likelihood. In practice though, the likelihood is teeny-tiny, and multiplying tiny numbers is hard! Fortunately… the log of a tiny value makes it bigger and easier to read and compute. the log of a product of values actually equals the sum of the log of those values (see equation below!). \\[ log ( \\ L(t) \\ ) = \\log\\prod_{i=1}^{n}{ f(t_i) = \\sum_{i=1}^{n}\\log( \\ f(t_i) \\ ) } \\] ll = function(data, lambda){ # Calculate Log-Likelihood, # by summing the log d(t = data, lambda) %&gt;% log() %&gt;% sum() } 6.1.2 Maximizing with optim() We can use the optim() function from base R to: run our function (fn = …) many times, varying the value of lambda (or any parameter). supply any other parameters like data = crops$days to each run. identify the log-likelihood that is greatest (if control = list(fnscale = -1)); by default, optim() actually minimizes otherwise. return the corresponding value for lambda (or any other parameter). optim() will output a $par (our parameter estimate(s)) and $value (the maximum log-likelihood). # Maximize the log-likelihood! optim(par = c(0.01), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.014375 ## ## $value ## [1] -262.167 ## ## $counts ## function gradient ## 18 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 6.1.3 Under the Hood Wondering what’s happening inside optim()? Good news: it’s not too tough. We can actually code and visualize it ourselves! First, let’s get all the log-likelihoods in that interval. # Let&#39;s manyll &lt;- data.frame(parameter = seq(from = 0.00001, to = 1, by = 0.001)) %&gt;% # For each parameter, get the loglikelihood group_by(parameter) %&gt;% summarize(loglik = ll(data = crops$days, lambda = parameter)) # Check a few manyll %&gt;% head(3) ## # A tibble: 3 × 2 ## parameter loglik ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00001 -576. ## 2 0.00101 -348. ## 3 0.00201 -317. Let’s maximize the log-likelihood manually… # Now find the parameter that has the greatest log-likelihood output &lt;- manyll %&gt;% filter(loglik == max(loglik)) #check it output ## # A tibble: 1 × 2 ## parameter loglik ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0140 -262. Now, let’s visualize it! ggplot() + geom_line(data = manyll, mapping = aes(x = parameter, y = loglik), color = &quot;steelblue&quot;) + geom_vline(xintercept = output$parameter, linetype = &quot;dashed&quot;) + theme_classic(base_size = 14) + labs(x = &quot;parameter (lambda)&quot;, y = &quot;loglik (Log-Likelihood)&quot;, subtitle = &quot;Maximizing the Log-Likelihood (Visually)&quot;) + # We can actually adust the x-axis to work better with log-scales here scale_x_log10() + # We can also annnotate our visuals like so. annotate(&quot;text&quot;, x = 0.1, y = -2000, label = output$parameter) 6.1.4 Multi-parameter optimization Multi-parameter optimization works pretty similarly. We can use optim() for this too! The key here is that optim() only varies whatever values we supply to optim(par = ...). So if we have multiple parameters, like the mean and sd for the normal distribution, we need to put both parameters into one vector in our input par, like so: # Let&#39;s write a new function ll = function(data, par){ # Our parameters input is now going to be vector of 2 values # par[1] gives the first value, the mean # par[2] gives the second value, the standard deviation dnorm(data, mean = par[1], sd = par[2]) %&gt;% log() %&gt;% sum() } Now, let’s maximize! # Let&#39;s optimize it! # put in some reasonable starting values for &#39;par&#39; and it optim() will do the rest. optim(par = c(90, 15), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 69.65196 47.56853 ## ## $value ## [1] -264.0527 ## ## $counts ## function gradient ## 51 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL That was ridiculously easy! Let’s compare to their known values: crops %&gt;% summarize(mu = mean(days), sigma = sd(days)) ## # A tibble: 1 × 2 ## mu sigma ## &lt;dbl&gt; &lt;dbl&gt; ## 1 69.6 48.0 Almost spot on! Pretty good for an estimation technique! 6.1.5 MLE with Censored Data But what do we do if we need to estimate parameters with MLE, but our data is censored (eg. time censored, or cross-tabulated?) Well, fortunately, same general rules apply! \\[ LIK = k \\times ( \\Pi_{i = 1}^{r}{f(t_i) \\ } \\times [1 - F(T_{max})]^{n-r} ) \\approx \\prod_{i=1}^{r}{ f(t_i) } \\times R(T_{max})^{n-r} \\] We can re-write the likelihood function as the joint probability of each time to failure \\(f(t_i)\\) for \\(r\\) total failures that did occur, times the cumulative probability that they did not fail by the end of the study period \\(T_{max}\\) for \\(n - r\\) potential failures that did not occur. Just like when estimating lambda though, we know that there’s room for error, so we can calculate a \\(k\\) factor like before to upweight or downweight our likelihood statistic, giving us upper and lower confidence intervals. Most often, we are looking for the precise estimate of parameters, not upper and lower confidence intervals for them, so in those cases, \\(k\\) can be omitted, simplifying down to the right-side equation above. Let’s try a few applications of this formula below. 6.1.5.1 Time Censored Data Suppose we actually had \\(n = 75\\) crops, but we only ever saw \\(r = 50\\) of them fail, and we stopped recording after 200 days. How would we estimate the maximum likelihood to get our parameter of interest \\(\\lambda\\)? First, let’s write our functions. # Let&#39;s write a basic pdf function &#39;d&#39; and cdf function &#39;f&#39; d = function(t, lambda){ lambda * exp(-t*lambda) } f = function(t, lambda){ 1 - exp(-t*lambda)} Now, using our observed (but incomplete data in crops), let’s calculate the log-likelihood if \\(\\lambda = 0.01\\). crops %&gt;% summarize( # Get total failures observed r = n(), # Get total sample size, n = 75, # Get last timestep tmax = 200, # Take the product of the PDF at each timestep prob_d = d(t = days, lambda = 0.01) %&gt;% prod(), # Get probability of survival by the last time step, # for as many observations as did not fail prob_r = (1 - f(t = tmax, lambda = 0.01))^(n - r), # Get likelihood lik = prob_d * prob_r, # Get loglikelihood loglik = log(lik)) ## # A tibble: 1 × 7 ## r n tmax prob_d prob_r lik loglik ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 50 75 200 7.55e-116 1.93e-22 1.46e-137 -315. Great! Let’s formalize that in a function, for any value of \\(\\lambda\\). # We&#39;ll write a log-likelihood function &#39;ll&#39; # that takes a data input and a single numeric parameter par ll = function(data, par){ output &lt;- data %&gt;% summarize( r = n(), n = 75, tmax = 200, prob_d = d(t = days, lambda = par) %&gt;% prod(), prob_r = (1 - f(t = tmax, lambda = par))^(n - r), loglik = log(prob_d * prob_r)) # Return the output output$loglik } # Last, we can run &#39;optim&#39; to get the MLE, with a starter guess of lambda at 0.05 optim(par = c(0.01), data = crops, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.005898438 ## ## $value ## [1] -306.6839 ## ## $counts ## function gradient ## 22 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 6.1.5.2 Cross-Tabulated, Time-Censored Data Alternatively, what if our data were not only time-censored, but had been cross-tabulated!? The crosstab data.frame below encodes days to failure for our crops, tallied up in 40 day intervals, supposing a total sample of 75 sampled crops evaluated for 200 days. We can still estimate parameters with MLE using the formula above! crosstab &lt;- data.frame( label = c(&quot;[0,40]&quot;, &quot;(40,80]&quot;, &quot;(80,120]&quot;, &quot;(120,160]&quot;, &quot;(160,200]&quot;), t = c(20, 60, 100, 140, 180), count = c(18, 14, 10, 5, 3)) We can write our log-likelihood function very similarly to above. The main change is that we add up the counts to get r, the total observed failures. To estimate our probabilities, we just use the interval midpoints as our days to failure t. ll = function(data, par){ output &lt;- data %&gt;% summarize( # Get total failures observed (sum of all tallies) r = sum(count), # Get total sample size, n = 75, # Get last timestep tmax = 200, # Get PDF at each timestep; take product for each failure observed prob_d = d(t = t, lambda = par)^count %&gt;% prod(), # Get probability of survival by the last time step, # for as many n-r observations that did not fail prob_r = (1 - f(t = tmax, lambda = par))^(n - r), # Get log-likelihood loglik = log(prob_d * prob_r)) # Return the output output$loglik } # Last, we can run &#39;optim&#39; to get the MLE, with a starter guess of lambda at 0.05 optim(par = c(0.01), data = crosstab, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.005925781 ## ## $value ## [1] -306.4357 ## ## $counts ## function gradient ## 24 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL The estimate provided is a little different than the one above, but it’s still in a very similar ballpark. When working with cross-tabulations but estimating complex parameters, this estimation strategy can be a life-saver (literally). LC 1 Question We’ve learned several probability density functions for different distributions, including dexp(), dgamma(), dpois(), and dweibull(). Use optim() to maximize the likelihood of each of these PDFs’ likelihood, using our crops data. Answer For the exponential distribution… ll = function(data, par){ dexp(data, rate = par) %&gt;% log() %&gt;% sum() } optim(par = c(0.01), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.014375 ## ## $value ## [1] -262.167 ## ## $counts ## function gradient ## 18 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL For the gamma distribution… ll = function(data, par){ dgamma(data, shape = par[1], scale = par[2]) %&gt;% log() %&gt;% sum() } optim(par = c(1, 1), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 1.921441 36.239762 ## ## $value ## [1] -256.9295 ## ## $counts ## function gradient ## 83 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL For the Poisson distribution… ll = function(data, lambda){ dpois(data, lambda) %&gt;% log() %&gt;% sum() } # Optimize! optim(par = 90, data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 69.63574 ## ## $value ## [1] -942.9518 ## ## $counts ## function gradient ## 26 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL For the Weibull distribution… ll = function(data, par){ dweibull(data, shape = par[1], scale = par[2]) %&gt;% log() %&gt;% sum() } # Optimize! optim(par = c(1,1), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 1.492828 77.106302 ## ## $value ## [1] -256.775 ## ## $counts ## function gradient ## 107 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL remove(ll, d, output, mylambda, manyll) 6.2 Gamma Distribution Alternatively, the Gamma distribution is well suited to modeling products exposed to a series of shocks over time at a given rate over time. 6.2.1 \\(\\Gamma\\) function gamma() In the gamma distribution, events are exposed to \\(k\\) shocks. We often will use something called a gamma function \\(\\Gamma\\) of \\(k\\) to show the total ways a series of shocks could have occurred. Gamma functions are written \\((k - 1)!\\), where \\(!\\) means “factorial”, or taking the product of k by every prior integer. Since, \\((k - 1)! = \\Gamma(k)\\), then if \\(k = 4\\), then \\(\\Gamma(k = 4) = (4 - 1) \\times (3 - 1) \\times (2 - 1) = 3 \\ \\times 2 \\times 1 = 6\\). We can quickly code in this in r using the gamma() function. For example, in the prior example, gamma(4) renders 6. Pretty quick, right? We can also code this directly using the factorial() function. For example, if k = 4, we can write factorial(4 - 1). # Let&#39;s try out the gamma function. gamma(4) ## [1] 6 # Produces same output as factorial(k - 1) factorial(4 - 1) ## [1] 6 6.2.2 \\(f(t)\\) or d(t)(Probability Density Function, aka PDF) Excitingly, we can reapply most of the same rules we learned for exponential distributions; we just have to update our functions. In the gamma distribution, events are exposed to \\(k\\) shocks that occur at rate \\(\\lambda\\) over time \\(t_1, t_2, ... t_n\\). We can model the time to failure for such a function like so! \\[f(t) = \\frac{\\lambda}{(k - 1)!}(\\lambda t)^{k-1}e^{-\\lambda t}\\] We can also write \\((k - 1)!\\) above as \\(\\Gamma(k)\\). As mentioned above, this is called a \"Gamma function of k\", which is where the distribution’s name comes from. \\(k\\) effectively controls the shape of the function. # Let&#39;s write our new PDF function, d(t); written as f(t) above d = function(t, k, lambda){ lambda / factorial(k - 1) * (lambda*t)^(k-1) * exp(-t*lambda) } # Try it out! d(t = 1, k = 1, lambda = 1) ## [1] 0.3678794 # Compare with our dgamma() function! dgamma(x = 1, shape = 1, rate = 1) ## [1] 0.3678794 # They&#39;re the same! 6.2.3 \\(F(t)\\) Failure Function and \\(R(t)\\) Reliability Function The failure and reliability function are always closely related, no matter the distribution, where \\(R(t) = 1 - F(t)\\). We can write the failure function \\(F(t)\\) (a.k.a. the CDF) as: \\[ F(t) = 1 - \\sum_{n = 0}^{k - 1}{\\frac{(\\lambda t)^n }{n!}e^{-\\lambda t}}\\] Therefore, we can also write the reliability function \\(R(t)\\) as: \\[ R(t) = \\sum_{n = 0}^{k - 1}{\\frac{(\\lambda t)^n }{n!}e^{-\\lambda t}}\\] # Write the failure function for Gamma distribution f = function(t, k, lambda){ # Make a vector of values from 0 to k-1 n = seq(from = 0, to = k - 1) # Now compute the failure function 1 - sum( (lambda*t)^n / factorial(n) * exp(-lambda*t) ) } # We can also integrate the PDF to get the CDF, alternatively, using mosaicCalc fc = antiD(tilde = d(t, k, lambda) ~ t) # Let&#39;s compare! # Using pgamma() pgamma(q = 10,shape = 3,rate = 1) ## [1] 0.9972306 # Using our calculate-based fc() fc(t = 10, k = 3, lambda = 1) ## [1] 0.9972306 # Using our direct function f() f(t = 10, k = 3, lambda = 1) ## [1] 0.9972306 # All the same! Correspondingly, the reliability function \\(R(t)\\) would be… # Write the reliability function r = function(t, k, lambda){ # Make a vector of values from 0 to k-1 n = seq(from = 0, to = k - 1) # Now compute the reliability function sum( (lambda*t)^n / factorial(n) * exp(-lambda*t) ) } # Get reliability function via calculus... rc = function(t,k,lambda){ # compute CDF fc = antiD(tilde = d(t, k, lambda) ~ t) # return 1 - CDF 1 - fc(t = t, k = k, lambda = lambda) } # Compare outputs! # with pgamma() 1 - pgamma(q = 10, shape = 3, rate = 1) ## [1] 0.002769396 # with rc() rc(t = 10, k = 3, lambda = 1) ## [1] 0.002769396 # with r() r(t = 10, k = 3, lambda = 1) ## [1] 0.002769396 # They&#39;re the same! \\(z(t)\\) Failure Rate function The failure rate remains \\(z(t) = \\frac{f(t)}{R(t)}\\). Further, in the gamma distribution, the rate remains constant, although the size parameter causes a result quite different from the exponential! # Let&#39;s write the failure rate function z = function(t,k,lambda){ # We&#39;ll break it up into parts to help readability # Compute the PDF d(t) (or f(t)) d_of_t = lambda / factorial(k - 1) * (lambda*t)^(k-1) * exp(-t*lambda) # Make a vector of values from 0 to k-1 n = seq(from = 0, to = k - 1) # Now compute the reliability function R(t) r_of_t = sum( (lambda*t)^n / factorial(n) * exp(-lambda*t) ) # Divide the two to get the failure rate! d_of_t / r_of_t } # Or, using calculus.... zc = function(t, k, lambda){ # Get CDF via integration fc = antiD(tilde = d(t, k, lambda) ~ t) # Get r(t) using 1 - f(t) # Take PDF / (1 - CDF) d(t = t, k = k, lambda = lambda) / (1 - fc(t = t, k = k, lambda = lambda) ) } # Try it out! # Using dgamma() and pgamma() dgamma(1,shape = 1, rate = 0.1) / (1 - pgamma(1,shape = 1, rate = 0.1)) ## [1] 0.1 # Using our calculus based function zc(t = 1, k = 1, lambda = 0.1) ## [1] 0.1 # Using our home cooked function z(t = 1, k = 1, lambda = 0.1) ## [1] 0.1 6.2.4 MTTF and Variance We can also compute the mean time to fail (MTTF) and variance using the following formulas. \\[ MTTF = \\int_{0}^{\\infty}{ t \\ f(t) dt} = \\frac{k}{\\lambda} \\] \\[ Var(t) = \\frac{k}{ \\lambda^2} \\] We can encode these as functions as follows. # Mean (aka mean time to fail) mttf = function(k, lambda){ k / lambda } # Check it! mttf(k = 1, lambda = 0.1) ## [1] 10 # Variance variance = function(k, lambda){ k / lambda^2 } # Try it! variance(k = 1, lambda = 0.1) ## [1] 100 LC 2 Question A car bumper has demonstrated a gamma distribution with a failure rate of \\(\\lambda = 0.005\\) per day given \\(k = 3\\) potential shocks. Determine the reliability of this bumper over a 30, 300, and 600 day period. Answer # Compute a reliability function using pgamma() r = function(t,k,lambda){ 1 - pgamma(t,shape = k, rate = lambda) } # For 24 hours r(t = c(30, 300, 600), k = 3, lambda = 0.005) ## [1] 0.9994971 0.8088468 0.4231901 6.3 Weibull Distribution Another popular distribution is the Weibull distribution. The exponential is a special case of Weibull distribution where the failure rate \\(\\lambda\\) is held constant such that \\(m = 1\\). But in a usual Weibull distribution, the failure rate can change over time! This makes it very flexible, able to take on the shape of an exponential, gamma, or normal distribution depending on the parameters. We can show this easily using the Weibull’s cumulative hazard function \\(H(t) = (\\lambda t)^m\\). If \\(m = 1\\), as in the exponential distribution, then \\(H(t) = \\lambda t\\), as in the exponential distribution. But if \\(m \\neq 1\\), then the accumulative hazard rate increases to the \\(m\\) power with ever passing hour. We can use this to derive the failure function \\(F(t)\\), reliability function \\(R(t)\\), and failure rate \\(z(t)\\). (Similarly, we can use all the same tricks from before to get the \\(AFR(t_1, t_2)\\), like using the accumulative hazard function \\(\\frac{H(t_2) - H(t_1)}{t_2 - t_1}\\).) 6.3.1 \\(F(t)\\) Failure Function and \\(R(t)\\) Reliability Function To derive \\(F(t)\\), we can sub in the new formula for the accumulative hazard function into the formula for the failure function. The Weibull failure function is also commonly written replacing \\(\\lambda\\) with \\(c = \\frac{1}{\\lambda}\\), the characteristic life. \\[ F(t) = 1 - e^{-H(t)} = 1 - e^{-(\\lambda t)^m} = 1 - e^{-(t/c)^m}\\] We can code it like so: f = function(t, m, c){ 1 - exp(-1*(t/c)^m) } # Compare! # Using pweibull() pweibull(1, shape = 1, scale = 1) ## [1] 0.6321206 # Using our home-made function! f(t = 1, m = 1, c = 1) ## [1] 0.6321206 Similarly, we can write the reliability function as… r = function(t, m, c){ exp(-1*(t/c)^m) } # Try it with pweibull() 1 - pweibull(1, shape = 1, scale = 1) ## [1] 0.3678794 # Using our home-made function! r(t = 1, m = 1, c = 1) ## [1] 0.3678794 6.3.2 \\(f(t)\\) or d(t) (PDF) While not nearly as straightforward to derive, we can literally take the derivative of the Failure function to get the PDF. \\[ f(t) = \\frac{m}{t} \\times (\\frac{t}{c})^m \\times e^{-(t/c)^m}\\] d = function(t, m, c){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) # alternatively written using calculus: # D(f(t,m,c) ~ t) } # Compare results! dweibull(1, shape = 1, scale = 1) ## [1] 0.3678794 d(t = 1, m = 1, c = 1) ## [1] 0.3678794 6.3.3 \\(z(t)\\) Failure Rate We can similarly derive the failure rate \\(z(t)\\) by taking the derivative of the accumulative hazard rate, which gives us: \\[z(t) = m \\lambda (\\lambda t)^{m-1} = (m/c) \\times (t/c)^{m-1} \\]/’ z = function(t, m, c){ (m/c) * (t/c)^(m-1) } # try it! z(1, m = 1, c = 0.1) ## [1] 10 # using dweibull() and pweibull() dweibull(1, shape = 1, scale = 0.1) / (1 - pweibull(1, shape = 1, scale = 0.1)) ## [1] 10 6.3.4 \\(m\\) and \\(c\\) Fortunately, if any 3 of the 4 parameters (\\(F(t), m, t, c\\) are known about a Weibull distribution, the remaining parameter can be calculated. Since \\(F(t) = 1 - e^{-(t/c)^m}\\), we can say: \\[ t = c \\times ( -log( 1 - F ))^{1/m} \\] \\[m = \\frac{log( -log( 1 - F) )}{log(t/c)}\\] \\[c = \\frac{t}{( -log(1 - F) )^{1/m}}\\] Characteristic life \\(c\\), in this case, really means the time at which 63.2% of units will consistently have failed. Shape parameter \\(m\\) is used to describe several different types of Weibull distributions. \\(m = 1\\) is an exponential; \\(m = 2\\) is a Reyleigh distribution, where the failure rate increases linearly. When \\(m &lt; 1\\), it looks like the left-end of a bath-tub. When \\(m &gt; 1\\), it looks like the right-end of a bath-tub. 6.3.5 Weibull Series Systems In a series system of independent components, which are each Weibull distributed with the same shape parameter \\(m\\), that system has a Weibull distribution, as follows: \\[ c_{series} = (\\sum_{i=1}^{n}{ \\frac{1}{c_i^m}})^{-\\frac{1}{m}} \\] 6.3.6 MTTF and Variance Finally, we could code the mean time to fail and variance as follows. \\[ MTTF = c \\Gamma(1 + \\frac{1}{m}) = c \\times (k - 1)! \\times (1 + \\frac{1}{m}) \\] \\[ Variance = c^2 \\Gamma(1 + \\frac{2}{m} ) - [ c \\Gamma(1 + \\frac{1}{m})]^{2} \\] # We could code these up simply like so mttf = function(c, m){ c * gamma( 1 + 1/m) } # Though we know that mttf = integral of R(t), so we could also write it like this mttf = antiD((1 - pweibull(t, shape = m, scale = c)) ~ t) # We could code these up simply like so variance = function(c, m){ c^2 * gamma( 1 + 2/m ) - ( c * gamma( 1 + 1/m) )^2 } LC 3 Question You’ve done this a bunch now - for this learning check, write your own function to find \\(t\\), \\(m\\), and \\(c\\) in a Weibull distribution. Answer # Write a function to get t get_t = function(f,m,c){ log(-log(1 - f) )/log(t/c) } # Write a function to get m get_m = function(t,f,c){ log(-log(1 - f) )/log(t/c) } # Write a function to get c get_c = function(t,f,m){ t / ( (-log(1 - f))^(1/m) ) } LC 4 Question Find the characteristic life necessary for 10% of failures to occur by 168 hours, if the shape parameter \\(m = 2\\). Then, using that characteristic life, plot the probability of failure when \\(m = 1\\), \\(m = 2\\), and \\(m = 3\\). Answer # Write our function to find c get_c = function(t, f, m){ t / ( (-log(1 - f))^(1/m) ) } get_c(t = 168, f = 0.10, m = 2) ## [1] 517.5715 # Looks like we expect a characteristic life of &gt;500 hours. # Write the failure function f = function(t, c, m){ 1 - exp(-1*(t/c)^m) } m1 &lt;- data.frame(hours = 1:100) %&gt;% mutate(prob = f(t = hours, c = 517.5715, m = 1), m = &quot;m = 1&quot;) m2 &lt;- data.frame(hours = 1:100) %&gt;% mutate(prob = f(t = hours, c = 517.5715, m = 2), m = &quot;m = 2&quot;) m3 &lt;- data.frame(hours = 1:100) %&gt;% mutate(prob = f(t = hours, c = 517.5715, m = 3), m = &quot;m = 3&quot;) bind_rows(m1,m2,m3) %&gt;% ggplot(mapping = aes(x = hours, y = prob, color = m)) + geom_line() 6.4 Lognormal Distribution The log-normal distribution can be very useful, often in modeling semi-conductors, among other product. To understand the log-normal distribution, let’s outline the normal distribution PDF, and compare it to the log-normal distribution PDF. 6.4.1 \\(f(t)\\) or d(t) (PDF) The normal distribution contains the parameters \\(\\mu\\) (mean) and \\(\\sigma\\) (standard deviation). \\[f(t) = \\frac{1}{\\sigma \\sqrt{2\\pi} } \\times e^{-(t - \\mu)^2 / 2\\sigma^2 } \\] The log-normal distribution is quite similar. Here, \\(t\\) becomes \\(e^t\\) and median \\(T_{50} = e^{\\mu}\\). \\(\\sigma\\) is really more of a shape parameter here than the standard deviation as we usually think of it. \\[ f(t) = \\frac{1}{\\sigma t \\sqrt{2\\pi}} \\times e^{-(1/2\\sigma^2) \\times (log(t) - log(T_{50} ))^2}\\] 6.4.2 \\(\\Phi\\) Interestingly, the CDF of the log-normal relies on the CDF of the normal distribution, sometimes written as \\(\\Phi\\). Both distributions’ CDFs are written below. \\[ Normal \\ F(t) = \\Phi ( \\frac{t - \\mu}{ \\sigma } ) \\] \\[ Log-Normal \\ \\ F(t) = \\Phi (\\frac{log(t / T_{50})}{\\sigma}) \\] We can write the normal distribution’s CDF, \\(\\Phi\\), as the cumulative probability of a lifespan \\(t\\) that is \\(\\frac{t - \\mu}{\\sigma}\\) standard deviations away from the mean. In the log-normal CDF, we transform our value \\(t\\) into a point measured in standard deviations from the mean. These points are called z-scores. We can feed that value to pnorm() to find out the cumulative probability of reaching that point on a normal distribution, with a mean of 0 and a standard deviation of 1. # Suppose... t = 50 t50 = 100 sigma = 2 # Get the cumulative probability of failure at time t p &lt;- pnorm( log(t/t50) / sigma ) # Check it! p ## [1] 0.3644558 Similarly, if we have a cumulative probability like p, we can solve for the z-score that made it using qnorm(). qnorm() is sometimes referred to as inverse-phi (\\(\\Phi^{-1}\\)). # Get the z-score for F(t) = 0.36! z &lt;- qnorm(p) # Check it! z ## [1] -0.3465736 Now that we know the z-score, we can easily solve for t, t50, or sigma with algebra if we know the other parameters, since \\(z = \\frac{log(t/T_{50})}{\\sigma}\\)! To get t… # t = exp(z * sigma) * t50 exp(z * sigma) * t50 ## [1] 50 To get t50… # t50 = t / exp(z * sigma) t / exp(z * sigma) ## [1] 100 To get sigma… # sigma = log(t/t50) / z log(t/t50) / z ## [1] 2 Pretty quick, right? 6.4.3 Key Functions This one’s pretty messy. (One can use the dlnorm() function, but it contains 2 parameters, meanlog and sdlog, which are NOT \\(T_{50}\\) (median) and \\(\\sigma\\) (shape). For six-sigma, it tends to be more helpful for us to write the log-normal functions in terms of \\(T_{50}\\) and \\(\\sigma\\) instead.) # Write the pdf of the log-normal, f(t) or d(t)! d = function(t, t50, sigma){ 1 / (sigma * t * sqrt(2*pi)) * exp( -(1/ 2 * sigma^2 )*(log(t) - log(t50))^2) } # Write the failure function of the log-normal F(t)! f = function(t, t50, sigma){ pnorm( log(t / t50) / sigma ) } # Or with mosaicCalc via derivation... # dc = D(f(t,t50, sigma)~t) # Write the reliability function R(t)! r = function(t, t50, sigma){ 1 - pnorm( log(t / t50) / sigma ) } # Write the failure rate z(t)! z = function(t, t50, sigma){ d(t, t50, sigma) / r(t, t50, sigma) } # Write the accumulative hazard function H(t)! h = function(t, t50, sigma){ -log(r(t, t50, sigma)) } # Write the average failure rate function AFR(t) afr = function(t1, t2, t50, sigma){ (h(t2) - h(t1) ) / (t2 - t1) } Wow! That was surprisingly easy! In this way, as we pick up more and more distributions in this course, our tools for generating failure and reliability functions get easier and easier too! 6.4.4 MTTF and Variance Finally, we can quickly calculate these quantities of interest too. \\[ MTTF = T_{50} \\times e^{\\sigma^2 / 2} \\] \\[ Variance = T_{50} \\times e^{\\sigma^2} \\times (e^{\\sigma^2} - 1)\\] # We could code these up simply like so mttf = function(mu, sigma){ mu * exp(sigma^2 / 2) } # We could code these up simply like so variance = function(mu, sigma){ mu * exp(sigma^2) * (exp(sigma^2) - 1) } LC 5 Question A crop tends to grow to a median height of 2 feet tall. We know from past data that this crop has a lognormal distribution, with a shape parameter \\(\\sigma\\) of about 0.2 feet. Market standards prefer crops between 1.5 and 8 feet tall. Given these standards, what percentage of crops are not eligible for market? Answer # Get percent under 1.5 feet below &lt;- f(1.5, t50 = 2, sigma = 0.2) # Get percentage over 8 feet above &lt;- 1 - f(8, t50 = 2, sigma = 0.2) # Get total percentage outside of these bounds below + above ## [1] 0.07515883 LC 6 Question A log normal distribution has a median time to failure equal to 50,000 hours and shape parameter \\(\\sigma\\) equal to 0.8. What is the mean time to failure and true standard deviation in this distribution? Answer # Let&#39;s write a mean time to failure function mttf = function(median, sigma){ median * exp(sigma^2 / 2) } mttf(median = 50000, sigma = 0.8) ## [1] 68856.39 # Looks like the MTTF is ~69,000 hours # Let&#39;s write the variance function variance = function(median, sigma){ median^2 * exp(sigma^2 / 2) * (exp(sigma^2) - 1) } # Now calculate variance, and take square root to get sd variance(median = 50000, sigma = 0.8) %&gt;% sqrt() ## [1] 55555.57 # with a very wide standard deviation 6.5 Conclusion Great work! You’ve learned to work with the Exponential, Gamma, Weibull, and Log-normal distribution. You’re ready to start coding some cool systems reliability analyses! Next week, we’ll learn some new techniques to help! "],["workshop-physical-acceleration-models.html", "7 Workshop: Physical Acceleration Models Getting Started 7.1 Acceleration Factor LC1 LC2 7.2 Modeling Normal Use Lifespan 7.3 Eyring Model (Multiple Stressors) 7.4 Degradation Model (Time Trends) 7.5 Burn-in Periods 7.6 Maximum Likelihood Estimation (MLE) for Physical Acceleration Models 7.7 Conclusion", " 7 Workshop: Physical Acceleration Models In this workshop, we’ll learn how to use physical acceleration models to convert results from reliability tests done under laboratory conditions (which may be slightly unrealistic) into estimates that match real outcomes in the field! Figure 7.1: Crash Testing with LEGOs: a very safe prototype Getting Started Load Packages Let’s start by loading the tidyverse and broom packages! Also, sometimes select gets overruled by other packages, so it can help to load it directly. # Load packages library(tidyverse) library(broom) # Load specific function to environment select = dplyr::select Helpful Functions We’ll be using the tibble() function; it works identically to the data.frame() function, but allows you to reference any vector that came before. For example: # This doesn&#39;t work... data.frame(x = c(1,2), y = x + 2) # But this does. tibble(x = c(1,2), y = x + 2) ## # A tibble: 2 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 ## 2 2 4 7.1 Acceleration Factor 7.1.1 Stress vs. Usage Conditions Imagine: You work in Dearborn, Michigan, testing airbag failure rates with car crash dummies every day. Your work helps people stay safe on the road! Unfortunately, lab conditions tend to be a little more extreme than the real world. Moisture levels, temperature, speed, friction, etc. are very hard (nay impossible) to perfectly match to real driving conditions. A product life distribution measured under lab “stress” conditions will always be slightly off from the real, “use”-case life distribution by a specific factor. What if we could measure and approximate that factor? Suppose researchers are stress testing airbags in the lab! Their empirical data reveals the airbag lifespans are Weibull distributed, with a characteristic life of c = 4100 hours and a shape parameter of m = 1.25. They write the Weibull density function (PDF) d() below, and use it to calculate and visualize the probability the airbags fail every hour from 1 to 8000 hours. They can visualize the blue density curve of the stress-tested airbags shown below (\\(s\\)), called \\(f_s(t)\\). But what they really want to know is the red dashed curve of the airbags under normal use conditions (\\(u\\)), called \\(f_u(t)\\)! But they don’t know it! Fortunately, they know that in past on-road and off-road studies, the median airbag’s time to failure was 250% times greater under normal usage as when under stress condition. We write 250% here the Acceleration Factor \\(AF\\) (or \\(A\\)), because it describes how stress-testing accelerates the failure by a factor of 2.5. We can write it like: \\[f_u(t) = f_s(t) \\times AF = f_s(t) \\times 2.5 \\] # Let&#39;s write ourselves a speedy weibull density function &#39;d()&#39; d = function(t, m, c){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) # or dweibull(t, scale = c, shape = m) } airbags &lt;- data.frame(t = seq(1, 12000, by = 20)) %&gt;% mutate(d_stress = d(t, c = 4100, m = 1.25)) %&gt;% # Suppose the lifespans under normal usage are just off by a factor of ~2.5 # then we could project the PDF under normal usage like: mutate(d_usage = d(t = t / 2.5, c = 4100, m = 1.25) / 2.5) ggplot() + # Plot the PDF under stress conditions geom_line(data = airbags, mapping = aes(x = t, y = d_stress, color = &quot;Stress&quot;), size = 1.5) + # Plot the PDF under normal usage conditions geom_line(data = airbags, mapping = aes(x = t, y = d_usage, color = &quot;Normal Usage&quot;), size = 1, linetype = &quot;dashed&quot;) + # Add a nice theme and clear labels theme_classic(base_size = 14) + labs(x = &quot;Airbag Lifespan in hours (t)&quot;, y = &quot;Probability Density d(t)&quot;, color = &quot;Conditions&quot;, subtitle = &quot;We want Lifespan under Normal Usage,\\nbut can only get Lifespan under Stress&quot;) 7.1.2 Acceleration as a Function \\(\\frac{f_{s}(t)}{f_{u}(t)}\\) In reality, \\(f_{u}(t)\\) probably doesn’t have exactly a constant relationship with \\(f_{s}(t)\\). In a perfect world, we could collect raw data for both lifetimes under normal conditions \\(t_u\\) and under stress-testing \\(t_s\\), and then estimate their density functions \\(f_u(t)\\) and \\(f_s(t)\\). We could then calculate \\(AF\\) exactly as a function af() that relates them. For example: # Let&#39;s write an acceleration factor function! af = function(t){ # Find density under stress ds &lt;- d(t, m = 1.25, c = 4100) # Find density function under normal conditions du &lt;- d(t, m = 1.5, c = 4500) # Since f_u(t) = f_s(t) x AF, # then AF = f_s(t) / f_u(t) = ds / du ds / du } Were we to plot it, we can see below AF is not constant here, but varies over time, because \\(f_u(t)\\) and \\(f_s(t)\\) vary over time. So, when we pick an AF, we’re usually picking the AF corresponding to a specific parameter, like the characteristic life or median of a distribution. data.frame( time = 1:1000, af = af(1:1000) ) %&gt;% # Visualize it! ggplot(mapping = aes(x = time, y = af)) + geom_line() + geom_point() + labs(subtitle = &quot;Acceleration Factor for Airbag Lifespan&quot;, y = &quot;Acceleration Factor function af()&quot;, x = &quot;Time in hours (t)&quot;) Supposing that \\(t_{u} = t_{s} \\times AF\\), we can say several more things: \\[ Failure \\ Function = F_{u}(t) = F_{s}(t/AF) \\] \\[ Density \\ Function = f_{u}(t) = \\frac{f_{s}(t/AF)}{AF} \\] \\[ Failure \\ Rate = z_{u}(t) = \\frac{z_{s}(t/AF)}{AF} \\] 7.1.3 Linear Acceleration However, it’s usually very difficult to obtain the density functions for both usage and stress conditions. That’s why we want acceleration factors (AF) - because they’ll let use estimate \\(f_u(t)\\) when we only have \\(f_s(t)\\). So in practice, we usually assume \\(AF\\) shows a constant, linear relationship between \\(f_{u}(t)\\) and \\(f_{s}(t)\\), like when \\(AF = 2.5\\). This is called linear acceleration. Linear acceleration requires us to choose a constant value of \\(AF\\) from just 1 time-step from the plot above. How can we choose!? We should probably choose a fairly representative lifespan, based off a parameter like the median time to fail \\(T_{50}\\) (or the mean time to fail \\(m\\), characteristic life \\(c\\), etc.). Even if we don’t have access to all the raw data, if we know the median lifespan under stress \\(T_{50_{s}}\\) and under normal conditions \\(T_{50_{u}}\\), we can estimate \\(AF\\) by taking \\(AF = \\frac{T_{50_{u}}}{T_{50_{s}}}\\). For example: # Let&#39;s write a weibull quantile function q = function(p, c, m){ qweibull(p, scale = c, shape = m) } # Get median under stress median_s &lt;- q(0.5, c = 4100, m = 1.25) # Get median under normal conditions median_u &lt;- q(0.5, c = 4500, m = 1.5) # Calculate it! af = median_u / median_s # Check the Acceleration Factor! af ## [1] 1.152529 Let’s clear this data. remove(af, median_s, median_u, q, d) LC1 Question A kitchen mixer has an exponential distributed lifespan. Labs stress tested the mixer at 125 degrees Celsius, yielding a mean time to fail of 4500 hours. But, it is usually heats to 32 degrees Celsius, and has an acceleration factor of 35. What proportion of mixers do we expect will fail by 40,000 hours under normal use? Answer # Given an acceleration factor of 35 af &lt;- 35 # Write a failure function f = function(t, lambda){ 1 - exp(-t*lambda) } # Method 1: # Failure Function = fu(t) = fs(t/AF) f(t = 40000 / af, lambda = 1 / 4500) ## [1] 0.2242836 # Method 2: # We know that lambda = zu(t) = zs(t/AF) / AF # so... plus that in for lambda f(t = 40000, lambda = 1 / (4500 * af) ) ## [1] 0.2242836 We expect ~22% of mixers will fail after 40,000 hours when running at 32 degrees Celsius. remove(af, f) LC2 Question Suppose we want no more than 10% of components to fail after 40,000 hours at normal usage conditions. So, we redesign the mixer to operate at a lower temperature. What (linear) acceleration factor is needed to project from our 125 degree conditions in lab to the normal use temperature of 32 degrees? Answer # Write failure function f = function(t, lambda){ 1 - exp(-t*lambda) } # Failure Function = fu(t) = fs(t/AF) # 0.10 = fu(t = 40000) = 1 - exp(-40000*lambda) # log(1 - 0.10) = -40000*lambda lambda_u = -log(0.90) / 40000 lambda_s = 1/4500 # Get ratio between these parameters # lambda_u = 1 / af * lambda_s # so... af = lambda_s / lambda_u # Check acceleration factor! af ## [1] 84.36641 We would need an acceleration factor af of ~84.4. remove(lambda_u, lambda_s, af, f) 7.2 Modeling Normal Use Lifespan 7.2.1 Models and Prediction A model is an equation approximating the relationship between two or more vectors of data. It’s not the real thing - it’s an approximation! Why use models? Usually we know quite a lot about the conditions under which a product was tested in lab, but we can’t actually observe the characteristic lifespan of that product under normal use conditions. But if we can make a really good model of the relationship between the outcome and those conditions, then we can predict for any set of conditions what the outcome - the characteristic lifespan of a product - would be under those conditions. Fortunately, R was built to make statistical models! To make a model of lifespans, we need 4 ingredients. an outcome vector, saved in a data.frame. 1 (or more) vectors of condition/predictors, saved in the same data.frame. the lm() function in R, which makes a linear model drawing the line of best fit between each value of the predictor(s) and outcome. The model is the equation of that line! the outcome vector needs to be log-transformed, because (a) most useful life distributions involve exponential processes and (b) lifespans are by nature non-negative, right-skewed variables. Taking log(outcome) adjusts for that and lets us plot a straight line of best fit. Note: We’ll learn more in later weeks about how* lm() makes models, but for now, we’ll assume it’s magic!* For example, let’s think about a car alternator, the device that converts mechanical energy into electrical energy in a car. We’ll make a few tiny example models! Suppose we tested 5 samples of 100 alternators each, and recorded the results in our alt data.frame below. We calculated the characteristic life in each of our 5 samples in the c, but our samples were all subjected to different stress conditions: different temperatures in Celsius (temp), which were converted into units of temperature factor scores (tf); different voltage levels in volts; at different points in the devices’ lifespan (time, in hours); and with different average performance ratings (rating) in amps. alt &lt;- tibble( # Characteristic life in hours c = c(1400, 1450, 1500, 1550, 1650), # Temperature in Celsius temp = c(160, 155, 152, 147, 144), # Temperature Factor (a standardized unit) tf = 1 / (1 / 11605 * (temp + 273.15)), # Voltage, in volts volts = c(17, 16.5, 14.5, 14, 13), # Hours of life spent by time of test time = c(1200, 1000, 950, 600, 500), # Performance Rating, in Amps rating = c(60, 70, 80, 90, 100)) Let’s use this data to explore some of the different classic physical acceleration models, including the Arrhenius Model, Eyring Model, and Degradation Model. 7.2.2 Arrhenius Model (Temperature) The Arrhenius Model is a simple equation that models the impact of temperature (tf) on the log() of lifespan (c). Let’s explore it visually using ggplot(), and then estimate it using lm(). 7.2.3 Visualizing the Arrhenius Model We could visualize the relationship between each of these conditions and the log() of the characteristic lifespan c using… geom_point() which makes a scatterplot between x and y coordinates in the aes(). geom_smooth(method = \"lm\"), which finds the line of best fit. g &lt;- alt %&gt;% ggplot(mapping = aes(x = tf, y = log(c) )) + geom_point(size = 5) + # Add scatterplot points geom_smooth(method = &quot;lm&quot;, se = FALSE) # Make line of best fit, using lm() - a linear model # we can write &#39;se = FALSE&#39; (standard error = FALSE) to get rid of the confidence interval We can also add in the equation of this line of best fit (which we’ll calculate below), plus other labels. g + # Add theme theme_classic(base_size = 14) + # Add labels labs(title = &quot;Arrhenius Model, Visualized&quot;, subtitle = &quot;Model Equation: log(c) = 3.1701 + 0.1518 TF \\nModel Fit: 96%&quot;, # We can add a line-break in the subtitle by writing \\n x = &quot;Temperature Factor (TF)&quot;, y = &quot;Characteristic Lifespan log(c)&quot;) 7.2.4 Estimating the Arrhenius Model So how did R calculate that line of best fit? It used the lm() function to make a linear model - an equation, which fits the data with a specific accuracy rate (eg. 96%). Let’s make a model m1 to predict the log() of characteristic lifespan c, based on our temperature factor vector tf in alt! First, let’s make the model with lm(), piping our vectors from alt. m1 &lt;- alt %&gt;% lm(formula = log(c) ~ tf) Second, let’s inspect the model’s fit with glance() from the broom package, and select() the r.squared statistic. 0% means terrible model fit. 100% means the model equation perfectly predicts every value of log(c) in our alt data.frame. We aim for excellent predictive power where possible. 96% is excellent! m1 %&gt;% glance() %&gt;% select(r.squared) ## # A tibble: 1 × 1 ## r.squared ## &lt;dbl&gt; ## 1 0.962 Third, we can now read the model equation for our line of best fit. m1 ## ## Call: ## lm(formula = log(c) ~ tf, data = .) ## ## Coefficients: ## (Intercept) tf ## 3.1701 0.1518 The lm() function estimated our model equation m1, including 2 constant coefficients named (Intercept) and tf. These coefficients show the y-intercept ((Intercept)), called \\(\\alpha\\) and the slope/effect/rate of change called \\(\\beta\\) for every 1 unit increase in the temperature factor tf. We can write this model equation formally as: \\[ \\begin{align*} log(c) =&amp; \\ Intercept + Slope \\ \\times TF \\\\ or:&amp; \\\\ log(c) =&amp; \\ \\alpha + \\beta \\times TF, \\\\ &amp; where \\ \\alpha = 3.1701 \\ and \\ \\beta = 0.1518, \\\\ so:&amp; \\\\ log(c) =&amp; \\ 3.1701 + 0.1518 \\ TF \\\\ so:&amp; \\\\ c =&amp; e^{\\alpha + \\beta \\times TF} \\\\ or:&amp; \\\\ c =&amp; \\ e^\\alpha \\times e^{\\beta \\times TF} \\\\ or:&amp; \\\\ c =&amp; e^{3.1701} + e^{0.1518 \\ TF} \\\\ or:&amp;\\\\ Arrhenius \\ Model:&amp; \\\\ c =&amp; A + e^{\\Delta H \\times TF} \\ \\\\ &amp;\\ where \\ A = e^{Intercept} = e^{\\beta} \\ \\ and \\ \\Delta H = Slope = \\beta \\\\ also:&amp; \\\\c =&amp; A + e^{\\Delta H \\times (1 / (k T))} \\\\ &amp;where \\ TF = 1 / (k \\times T_{Kelvin}) \\ and \\ k = 1 / 11605 \\end{align*} \\] As you can see above, there are many ways to write the Arrhenius model, but it boils down to this: any linear model of the log-characteristic life will involve: a y-intercept constant called \\(\\alpha\\) (\\(log(A)\\) in the Arrhenius model). \\(\\alpha\\) describes how much log(c) we get independent of any other factors (ie: if tf = 0). Interpreting our model: If the temperature factor \\(TF = 0\\), our model predicts that \\(log(c) = 3.1701\\). a slope constant called \\(\\beta\\) describing the effect of your variable (\\(TF\\) or tf), for every 1 unit increase in your variable. In the Arrhenius model, \\(\\beta\\) is written as \\(\\Delta H\\), the ‘activation energy’ rate at which a temperature factor increase of 1 unit affects log(c). Interpreting our model: If the temperature factor \\(TF\\) increases by 1, our model predicts that \\(log(c)\\) will change by \\(\\beta = \\Delta H = 0.1518\\). And that’s how we read any statistical model with two variables! &lt;br 7.2.5 Prediction Now that we have our model equation (and, importantly, a good fitting one), we can feed it values of our predictor \\(TF\\) (generically called \\(X\\)) to calculate our predicted log of the characteristic life \\(log(\\hat{c})\\) (generically called \\(log(\\hat{Y})\\)). We can do this two ways: (1) by writing a function or (2) using the predict() function in R. Writing our own function works the same way as writing our d(), f(), or r() function. We’ll call it c_hat(). # For any value of tf, we can now calculate c_hat. # We write the model equation for log(c), then exponentiate it with exp()! c_hat = function(tf){ exp( 3.1701 + 0.1518*tf) } # Test it! c_hat(tf = 28) ## [1] 1669.868 It works! The characteristic life for tf = 28 is ~1670. # Or better yet, let&#39;s calculate temperature factor &#39;tf&#39; too, # so we only have to supply a temperature in Celsius tf = function(temp){ k = 1 / 11605 # Get Boltzmann&#39;s constant 1 / (k * (temp + 273.15)) # Get TF! } # Now predict c_hat for 30, 60, and 90 degrees celsius! c_hat = function(temp){ exp( 3.1701 + 0.1518*tf(temp)) } c(30, 60, 90) %&gt;% c_hat() ## [1] 7952.275 4712.271 3044.511 So cool! Wouldn’t it be nice though, if we could simplify that process? The predict() function in R can help! The predict function will run your model equation on any newdata that you feed it, calculating the predicted outcomes. newdata must be formatted as a data.frame, containing vectors named to match each predictor from your original data (which we named alt). Let’s make a data.frame of fakedata with tibble(), varying temperature from 0 to 200 degrees Celsius, and then transform that into a temperature factor tf with our tf() function. fakedata &lt;- tibble( temp = seq(0, 200, by = 10), tf = tf(temp)) # Check the first 3 rows! fakedata %&gt;% head(3) ## # A tibble: 3 × 2 ## temp tf ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 42.5 ## 2 10 41.0 ## 3 20 39.6 Then, we’ll feet our 21 rows of fakedata and our model m1 to the predict() function, which will output 21 predictions for log(c). m1 %&gt;% predict(newdata = fakedata) ## 1 2 3 4 5 6 7 ## 9.619376 9.391606 9.179376 8.981148 8.795580 8.621497 8.457864 ## 8 9 10 11 12 13 14 ## 8.303769 8.158401 8.021038 7.891038 7.767824 7.650877 7.539733 ## 15 16 17 18 19 20 21 ## 7.433969 7.333203 7.237091 7.145316 7.057591 6.973655 6.893267 But we can exponentiate it with exp() to get c_hat! m1 %&gt;% predict(newdata = fakedata) %&gt;% exp() ## 1 2 3 4 5 6 ## 15053.6495 11987.3387 9695.1035 7951.7546 6604.9840 5549.6862 ## 7 8 9 10 11 12 ## 4711.9837 4039.0668 3492.5958 3044.3360 2673.2172 2363.3224 ## 13 14 15 16 17 18 ## 2102.4897 1881.3274 1692.5112 1530.2759 1390.0440 1268.1516 ## 19 20 21 ## 1161.6437 1068.1196 985.6159 We could even write the whole thing inside a tibble() function: fakedata &lt;- tibble( temp = seq(0, 200, by = 10), tf = tf(temp), # Predict c_hat c_hat = predict(m1, newdata = tibble(tf)) %&gt;% exp()) # View the first 3 rows! fakedata %&gt;% head(3) ## # A tibble: 3 × 3 ## temp tf c_hat ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 42.5 15054. ## 2 10 41.0 11987. ## 3 20 39.6 9695. And now, we can plot the line of best fit between temp and c_hat, which are the quantities we actually care about. g2 &lt;- fakedata %&gt;% ggplot(mapping = aes(x = temp, y = c_hat)) + geom_line() + geom_point() # add some labels! g2 + labs(x = &quot;Temperature in Celsius (T)&quot;, y = &quot;Predicted Characteristic Life (c-hat)&quot;, title = &quot;Arrhenius Model of Car Alternator Characteristic Life&quot;, # Here&#39;s a little trick for cleanly adding linebreaks in subtitles: # Just write a paste() function subtitle = paste( &quot;Equation: c = e^(3.1701 + 0.1518 TF) = e^3.1701 * e^(0.1518 * (1 / kT))&quot;, &quot;Model: log(c) = 3.1701 + 0.1518 TF&quot;, sep = &quot;\\n&quot;)) 7.3 Eyring Model (Multiple Stressors) But in our alt data, multiple conditions varied, including tf but also volts and time. These might each have independent effects on log(c)! So let’s estimate their effect by adding some more coefficients to our model! The Eyring Model provides an equation derived from chemical reaction theory and quantum mechanics, which supposes that we can predict lifespan parameters pretty well if we know temperature and any other stresses, such as voltage. The general form of the Eyring Model can also be distilled into a multivariate regression equation. This means instead of drawing a line of best fit approximating the relationship between 2 vectors (log(c) and tf), we can approximate the relationship between 3 vectors (log(c), tf, and volts), making a plane. \\[ \\begin{align*} c =&amp; AT^{\\alpha} \\times e^{\\frac{\\Delta H}{kT}} \\times e^{(B + C/T) \\times S} \\\\ &amp;where: \\ T = Temperature \\ in \\ Kelvin, \\\\ &amp;so: \\ if \\ \\alpha = 0, \\ T^{\\alpha} = 1 \\\\ &amp;also:\\\\ &amp;where: \\ S = Stressor \\ S, \\\\ &amp;and: \\ B + C/T = \\ temperature \\ contigent \\ effect \\ of \\ S\\\\ \\\\ &amp;simplifies \\ to: \\\\ c=&amp;e^{intercept} \\times T^{\\alpha} + e^{\\Delta H \\times (1 / (kT)) \\ + \\ (B + C/T)^S } \\\\ \\\\ &amp; so \\ assuming \\ \\ T^{\\alpha} = 1: \\\\ log(c) =&amp; intercept \\ + \\ \\Delta H \\times (1 / (kT)) + (B + C/T)\\times S \\\\ &amp; can \\ be \\ rewritten \\ as: \\\\ log(c) =&amp; \\alpha \\ + \\ \\beta_1 X_1 + B_2 X_2 + ... \\\\ &amp; where \\ \\alpha = intercept, \\\\ &amp;X_1 = Temperature \\ Factor = TF = 1 / (kT), \\\\ &amp;\\beta_1 = effect \\ of X_1 \\ [Temperature \\ Factor], \\\\ &amp;X_2 = Stressor \\ S, \\\\ &amp;B_2 = net \\ effect \\ of \\ X_2 \\ [Stressor \\ S] \\\\ &amp;... = any \\ other \\ stressors \\end{align*}\\] This is a monster of an equation, but we can still model it relatively simply using a linear model lm() of the log(c). Often, we care about voltage, which can be written as: \\[ \\beta_{2}X_{2} = B \\times (-log(V)), \\ where \\ \\beta_2 = B = effect \\ of \\ -log(Voltage) \\] We can write model this as m2, like so: m2 &lt;- alt %&gt;% lm(formula = log(c) ~ tf + log(volts) ) # See our model equation! m2 ## ## Call: ## lm(formula = log(c) ~ tf + log(volts), data = .) ## ## Coefficients: ## (Intercept) tf log(volts) ## 5.0086 0.1027 -0.1837 Then, we can just supply predict() with any values of tf and volts to predict log(c_hat), and exponeniate the result. fakedata &lt;- tibble( # Hold temperature constant temp = 30, tf = tf(temp), # But vary volts volts = seq(from = 1, to = 30, by = 1), # Predict c_hat c_hat = predict(m2, newdata = tibble(tf, volts)) %&gt;% exp()) And we can visualize our fakedata to see the impact of changing volts on c_hat as temp and tf were held constant. fakedata %&gt;% ggplot(mapping = aes(x = volts, y = c_hat)) + geom_line() + geom_point() + theme_classic(base_size = 14) + labs(title = &quot;Eyring Model of Effect of Voltage on Lifespan (30 Deg. Celsius)&quot;, subtitle = &quot;Equation: c = e^(5.0086 + 0.1027 TF - 0.1837 * log(volts))&quot;, x = &quot;Voltage (volts)&quot;, y = &quot;Predicted Characteristic Life (c-hat)&quot;) 7.4 Degradation Model (Time Trends) One final common impact on lifespan is simple degradation in functioning over time. With each passing day, the product is exposed to a stressor an additional time, bringing it a little closer to whatever we define as failure, and in turn, impacting the estimation of performance across our samples. We can model this as… \\[ \\begin{align*} Q_t =&amp; \\ Q_0 \\times e^{-R(S)t} \\\\ &amp;where: \\\\&amp; Q_t = performance \\ metric \\ at \\ time \\ t, \\\\&amp; Q_0 = starting \\ value \\ at \\ t = 0, \\ and \\\\&amp; R(S) = effect \\ of \\ stressor \\ S \\ \\\\ \\\\&amp; modeled \\ as: \\\\ log(Y_t) =&amp; \\ \\alpha + \\beta X \\times t \\\\ &amp;where: \\\\&amp;log(Y_t) = log(Q_t), \\\\ &amp; \\alpha = intercept = log(Q_0), \\\\&amp; \\beta = -R = degradation \\ effect \\ of \\ S,\\\\&amp; X = stressor \\ S \\ (sometimes \\ excluded), \\ and \\\\ &amp;t = time \\ t \\\\&amp;or: \\\\ log(Y_t) =&amp; \\ \\alpha + \\beta \\times t \\\\&amp; where: \\beta = overall \\ degradation \\ effect \\ of \\ time \\ t \\end{align*} \\] Let’s try this out using our alt data! We might expect the power rating of an alternator might decline over time. We could model the overall degradation effect on the log(rating) by regressing a single vector time against our model. This produces a great fit of ~94%. alt %&gt;% lm(formula = log(rating) ~ time) %&gt;% glance() %&gt;% select(r.squared) ## # A tibble: 1 × 1 ## r.squared ## &lt;dbl&gt; ## 1 0.940 Alternatively, if the degradation effect depends on another condition, like how much voltage was run on it, we could write an interaction effect using I(volts * time). This forces our model to be written as \\(log(Y) = \\alpha + \\beta X \\times t\\) instead of just \\(log(Y) = \\alpha + \\beta \\times t\\). alt %&gt;% lm(formula = log(rating) ~ I(volts * time) ) ## ## Call: ## lm(formula = log(rating) ~ I(volts * time), data = .) ## ## Coefficients: ## (Intercept) I(volts * time) ## 4.825e+00 -3.497e-05 We could even apply these effects right into our estimation of characteristic life \\(\\hat{c}\\) for our alternators! Perhaps we think the characteristic life would tend to be lower if the sample were measured later in time, after being exposed to higher volts. Let’s estimate this model as m3! m3 &lt;- alt %&gt;% lm(formula = log(c) ~ tf + log(volts) + I(volts * time)) # Really good fit! m3 %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.981 0.924 0.0174 17.3 0.174 3 17.2 ## # ℹ 5 more variables: AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, ## # df.residual &lt;int&gt;, nobs &lt;int&gt; As you can see, we can build nearly as complex models as you can imagine! A good rule of thumb is to seek to build the most parsimonious (simplest) model that explains the most variation (maximizes the r.squared) Figure 7.2: Wasn’t that fun? 7.5 Burn-in Periods Sometimes, manufacturers will deliberately test-run their products for several hundred hours before selling them, just to ‘burn-in’ to their lifespan distributions. The idea is, if they can force the defective products to fail early on, then the products that remain are less likely to fail. Acceleration Factors can help us identify the ideal burn in period. We can say: \\(z(t)\\) = failure rate curve with no burn-in. \\(z_b(t)\\) = failure rate curve after a burn-in period of \\(t_b\\) hours. \\(a\\) = acceleration factor between normal use vs. stress (burn-in), so: \\(z_b(t = 0) = z(a \\times t_b)\\): failure rate after 0 hours of burn-in \\(z_b(t = 0)\\) should equal the normal failure rate at a time \\(a\\) times greater than the burn-in period of \\(t_b\\) hours. In other words, we can use the acceleration factor \\(a\\) to project the conditional probability of failure after surviving burn-in. We can say, hey, what’s the probability of failure under normal use \\(t\\) hours after burn-in \\(F_b(t)\\), given that we know it survived up through the burn-in period \\(t_b\\) and its use-to-stress relationship is characterized by an acceleration factor of \\(a\\)? # Let&#39;s write the Weibull density and failure function, as always... d = function(t, c, m){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) } f = function(t, c, m){ 1 - exp(-1*((t/c)^m)) } Since we can calculate \\(F(t)\\) as f(t,c,m) and \\(f(t)\\) as d(t,c,m), we can write the conditional probability of failure post-burn-in as: \\[ F_b(t) = \\frac{ F(t + a t_b) - F(at_b)}{ 1 - F(at_b) } \\] And we can code it as: fb = function(t, tb, a, c, m){ # Change in probability of failure delta_failure &lt;- f(t = t + a*tb, c, m) - f(t = a*tb, c, m) # Reliability after burn-in period reliability &lt;- 1 - f(t = a*tb, c, m) # conditional probability of failure delta_failure / reliability } Let’s try it! # 1000 hours after burn-in # with a burn-in period of 100 hours # an acceleration factor of 20 # characteristic life c = 2000 hours # and # shape parameter m = 1.5 fb(t = 1000, tb = 100, a = 20, c = 2000, m = 1.5) ## [1] 0.5670432 Likewise, the conditional density function \\(f_b(t)\\) can be written as: \\[ f_b(t) = \\frac{ f(t + at_b)}{ 1 - F(at_b)} \\] # And we&#39;ll write the condition db = function(t, tb, a, c, m){ density &lt;- d(t = t + a*tb, c, m) reliability &lt;- 1 - f(t = a*tb, c, m) # get conditional density density / reliability } Let’s try it! What’s the conditional probability of having a lifespan of 1000 hours, given that you had a burn-in period of 100 hours? Let’s assume an acceleration factor of 20, characteristic life of 2000 hours, and a shape parameter of 1.5, like before. db(t = 1000, tb = 100, a = 20, c = 2000, m = 1.5) ## [1] 0.0003976962 7.6 Maximum Likelihood Estimation (MLE) for Physical Acceleration Models But how in the world did we get all these estimates of c in the first place? Often, we end up calculating it via maximum likelihood estimation (MLE) from cross-tabulated readout data. Suppose we have several cross-tabulations of readout data available to us about the occurrence of failure for wheels by temperature. We know these are Weibull distributed, but we don’t know the distributions’ parameters for each temperature level, let alone the \\(\\Delta H\\) or the Acceleration Factor \\(AF\\)! The examples below will focus on samples of car wheels, each with a Weibull distribution, but they are equally applicable to other distributions. # Let&#39;s write Weibull density, failure, and reliability functions d = function(t, c, m){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) } f = function(t, c, m){ 1 - exp(-1*((t/c)^m)) } r = function(t, c, m){ 1 - f(t,c,m) } 7.6.1 MLE for an Example Arrhenius Model # Let&#39;s load in our crosstable wheels &lt;- tibble( label = c(&quot;[0,1000]&quot;, &quot;(1000,2000]&quot;, &quot;(2000,3000]&quot;, &quot;(3000,4000]&quot;, &quot;(4000,5000]&quot;), t = c(500, 1500, 2500, 3500, 4500), temp_100 = c(106, 66, 22, 4, 2), # out of 200 wheels temp_150 = c(125, 25, 25, 15, 10), # out of 175 wheels temp_200 = c(140, 30, 15, 10, 15)) # out of 300 wheels # Check it! wheels ## # A tibble: 5 × 5 ## label t temp_100 temp_150 temp_200 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [0,1000] 500 106 125 140 ## 2 (1000,2000] 1500 66 25 30 ## 3 (2000,3000] 2500 22 25 15 ## 4 (3000,4000] 3500 4 15 10 ## 5 (4000,5000] 4500 2 10 15 We can write a maximum likelihood estimation function to find the most likely parameters for our products stress tested at 100 degrees Celsius, using optim() to perform MLE. # Let&#39;s write our crosstable&#39;s likelihood function ll = function(t, x, par){ r = sum(x) # Get total failures n = 200 # Record total sampel size tmax = max(t) # Record last time step # Get the product of the log-densities at each time step, for all failures then prob_d = ((d(t, c = par[1], m = par[2]) %&gt;% log()) * x) %&gt;% sum() # For the last time step, get the probability of each remaining unit surviving prob_r = r(t = tmax, c = par[1], m = par[2])^(n - r) %&gt;% log() # Get joint log-likelihood prob_d + prob_r } # And let&#39;s run MLE! mle100 &lt;- optim(par = c(1000, 1), t = wheels$t, x = wheels$temp_100, fn = ll, control = list(fnscale = -1)) # Our characteristic life and shape parameter m! mle100$par ## [1] 1287.316800 1.511327 But doesn’t it seem like a waste that we have all this data at multiple temperature readouts, but we’re just relying on one temperature to estimate parameters? We can do better! Let’s write a maximum likelihood estimator that maximizes more parameters! We’ll assume that the shape parameter \\(m\\) is the same for each distribution, but the characteristic life varies (a common assumption in physical acceleration models). # Let&#39;s write our crosstable&#39;s likelihood function ll = function(t, x1, x2, x3, par){ # Get total failures r1 = sum(x1) r2 = sum(x2) r3 = sum(x3) # Record total sample size in each n1 = 200 n2 = 175 n3 = 300 tmax = max(t) # Record last time step # Get the product of the log-densities at each time step, for all failures then prob_d1 = ((d(t, c = par[1], m = par[4]) %&gt;% log()) * x1) %&gt;% sum() prob_d2 = ((d(t, c = par[2], m = par[4]) %&gt;% log()) * x2) %&gt;% sum() prob_d3 = ((d(t, c = par[3], m = par[4]) %&gt;% log()) * x3) %&gt;% sum() # For the last time step, get the probability of each remaining unit surviving prob_r1 = r(t = tmax, c = par[1], m = par[4])^(n1 - r1) %&gt;% log() prob_r2 = r(t = tmax, c = par[2], m = par[4])^(n2 - r2) %&gt;% log() prob_r3 = r(t = tmax, c = par[3], m = par[4])^(n3 - r3) %&gt;% log() # Get joint log-likelihood, across ALL vectors prob_d1 + prob_r1 + prob_d2 + prob_r2 + prob_d3 + prob_r3 } # And let&#39;s run MLE! mle &lt;- optim(par = c(1000, 1000, 1000, 1), t = wheels$t, x1 = wheels$temp_100, x2 = wheels$temp_150, x3 = wheels$temp_200, fn = ll, control = list(fnscale = -1)) # Check out our 3 characteristic life parameters, # for temp_100, temp_150, and temp_200, and our shared shape parameter! mle$par ## [1] 794.06372 1491.73881 1747.74451 1.02723 We can apply MLE to estimate as many parameters as our computers and patience for coding functions will allow! 7.6.2 Estimating \\(\\Delta H\\) with MLE Next, let’s use our MLE values to estimate \\(\\Delta H\\), the impact of temperature on lifespan parameters. # Remember our function to calculate temperature factors tf = function(temp){ 1 / ((1 / 11605) * (temp + 273.15)) } # Let&#39;s collect our parameter estimates param &lt;- tibble( # For each temperature temp = c(100, 150, 200), # report the MLE c estimates c = mle$par[1:3], # and the shared MLE m estimate m = mle$par[4], # and Calculate TF (for each temperature...) # This will be our independent variable tf = tf(temp)) # Check it! param ## # A tibble: 3 × 4 ## temp c m tf ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 794. 1.03 31.1 ## 2 150 1492. 1.03 27.4 ## 3 200 1748. 1.03 24.5 Now, we’ve got three different \\(c\\) estimates. We’d really like to project, for any temperature temp, what would the characteristic life c be? Fortunately, we know we can estimate that with a line of best fit. m4 &lt;- param %&gt;% lm(formula = log(c) ~ tf) # Pretty good fit (93%) m4 %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.932 0.863 0.154 13.6 0.168 1 3.00 ## # ℹ 5 more variables: AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, ## # df.residual &lt;int&gt;, nobs &lt;int&gt; Now that we’ve built a fairly good model, we can use it to predict the characteristic life \\(c\\) for any temperature temp. And if we have c_hat(), then all of a sudden, we can calculate the probability of failure at any time t! Suppose our wheel was used at 30 degrees Celsius. Our model projects a probability of failure of 32% by the 100th hour! (Don’t buy that wheel!) tibble( t = 100, temp = 10, tf = tf(temp), # Predict c-hat for our specified temperature c_hat = predict(m4, newdata = tibble(tf)) %&gt;% exp(), # Grab our MLE estimate of m m = mle$par[4], # And calculate the probability of failure, given a use temperature of 30 prob_f = f(t, c = c_hat, m = m) ) 7.6.3 Visualizing Probabilities We can even then make some rad graphs! For example, we can vary time but hold constant temperature to calculate the probability of failure over time at a specific temperature. tibble( t = seq(0, 2000, by = 10), tf = tf(temp = 30), # Get parameters c_hat = predict(m4, newdata = tibble(tf)) %&gt;% exp(), m = mle$par[4], # Calculate Probability prob_f = f(t, c = c_hat, m = m) ) %&gt;% ggplot(mapping = aes(x = t, y = prob_f)) + geom_area() + labs(x = &quot;Time&quot;, y = &quot;Failure Function F(t)&quot;, subtitle = &quot;Probability of Failure at 30 Degrees Celsius&quot;) Or, we can hold constant time but vary temperature, to show the changing probability of failure given different stress levels by temperature. tibble( t = 1000, temp = seq(0, 200, by = 10), tf = tf(temp), # Get parameters c_hat = predict(m4, newdata = tibble(tf)) %&gt;% exp(), m = mle$par[4], # Calculate Probability prob_f = f(t, c = c_hat, m = m) ) %&gt;% ggplot(mapping = aes(x = temp, y = prob_f)) + geom_area() + labs(x = &quot;Temperature (Celsius)&quot;, y = &quot;Failure Function F(t)&quot;, subtitle = &quot;Probability of Failure after 1000 hours&quot;) 7.7 Conclusion All done! You have covered several major models for estimating change in lifespan parameters! Hooray! "],["workshop-statistical-process-control-in-r.html", "8 Workshop: Statistical Process Control in R Getting Started 8.1 Visualizing Quality Control LC 1 8.2 Average and Standard Deviation Graphs LC 2 8.3 Moving Range Charts 8.4 Constants LC 3", " 8 Workshop: Statistical Process Control in R Figure 8.1: Statistical Process Control! In this workshop, we will learn how to perform statistical process control in R, using statistical tools and ggplot visualizations! Statistical Process Control refers to using statistics to (1) measure variation in product quality over time and (2) identify benchmarks to know when intervention is needed. Let’s get started! Getting Started Packages We’ll be using the tidyverse package for visualization, viridis for color palletes, moments for descriptive statistics, plus ggpubr for some add-on functions in ggplot. library(tidyverse) library(viridis) # you&#39;ll probably need to install these packages! # install.packages(c(&quot;ggpubr&quot;, &quot;moments&quot;)) library(ggpubr) library(moments) Our Case Figure 8.2: Obanazawa City, Yamagata Prefecture - A Hot Springs Economy. Photo credit and more here. For today’s workshop, we’re going to think about why quality control matters in a local economy, by examining the case of the Japanese Hot Springs bath economy! Hot springs, or onsen, are a major source of tourism and recreation for families in Japan, bringing residents from across the country every year to often rural communities where the right geological conditions have brought on naturally occurring hot springs. Restaurants, taxi and bus companies, and many service sector firms rely on their local onsen to bring in a steady stream (pun intended) of tourists to the local economy. So, it’s often in the best interest of onsen operators to keep an eye on the temperature, minerals, or other aspects of their hot springs baths to ensure quality control, to keep up their firm (and town’s!) reputation for quality rest and relaxation! Onsen-goers often seek out specific types of hot springs, so it’s important for an onsen to actually provide what it advertises! Serbulea and Payyappallimana (2012) describe some of these benchmarks. Temperature: Onsen are divided into “Extra Hot Springs” (&gt;42 °C), “Hot Springs” (41~34°C), and “Warm Springs” (33~25°C). pH: Onsen are classified into “Acidic” (pH &lt; 3), “Mildly Acidic” (pH 3~6), “Neutral” (pH 6~7.5), “Mildly alkaline” (ph 7.5~8.5), and “Aklaline” (pH &gt; 8.5). Sulfur: Sulfur onsen typically have about 2mg of sulfur per 1kg of hot spring water; sulfur levels must exceed 1 mg to count as a Sulfur onsen. (It smells like rotten eggs!) These are decent examples of quality control metrics that onsen operators might want to keep tabs on! Figure 8.3: Monkeys are even fans of onsen! Read more here! Our Data You’ve been hired to evaluate quality control at a local onsen in sunny Kagoshima prefecture! Every month, for 15 months, you systematically took 20 random samples of hot spring water and recorded its temperature, pH, and sulfur levels. How might you determine if this onsen is at risk of slipping out of one sector of the market (eg. Extra Hot!) and into another (just normal Hot Springs?). Let’s read in our data from workshops/onsen.csv! # Let&#39;s import our samples of bathwater over time! water = read_csv(&quot;workshops/onsen.csv&quot;) # Take a peek! water %&gt;% glimpse() ## Rows: 160 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, … ## $ time &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ temp &lt;dbl&gt; 43.2, 45.3, 45.5, 43.9, 45.9, 45.0, 42.3, 44.2, 42.… ## $ ph &lt;dbl&gt; 5.1, 4.8, 6.2, 6.4, 5.1, 5.6, 5.5, 5.3, 5.2, 5.9, 5… ## $ sulfur &lt;dbl&gt; 0.0, 0.4, 0.9, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0… Our dataset contains: id: unique identifer for each sample of onsen water. time: categorical variable describing date of sample (month 1, month 3, … month 15). temp: temperature in celsius. ph: pH (0 to 14) sulfur: milligrams of sulfur ions. 8.1 Visualizing Quality Control Let’s learn some key techniques for visualizing quality control! 8.1.1 theme_set() First, when you’re about to make a bunch of ggplot visuals, it can help to set a common theme across them all with theme_set(). # By running theme_set() theme_set( # we tell ggplot to give EVERY plot this theme theme_classic(base_size = 14) + # With these theme traits, including theme( # Putting the legend on the bottom, if applicable legend.position = &quot;bottom&quot;, # horizontally justify plot subtitle and caption in center plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0.5), # Getting rid of busy axis ticks axis.ticks = element_blank(), # Getting rid of busy axis lines axis.line = element_blank(), # Surrounding the plot in a nice grey border panel.border = element_rect(fill = NA, color = &quot;grey&quot;), # Remove the right margin, for easy joining of plots plot.margin = margin(r = 0) ) ) 8.1.2 Process Descriptive Statistics First, let’s describe our process, using favorite description statistics. We’re going to want to do this a bunch, so why don’t we just write a function for it? Let’s write describe(), which will take a vector x and calculate the mean(), sd(), skewness(), and kurtosis(), and then paste() together a nice caption describing them. (I encourage you to write your own functions like this to help expedite your coding! Start simple!) describe = function(x){ # Put our vector x in a tibble tibble(x) %&gt;% # Calculate summary statistics summarize( mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE), # We&#39;ll use the moments package for these two skew = skewness(x, na.rm = TRUE), kurtosis = kurtosis(x, na.rm = TRUE)) %&gt;% # Let&#39;s add a caption, that compiles all these statistics mutate( # We&#39;ll paste() the following together caption = paste( # Listing the name of each stat, then reporting its value and rounding it, then separating with &quot; | &quot; &quot;Process Mean: &quot;, mean %&gt;% round(2), &quot; | &quot;, &quot;SD: &quot;, sd %&gt;% round(2), &quot; | &quot;, &quot;Skewness: &quot;, skew %&gt;% round(2), &quot; | &quot;, &quot;Kurtosis: &quot;, kurtosis %&gt;% round(2), # Then make sure no extra spaces separate each item sep = &quot;&quot;)) %&gt;% return() } # Run descriptives! tab = water$temp %&gt;% describe() # Check it out! tab ## # A tibble: 1 × 5 ## mean sd skew kurtosis caption ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 44.8 1.99 0.0849 3.62 Process Mean: 44.85 | SD: 1.99 | Skew… 8.1.3 Process Overview Visual Your first step should always be to look at the data overall! geom_jitter(), geom_boxplot(), and ggMarginal() can help you do this. geom_jitter() is a jittered scatterplot, jittering points a little to help with visibility. Since we want to be really precise on our quality control metrics, we could jitter the width a little (width = 0.25), but hold the y-axis (quality metric) constant at height = 0. These are in your x and y axis units, so decide based on your data each time. geom_boxplot() makes a boxplot for each group (time) in our data, showing the interquartile range (25th, 50th, and 75th percentiles) of our y variable for each group. Helpful way to view distributions. (Alternatively, you can try geom_violin(), which works the same way.) geom_hline() makes a horizontal line at the yintercept; we can tell it to show the mean() of y (in this case, temp). geom_histogram() is a histogram! We can use coord_flip() to turn it vertical to match the y-axis. ggarrange() from the ggpubr package binds two plots together into one, giving each a specific proportional width (eg. c(0.25, 0.75) percent, or c(5, 1) would be 5/6 and 1/6.) # Make the initial boxplot... g1 = water %&gt;% ggplot(mapping = aes(x = time, y = temp, group = time)) + # Plot grand mean geom_hline(mapping = aes(yintercept = mean(temp)), color = &quot;lightgrey&quot;, size = 3) + # Plot points and boxplots geom_jitter(height = 0, width = 0.25) + geom_boxplot() + labs(x = &quot;Time (Subgroup)&quot;, y = &quot;Temperature (Celsius)&quot;, subtitle = &quot;Process Overview&quot;, # Add our descriptive stats in the caption! caption = tab$caption) # Part 1 of plot g1 # Make the histogram, but tilt it on its side g2 = water %&gt;% ggplot(mapping = aes(x = temp)) + geom_histogram(bins = 15, color = &quot;white&quot;, fill = &quot;grey&quot;) + theme_void() + # Clear the them coord_flip() # tilt on its side # Part 2 of plot g2 # Then bind them together into 1 plot, &#39;h&#39;orizontally aligned. p1 = ggarrange(g1,g2, widths = c(5,1), align = &quot;h&quot;) # Check it out! p1 We can tell from this visual several things! Side Histogram: Our overall distribution is pretty centered. Descriptive Statistics: Our distribution has little skew (~0) and has slightly higher-than-average kurtosis (&lt;3) (very centered) (Review Skewness and Kurtosis here.) Line vs. Boxplots: Over time, our samples sure do seem to be getting slightly further from the mean! LC 1 Question We analyzed temperature variation above, but our hot springs owner wants to know about variation in pH too! Write a function to produce a process overview plot given any 2 vectors (water$time and water$pH, in this case), and visualize the process overview for pH! (You can do it!) Answer ggprocess = function(x, y, xlab = &quot;Subgroup&quot;, ylab = &quot;Metric&quot;){ # Get descriptive statistics tab = describe(y) # Make the initial boxplot... g1 = ggplot(mapping = aes(x = x, y = y, group = x)) + # Plot grand mean geom_hline(mapping = aes(yintercept = mean(y)), color = &quot;lightgrey&quot;, size = 3) + # Plot points and boxplots geom_jitter(height = 0, width = 0.25) + geom_boxplot() + labs(x = xlab, y = ylab, subtitle = &quot;Process Overview&quot;, # Add our descriptive stats in the caption! caption = tab$caption) # Make the histogram, but tilt it on its side g2 = ggplot(mapping = aes(x = y)) + geom_histogram(bins = 15, color = &quot;white&quot;, fill = &quot;grey&quot;) + theme_void() + # Clear the them coord_flip() # tilt on its side # Then bind them together into 1 plot, &#39;h&#39;orizontally aligned. p1 = ggarrange(g1,g2, widths = c(5,1), align = &quot;h&quot;) return(p1) } # Visualize it! ggprocess(x = water$time, y = water$ph) In comparison to temp, pH is a much more controlled process. I encourage you to use this ggprocess() function you just created! s 8.2 Average and Standard Deviation Graphs 8.2.1 Key Statistics Next, to analyze these processes more in depth, we need to assemble statistics at 2 levels: within-group statistics measure quantities of interest within each subgroup (eg. each monthly slice time in our onsen data). between-group statistics measure total quantities of interest for the overall process (eg. the overall “grand” mean, overall standard deviation in our onsen data). 8.2.2 Subgroup (Within-Group) Statistics Let’s apply these to our onsen data to get statistics describing each subgroup’s distribution, a.k.a. short-term or within-group statistics. # Calculate short-term statistics within each group stat_s = water %&gt;% # For each timestpe group_by(time) %&gt;% # Calculate these statistics of interest! summarize( # within-group mean xbar = mean(temp), # within-group range r = max(temp) - min(temp), # within-group standard deviation sd = sd(temp), # within-group sample size nw = n(), # Degrees of freedom within groups df = nw - 1) %&gt;% # Last, we&#39;ll calculate sigma_short (within-group variance) # We&#39;re going to calculate the short-term variation parameter sigma_s (sigma_short) # by taking the square root of the average of the standard deviation # Essentially, we&#39;re weakening the impact of any special cause variation # so that our sigma is mostly representative of common cause (within-group) variation mutate( # these are equivalent sigma_s = sqrt( sum(df * sd^2) / sum(df) ), sigma_s = sqrt(mean(sd^2)), # And get standard error (in a way that retains each subgroup&#39;s sample size!) se = sigma_s / sqrt(nw), # Calculate 6-sigma control limits! upper = mean(xbar) + 3*se, lower = mean(xbar) - 3*se) # Check it! stat_s %&gt;% head(3) ## # A tibble: 3 × 10 ## time xbar r sd nw df sigma_s se upper lower ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 44.6 4.20 1.34 20 19 1.99 0.444 46.2 43.5 ## 2 3 45.3 7.9 2.00 20 19 1.99 0.444 46.2 43.5 ## 3 5 44.8 5.90 1.63 20 19 1.99 0.444 46.2 43.5 8.2.3 Total Statistics (Between Groups) # To get between-group estimates.... stat_t = stat_s %&gt;% summarize( xbbar = mean(xbar), rbar = mean(r), sdbar = mean(sd), # We can also recalculate sigma_short here too sigma_s = sqrt( mean(sd^2) ), # Or we can calculate overall standard deviation sigma_t = sd(water$temp) ) So, now that we have estimated within-group, common cause variation via \\(\\sigma_{short}\\) (sigma_s) and the standard error (se), what can we say about our process? 8.2.4 Average and Standard Deviation Charts The preferred method for measuring within-group variability is the standard deviation, rather than the range, so we generally recommend (a) Average (\\(\\bar{X}\\)) and Standard Deviation (\\(S\\)) charts over (b) Average (\\(\\bar{X}\\)) and Range (\\(R\\)) charts. # Let&#39;s extract some labels labels = stat_s %&gt;% summarize( time = max(time), type = c(&quot;xbbar&quot;, &quot;upper&quot;, &quot;lower&quot;), name = c(&quot;mean&quot;, &quot;+3 s&quot;, &quot;-3 s&quot;), value = c(mean(xbar), unique(upper), unique(lower)), value = round(value, 2), text = paste(name, value, sep = &quot; = &quot;)) stat_s %&gt;% ggplot(mapping = aes(x = time, y = xbar)) + geom_hline(mapping = aes(yintercept = mean(xbar)), color = &quot;lightgrey&quot;, size = 3) + geom_ribbon(mapping = aes(ymin = lower, ymax = upper), fill = &quot;steelblue&quot;, alpha = 0.2) + geom_line(size = 1) + geom_point(size = 5) + # Plot labels geom_label(data = labels, mapping = aes(x = time, y = value, label = text), hjust = 1) + labs(x = &quot;Time (Subgroups)&quot;, y = &quot;Average&quot;, subtitle = &quot;Average and Standard Deviation Chart&quot;) This tells us that excitingly, our onsen temperatures are quite firmly within range. While the average varies quite a bit, it remains comfortably within 3 standard deviations of the mean. LC 2 Question Well, that was nifty, but can we do it all over again for pH? Make some rad upper and lower confidence intervals for \\(\\bar{X}\\) for pH! Answer # Get the within-group stats for ph! ph_s = water %&gt;% group_by(time) %&gt;% summarize( xbar = mean(ph), r = max(ph) - min(ph), sd = sd(ph), nw = n(), df = nw - 1) %&gt;% mutate( sigma_s = sqrt(mean(sd^2)), se = sigma_s / sqrt(nw), upper = mean(xbar) + 3*se, lower = mean(xbar) - 3*se) # Let&#39;s extract some labels labels = ph_s %&gt;% summarize( time = max(time), type = c(&quot;xbbar&quot;, &quot;upper&quot;, &quot;lower&quot;), name = c(&quot;mean&quot;, &quot;-3 s&quot;, &quot;+3 s&quot;), value = c(mean(xbar), unique(upper), unique(lower)), value = round(value, 2), text = paste(name, value, sep = &quot; = &quot;)) # and let&#39;s visualize it! ph_s %&gt;% ggplot(mapping = aes(x = time, y = xbar)) + geom_hline(mapping = aes(yintercept = mean(xbar)), color = &quot;lightgrey&quot;, size = 3) + geom_ribbon(mapping = aes(ymin = lower, ymax = upper), fill = &quot;steelblue&quot;, alpha = 0.2) + geom_line(size = 1) + geom_point(size = 5) + # Plot labels geom_label(data = labels, mapping = aes(x = time, y = value, label = text), hjust = 1) + labs(x = &quot;Time (Subgroups)&quot;, y = &quot;Average pH&quot;, subtitle = &quot;Average and Standard Deviation Chart&quot;) 8.3 Moving Range Charts 8.3.1 Individual and Moving Range Charts Suppose we only had 1 observation per subgroup! There’s no way to calculate standard deviation for that - after all, there’s no variation within each subgroup! Instead, we can generate an individual and moving range chart. # Suppose we sample just the first out of each our months. indiv = water %&gt;% filter(id %in% c(1, 21, 41, 61, 81, 101, 121, 141)) The average moving range \\(m\\bar{R}\\) aptly refers to the average of the moving Range \\(mR\\), the difference in values over time. We can calculate the moving range using the diff() function on a vector like temp, shown below. abs() converts each value to be positive, since ranges are always 0 to infinity. # Let&#39;s see our original values indiv$temp ## [1] 43.2 46.0 46.6 42.1 44.4 46.8 43.7 45.9 # diff() gets range between second and first, third and second, and so on indiv$temp %&gt;% diff() %&gt;% abs() ## [1] 2.8 0.6 4.5 2.3 2.4 3.1 2.2 Just like all statistics, \\(m\\bar{R}\\) too has its own distribution, containing a range of slightly higher and lower \\(m\\bar{R}\\) statistics we might have gotten had our sample been just slightly different due to chance. As a result, we will want to estimate a confidence interval around \\(m\\bar{R}\\), but how are we to do that if we have no statistic like \\(\\sigma\\) to capture that variation? Well, good news: We can approximate \\(\\sigma_s\\) by taking the ratio of \\(m\\bar{R}\\) over a factor called \\(d_{2}\\). What is \\(d_{2}\\)? I’m glad you asked! \\[\\sigma_{short} \\approx \\frac{m\\bar{R}}{d_{2}} \\] 8.3.2 Factors \\(d_{2}\\) and Friends As discussed above, any statistics has a latent distribution of other values you might have gotten for your statistic had your sample been just slightly different due to random error. Fun fact: we can actually see those distributions pretty easily thanks to simulation! Suppose we have a subgroup of size n = 1, so we calculate a moving range of length 1. This subgroup and its moving range is just one of the possible subgroups we could have encountered by chance, so we can think of it as a random draw from an archetypal normal distribution (mean = 0 and sd = 1). If we take enough samples of moving ranges from that distribution, we can plot the distribution. Below, we take n = 10000 samples with rnorm() and plot the vector with hist(). We find a beautiful distribution of moving range statistics for n=1 size subgroups. mrsim = rnorm(n = 10000, mean = 0, sd = 1) %&gt;% diff() %&gt;% abs() mrsim %&gt;% hist() Much like \\(k\\) factors in the exponential distribution, we can use this distribution of \\(mR\\) stats to produce a series of factors that can help us estimate any upper or lower confidence interval in a moving range distribution. For example, we can calculate: \\(d_{2}\\), the mean of this archetypal \\(mR\\) distribution. \\(d_{3}\\), the standard deviation of this \\(mR\\) distribution. Technically, \\(d_{2}\\) is a ratio, which says that in a distribution with a standard deviation of 1, the mean mR is \\(d_{2}\\). In other words, \\(d_{2} = \\frac{m\\bar{R}_{normal, n = 1} }{\\sigma_{normal,n=1} }\\). So, if we have observed a real life average moving range \\(m\\bar{R}_{observed,n=1}\\), we can use this \\(d_{2}\\) factor to convert out of units of \\(1 \\sigma_{normal,n=1}\\) into units the \\(\\sigma_{short}\\) of our observed data! # For example, the mean of our vector mrsim, for subgroup size n = 1, # says that d2 (mean of these mR stats) is... mrsim %&gt;% mean() ## [1] 1.122634 # While d3 (standard deviation is...) mrsim %&gt;% sd() ## [1] 0.8469419 But why stop there? We can calculate loads of other interesting statistics! # For example, these statistics # estimate the median, upper 90, and upper 95% of the distribution! mrsim %&gt;% quantile(probs = c(0.50, 0.90, .95)) %&gt;% round(3) ## 50% 90% 95% ## 0.944 2.320 2.749 8.3.3 Estimating \\(\\sigma_{short}\\) for Moving Range Statistics Let’s apply our new knowledge about \\(d_{2}\\) to calculate some upper bounds (\\(+3 \\sigma\\)) for our average moving range estimates! istat_s = indiv %&gt;% summarize( time = time[-1], # get moving range mr = temp %&gt;% diff() %&gt;% abs(), # Get average moving range mrbar = mean(mr), # Get total sample size! d2 = rnorm(n = 10000, mean = 0, sd = 1) %&gt;% diff() %&gt;% abs() %&gt;% mean(), # If we approximate sigma_s.... # pretty good! sigma_s = mrbar / d2, # Our subgroup size was 1, right? n = 1, # so this means sigma_s just equals the standard error here se = sigma_s / sqrt(n), # compute upper 3-se bound upper = mrbar + 3 * se, # and lower ALWAYS equals 0 for moving range lower = 0) Why stop there? Let’s visualize it! # Let&#39;s get our labels! labels = istat_s %&gt;% summarize( time = max(time), type = c(&quot;mean&quot;, &quot;+3 s&quot;, &quot;lower&quot;), value = c(mrbar[1], upper[1], lower[1]) %&gt;% round(2), name = paste(type, value, sep = &quot; = &quot;)) # Now make the plot! istat_s %&gt;% ggplot(mapping = aes(x = time, y = mr)) + # Plot the confidence intervals geom_ribbon(mapping = aes(x = time, ymin = lower, ymax = upper), fill = &quot;steelblue&quot;, alpha = 0.25) + # Plot mrbar geom_hline(mapping = aes(yintercept = mean(mr)), size = 3, color = &quot;darkgrey&quot;) + # Plot moving range geom_line(size = 1) + geom_point(size = 5) + geom_label(data = labels, mapping = aes(x = time, y = value, label = name), hjust = 1) + labs(x = &quot;Time (Subgroup)&quot;, y = &quot;Moving Range&quot;, subtitle = &quot;Moving Range Chart&quot;) Again, this process looks pretty sound, firmly within range. 8.4 Constants 8.4.1 Find any \\(d_x\\) Factor While our book writes extensively about \\(d_{2}\\) and other \\(d_{whatever}\\) factors, it’s not strictly necessary to calculate them unless you need them. Usually, we do this when we can’t calculate the standard deviation normally (eg. when we have only moving range statistics or only a subgroup sample size of n=1). If you’re working with full data from your process though, you can easily calculate \\(\\sigma_{short}\\) right from the empirical data, without ever needing to use \\(d_x\\) factors. But let’s say you did need a \\(d_x\\) factor for a subgroup range of a given sample size n = 1, 2, 3.... n. Could we calculate some kind of function to give us it? Funny you should ask! I’ve written a little helper function you can use. # Let&#39;s calculate our own d function dn = function(n, reps = 1e4){ # For 10,0000 reps tibble(rep = 1:reps) %&gt;% # For each rep, group_by(rep) %&gt;% # Simulate the ranges of n values summarize(r = rnorm(n = n, mean = 0, sd = 1) %&gt;% range() %&gt;% diff() %&gt;% abs()) %&gt;% ungroup() %&gt;% # And calculate... summarize( # Mean range d2 = mean(r), # standard deviation of ranges d3 = sd(r), # and constants for obtaining lower and upper ci for rbar D3 = 1 - 3*(d3/d2), # sometimes written D3 D4 = 1 + 3*(d3/d2), # sometimes written D4 # Sometimes D3 goes negative; we need to bound it at zero D3 = if_else(D3 &lt; 0, true = 0, false = D3) ) %&gt;% return() } # Let&#39;s try it, where subgroup size is n = 2 dn(n = 2) ## # A tibble: 1 × 4 ## d2 d3 D3 D4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.12 0.852 0 3.27 # Let&#39;s get the constants we need too. # Each of our samples has a sample size of 20 d = dn(n = 20) # Check it! d ## # A tibble: 1 × 4 ## d2 d3 D3 D4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.75 0.731 0.415 1.59 8.4.2 Using \\(d_x\\) factors Using d_n(), we can make a quick approximation for the upper and lower control limits for the range \\(\\bar{R}\\) as well (as opposed to \\(m\\bar{R}\\))! # Let&#39;s get within group range for temperature... stat_w = water %&gt;% group_by(time) %&gt;% summarize(r = temp %&gt;% range() %&gt;% diff() %&gt;% abs(), n_w = n()) # get subgroup size # Let&#39;s get average within group range for temperature... stat = stat_w %&gt;% summarize(rbar = mean(r), # get Rbar n_w = unique(n_w)) # assuming constant subgroup size... # Check it! stat ## # A tibble: 1 × 2 ## rbar n_w ## &lt;dbl&gt; &lt;int&gt; ## 1 7.26 20 # We find that dn() gives us constants D3 and D4... mydstat = dn(n = stat$n_w) mydstat ## # A tibble: 1 × 4 ## d2 d3 D3 D4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.74 0.730 0.414 1.59 And use these constants to estimate the upper and lower CI for \\(\\bar{r}\\)! stat %&gt;% mutate(rbar_lower = rbar * mydstat$D3, rbar_upper = rbar * mydstat$D4) %&gt;% select(rbar, rbar_lower, rbar_upper) ## # A tibble: 1 × 3 ## rbar rbar_lower rbar_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.26 3.00 11.5 So quick! You could use these values to make a range chart now. remove(stat, stat_w) 8.4.3 Finding any \\(b_x\\) Factor. We might also want to know how much the standard deviation could possible vary due to sampling error. To figure this out, we’ll simulate many many standard deviations from a normal distribution, like in dn(), for a given subgroup size n. Then, we can calculate some quantities of interest like \\(C_{4}\\) (the mean standard deviation from an archetypal normal distribution), \\(B_{3}\\) (a multiplier for getting the lower control limit for 3 sigmas), and \\(B_{4}\\) (a multiplier for getting the upper control limit for 3 sigmas.) # Let&#39;s write a function bn() to calculate our B3 and B4 statistics for any subgroup size n bn = function(n, reps = 1e4){ tibble(rep = 1:reps) %&gt;% group_by(rep) %&gt;% summarize(s = rnorm(n, mean = 0, sd = 1) %&gt;% sd()) %&gt;% summarize(b2 = mean(s), b3 = sd(s), C4 = b2, # this is sometimes called C4 A3 = 3 / (b2 * sqrt( n )), B3 = 1 - 3 * b3/b2, B4 = 1 + 3 * b3/b2, # bound B3 at 0, since we can&#39;t have a standard deviation below 0 B3 = if_else(B3 &lt; 0, true = 0, false = B3)) %&gt;% return() } Let’s apply this to our temp vector. First, we’ll calculate the standard deviation within each subgroup, saved in stat_w under s. # Let&#39;s get within group standard deviation for temperature... stat_w = water %&gt;% group_by(time) %&gt;% summarize(s = temp %&gt;% sd(), n_w = n()) # get subgroup size stat_w ## # A tibble: 8 × 3 ## time s n_w ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 1.34 20 ## 2 3 2.00 20 ## 3 5 1.63 20 ## 4 7 2.66 20 ## 5 9 2.57 20 ## 6 11 2.02 20 ## 7 13 1.65 20 ## 8 15 1.61 20 Second, we’ll calculate the average standard deviation across subgroups, saved in stat under sbar. # Let&#39;s get average within group range for temperature... stat = stat_w %&gt;% summarize(sbar = mean(s), # get Rbar n_w = unique(n_w)) # assuming constant subgroup size... # Check it! stat ## # A tibble: 1 × 2 ## sbar n_w ## &lt;dbl&gt; &lt;int&gt; ## 1 1.94 20 Third, we’ll get our constants \\(B_{3}\\) and \\(B_{4}\\)! # For a subgroup size of 20... stat$n_w ## [1] 20 # Let&#39;s get our B constants! mybstat = bn(n = stat$n_w) # Check it out! mybstat ## # A tibble: 1 × 6 ## b2 b3 C4 A3 B3 B4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.990 0.161 0.990 0.678 0.511 1.49 Finally, let’s calculate our control limits! stat = stat %&gt;% # Add our constants to the data.frame... mutate(mybstat) %&gt;% # Calculate 3 sigma control limits mutate(sbar_lower = sbar * B3, sbar_upper = sbar * B4) # Check it out! stat %&gt;% select(sbar, sbar_lower, sbar_upper) ## # A tibble: 1 × 3 ## sbar sbar_lower sbar_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.94 0.990 2.88 Now you’re all ready to make a control chart showing variation in the standard deviation! LC 3 Question Using our dn() function above, compile for yourself a short table of the \\(d_{2}\\) and \\(d_{3}\\) factors for subgroups sized 2 to 10. Answer # Let&#39;s bind them together! dx = bind_rows( dn(2), dn(3), dn(4), dn(5), dn(6), dn(7), dn(8), dn(9), dn(10)) %&gt;% mutate(n = 2:10) %&gt;% select(n, d2, d3) # Look at that cool table! dx ## # A tibble: 9 × 3 ## n d2 d3 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 1.13 0.857 ## 2 3 1.69 0.904 ## 3 4 2.06 0.871 ## 4 5 2.32 0.865 ## 5 6 2.53 0.840 ## 6 7 2.69 0.827 ## 7 8 2.86 0.826 ## 8 9 2.98 0.805 ## 9 10 3.08 0.798 All done! Great work! "],["workshop-indices-and-confidence-intervals-for-statistical-process-control-in-r.html", "9 Workshop: Indices and Confidence Intervals for Statistical Process Control in R Getting Started 9.1 Process Capability vs. Stability 9.2 Index Functions LC1 9.3 Confidence Intervals 9.4 CIs for any Index!", " 9 Workshop: Indices and Confidence Intervals for Statistical Process Control in R Figure 9.1: Bootstrapping Sampling Distributions for Statistics!! This workshop extends our toolkit developed in Workshop 9, discussing Process Capability and Stability Indices, and introducing means to calculate confidence intervals for these indices. Getting Started Packages We’ll be using the tidyverse package for visualization, viridis for color palletes, moments for descriptive statistics, plus ggpubr for some add-on functions in ggplot. library(tidyverse) library(viridis) # you&#39;ll probably need to install these packages! # install.packages(c(&quot;ggpubr&quot;, &quot;moments&quot;)) Our Data We’ll be continuing to analyze our quality control data from a local hot springs inn (onsen) in sunny Kagoshima Prefecture, Japan. Every month, for 15 months, you systematically took 20 random samples of hot spring water and recorded its temperature, pH, and sulfur levels. How might you determine if this onsen is at risk of slipping out of one sector of the market (eg. Extra Hot!) and into another (just normal Hot Springs?). Let’s read in our data from workshops/onsen.csv! # Let&#39;s import our samples of bathwater over time! water = read_csv(&quot;workshops/onsen.csv&quot;) # Take a peek! water %&gt;% glimpse() ## Rows: 160 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, … ## $ time &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ temp &lt;dbl&gt; 43.2, 45.3, 45.5, 43.9, 45.9, 45.0, 42.3, 44.2, 42.… ## $ ph &lt;dbl&gt; 5.1, 4.8, 6.2, 6.4, 5.1, 5.6, 5.5, 5.3, 5.2, 5.9, 5… ## $ sulfur &lt;dbl&gt; 0.0, 0.4, 0.9, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0… Our dataset contains: id: unique identifer for each sample of onsen water. time: categorical variable describing date of sample (month 1, month 3, … month 15). temp: temperature in celsius. ph: pH (0 to 14) sulfur: milligrams of sulfur ions. 9.1 Process Capability vs. Stability 9.1.1 Definitions Production processes can be categorized in terms of Capability (does it meet required specifications?) and Stability (is production consistent and predictable?) Both are vital. A capable process delivers goods that can actually perform their function, like a 20-foot ladder that is actually 20 feet! A stable process delivers products with consistent and predictable traits (regardless of whether those traits are good). We need to maximize both process capability and stability to make an effective process, be it in manufacturing, health care, local businesses, or social life! Depending on the shape and stability of our data, we should choose one of the 4 statistics to evaluate our data. 9.1.2 Table of Indices These statistics rely on some combination of (1) the mean \\(\\mu\\), (2) the standard deviation \\(\\sigma\\), and (3) the upper and lower “specification limits”; the specification limits are our expected values \\(E_{upper}\\) and \\(E_{lower}\\), as compared to our actual observed values, summarized by \\(\\mu\\) and \\(\\sigma\\). Index Shape Sided Stability Formula Meaning Capability Indices (How \\(\\textit{could}\\) it perform, if stable?) \\(C_{p}\\) Centered 2-sided Stable \\(\\frac{E_{upper} - E_{lower}}{6 \\sigma_{short}}\\) How many times wider is the expected range than the observed range, assuming it is stable? \\(C_{pk}\\) Uncentered 1-sided Stable \\(\\frac{\\mid E_{limit} - \\mu \\mid}{3 \\sigma_{short}}\\) How many times wider is the expected vs. observed range for the left/right side, assuming it is stable? Process Performance Indices (How is it performing, stable or not?) \\(P_{p}\\) Centered 2-sided Unstable \\(\\frac{E_{upper} - E_{lower}}{6 \\sigma_{total}}\\) How many times wider is the expected vs. observed range, stable or not? \\(P_{pk}\\) Uncentered 1-sided Unstable \\(\\frac{\\mid E_{limit} - \\mu \\mid}{3 \\sigma_{total}}\\) How many times wider is the expected vs. observed range for the left/right side, stable or not? 9.2 Index Functions Let’s do ourselves a favor and write up some simple functions for these. # Capability Index (for centered, normal data) cp = function(sigma_s, upper, lower){ abs(upper - lower) / (6*sigma_s) } # Process Performance Index (for centered, normal data) pp = function(sigma_t, upper, lower){ abs(upper - lower) / (6*sigma_t) } # Capability Index (for skewed, uncentered data) cpk = function(mu, sigma_s, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_s) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_s) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } # Process Performance Index (for skewed, uncentered data) ppk = function(mu, sigma_t, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_t) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_t) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } How might we use these indices to describe our process data? For example, recall that our onsen operator needs to be sure that their hot springs water is consistently falling into the temperature bins for Extra Hot Springs, which start at 42 degrees Celsius (and go as high as 80). Let’s use those as our specification limits (pretty easy-going limits, I might add). Let’s start by calculating our quantities of interest. stat = water %&gt;% group_by(time) %&gt;% summarize( xbar = mean(temp), # Get within group mean sd = sd(temp), # Get within group sigma n_w = n() # Get within subgroup size ) %&gt;% summarize( xbbar = mean(xbar), # Get grand mean sigma_s = sqrt(mean(sd^2)), # Get sigma_short sigma_t = sd(water$temp), # get sigma_total n = sum(n_w), # Get total observations n_w = unique(n_w), # get size of subgroups k = n()) # Get number of subgroups k # Check it! stat ## # A tibble: 1 × 6 ## xbbar sigma_s sigma_t n n_w k ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 44.8 1.99 1.99 160 20 8 9.2.1 Capacity Index \\(C_{p}\\) Our \\(C_{p}\\) Capacity index says, assuming that the distribution is centered and stable, how many times wider are our limits than our approximate observed distribution (\\(6 \\sigma_{short}\\))? mycp = cp(sigma_s = stat$sigma_s, lower = 42, upper = 80) # Check it! mycp ## [1] 3.18871 Great! This says, our observed variation is many times (3.1887098 times) narrower than the expected specification limits. 9.2.2 Process Performance Index \\(P_{p}\\) Our \\(P_{p}\\) Process Performance Index asks, even if the distribution is not stable (meaning it varies not just due to common causes), how many times wider are our specification limits than our approximate observed distribution (\\(6 \\sigma_{total}\\))? mypp = pp(sigma_t = stat$sigma_t, lower = 42, upper = 80) mypp ## [1] 3.183378 Much like before, the specification limit range remains quite bigger than the observed distribution. 9.2.3 Capacity Index \\(C_{pk}\\) Our \\(C_{pk}\\) Capacity index says, assuming the distribution is pretty stable across subgroups, how many times wider is (a) the distance from the tail of interest to the mean than (b) our approximate observed tail (3 sigma)? This always looks at the shorter tail. mycpk = cpk(sigma_s = stat$sigma_s, mu = stat$xbbar, lower = 42, upper = 80) mycpk ## [1] 0.4783065 If we only care about one of the tails, eg. the lower specification limit of 42, which is much closer than the upper limit of 80, we can just write the lower limit only. cpk(sigma_s = stat$sigma_s, mu = stat$xbbar, lower = 42) ## [1] 0.4783065 This says, our observed variation is much wider than the lower specification limit, since \\(C_{pk}\\) is far from 1, which which show equality. 9.2.4 Process Performance Index \\(P_{pk}\\) Our \\(P_{pk}\\) process performance index says, even if the distribution is neither stable nor centered, how much wider is the observed variation \\(3 \\sigma_{total}\\) than the distance from the tail of interest to the mean? We use \\(\\sigma_{total}\\) here to account for instability (considerable variation between subgroups) and one-tailed testing to account for the uncentered distribution. myppk = ppk(sigma_t = stat$sigma_t, mu = stat$xbbar, lower = 42, upper = 80) myppk ## [1] 0.4775067 9.2.5 Equality A final reason why these quantities are neat is that these 4 indices are related; if you know 3 of them, we can always calculate the 4th! See the identity below: \\[P_{p} \\times C_{pk} = P_{pk} \\times C_{p}\\] # whaaaaaat? They&#39;re equal!!!! mypp * mycpk == myppk * mycp ## [1] TRUE LC1 Question Let’s apply this to some tasty examples! A manufacturer of granola bars is aiming for a weight of 2 ounces (oz), plus or minus 0.5 oz. Suppose the standard deviation of our granola bar machine is 0.02 oz, and the mean weight is 2.01 oz. What’s the process capability index? (How many times greater is the expected variation than the observed variation?) Answer lower = 2 + 0.05 upper = 2 - 0.05 sigma = 0.02 mu = 2.01 cp(sigma = 0.02, upper = 2.05, lower = 1.95) ## [1] 0.8333333 cpk(mu = 2.01, sigma = 0.02, lower = 1.95, upper = 2.05) ## [1] 0.6666667 9.3 Confidence Intervals Any statistic is really just 1 of the many possible values of statistics you could have gotten from your sample, had you taken just a slightly different random sample. So, when we calculate our indices, we should be prepared that our indices might vary due to chance (sampling error), so we should build in confidence intervals. This helps us benchmark how trustworthy any given index is. 9.3.1 Confidence Intervals show us Sampling Distributions Let’s quickly go over what confidence intervals are trying to show us! Suppose we take a statistic like the mean \\(\\mu\\) to describe our vector temp. water %&gt;% summarize(mean = mean(temp)) ## # A tibble: 1 × 1 ## mean ## &lt;dbl&gt; ## 1 44.8 We might have gotten a slightly different statistic had we had a slightly different sample. We can approximate what slightly different sample might look like by using bootstrapped resamples. This means, randomly sampling a bunch of observations from our dataframe water, sometimes taking the same observation multiple times, sometimes leaving out some observations by chance. We can use the sample(x, size = ..., replace = TRUE) function to take a bootstrapped sample. water %&gt;% # Grab n() randomly sampled temperatures, with replacement, # we&#39;ll call those &#39;boot&#39;, since they were &#39;bootstrapped&#39; summarize(boot = sample(temp, size = n(), replace = TRUE)) %&gt;% # and take the mean! summarize(mean = mean(boot)) ## # A tibble: 1 × 1 ## mean ## &lt;dbl&gt; ## 1 45.0 Our bootstrapped mean is very, very close to the original mean - just slightly off due to sampling error! Bootstrapping is a very powerful tool, as it lets us circumvent many long formulas, as long as you take enough samples. Let’s take 1000 resamples below: # Get a vector of ids from 1 to 1000 myboot = tibble(rep = 1:1000) %&gt;% # For each repetition, group_by(rep) %&gt;% # Get a random bootstrapped sample of temperatures summarize(boot = water$temp %&gt;% sample(size = n(), replace = TRUE)) %&gt;% # For that repetition, group_by(rep) %&gt;% # Calculate the mean of the bootstrapped samples! summarize(mean = mean(boot)) # Let&#39;s view them! myboot$mean %&gt;% hist() We can see above the latent distribution of 1000 statistics we could have gotten due to random sampling error. This is called a sampling distribution. Whenever we make confidence intervals, we are always drawing from a sampling distribution. 9.3.2 Bootstrapped or Theoretical Sampling Distributions? The question is, are you relying on a bootstrapped sampling distribution or a theoretical sampling distribution? If we assume a perfectly normal distribution, then we’re relying on a theoretical sampling distribution, and we need formulas to calculate our confidence intervals. This is a big assumption! If we are comfortable with computing 1000 or more replicates, then we can bootstrap those confidence intervals instead, gaining accuracy at the expense of computational power. Let’s learn how to make confidence intervals for our indices from (1) a theoretical sampling distribution, and then we’ll learn to make them from (2) a bootstrapped sampling distribution. 9.3.3 Confidence Intervals with Theoretical Sampling Distributions Suppose our lower and upper specification limits - the expectations of the market and/or regulators - are that our onsen’s temperature should be between 42 and 50 degrees Celsius if we advertise ourselves as an Extra Hot Onsen. For any index, you’ll need to get the ingredients needed to calculate the index and to calculate its standard error (the standard deviation of the sampling distribution you’re trying to approximate). So, let’s first get our ingredients… stat = water %&gt;% group_by(time) %&gt;% summarize(xbar = mean(temp), s = sd(temp), n_w = n()) %&gt;% summarize( xbbar = mean(xbar), # grand mean x-double-bar sigma_s = sqrt(mean(s^2)), # sigma_short sigma_t = water$temp %&gt;% sd(), # sigma_total! n = sum(n_w), # or just n = n() # Total sample size n_w = unique(n_w), k = time %&gt;% unique() %&gt;% length()) # Get total subgroups # Check it! stat ## # A tibble: 1 × 6 ## xbbar sigma_s sigma_t n n_w k ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 44.8 1.99 1.99 160 20 8 Now, let’s calculate our Capability Index \\(C_{p}\\), which assumes a process centered between the upper and lower specification limits and a stable process. # Capability Index (for centered, normal data) cp = function(sigma_s, upper, lower){ abs(upper - lower) / (6*sigma_s) } stat %&gt;% summarize( limit_lower = 42, limit_upper = 50, # index estimate = cp(sigma_s = sigma_s, lower = limit_lower, upper = limit_upper)) ## # A tibble: 1 × 3 ## limit_lower limit_upper estimate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 42 50 0.671 That was surprisingly painless! Now, let’s estimate the two-sided, 95% confidence interval of our sampling distribution for the statistic cp, assuming that this sampling distribution has a normal shape. We’re getting the interval that spans 95%, so it’s got to start at 2.5% and end at 97.5%, covering the 95% most frequently occurring statistics in the sampling distribution. bands = stat %&gt;% summarize( limit_lower = 42, limit_upper = 50, # index estimate = cp(sigma_s = sigma_s, lower = limit_lower, upper = limit_upper), # Get our extra quantities of interest v_short = k*(n_w - 1), # get degrees of freedom # Get standard error for estimate se = estimate * sqrt(1 / (2*v_short)), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Now if z, the 97.5th percentile, # is 1.96 standard deviations from the mean in the normal, # Then so too is the 2.5th percentile in the normal. # Give me 1.96 standard deviations above cp # in the sampling distribution of cp! # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) bands ## # A tibble: 1 × 8 ## limit_lower limit_upper estimate v_short se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 42 50 0.671 152 0.0385 1.96 0.596 0.747 9.3.4 Visualizing Confidence Intervals Were we to visualize this, it might look like… bands %&gt;% ggplot(mapping = aes(x = &quot;Cp Index&quot;, y = estimate, ymin = lower, ymax = upper)) + # Get draw us some benchmarks to make our chart meaningful geom_hline(yintercept = c(0,1,2), color = c(&quot;grey&quot;, &quot;black&quot;, &quot;grey&quot;)) + # Draw the points! geom_point() + geom_linerange() + # Add theming theme_classic(base_size = 14) + coord_flip() + labs(y = &quot;Index Value&quot;, x = NULL) It’s not the most exciting plot, but it does show very clearly that the value of \\(C_{p}\\) and its 95% confidence interval are nowhere even close to 1.0, the key threshold. This means we can say with 95% confidence that the true value of \\(C_{p}\\) is less than 1. 9.3.5 Bootstrapping \\(C_{p}\\) How might we estimate this using the bootstrap? Well, we could… myboot = tibble(rep = 1:1000) %&gt;% # For each rep, group_by(rep) %&gt;% # Give me the data.frame water, summarize(water) %&gt;% # And give me a random sample of observations sample_n(size = n(), replace = TRUE) myboot %&gt;% glimpse() ## Rows: 160,000 ## Columns: 6 ## Groups: rep [1,000] ## $ rep &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ id &lt;dbl&gt; 154, 122, 31, 111, 67, 158, 110, 67, 42, 140, 3, 88… ## $ time &lt;dbl&gt; 15, 13, 3, 11, 7, 15, 11, 7, 5, 13, 1, 9, 7, 3, 3, … ## $ temp &lt;dbl&gt; 43.2, 45.3, 45.8, 43.7, 44.8, 44.7, 45.5, 44.8, 44.… ## $ ph &lt;dbl&gt; 4.6, 4.9, 6.3, 6.1, 5.6, 5.2, 5.6, 5.6, 6.3, 5.0, 6… ## $ sulfur &lt;dbl&gt; 0.4, 0.8, 0.0, 0.4, 0.1, 1.0, 4.0, 0.1, 1.4, 0.5, 0… This produces a very, very big data.frame! Let’s now, for each rep, calculate our statistics from before! mybootstat = myboot %&gt;% # For each rep, and each subgroup... group_by(rep, time) %&gt;% summarize( xbar = mean(temp), # Get within group mean sigma_w = sd(temp), # Get within group sigma n_w = n() # Get within subgroup size ) %&gt;% # For each rep... group_by(rep) %&gt;% summarize( limit_upper = 42, limit_lower = 50, xbbar = mean(xbar), # Get grand mean sigma_s = sqrt(mean(sigma_w^2)), # Get sigma_short n = sum(n_w), # Get total observations n_w = unique(n_w)[1], k = n(), # Get number of subgroups k limit_lower = 42, limit_upper = 50, estimate = cp(sigma_s, upper = limit_upper, lower = limit_lower)) So cool! We’ve now generated the sampling distributions for xbbar, sigma_s, and cp! We can even visualize the raw distributions now! Look at those wicked cool bootstrapped sampling distributions!!! g = mybootstat %&gt;% # For each rep, group_by(rep) %&gt;% # Stack our values atop each other... summarize( # Get these names, and repeat them each n times type = c(&quot;xbbar&quot;, &quot;sigma_s&quot;, &quot;cp&quot;) %&gt;% rep(each = n()), value = c(xbbar, sigma_s, estimate)) %&gt;% ggplot(mapping = aes(x = value, fill = type)) + geom_histogram() + facet_wrap(~type, scales = &quot;free&quot;) + guides(fill = &quot;none&quot;) # View it! g So last, let’s take our boostrapped \\(C_{p}\\) statistics in mybootstat$cp and estimate a confidence interval and standard error for this sampling distribution. Because we have the entire distribution, we can extract values at specific percentiles in the distribution using quantiles(), rather than qnorm() or such theoretical distributions. # We&#39;ll save it as &#39;myqi&#39;, for quantities of interest myqi = mybootstat %&gt;% summarize( # Let&#39;s grab the original cp statistic cp = bands$estimate, # Get the lower and upper 95% confidence intervals lower = quantile(estimate, probs = 0.025), upper = quantile(estimate, probs = 0.975), # We can even get the standard error, # which is *literally* the standard deviation of this sampling distribution se = sd(estimate)) # Check it out! myqi ## # A tibble: 1 × 4 ## cp lower upper se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.671 0.614 0.779 0.0434 This suggests a wider confidence interval that our normal distribution assumes by default - interesting! We can perform bootstrapping to estimate confidence intervals for any statistic, including \\(C_{p}\\), \\(C_{pk}\\), \\(P_{p}\\), or \\(P_{pk}\\). The only limit is your computational power! Wheee! Note: Whenever you bootstrap, it’s important that you clear out your R environment to keep things running quickly, because you tend to accumulate a lot of really big data.frames. You can use remove() to do this. remove(myboot, mybootstat) 9.4 CIs for any Index! Let’s practice calculating confidence intervals (CIs) for each of these indices. 9.4.1 CIs for \\(P_p\\) Now that we have our ingredients, let’s get our index and its confidence intervals! # Capability Index (for centered, normal data) cp = function(sigma_s, upper, lower){ abs(upper - lower) / (6*sigma_s) } stat %&gt;% summarize( # index estimate = cp(sigma_s = sigma_s, lower = 42, upper = 50), # Get our extra quantities of interest v_short = k*(n_w - 1), # get degrees of freedom # Get standard error for cpk se = estimate * sqrt(1 / (2*v_short)), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 × 6 ## estimate v_short se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.671 152 0.0385 1.96 0.596 0.747 9.4.2 CIs for \\(C_{pk}\\) Write the function and generate the confidence interval for \\(P_{pk}\\)! # Capability Index (for skewed, uncentered data) cpk = function(mu, sigma_s, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_s) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_s) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } stat %&gt;% summarize( # index estimate = cpk(mu = xbbar, sigma_s = sigma_s, lower = 42, upper = 50), # Get our extra quantities of interest v_short = k*(n_w - 1), # get degrees of freedom # Get standard error for ppk se = estimate * sqrt( 1 / (2*v_short) + 1 / (9 * n * estimate^2) ), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 × 6 ## estimate v_short se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.478 152 0.0380 1.96 0.404 0.553 9.4.3 CIs for \\(P_p\\) Now that we have our ingredients, let’s get our index and its confidence intervals! # Suppose we&#39;re looking at the entire process! # Process Performance Index (for centered, normal data) pp = function(sigma_t, upper, lower){ abs(upper - lower) / (6*sigma_t) } stat %&gt;% summarize( # index estimate = pp(sigma_t = sigma_t, lower = 42, upper = 50), # Get our extra quantities of interest v_total = n_w*k - 1, # get degrees of freedom # Get standard error for cpk se = estimate * sqrt(1 / (2*v_total)), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 × 6 ## estimate v_total se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.670 159 0.0376 1.96 0.597 0.744 9.4.4 CIs for \\(P_{pk}\\) Write the function and generate the confidence interval for \\(P_{pk}\\)! # Process Performance Index (for skewed, uncentered data) ppk = function(mu, sigma_t, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_t) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_t) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } stat %&gt;% summarize( # index estimate = ppk(mu = xbbar, sigma_t = sigma_t, lower = 42, upper = 50), # Get our extra quantities of interest v_total = n_w*k - 1, # get degrees of freedom # Get standard error for cpk se = estimate * sqrt( 1 / (2*v_total) + 1 / (9 * n * estimate^2) ), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 × 6 ## estimate v_total se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.478 159 0.0376 1.96 0.404 0.551 Alright! You are now a confidence interval wizard! Go forth and make confidence intervals! "],["references.html", "10 References", " 10 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
