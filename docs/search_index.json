[["index.html", "System Reliability and Six Sigma in R and Python Introduction", " System Reliability and Six Sigma in R and Python Timothy Fraser, PhD 2025-11-13 Introduction Your online textbook for learning reliability and six sigma techniques in R and Python! These coding workshops were made for Cornell University Course SYSEN 5300. Follow along with Posit.Cloud to learn to apply six sigma techniques in R and Python! Figure 0.1: Photo by Naser Tamimi on Unsplash "],["about-this-book.html", "1 About this Book 1.1 What Youâ€™ll Learn 1.2 Learning Approach 1.3 Prerequisites 1.4 About the Author 1.5 Acknowledgments 1.6 How to Use This Book", " 1 About this Book This comprehensive textbook provides hands-on training in System Reliability and Six Sigma techniques using R and Python. Designed specifically for Cornell Universityâ€™s SYSEN 5300 course, this book bridges the gap between statistical theory concepts and practical coding implementation of these advanced statistical techniques. 1.1 What Youâ€™ll Learn Through a series of interactive workshops and skill-building exercises, youâ€™ll master: Statistical Process Control - Monitoring and improving manufacturing and service processes Reliability Engineering - Predicting and improving system performance over time Life Distributions - Modeling failure patterns using exponential, Weibull, gamma, and log-normal distributions Fault Tree Analysis - Visualizing and quantifying system failure modes Physical Acceleration Models - Converting lab test results to field performance estimates Regression Analysis - Modeling relationships between variables in complex systems Design of Experiments - Planning and analyzing controlled experiments Response Surface Methodology - Optimizing processes through systematic experimentation 1.2 Learning Approach Each chapter combines: - Theoretical foundations with clear explanations - Hands-on coding workshops in both R and Python - Case studies from manufacturing, healthcare, and service industries - Interactive exercises with immediate feedback - Visual learning through plots, diagrams, and flowcharts 1.3 Prerequisites Basic familiarity with R programming Understanding of fundamental statistics Access to RStudio or Posit Cloud (recommended) 1.4 About the Author Figure 1.1: Dr.Â Timothy Fraser Dr.Â Timothy (Tim) Fraser is an Assistant Teaching Professor in the Systems Engineering Program at Cornell University, where he has been teaching since 2022. He also serves as Coordinator for the Center for Transportation, Environment, and Community Health (CTECH). As a computational social scientist, Dr.Â Fraser develops methods, systems, and software to help communities combat environmental crises. His research spans urban resilience and environmental systems in the US and Japan, with 51 peer-reviewed publications, plus numerous dashboards, R packages, and software tools. Dr.Â Fraser has advised researchers on social infrastructure at the OECD and UN Development Programmeâ€™s Accelerator Labs. He earned his PhD from Northeastern University in 2022 and has received funding from multiple national grants, including Fulbright. His current research focuses on developing AI and cloud computing tools to quantify, visualize, and communicate environmental policy data to the public, supporting community action and resilience in socio-technical systems. Contact: www.timothyfraser.com 1.5 Acknowledgments Special thanks to the students of Cornellâ€™s Systems Engineering Program for their invaluable feedback, testing, and suggestions that helped refine this textbook. Your engagement and questions have made this resource more effective and user-friendly. 1.6 How to Use This Book Start with the basics - Begin with Part 1 for fundamental R and Python programming skills Follow the sequence - Each chapter builds on previous concepts Find the code - Find the code for each chapter in the code/ folder and the workshops/ folder at github.com/timothyfraser/sigma Practice actively - Run all code examples and complete exercises Use both languages - Compare R and Python implementations to deepen understanding Apply to your field - Adapt examples to your specific industry or research area Happy learning! ðŸš€ "],["coding-in-r.html", "2 Coding in R Getting Started 2.1 Introduction to R 2.2 Basic Calculations in R Learning Check 1 2.3 Types of Values in R 2.4 Types of Data in R Learning Check 2 2.5 Common Functions in R 2.6 Missing Data Learning Check 3 2.7 Packages The Pipeline 2.8 Visualizing Data with Histograms Learning Check 4 Learning Check 5 Conclusion", " 2 Coding in R Welcome to Posit Cloud! You made it! This document will introduce you to how to start coding in R, using Posit Cloud. We will use the R statistical coding language frequently in class to conduct analyses and visualization. Hello world! We are coding in R! Getting Started Making an Posit.Cloud account Weâ€™ll be using Posit.Cloud, a virtual version of R you can access from any computer with an internet browser (PC, Mac, Chromebook, anything). To get set up, please follow the steps in this short Video playlist! Using R for the First Time For a quick visual orientation, take a peek at the image below. Read and follow along with the instructions on the webpage! Read the tutorial code (below), and then type it in and run it in your R session! (#fig:graphic_1)Visual Intro to Using Posit.Cloud 2.1 Introduction to R The document in your Posit Cloud project document is an â€˜R script.â€™ (its name ends in .R). It contains two kinds of text: â€˜codeâ€™ - instructions to our statistical calculator â€˜commentsâ€™ - any text that immediately follows a â€˜#â€™ sign. # For example, # Comments are ignored by the calculator, so we can write ourselves notes. Notice: 4 windows in R. Window 1 (upper left): Scripts! Window 2 (bottom left): Console (this shows the output for our calculator) Window 3 (upper right): Environment (this shows any data the computer is holding onto for us) Window 4 (bottom right): Files (this shows our working project folder, our scripts, and any data files.) A few tips: To change the background theme (and save your eyes), go to Tools &gt;&gt; Global Options &gt;&gt; Appearance &gt;&gt; Editor Theme &gt;&gt; Dracula To increase the font size, go to Tools &gt;&gt; Global Options &gt;&gt; Appearance &gt;&gt; Editor Font Size To make a script, go to File &gt;&gt; New File &gt;&gt; R Script, then save it and name it. (#fig:image_1_1)Open New Script (#fig:image_1_2)Save New Script! Letâ€™s learn to use R! 2.2 Basic Calculations in R Try highlighting the following with your cursor, and then press CTRL and ENTER simultaneously, or the â€˜Runâ€™ button above. Addition: 1 + 5 ## [1] 6 Subtraction: 5 - 2 ## [1] 3 Multiplication: 2 * 3 ## [1] 6 Division: 15 / 5 ## [1] 3 Exponents: 2^2 ## [1] 4 Square-Roots: sqrt(4) ## [1] 2 Order of Operations: Still applies! Like in math normally, R calculations are evaluated from left to right, prioritizing parentheses, then multiplication and division, then addition and subtraction. 2 * 2 - 5 ## [1] -1 Use Parentheses! 2 * (2 - 5) ## [1] -6 Learning Check 1 Learning Checks (LC) are short questions that appear throughout this book, providing short coding challenges to try and work through. Below is the question tab. Read the question, and try to answer it on your own! Then, click the answer button to see the answer. (Note: There are often many different ways to code the same thing!) Feeling stumped? You can check the answer, but be sure to code it yourself afterwards! Question Try calculating something wild in R! Solve for x below using the commands you just learned in R! \\(x = \\sqrt{ (\\frac{2 - 5 }{5})^{4} }\\) \\(x = (1 - 7)^{2} \\times 5 - \\sqrt{49}\\) \\(x = 2^{2} + 2^{2} \\times 2^{2} - 2^{2} \\div 2^{2}\\) [View Answer!] Hereâ€™s how we coded it! How does yours compare? If your result is different, compare code. Whatâ€™s different? Be sure to go back and adjust your code so you understand the answer! \\(x = \\sqrt{ (\\frac{2 - 5 }{5})^{4} }\\) sqrt( ((2 - 5) / 5)^4 ) ## [1] 0.36 \\(x = (1 - 7)^{2} \\times 5 - \\sqrt{49}\\) (1 - 7)^2 * 5 - sqrt(49) ## [1] 173 \\(x = 2^{2} + 2^{2} \\times 2^{2} - 2^{2} \\div 2^{2}\\) 2^2 + 2^2 * 2^2 - 2^2 / 2^2 ## [1] 19 2.3 Types of Values in R R accepts 2 type of data: # Numeric Values 15000 ## [1] 15000 0.0005 ## [1] 5e-04 -8222 # notice no commas allowed ## [1] -8222 and # Character Strings &quot;Coding!&quot; # Uses quotation marks ## [1] &quot;Coding!&quot; &quot;Corgis!&quot; # Can contain anything - numbers, characters, etc. ## [1] &quot;Corgis!&quot; &quot;Coffee!&quot; ## [1] &quot;Coffee!&quot; (Note: R also uses something called factors, which are characters, but have a specific order. Weâ€™ll learn them later.) 2.4 Types of Data in R 2.4.1 Values First, R uses values - which are single numbers or characters. 2 # this is a value ## [1] 2 &quot;x&quot; # this is also a value ## [1] &quot;x&quot; You can save a value as a named object in the R Environment. That means, we tell R to remember that whenever you use a certain name, it means that value. To name something as an object, use an arrow! myvalue &lt;- 2 Now letâ€™s highlight and press CTRL ENTER on myvalue (or the Mac Equivalent). myvalue ## [1] 2 Notice how itâ€™s listed in the R Environment (upper right), and how it outputs as 2 in the console? We can do operations too! secondvalue &lt;- myvalue + 2 # add 2 to myvalue secondvalue # check new value - oooh, it&#39;s 4! ## [1] 4 We can also overwrite old objects with new objects. myvalue &lt;- &quot;I overwrote it!&quot; myvalue ## [1] &quot;I overwrote it!&quot; And we can also remove objects from the Environment, with remove(). remove(myvalue, secondvalue) 2.4.2 Vectors Second, R contains values in vectors, which are sets of values. # This is a numeric vector c(1, 4, 8) # is the same as 1, 4, 8 ## [1] 1 4 8 andâ€¦ # This is a character vector c(&quot;Boston&quot;, &quot;New York&quot;, &quot;Los Angeles&quot;) ## [1] &quot;Boston&quot; &quot;New York&quot; &quot;Los Angeles&quot; But if you combine numeric and character values in one vectorâ€¦ # This doesn&#39;t work - R immediately makes it into a character vector c(1, &quot;Boston&quot;, 2) ## [1] &quot;1&quot; &quot;Boston&quot; &quot;2&quot; Why do we use vectors? Because you can do mathematical operations on entire vectors of values, all at once! c(1,2,3,4) * 2 # this multiplies each value by 2! ## [1] 2 4 6 8 c(1,2,3,4) + 2 # this adds 2 to each value! ## [1] 3 4 5 6 We can save vectors as objects too! # Here&#39;s a vector of (hypothetical) seawall heights in 10 towns. myheights &lt;- c(4, 4.5, 5, 5, 5, 5.5, 5.5, 6, 6.5, 6.5) # And here&#39;s a list of hypothetical names for those towns mytowns &lt;- c(&quot;Gloucester&quot;, &quot;Newburyport&quot;, &quot;Provincetown&quot;, &quot;Plymouth&quot;, &quot;Marblehead&quot;, &quot;Chatham&quot;, &quot;Salem&quot;, &quot;Ipswich&quot;, &quot;Falmouth&quot;, &quot;Boston&quot;) # And here&#39;s a list of years when those seawalls were each built. myyears &lt;- c(1990, 1980, 1970, 1930, 1975, 1975, 1980, 1920, 1995, 2000) Plus, we can still do operations on entire vectors! myyears + 1 ## [1] 1991 1981 1971 1931 1976 1976 1981 1921 1996 2001 2.4.3 Dataframes Third, R bundles vectors into data.frames. # Using the data.frame command, we make a data.frame, data.frame( height = myheights, # length 10 town = mytowns, # length 10 year = myyears) # length 10 ## height town year ## 1 4.0 Gloucester 1990 ## 2 4.5 Newburyport 1980 ## 3 5.0 Provincetown 1970 ## 4 5.0 Plymouth 1930 ## 5 5.0 Marblehead 1975 ## 6 5.5 Chatham 1975 ## 7 5.5 Salem 1980 ## 8 6.0 Ipswich 1920 ## 9 6.5 Falmouth 1995 ## 10 6.5 Boston 2000 And inside, we put a bunch of vectors of EQUAL LENGTHS, giving each vector a name. And when it outputs in the console, it looks like a spreadsheet! BECAUSE ALL SPREADSHEETS ARE DATAFRAMES! AND ALL COLUMNS ARE VECTORS! AND ALL CELLS ARE VALUES! Actually, we can make data.frames into objects too! # Let&#39;s name our data.frame about seawalls &#39;sw&#39; sw &lt;- data.frame( height = myheights, town = mytowns, year = myyears) # Notice this last parenthesis; very important # Check the contents of sw! sw ## height town year ## 1 4.0 Gloucester 1990 ## 2 4.5 Newburyport 1980 ## 3 5.0 Provincetown 1970 ## 4 5.0 Plymouth 1930 ## 5 5.0 Marblehead 1975 ## 6 5.5 Chatham 1975 ## 7 5.5 Salem 1980 ## 8 6.0 Ipswich 1920 ## 9 6.5 Falmouth 1995 ## 10 6.5 Boston 2000 Although, we could do this too, and it would be equivalent: sw &lt;- data.frame( # It&#39;s okay to split code across multiple lines. # It keeps things readable. height = c(4, 4.5, 5, 5, 5, 5.5, 5.5, 6, 6.5, 6.5), town = c(&quot;Gloucester&quot;, &quot;Newburyport&quot;, &quot;Provincetown&quot;, &quot;Plymouth&quot;, &quot;Marblehead&quot;, &quot;Chatham&quot;, &quot;Salem&quot;, &quot;Ipswich&quot;, &quot;Falmouth&quot;, &quot;Boston&quot;), year = c(1990, 1980, 1970, 1930, 1975, 1975, 1980, 1920, 1995, 2000)) # Let&#39;s check out our dataframe! sw ## height town year ## 1 4.0 Gloucester 1990 ## 2 4.5 Newburyport 1980 ## 3 5.0 Provincetown 1970 ## 4 5.0 Plymouth 1930 ## 5 5.0 Marblehead 1975 ## 6 5.5 Chatham 1975 ## 7 5.5 Salem 1980 ## 8 6.0 Ipswich 1920 ## 9 6.5 Falmouth 1995 ## 10 6.5 Boston 2000 But what if we want to work with the vectors again? We can use the â€˜$â€™ sign to say, â€˜grab the following vector from inside this data.frame.â€™ sw$height ## [1] 4.0 4.5 5.0 5.0 5.0 5.5 5.5 6.0 6.5 6.5 We can also do operations on that vector from within the dataframe. sw$height + 1 ## [1] 5.0 5.5 6.0 6.0 6.0 6.5 6.5 7.0 7.5 7.5 We can also update values, like the following: # sw$height &lt;- sw$height + 1 # I&#39;ve put this in comments, since I don&#39;t actually want to do it (it&#39;ll change our data) # but good to know, right? Learning Check 2 Question How would you make your own data.frame? Please make up a data.frame of with 3 vectors and 4 values each. Make 1 vector numeric and 2 vectors character data. How many rows are in that data.frame? [View Answer!] Hereâ€™s my example! # Make a data.frame called &#39;mayhem&#39; mayhem &lt;- data.frame( # make a character vector of 4 dog by their names dogs = c(&quot;Mocha&quot;, &quot;Domino&quot;, &quot;Latte&quot;, &quot;Dot&quot;), # Classify the type of dog as a character vector types = c(&quot;corgi&quot;, &quot;dalmatian&quot;, &quot;corgi&quot;, &quot;dalmatian&quot;), # Record the number of treats eaten per year per dog treats_per_year = c(5000, 3000, 2000, 10000)) # View the resulting &#39;mayhem&#39;! mayhem ## dogs types treats_per_year ## 1 Mocha corgi 5000 ## 2 Domino dalmatian 3000 ## 3 Latte corgi 2000 ## 4 Dot dalmatian 10000 2.5 Common Functions in R We can also run functions that come pre-installed to analyze vectors. These include: mean(), median(), sum(), min(), max(), range(), quantile(), sd(), var(), and length(). (#fig:image_1_4)Descriptive Stats function Cheatsheet! 2.5.1 Measures of Central Tendency mean(sw$height) # the mean seawall height among these towns ## [1] 5.35 median(sw$height) # the median seawall height ## [1] 5.25 sum(sw$height) # total meters of seawall height! (weird number, but okay) ## [1] 53.5 2.5.2 Measures of Dispersion min(sw$height) # smallest seawall height ## [1] 4 max(sw$height) # tallest seawall height ## [1] 6.5 range(sw$height) # range of seawalls (min &amp; max) ## [1] 4.0 6.5 quantile(sw$height, probs = 0.25) # 25th percentile ## 25% ## 5 quantile(sw$height, probs = 0.75) # 75th percentile ## 75% ## 5.875 sd(sw$height) # the standard deviation of seawall heights ## [1] 0.8181958 var(sw$height) # the variance of seawall heights (= standard deviation squared) ## [1] 0.6694444 2.5.3 Other Good Functions length(sw$height) # the number of values in this vector ## [1] 10 length(sw) # the number of vectors in this data.frame ## [1] 3 Thatâ€™s really fast! Weâ€™ll learn more about these descriptive statistics in later lessons! 2.6 Missing Data Sometimes, data.frames include missing data for a case/observation. For example, letâ€™s say there is an 11th town, where the seawall height is unknown. # We would write: mysw &lt;- c(4, 4.5, 5, 5, 5, 5.5, 5.5, 6, 6.5, 6.5, NA) # see the &#39;NA&#39; for non-applicable If you run mean(mysw) now, R doesnâ€™t know how to add 6.5 + NA. The output will become NA instead of 5.35. mean(mysw) ## [1] NA To fix this, we can add an â€˜argumentâ€™ to the function, telling it to omit NAs from the calculation. mean(mysw, na.rm = TRUE) # short for, &#39;remove NAs&#39; ## [1] 5.35 Pretty cool, no? Each function is unique, often made by different people, so only these functions have na.rm as an argument. Learning Check 3 Question Jun Kanda (2015) measured max seawall heights (seawall_m) in 13 Japanese towns (town) after the 2011 tsunami in Tohoku, Japan, compared against the height of the tsunami wave (wave_m). Using this table, please code and answer the questions below. town seawall_m wave_m Kuji South 12.0 14.5 Fudai 15.5 18.4 Taro 13.7 16.3 Miyako 8.5 11.8 Yamada 6.6 10.9 Ohtsuchi 6.4 15.1 Tohni 11.8 21.0 Yoshihama 14.3 17.2 Hirota 6.5 18.3 Karakuwa East 6.1 14.4 Onagawa 5.8 18.0 Souma 6.2 14.5 Nakoso 6.2 7.7 Reproduce this table as a data.frame in R, and save it as an object named jp. How much greater was the mean height of the tsunami than the mean height of seawalls? Evaluate how much these heights varied on average among towns. Did seawall height vary more than tsunami height? How much more/less? [View Answer!] Reproduce this table as a data.frame in R, and save it as an object named jp. # Make a dataframe named jp, jp &lt;- data.frame( # containing a character vector of 13 town names, town = c(&quot;Kuji South&quot;, &quot;Fudai&quot;, &quot;Taro&quot;, &quot;Miyako&quot;, &quot;Yamada&quot;, &quot;Ohtsuchi&quot;, &quot;Tohni&quot;, &quot;Yoshihama&quot;, &quot;Hirota&quot;, &quot;Karakuwa East&quot;, &quot;Onagawa&quot;, &quot;Souma&quot;, &quot;Nakoso&quot;), # and a numeric vector of 13 max seawall heights in meters seawall_m = c(12.0, 15.5, 13.7, 8.5, 6.6, 6.4, 11.8, 14.3, 6.5, 6.1, 5.8, 6.2, 6.2), # and a numeric vector of 13 max tsunami heights in meters wave_m = c(14.5, 18.4, 16.3, 11.8, 10.9, 15.1, 21.0, 17.2, 18.3, 14.4, 18.0, 14.5, 7.7) ) # View contents! jp ## town seawall_m wave_m ## 1 Kuji South 12.0 14.5 ## 2 Fudai 15.5 18.4 ## 3 Taro 13.7 16.3 ## 4 Miyako 8.5 11.8 ## 5 Yamada 6.6 10.9 ## 6 Ohtsuchi 6.4 15.1 ## 7 Tohni 11.8 21.0 ## 8 Yoshihama 14.3 17.2 ## 9 Hirota 6.5 18.3 ## 10 Karakuwa East 6.1 14.4 ## 11 Onagawa 5.8 18.0 ## 12 Souma 6.2 14.5 ## 13 Nakoso 6.2 7.7 How much greater was the mean height of the tsunami than the mean height of seawalls? # Get mean of wave height mean(jp$wave_m) ## [1] 15.23846 The average wave was 15.24 meters tall. # Get mean of seawall height mean(jp$seawall_m) ## [1] 9.2 The average seawall was 9.2 meters tall. # Get difference in mean seawall height mean(jp$wave_m) - mean(jp$seawall_m) ## [1] 6.038462 The average wave was 6.04 meters taller than the average seawall. Evaluate how much these heights varied on average among towns. Did seawall height vary more than tsunami height? How much more/less? # Get standard deviation of wave height sd(jp$wave_m) ## [1] 3.587603 On average, wave height varied by 3.59 meters. # Get standard deviation of seawall height sd(jp$seawall_m) ## [1] 3.675368 On average, seawall height varied by 3.68 meters. # Get difference sd(jp$wave_m) - sd(jp$seawall_m) ## [1] -0.08776516 That means wave height varied by -0.09 meters less than seawall height. 2.7 Packages 2.7.1 Using Packages Some functions come pre-built into R, but lots of people have come together to build â€˜packagesâ€™ of functions that help R users all over the world do more, cool things, so we donâ€™t each have to â€˜reinvent the wheel.â€™ ggplot2, which we use below, is one of these! 2.7.2 Installing Packages We can use the library() function to load a package (like fipping an â€˜onâ€™ switch for the package). After loading it, R will recognize that packageâ€™s functions when you run them! But if you try to load a package that has never been installed on your computer, you might get this error: library(ggplot2) Error in library(ggplot2) : there is no package called â€˜ggplot2â€™ In this case, we need to install those packages (only necessary once), using install.packages(). (If a message pops up, just accept â€˜Yesâ€™.) install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;)) After a successful install, youâ€™ll get a message like this: ================================================== downloaded 1.9 MB * installing *binary* package â€˜ggplot2â€™ ... * DONE (ggplot2) * installing *binary* package â€˜dplyrâ€™ ... * DONE (dplyr) The downloaded source packages are in â€˜/tmp/RtmpefCnYe/downloaded_packagesâ€™ 2.7.3 Loading Packages Finally, we can load our packages with library(). library(ggplot2) library(dplyr) Tada! You have turned on your packages! The Pipeline In much of this course, weâ€™re going to use a coding symbol %&gt;%, called a pipeline. The pipeline is not built into base R, so you always need the dplyr package loaded in order to use it. Fortunately, we just loaded dplyr, our data wrangling toolkit, above using library(dplyr), so weâ€™re good to go! Pipelines let us connect data to functions, with fewer parentheses! It helps more clearly show and code a process of input data to function A to function B to output data (for example). Figure 2.1: Old-School Pipeline For example: # let&#39;s make a vector `x` and do some operations on it. x &lt;- c(1,2,3) # These are the same! mean(x) ## [1] 2 x %&gt;% mean() ## [1] 2 Using pipelines keeps our code neat and tidy. It lets us run long sequences of code without saving it bit by bit as objects. For example, we can take them mean() of x and then get the length() of the resulting vector, all in one sequence. Without a pipeline, you end up in parenthesis hell very quickly. # without pipe length(mean(x)) ## [1] 1 # with pipe x %&gt;% mean() %&gt;% length() ## [1] 1 Handy, right? To simplify things, thereâ€™s a special â€˜hotkeyâ€™ shortcut for making pipelines too. In Windows and Linux, use Ctrl Shift M. In Mac, use Cmd Shift M. 2.8 Visualizing Data with Histograms The power of R is that you can process data, calculate statistics, and visualize it all together, very quickly. We can do this using hist() and geom_histogram(), among other functions. 2.8.1 hist() For example, letâ€™s imagine that we had seawall height data from cities in several states. We might want to compare those states. # Create 30 cities, ten per state (MA, RI, ME) allsw &lt;- data.frame( height = c(4, 4.5, 5, 5, 5.5, 5.5, 5.5, 6, 6, 6.5, 4, 4,4, 4, 4.5, 4.5, 4.5, 5, 5, 6, 5.5, 6, 6.5, 6.5, 7, 7, 7, 7.5, 7.5, 8), states = c(&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;, &quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;, &quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;)) # Take a peek! allsw ## height states ## 1 4.0 MA ## 2 4.5 MA ## 3 5.0 MA ## 4 5.0 MA ## 5 5.5 MA ## 6 5.5 MA ## 7 5.5 MA ## 8 6.0 MA ## 9 6.0 MA ## 10 6.5 MA ## 11 4.0 RI ## 12 4.0 RI ## 13 4.0 RI ## 14 4.0 RI ## 15 4.5 RI ## 16 4.5 RI ## 17 4.5 RI ## 18 5.0 RI ## 19 5.0 RI ## 20 6.0 RI ## 21 5.5 ME ## 22 6.0 ME ## 23 6.5 ME ## 24 6.5 ME ## 25 7.0 ME ## 26 7.0 ME ## 27 7.0 ME ## 28 7.5 ME ## 29 7.5 ME ## 30 8.0 ME Every vector is a distribution - a range of low to high values. We can use the hist() function to quickly visualize a vectorâ€™s distribution. hist(allsw$height) Learning Check 4 Question Using the hist() function we just learned, draw the histogram of a vector of seawalls, naming the vector sw! The vector should include the following seawall heights (in meters): 4.5 m, 5 m, 5.5 m, 5 m, 5.5 m, 6.5 m, 6.5 m, 6 m, 5 m, and 4 m. [View Answer!] Using the hist() function we just learned, draw the histogram of a vector of seawalls, naming the vector sw! The vector should include the following seawall heights (in meters): 4.5 m, 5 m, 5.5 m, 5 m, 5.5 m, 6.5 m, 6.5 m, 6 m, 5 m, and 4 m. # Many options! # You could code it as a vector, save it as an object, then use your functions! sw &lt;- c(4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4) sw %&gt;% hist() # or you could do it like this! # hist(sw) 2.8.2 geom_histogram() in ggplot2 hist() is great for a quick check, but for anything more complex, weâ€™re going to use ggplot2, the most popular visualization package in R. # Load ggplot2 package library(ggplot2) # Tell the ggplot function to... ggplot( # draw data from the &#39;allsw&#39; data.frame data = allsw, # and &#39;map&#39; the vector &#39;height&#39; to be an &#39;aes&#39;thetic on the &#39;x&#39;-axis. mapping = aes(x = height)) + # make histograms of distribution, geom_histogram( # With white outlines color = &quot;white&quot;, # With blue inside fill fill = &quot;steelblue&quot;, # where every half meter gets a bin (binwidth = 0.5) binwidth = 0.5) + # add labels labs(x = &quot;Seawall Height&quot;, y = &quot;Frequency (# of cities)&quot;) Looks much nicer, right? Lots more code, but lots more options for customizing. Weâ€™ll learn ggplot2 more over this term, and it will become second nature in time! (Just takes practice!) The value of ggplot2 really comes alive when we make complex visuals. For example, our data allsw$height essentially contains 3 vectors, one per state; one for MA, one for RI, one for ME. Can we visualize each of these vectorsâ€™ distributions using separate histograms? # Repeat code from before... ggplot(data = allsw, mapping = aes(x = height)) + geom_histogram(color = &quot;white&quot;, fill = &quot;steelblue&quot;, binwidth = 0.5) + labs(x = &quot;Seawall Height&quot;, y = &quot;Frequency (# of cities)&quot;) + # don&#39;t forget the &#39;+&#39;! # But also ## Split into panels by state! facet_wrap(~states) We can now see, according to our hypothetical example, that states host different distributions of seawall heights. Massachusetts (MA) has lower seawalls, evenly distributed around 5.5 m. Maine (ME) has higher seawalls, skewed towards 7 m. Rhode Island (RI) has lower seawalls, skewed towards 4 m. Learning Check 5 Question Challenge: Please make a histogram of Jun Kandaâ€™s sample of seawall heights (seawall_m) in the jp object from LC 3. First, make a histogram using the hist() function. Then, try and use the geom_histogram() function from ggplot2! [View Answer!] First, make a histogram using the hist() function. # Tell R to make a histogram from the &#39;seawall_m&#39; vector inside &#39;jp&#39;! hist(jp$seawall_m) Then, try and use the geom_histogram() function from ggplot2! # Tell ggplot to grab the &#39;seawall_m&#39; vector from the &#39;jp&#39; data.frame, # and make a histogram! ggplot(data = jp, mapping = aes(x = seawall_m)) + geom_histogram() Looks pretty weird, huh? hist() automatically chooses the binwidth, but ggplot() gives us more control over the whole plot. Weâ€™ll learn more about this soon! Conclusion Next Steps Throughout the rest of the course, weâ€™re going to advance each of these skills: working with types of data in R calculating meaningful statistics in R visualizing meaningful trends in R Advice Be sure to clear your environment often. That means, using remove() or the broom tool in the upper right hand corner. remove(allsw, mysw, sw, myheights, mytowns, myyears) You can clean your console too, using broom in consoleâ€™s upper right corner. Save often. (Control + Save usually works on PC.) You can download files using more / export, or upload them. Youâ€™ll be a rockstar at using R in no time! Stay tuned for our next Workshop! Troubleshooting If your session freezes, go to â€˜Sessionâ€™ &gt;&gt; â€˜Restart R.â€™ If that doesnâ€™t work, go to â€˜Sessionâ€™ &gt;&gt; â€˜Terminateâ€™. If that doesnâ€™t work, click on the elipsis (â€¦) in the white banner at the top, and select Relaunch Project. If that doesnâ€™t work, let me know! Having problems? There are three causes of most all problems in R. thereâ€™s a missing parenthesis or missing quotation mark in oneâ€™s code. Youâ€™re using a function from a package that needs to be loaded (weâ€™ll talk about this in later workshops). Too much data in your environment is causing R to crash. Clear the environment. "],["coding-in-python.html", "3 Coding in Python Getting Started 3.1 Introduction to Python 3.2 Basic Calculations in Python Learning Check 1 3.3 Types of Values in Python 3.4 Types of Data in Python Learning Check 2 3.5 Common Functions in Python 3.6 Missing Data Learning Check 3 3.7 Packages The Pipeline 3.8 Visualizing Data with Histograms Learning Check 4 Learning Check 5 Conclusion", " 3 Coding in Python Welcome to Posit Cloud! You made it! This document will introduce you to how to start coding in Python using Posit Cloud. We will use the Python language frequently to conduct analyses and visualization. Hello world! We are coding in Python! Getting Started Making a Posit Cloud account Weâ€™ll be using Posit Cloud, a browser-based environment you can access from any computer. To get set up, follow your course setup guide, then open the project for this workshop. Using Python for the First Time For a quick visual orientation, take a peek at the image below. Read and follow along with the instructions on this page. Type the tutorial code and run it in your Python session. (#fig:graphic_py_1)Visual Intro to Using Posit Cloud 3.1 Introduction to Python Your project includes a Python script file (its name ends in .py). It contains two kinds of text: code - instructions to our calculator comments - any text that immediately follows a # sign # For example, # Comments are ignored by the calculator, so we can write notes. Notice: the IDE has four panes, including the editor, console, environment/history, and files. The console shows outputs from Python (or R). To create a new script, go to File &gt;&gt; New File &gt;&gt; Text File (or Python Script), then save it and name it. (#fig:image_py_1_1)Open New Script (#fig:image_py_1_2)Save New Script as a .py file! Letâ€™s learn to use Python! 3.2 Basic Calculations in Python Try highlighting the following and pressing Ctrl+Enter, or click Run. Addition: 1 + 5 ## 6 Subtraction: 5 - 2 ## 3 Multiplication: 2 * 3 ## 6 Division: 15 / 5 ## 3.0 Exponents: 2 ** 2 ## 4 Square roots: 16 ** 0.5 ## 4.0 Order of operations still applies. Use parentheses to control order: 2 * 2 - 5 ## -1 2 * (2 - 5) ## -6 Learning Check 1 Question Try calculating something wild in Python! Solve for x below using the commands above. \\(x = \\sqrt{ (\\frac{2 - 5}{5})^{4} }\\) \\(x = (1 - 7)^{2} \\times 5 - \\sqrt{49}\\) \\(x = 2^{2} + 2^{2} \\times 2^{2} - 2^{2} \\div 2^{2}\\) [View Answer!] ((2 - 5) / 5) ** 4 ## 0.1296 (( (2 - 5) / 5) ** 4) ** 0.5 ## 0.36 (1 - 7) ** 2 * 5 - 49 ** 0.5 ## 173.0 2**2 + 2**2 * 2**2 - 2**2 / 2**2 ## 19.0 3.3 Types of Values in Python Python commonly uses numeric values and character strings. 15000 ## 15000 0.0005 ## 0.0005 -8222 ## -8222 and &quot;Coding!&quot; ## &#39;Coding!&#39; &quot;Corgis!&quot; ## &#39;Corgis!&#39; &quot;Coffee!&quot; ## &#39;Coffee!&#39; 3.4 Types of Data in Python 3.4.1 Values and Variables Save a value as a named variable in memory. 2 ## 2 &quot;x&quot; ## &#39;x&#39; myvalue = 2 myvalue ## 2 Do operations too! secondvalue = myvalue + 2 secondvalue ## 4 Overwrite variables as needed. myvalue = &quot;I overwrote it!&quot; myvalue ## &#39;I overwrote it!&#39; Remove variables from memory if needed. del myvalue del secondvalue 3.4.2 Lists (like R vectors) Lists hold multiple values. [1, 4, 8] ## [1, 4, 8] and [&quot;Boston&quot;, &quot;New York&quot;, &quot;Los Angeles&quot;] ## [&#39;Boston&#39;, &#39;New York&#39;, &#39;Los Angeles&#39;] Python will coerce types inside a list only if you mix them when converting to arrays or series. Keep types consistent when possible. Do math element-wise using pandas Series: import pandas as p p.Series([1,2,3,4]) * 2 p.Series([1,2,3,4]) + 2 3.4.3 DataFrames with pandas Bundle columns into a table using pandas DataFrame. import pandas as p myheights = [4, 4.5, 5, 5, 5, 5.5, 5.5, 6, 6.5, 6.5] mytowns = [&quot;Gloucester&quot;, &quot;Newburyport&quot;, &quot;Provincetown&quot;, &quot;Plymouth&quot;, &quot;Marblehead&quot;, &quot;Chatham&quot;, &quot;Salem&quot;, &quot;Ipswich&quot;, &quot;Falmouth&quot;, &quot;Boston&quot;] myyears = [1990, 1980, 1970, 1930, 1975, 1975, 1980, 1920, 1995, 2000] sw = p.DataFrame({ &#39;height&#39;: myheights, &#39;town&#39;: mytowns, &#39;year&#39;: myyears }) sw Access a column (Series) with dot or bracket notation and do operations. sw.height sw.height + 1 Update values as needed. # sw[&quot;height&quot;] = sw[&quot;height&quot;] + 1 Learning Check 2 Question How would you make your own DataFrame? Make a DataFrame with 3 columns and 4 rows. Make 1 numeric column and 2 character columns. How many rows are in that DataFrame? [View Answer!] import pandas as p mayhem = p.DataFrame({ &#39;dogs&#39;: [&quot;Mocha&quot;, &quot;Domino&quot;, &quot;Latte&quot;, &quot;Dot&quot;], &#39;types&#39;: [&quot;corgi&quot;, &quot;dalmatian&quot;, &quot;corgi&quot;, &quot;dalmatian&quot;], &#39;treats_per_year&#39;: [5000, 3000, 2000, 10000] }) mayhem 3.5 Common Functions in Python We can compute descriptive statistics using pandas Series methods. 3.5.1 Measures of Central Tendency sw.height.mean() sw.height.median() sw.height.sum() 3.5.2 Measures of Dispersion sw.height.min() sw.height.max() sw.height.quantile(q=0.25) sw.height.quantile(q=0.75) sw.height.std() sw.height.var() 3.5.3 Other Good Functions len(sw.height) sw.shape[1] # number of columns 3.6 Missing Data Sometimes data include missing values. In pandas these are NaN. Many pandas functions ignore NaN by default. import pandas as p mysw = p.Series([4, 4.5, 5, 5, 5, 5.5, 5.5, 6, 6.5, 6.5, None]) mysw.mean() # returns 5.35, skips None/NaN by default ## 5.35 If you need to include missing values in a calculation, convert them or use numpy functions explicitly, but usually skipping them is desired. Learning Check 3 Question Recreate the table below as a pandas DataFrame named jp, then answer the questions. town seawall_m wave_m Kuji South 12.0 14.5 Fudai 15.5 18.4 Taro 13.7 16.3 Miyako 8.5 11.8 Yamada 6.6 10.9 Ohtsuchi 6.4 15.1 Tohni 11.8 21.0 Yoshihama 14.3 17.2 Hirota 6.5 18.3 Karakuwa East 6.1 14.4 Onagawa 5.8 18.0 Souma 6.2 14.5 Nakoso 6.2 7.7 Reproduce this table as a DataFrame named jp. How much greater was the mean tsunami height than the mean seawall height? Which varied more across towns: seawall height or tsunami height? By how much? [View Answer!] import pandas as p jp = p.DataFrame({ &#39;town&#39;: [&quot;Kuji South&quot;, &quot;Fudai&quot;, &quot;Taro&quot;, &quot;Miyako&quot;, &quot;Yamada&quot;, &quot;Ohtsuchi&quot;, &quot;Tohni&quot;, &quot;Yoshihama&quot;, &quot;Hirota&quot;, &quot;Karakuwa East&quot;, &quot;Onagawa&quot;, &quot;Souma&quot;, &quot;Nakoso&quot;], &#39;seawall_m&#39;: [12.0, 15.5, 13.7, 8.5, 6.6, 6.4, 11.8, 14.3, 6.5, 6.1, 5.8, 6.2, 6.2], &#39;wave_m&#39;: [14.5, 18.4, 16.3, 11.8, 10.9, 15.1, 21.0, 17.2, 18.3, 14.4, 18.0, 14.5, 7.7] }) jp jp.wave_m.mean() jp.seawall_m.mean() jp.wave_m.mean() - jp.seawall_m.mean() jp.wave_m.std() jp.seawall_m.std() jp.wave_m.std() - jp.seawall_m.std() 3.7 Packages 3.7.1 Installing packages Use pip to install packages. Do this once per environment. import ensurepip ensurepip.bootstrap() %pip install pandas plotnine dfply 3.7.2 Importing packages import pandas as p from plotnine import * from dfply import * import matplotlib.pyplot as plt The Pipeline In Python we can use dfplyâ€™s pipeline operator &gt;&gt; to connect data to functions. This reduces parentheses and keeps sequences readable. But it is not as usable as the pipe operator in R. It can only pipe dataframes to common dfply / dplyr functions like select, mutate, summarize, etc. from dfply import * sw &gt;&gt; select(X.height) sw &gt;&gt; mutate(y = X.height ** X.height) sw &gt;&gt; summarize(mean_value = mean(X.height)) 3.8 Visualizing Data with Histograms We can visualize with matplotlib/pandas, or use plotnine (a Python port of Râ€™s ggplot2) to develop detailed, customized visuals. 3.8.1 pandas/matplotlib import pandas as p import matplotlib.pyplot as pltI allsw = p.DataFrame({ &#39;height&#39;: [4, 4.5, 5, 5, 5.5, 5.5, 5.5, 6, 6, 6.5, 4, 4, 4, 4, 4.5, 4.5, 4.5, 5, 5, 6, 5.5, 6, 6.5, 6.5, 7, 7, 7, 7.5, 7.5, 8], &#39;states&#39;: [&quot;MA&quot;]*10 + [&quot;RI&quot;]*10 + [&quot;ME&quot;]*10 }) allsw.hist() ## array([[&lt;Axes: title={&#39;center&#39;: &#39;height&#39;}&gt;]], dtype=object) 3.8.2 geom_histogram() in plotnine from plotnine import * g = (ggplot(allsw, aes(x=&#39;height&#39;)) + geom_histogram(color=&quot;white&quot;, fill=&quot;steelblue&quot;, binwidth=0.5) + labs(x=&quot;Seawall Height&quot;, y=&quot;Frequency (# of cities)&quot;) ) g ## &lt;plotnine.ggplot.ggplot object at 0x312406e40&gt; Facet by state: g = (ggplot(allsw, aes(x=&#39;height&#39;)) + geom_histogram(color=&quot;white&quot;, fill=&quot;steelblue&quot;, binwidth=0.5) + labs(x=&quot;Seawall Height&quot;, y=&quot;Frequency (# of cities)&quot;) + facet_wrap(&#39;~states&#39;)) g ## &lt;plotnine.ggplot.ggplot object at 0x313d57f50&gt; Learning Check 4 Question Using a list named sw, draw a histogram of the seawall heights: 4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4. Use pandas or plotnine. [View Answer!] import pandas as p sw = [4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4] g = p.Series(sw).hist() g # or you could do it like this! # hist(sw) Learning Check 5 Question Make a histogram of jp['seawall_m'] from Learning Check 3 using (1) pandas and (2) plotnine. [View Answer!] g = jp.seawall_m.hist() g g = (ggplot(jp, aes(x=&#39;seawall_m&#39;)) + # adjust binwidth for clearer visualization geom_histogram(binwidth=0.5)) g ## &lt;plotnine.ggplot.ggplot object at 0x313fe9cd0&gt; Conclusion Next Steps Weâ€™ll keep building skills: working with data types in Python calculating meaningful statistics in Python visualizing meaningful trends in Python Advice Clear variables as needed with del or restart the session. Clear console outputs and save often. Download or upload files from the Files pane as needed. Youâ€™ll be a rockstar at using Python in no time! Stay tuned for our next Workshop! Troubleshooting If your session freezes, use Session &gt;&gt; Restart Session. If that doesnâ€™t work, relaunch the project from the top banner menu. If that doesnâ€™t work, let me know! "],["fmea-in-r.html", "4 FMEA in R Getting Started 4.1 Example: Ben and Jerryâ€™s Ice Cream 4.2 Calculating Criticality Learning Check 1 Conclusion", " 4 FMEA in R This tutorial will introduce you to Failure Modes and Effects Analysis (FMEA) in R! Getting Started Please open up your project on Posit.Cloud, for our Github class repository. Start a new R script (File &gt;&gt; New &gt;&gt; R Script). Save the R script as lesson_3.R. And letâ€™s get started! Load Packages # Load tidyverse, which contains dplyr and most data wrangling functions library(tidyverse) # Load DiagrammeR, which we&#39;ll use to make diagrams today! library(DiagrammeR) 4.1 Example: Ben and Jerryâ€™s Ice Cream Ben and Jerryâ€™s main headquarters is in Waterbury, VT, just outside of Burlington, where it makes a lot of ice cream. (Itâ€™s also fun to visit.) Their staff likely has to take considerable care to make sure that all that ice cream stays refrigerated! Suppose Ben and Jerryâ€™s has decided to build a new ice cream production plant in Ithaca, NY. For the sake of Ben and Jerryâ€™s (nay, the world!) letâ€™s use Failure Modes and Effects Analysis (FMEA) to identify any hypothetical vulnerability that might occur at this new ice cream business! 4.1.1 Scope &amp; Resolution As our scope, weâ€™re going to just focus on melting. What are all the possible ways that ice cream could melt during this process? Melting would have several negative impacts, such as getting exposed to heat, bacteria, and, worst of all, melting the ice cream! This example is primarily people-centric, because itâ€™s important to remember that people are part of our technological systems! 4.1.2 Measuring Criticality FMEA includes uses 3 measures to calculate a criticality index, meaning the overall risk of each combination of severity and underlying conditions. $ severity occurence detection = criticality $ Each gets classified on a scale from 1-10: severity: 1 = none, 10 = hazardous/catastrophic occurrence: 1 = almost impossible, 10 = almost certain detection: 1 = almost certain, 10 = almost impossible These will produce a criticality index from 1 to 1000. Suppose we want to be 99% sure that our technology wonâ€™t fail and negatively impact society. We would need a criticality index (also known as RPN) of 990 points or less! (Because 1000 - 10 = 990).) So, letâ€™s analyze them! 4.1.3 Block Diagram Below, weâ€™ve visualized what the process of shipping out ice cream looks like once it has been made. This involves the following several steps: Worker 1 puts ice cream in Freezer Worker 2 loads ice cream into Truck Worker 3 transports ice cream to Store Also, Worker 2 takes the ice cream from the Freezer for loading And Worker 3 drives the ice cream from loading dock to Store Plus, several possible failure modes are involved, as discussed below. Figure 4.1: Ben &amp; Jerryâ€™s Ice Cream Block Diagram 4.1.4 Failure Modes Weâ€™ll make a tidy data.frame() of each of the ways our block diagram above could fail, which were contained above in failures. Weâ€™ll call this data.frame f. f &lt;- tibble( # Make a vector of routes to failure failure_mode = c( &quot;freezer --&gt; fail_break&quot;, &quot;loading --&gt; fail_time&quot;, &quot;loading --&gt; fail_eat&quot;, &quot;transport --&gt; fail_time&quot;, &quot;transport --&gt; fail_eat&quot;) ) # Worker 2 could leave ice cream out while loading # Worker 2 could eat the ice cream while loading # Worker 3 could leave the ice cream out in transit # Worker 3 could eat the ice cream in transit 4.2 Calculating Criticality Next, weâ€™re going to make a few judgement calls, to calculate the overall risk for this FMEA. 4.2.1 Estimate Severity Whatâ€™s the severity of the effects of these failures, on a scale from 1 (low) to 10 (high)? Weâ€™ll mutate() the data.frame to include a new column severity, and save it as a new data.frame f1. fail_break: Itâ€™s pretty bad it the freezer breaks; that could ruin days worth of product. Letâ€™s call that an 8. Not catastrophic, but not good for the company! fail_time: Itâ€™s not great it a single shipment gets left out and melts while waiting for pickup. But itâ€™s just one shipment. Letâ€™s call that a 5. fail_eat: How much ice cream could one worker really eat? Thatâ€™s probably a 1. f1 &lt;- f %&gt;% mutate(severity = c(8, 5, 1, 5, 1)) # Check out the contents! f1 ## # A tibble: 5 Ã— 2 ## failure_mode severity ## &lt;chr&gt; &lt;dbl&gt; ## 1 freezer --&gt; fail_break 8 ## 2 loading --&gt; fail_time 5 ## 3 loading --&gt; fail_eat 1 ## 4 transport --&gt; fail_time 5 ## 5 transport --&gt; fail_eat 1 4.2.2 Estimate Occurrence How often does this occur, from 1 (almost never) to 10 (almost always)? Letâ€™s rank occurrence as follows: fail_break: Itâ€™s pretty rare that the freezer would break (eg. 2). fail_time: Itâ€™s probably somewhat rare that shipments melt (eg. 5). fail_eat: If I were a worker, I would eat that all the time (eg. 8). f2 &lt;- f1 %&gt;% mutate(occurrence = c(2, 5, 8, 5, 8)) 4.2.3 Estimate Detection Finally, how likely is it that we would detect the occurrence? If very likely, thatâ€™s a 1. If very unlikely, thatâ€™s a 10. fail_break: Workers would very quickly detect if the freezer were broken. (eg. 1). fail_time: You might not know it had melted until the product gets to the store. (eg. 8) fail_eat: Might get caught. Low chance. (eg. 3). f3 &lt;- f2 %&gt;% mutate(detection = c(1, 8, 3, 8, 3)) 4.2.4 Estimate Criticality (RPN) Using our data in f3, letâ€™s estimate criticality (aka RPN, the risk priority number). f4 &lt;- f3 %&gt;% mutate(criticality = severity * occurrence * detection) We can add up the criticality/RPN to estimate the total risk priority, out of 1000, which is the max_criticality possible. We can divide these two to get the probability of system failure. Is that risk greater than 0.010, aka 0.1%? If so, bad news! f4 %&gt;% summarize( total_criticality = sum(criticality), max_criticality = 10*10*10, probability = total_criticality / max_criticality) ## # A tibble: 1 Ã— 3 ## total_criticality max_criticality probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 464 1000 0.464 Well, thatâ€™s not good! Looks like the new factory will need to figure out a way to keep their product from melting! (In reality, Iâ€™m sure Ben and Jerryâ€™s has strict quality control!) Learning Check 1 Question What other ways could failure occur here? Add three more kinds of failure to your tibble f, then estimate their severity, occurence, detection, and criticality, and recalculate the total probability of failure at this ice cream plant. [View Answer!] fprime &lt;- tibble( # Make a vector of routes to failure failure_mode = c( &quot;freezer --&gt; fail_break&quot;, &quot;loading --&gt; fail_time&quot;, &quot;loading --&gt; fail_eat&quot;, &quot;transport --&gt; fail_time&quot;, &quot;transport --&gt; fail_eat&quot;, # Technically, worker 1 could eat it before taking it to freezer &quot;w1 --&gt; fail_eat&quot;, # A fourth worker at the store could eat it before delivering it &quot;w4 --&gt; fail_eat&quot;, # The fourth worker could also leave it out! &quot;w4 --&gt; fail_time&quot;) ) %&gt;% # Estimate quantities of interest mutate(severity = c(8, 5, 1, 5, 1, 1, 1, 5), occurrence = c(2, 5, 8, 5, 8, 8, 8, 5), detection = c(1, 8, 3, 8, 3, 3, 3, 8)) %&gt;% # Calculate criticality mutate(criticality = severity * occurrence * detection) # Let&#39;s check it out! fprime ## # A tibble: 8 Ã— 5 ## failure_mode severity occurrence detection criticality ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 freezer --&gt; fail_break 8 2 1 16 ## 2 loading --&gt; fail_time 5 5 8 200 ## 3 loading --&gt; fail_eat 1 8 3 24 ## 4 transport --&gt; fail_time 5 5 8 200 ## 5 transport --&gt; fail_eat 1 8 3 24 ## 6 w1 --&gt; fail_eat 1 8 3 24 ## 7 w4 --&gt; fail_eat 1 8 3 24 ## 8 w4 --&gt; fail_time 5 5 8 200 # Let&#39;s calculate the total risk! fprime %&gt;% summarize( total_criticality = sum(criticality), max_criticality = 10*10*10, probability = total_criticality / max_criticality) ## # A tibble: 1 Ã— 3 ## total_criticality max_criticality probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 712 1000 0.712 Ooph! Not good! Conclusion Great work! All done! See you in class! "],["visualization-with-ggplot-in-r.html", "5 Visualization with ggplot in R Getting Started 5.1 Your first scatterplot Learning Check 1 Learning Check 2 Learning Check 3 5.2 Improving our Visualizations Learning Check 4 Learning Check 5 5.3 Visualizing diamonds data Learning Check 6 5.4 Visualizing Distributions Learning Check 9 Conclusion", " 5 Visualization with ggplot in R Visualization is a key part of statistical analyses, especially in systems engineering! Visuals themselves are often the analysis themselves! In this tutorial, weâ€™re going to learn how to visualize data in the ggplot2 package. Please follow along using the code below! Getting Started Loading Packages Letâ€™s load our packages with library(). # Data viz and data manipulation packages library(ggplot2) library(dplyr) # Data sources library(gapminder) Notes: SAVE YOUR SCRIPT. Always comment your code (what Iâ€™m doing now), use lots of spaces, and keep it clean. Gapminder data Economist Hans Rosling made a dataset that examines change in life expectancy over time for most countries in the world. It is contained in the gapminder package! # Let&#39;s view it. (see console below) gapminder ## # A tibble: 1,704 Ã— 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # â„¹ 1,694 more rows Each row is a country-year, marking the life expectancy, population, and gross domestic product (GDP) per capita. On your end, you can only can see some of it, right? Letâ€™s check out what vectors are in this dataframe, using the glimpse function from the dplyr package. # (Remember, a vector is a column in a spreadsheet; # a data.frame is a spreadsheet.) glimpse(gapminder) ## Rows: 1,704 ## Columns: 6 ## $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, â€¦ ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, â€¦ ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, â€¦ ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8â€¦ ## $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12â€¦ ## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, â€¦ # Nice, we can see things more concisely. Our data has six variables. Great! 5.1 Your first scatterplot Using the gapminder data, letâ€™s map a series of vectors to become aesthetic features in the visualization (point, colors, fills, etc.). ggplot(data = gapminder, mapping = aes( # Let&#39;s make the x-axis gross-domestic product per capita (wealth per person) x = gdpPercap, # Let&#39;s make the y-axis country life expectancy y = lifeExp)) Huh! We made an empty graph. Cool. Thatâ€™s because ggplot needs helper functions to add aesthetic features to the graph. For example, adding + geom_point() will overlay a scatterplot. # Make a scatterplot ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + # same as above, except &quot;+&quot; geom_point() Learning Check 1 Question What kind of relationship does this graph show? Why might it matter to policymakers? [View Answer!] The graph above shows that as average wealth (GDP per capita) in a country increases, those countriesâ€™ life expectancy increases swiftly, but then tapers off. This highlights that there is a strong relationship between wealth and health globally. Learning Check 2 Question What happens when you add the alpha, changing its values in the 3 visuals below? # Run the following code: ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.2) ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.5) ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 1) [View Answer!] alpha ranges from 0 to 1 and describes feature transparency. Increasing alpha to 1 makes points fully opaque! Decreasing alpha to 0 makes points fully transparent! Learning Check 3 Question We can make it more visually appealing. What happens when we do each of the following? If you want to make it a single color, where do you need to write color = ...? If you want to make it multiple colors according to a vector, where do you need to write color =? # Run the following code: # Version 1 ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.5, color = &quot;steelblue&quot;) # Version 2 ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(alpha = 0.5) [View Answer!] To assign a single color, you need to put color outside the aes() phrase, and write the name of the color. To assign multiple colors, you need to put the color inside the aes(...) phrase, and write the name of the vector in the data that it corresponds to (eg. continent). 5.2 Improving our Visualizations We can (and should!) make our visualizations much more readable by adding appropriate labels. ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(alpha = 0.5) + # Add labels! labs(x = &quot;GDP per capita (USD)&quot;, # label for x-values y = &quot;Life Expectancy (years)&quot;, # label for y-values color = &quot;Continent&quot;, # label for colors title = &quot;Does Wealth affect Health?&quot;, # overall title subtitle = &quot;Global Health Trends by Continent&quot;, # subtitle! caption = &quot;Points display individual country-year observations.&quot;) # caption We can actually save visualizations as objects too, which can make things faster. Letâ€™s save our visual as myviz myviz &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(alpha = 0.5) + labs(x = &quot;GDP per capita (USD)&quot;, y = &quot;Life Expectancy (years)&quot;, color = &quot;Continent&quot;, title = &quot;Does Wealth affect Health?&quot;, # overall title subtitle = &quot;Global Health Trends by Continent&quot;, # subtitle! caption = &quot;Points display individual country-year observations.&quot;) # caption Next, letâ€™s try a few more learning check that will ask you to try our ways to improve the quality and readability of your visuals! Learning Check 4 Question Now run myviz - what happens? myviz [View Answer!] When you save a ggplot to an object, eg. naming it myviz, you can call up the visual again as many times as you want by just running the myviz object, just like any other object. Learning Check 5 Question We can do better, adding things onto our myviz object! Try changing themes. What happens below? # Version theme_bw myviz + # How about this theme? theme_bw() # Version theme_dark myviz + # How about this theme? theme_dark() # Version theme_classic myviz + # How about this theme? theme_classic() [View Answer!] theme_bw() makes a nice black-and-white graph; theme_dark() makes a funky graph with a dark grey background; theme_classic() makes a very simple graph, with fewer distractions. I personally really like the default theme or theme_bw(). Sometimes theme_classic() can be really helpful if you have a particularly busy visual. 5.3 Visualizing diamonds data Next, letâ€™s use the diamonds dataset, which comes with the ggplot2 package This is a dataset of over 50,000 diamond sales. # Check out first 3 rows... diamonds %&gt;% head(3) ## # A tibble: 3 Ã— 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 We can use this visualization to check whether the cut of diamonds really has any relationship with price. glimpse(diamonds) ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.â€¦ ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Verâ€¦ ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,â€¦ ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, â€¦ ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64â€¦ ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58â€¦ ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34â€¦ ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.â€¦ ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.â€¦ ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.â€¦ Looks like cut is an ordinal variable (fair, good, ideal, etc.), while price is numeric (eg. dollars). A boxplot might be helpful! ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut)) + # notice how we added group = cut, to tell it to use 5 different boxes, one per cut? geom_boxplot() Huh. How odd. Looks like the cut of diamonds has very little impact on what price they are sold at! We can see lots of outliers at the top - really expensive diamonds for that cut. Learning Check 6 Question Letâ€™s make this visualization more visually appealing. What changed in the code to make these two different visual effects? Why? (Hint: fill.) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut)) + geom_boxplot(fill = &quot;steelblue&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() [View Answer!] In the first visual, we assigned all the boxplots to have the same fill (fill = \"steelblue\"), but in the second visual, we assigned the boxplot fill to be shaded based on the cut of diamond. This adds a cool color range! 5.4 Visualizing Distributions Different geom_ functions use colors in different ways, but this is a good example. For example, below is a histogram. It visualizes the approximate distribution of a set of values. We can see how frequently diamonds are sold for certain prices versus others. ggplot(data = diamonds, mapping = aes(x = price, group = cut, fill = cut)) + geom_histogram(color = &quot;white&quot;) + # notice new function here labs(x = &quot;Price (USD)&quot;, y = &quot;Frequency of Price (Count)&quot;, title = &quot;US Diamond Sales&quot;) Learning Check 9 Question Are most diamonds cheap or expensive? What type of distribution would you call this? Normal? Uniform? Left Skewed? Right Skewed? [View Answer!] This is strongly right-skewed distribution, because the majority of the distribution leans to the left (the clump of the data), while it has a long tail that skews to the right. The median is less than the mean in a right skewed distribution. Conclusion You made it! You have now tried out a series of visuals in ggplot. We will use ggplot a lot in this course, so please be sure to reach out when you have questions, talk with others in your group, and work together to build great visualization skills! (Plus, itâ€™s super applicable professionally!) "],["visualization-with-plotnine-in-python.html", "6 Visualization with plotnine in Python Getting Started 6.1 Your first scatterplot Learning Check 1 6.2 Transparency (alpha) Learning Check 2 6.3 Color: constant vs mapped Learning Check 3 6.4 Improving our visualizations 6.5 Visualizing diamonds data Learning Check 4 6.6 Visualizing Distributions Learning Check 5 Conclusion", " 6 Visualization with plotnine in Python Visualization is a key part of statistical analyses, especially in systems engineering. In this tutorial, weâ€™ll learn to visualize data with plotnine (the Python port of ggplot2). Please follow along using the code below! Getting Started Loading Packages import pandas as p from plotnine import * from gapminder import gapminder as gapminder from seaborn import load_dataset # diamonds from seaborn diamonds = load_dataset(&#39;diamonds&#39;) Gapminder data # View it in the console gapminder ## country continent year lifeExp pop gdpPercap ## 0 Afghanistan Asia 1952 28.801 8425333 779.445314 ## 1 Afghanistan Asia 1957 30.332 9240934 820.853030 ## 2 Afghanistan Asia 1962 31.997 10267083 853.100710 ## 3 Afghanistan Asia 1967 34.020 11537966 836.197138 ## 4 Afghanistan Asia 1972 36.088 13079460 739.981106 ## ... ... ... ... ... ... ... ## 1699 Zimbabwe Africa 1987 62.351 9216418 706.157306 ## 1700 Zimbabwe Africa 1992 60.377 10704340 693.420786 ## 1701 Zimbabwe Africa 1997 46.809 11404948 792.449960 ## 1702 Zimbabwe Africa 2002 39.989 11926563 672.038623 ## 1703 Zimbabwe Africa 2007 43.487 12311143 469.709298 ## ## [1704 rows x 6 columns] # Glimpse-like summary gapminder.dtypes, gapminder.shape ## (country object ## continent object ## year int64 ## lifeExp float64 ## pop int64 ## gdpPercap float64 ## dtype: object, (1704, 6)) 6.1 Your first scatterplot ggplot(data=gapminder, mapping=aes(x=&#39;gdpPercap&#39;, y=&#39;lifeExp&#39;)) ## &lt;plotnine.ggplot.ggplot object at 0x31651ac90&gt; Add points with + geom_point(). (ggplot(gapminder, aes(x=&#39;gdpPercap&#39;, y=&#39;lifeExp&#39;)) + geom_point()) ## &lt;plotnine.ggplot.ggplot object at 0x3165188c0&gt; Learning Check 1 Question What kind of relationship does this graph show? Why might it matter to policymakers? [View Answer!] As wealth per person (GDP per capita) increases, life expectancy rises quickly then tapers off. This shows a strong relationship between wealth and health. 6.2 Transparency (alpha) (ggplot(gapminder, aes(x=&#39;gdpPercap&#39;, y=&#39;lifeExp&#39;)) + geom_point(alpha=0.2)) ## &lt;plotnine.ggplot.ggplot object at 0x31655b0e0&gt; (ggplot(gapminder, aes(x=&#39;gdpPercap&#39;, y=&#39;lifeExp&#39;)) + geom_point(alpha=0.5)) ## &lt;plotnine.ggplot.ggplot object at 0x31655af00&gt; (ggplot(gapminder, aes(x=&#39;gdpPercap&#39;, y=&#39;lifeExp&#39;)) + geom_point(alpha=1)) ## &lt;plotnine.ggplot.ggplot object at 0x3165a02c0&gt; Learning Check 2 Question What happens when you change alpha across the three visuals above? [View Answer!] alpha controls transparency from 0 to 1. Higher values are more opaque; lower values are more transparent. 6.3 Color: constant vs mapped # Single color (ggplot(gapminder, aes(x=&#39;gdpPercap&#39;, y=&#39;lifeExp&#39;)) + geom_point(alpha=0.5, color=&#39;steelblue&#39;)) ## &lt;plotnine.ggplot.ggplot object at 0x3165a0110&gt; # Color mapped by continent (ggplot(gapminder, aes(x=&#39;gdpPercap&#39;, y=&#39;lifeExp&#39;, color=&#39;continent&#39;)) + geom_point(alpha=0.5)) ## &lt;plotnine.ggplot.ggplot object at 0x3165886e0&gt; Learning Check 3 Question Where do you place color for a single color vs.Â multiple colors based on a variable? [View Answer!] Single color: set color inside geom_point(color='...') (outside aes). Mapped colors: set color inside aes(color='variable'). 6.4 Improving our visualizations (ggplot(gapminder, aes(x=&#39;gdpPercap&#39;, y=&#39;lifeExp&#39;, color=&#39;continent&#39;)) + geom_point(alpha=0.5) + labs(x=&#39;GDP per capita (USD)&#39;, y=&#39;Life Expectancy (years)&#39;, color=&#39;Continent&#39;, title=&#39;Does Wealth affect Health?&#39;, subtitle=&#39;Global Health Trends by Continent&#39;, caption=&#39;Points display individual country-year observations.&#39;)) ## &lt;plotnine.ggplot.ggplot object at 0x31658ed50&gt; You can save visuals as objects to reuse them. myviz = (ggplot(gapminder, aes(x=&#39;gdpPercap&#39;, y=&#39;lifeExp&#39;, color=&#39;continent&#39;)) + geom_point(alpha=0.5) + labs(x=&#39;GDP per capita (USD)&#39;, y=&#39;Life Expectancy (years)&#39;, color=&#39;Continent&#39;, title=&#39;Does Wealth affect Health?&#39;, subtitle=&#39;Global Health Trends by Continent&#39;, caption=&#39;Points display individual country-year observations.&#39;)) myviz ## &lt;plotnine.ggplot.ggplot object at 0x31658a8d0&gt; myviz + theme_bw() ## &lt;plotnine.ggplot.ggplot object at 0x31655a900&gt; myviz + theme_dark() ## &lt;plotnine.ggplot.ggplot object at 0x316518f80&gt; myviz + theme_classic() ## &lt;plotnine.ggplot.ggplot object at 0x31658bf50&gt; 6.5 Visualizing diamonds data diamonds.head(3) ## carat cut color clarity depth table price x y z ## 0 0.23 Ideal E SI2 61.5 55.0 326 3.95 3.98 2.43 ## 1 0.21 Premium E SI1 59.8 61.0 326 3.89 3.84 2.31 ## 2 0.23 Good E VS1 56.9 65.0 327 4.05 4.07 2.31 diamonds.dtypes, diamonds.shape ## (carat float64 ## cut category ## color category ## clarity category ## depth float64 ## table float64 ## price int64 ## x float64 ## y float64 ## z float64 ## dtype: object, (53940, 10)) 6.5.1 Boxplots by cut (ggplot(diamonds, aes(x=&#39;cut&#39;, y=&#39;price&#39;, group=&#39;cut&#39;)) + geom_boxplot()) ## &lt;plotnine.ggplot.ggplot object at 0x3165a0770&gt; (ggplot(diamonds, aes(x=&#39;cut&#39;, y=&#39;price&#39;, group=&#39;cut&#39;)) + geom_boxplot(fill=&#39;steelblue&#39;)) ## &lt;plotnine.ggplot.ggplot object at 0x3165da510&gt; (ggplot(diamonds, aes(x=&#39;cut&#39;, y=&#39;price&#39;, group=&#39;cut&#39;, fill=&#39;cut&#39;)) + geom_boxplot()) ## &lt;plotnine.ggplot.ggplot object at 0x3165ece90&gt; Learning Check 4 Question Why do the two boxplot versions look different? What changed in the code to create those effects? [View Answer!] Constant fill uses geom_boxplot(fill='steelblue'). Mapped fill uses aes(fill='cut') to color by variable. 6.6 Visualizing Distributions (ggplot(diamonds, aes(x=&#39;price&#39;, fill=&#39;cut&#39;)) + geom_histogram(color=&#39;white&#39;) + labs(x=&#39;Price (USD)&#39;, y=&#39;Frequency&#39;, title=&#39;US Diamond Sales&#39;)) ## &lt;plotnine.ggplot.ggplot object at 0x3164ebce0&gt; 6.6.1 Try adjusting binwidth and theme (ggplot(diamonds, aes(x=&#39;price&#39;, fill=&#39;cut&#39;)) + geom_histogram(color=&#39;white&#39;, binwidth=500) + theme_classic() + labs(x=&#39;Price (USD)&#39;, y=&#39;Frequency&#39;, title=&#39;US Diamond Sales (binwidth=500)&#39;)) ## &lt;plotnine.ggplot.ggplot object at 0x31658f680&gt; Learning Check 5 Question Make a histogram of price with a narrower binwidth and apply a different theme. Which choices improve readability? [View Answer!] (ggplot(diamonds, aes(x=&#39;price&#39;, fill=&#39;cut&#39;)) + geom_histogram(color=&#39;white&#39;, binwidth=250) + theme_bw() + labs(x=&#39;Price (USD)&#39;, y=&#39;Frequency&#39;)) ## &lt;plotnine.ggplot.ggplot object at 0x3164eb650&gt; Conclusion You learned how to build scatterplots, boxplots, and histograms with plotnine and how to control transparency, color mapping, labels, and themes. "],["distributions-and-descriptive-statistics-in-r.html", "7 Distributions and Descriptive Statistics in R Getting Started 7.1 Distributions 7.2 Descriptive Statistics 7.3 Our Data 7.4 Size 7.5 Location 7.6 Spread (1) Learning Check 1 7.7 Spread (2) Learning Check 2 7.8 Shape Learning Check 3 7.9 Simulating Distributions 7.10 Finding Parameters for your Distribution 7.11 Common Distributions 7.12 Special Distributions 7.13 Comparing Distributions Learning Check 4 Conclusion", " 7 Distributions and Descriptive Statistics in R Figure 7.1: Yay Distributions! This tutorial will introduce you to how to code and analyses distributions in R, using descriptive statistics and visualization! Getting Started Please open up your Posit.Cloud project. Start a new R script (File &gt;&gt; New &gt;&gt; R Script). Save the R script as workshop_2.R. And letâ€™s get started! Load Packages Weâ€™re going to use extra functions from 3 packages today, including ggplot2, dplyr (pronounced DIP-LER), and MASS. [Note: Please be sure to load them first, otherwise your functions will not work.] library(ggplot2) # for visualization library(dplyr) # for pipelines! library(MASS) # for fitting distributions 7.1 Distributions Any vector can be expressed as a distribution (especially numeric vectors). A distribution stacks the values in a vector in order from lowest to highest to show the frequency of values. There are several ways to visualize distributions, including histograms, density plots, violin plots, jitter plots, ribbon plots, and more; the most common are histograms and density plots, which we will learn today. For example, Figure 2 shows our seawall vector from Workshop 1 in part A (left). In part B (right), that vector is shown as a distribution: its blocks are stacked to make a histogram (bars), while the distribution itself (line) is approximated by a curve, known as a density function. Figure 7.2: Figure 2: Seawall Vector as a Distribution Any distribution can be described with 4 traits, shown above in part C. These include: Size (how many values are in it), Location (eg. where is it clumped), Spread (how much do values vary?), and Shape (eg. bell curve). 7.2 Descriptive Statistics 7.2.1 Whatâ€™s a statistic? Whatâ€™s a statistic? A statistic is a single number that summarizes something about a sample. Thatâ€™s it! No magic! Statistics is the process of making statistics (eg. many single numbers) so we can understand samples of data! They help people make decisions when faced with uncertainty. Weâ€™ll learn several functions to make statistics that describe our distributions. Trait Meaning Type Functions Size How many values? statistics length() Location Where is it clumped? statistics mean(), median() Spread How much do values vary? statistics sd(), var(), range(), quantile() Shape What shape does it resemble? distributions rnorm(), rbinom(), rpois(),&lt;br&gt;[skewness &amp; kurtosis - no functions] 7.3 Our Data Below, we will learn several functions for describing Size, Location, and Spread in a distribution. (Weâ€™ll get to shape in a minute.) To do this, weâ€™re going to use a data sample of seawalls, describing the height in meters of several citiesâ€™ seawalls. Letâ€™s encode that vector below. # You could code it as a vector, save it as an object, then use your functions! sw &lt;- c(4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4) # View it sw ## [1] 4.5 5.0 5.5 5.0 5.5 6.5 6.5 6.0 5.0 4.0 7.4 Size 7.4.1 Length How big is our sample? Use length() on a vector to find the number of values in the vector sw we made in LC1. length(sw) ## [1] 10 7.5 Location Where is our sample clumped? Figure 7.3: Figure 3: Statistics for Location 7.5.1 Mean Use mean() and median() to find the most central values. sw %&gt;% mean() ## [1] 5.35 7.5.2 Median sw %&gt;% median() ## [1] 5.25 7.5.3 Mode Fun fact: mode() doesnâ€™t work in R; itâ€™s huge pain. You have to use this code instead. sw %&gt;% table() %&gt;% sort(decreasing = TRUE) ## . ## 5 5.5 6.5 4 4.5 6 ## 3 2 2 1 1 1 7.6 Spread (1) How much does our sample vary? Figure 7.4: Figure 4: Statistics for Spread 7.6.1 Percentile Use quantile() to check for any percentile in a vector, from 0 (min) to 0.5 (median) to 1 (max). If you have quantile(), you donâ€™t need to remember min(), max(), range(), or even median(). sw %&gt;% quantile(probs = 0) # min ## 0% ## 4 sw %&gt;% quantile(probs = 1) # max ## 100% ## 6.5 Learning Check 1 Question Your team took a series of air quality measurements from a sample of sensors across Ithaca. While there are 100 sensors in Ithaca, due to time limitations, you accessed just a random sample of sensors. Air Quality Index scores: 12, 24, 50, 35, 36, 37, 40, 25, 28, 30, 32, 28 Please convert the following values into a vector named aqi! What was the sample size of the vector? What was the interquartile range of air quality measurements, meaning the 25th to 75th percentiles?? [View Answer!] Please convert the following values into a vector named aqi! # Make a vector of air quality index scores... aqi = c(12, 24, 50, 35, 36, 37, 40, 25, 28, 30, 32, 28) What was the sample size of the vector? # Get the length of the vector... length(aqi) ## [1] 12 What was the interquartile range of air quality measurements, meaning the 25th to 75th percentiles?? # 25% of air quality measurements were at or below this value quantile(aqi, probs = 0.25) ## 25% ## 27.25 # 75% of air quality measurements were at or below this value: quantile(aqi, probs = 0.75) ## 75% ## 36.25 7.7 Spread (2) 7.7.1 Standard Deviation But we can also evaluate how much our values vary from the mean on average - the standard deviation, often abbreviated as \\(\\sigma\\) (sigma). This is written as: $ = $ Figure 7.5: Figure 5: Standard Deviation, the ultimate Statistic for Spread We can calculate this â€˜by handâ€™, or use the sd() function. # Calculating in R still faster than on your own! sqrt( sum((sw - mean(sw))^2) / (length(sw) - 1) ) ## [1] 0.8181958 # Get the standard deviation by code! sw %&gt;% sd() ## [1] 0.8181958 7.7.2 Variance Sometimes, we might want the variance, which is the standard deviation squared. This accentuates large deviations in a sample. # Get the variance! sw %&gt;% var() ## [1] 0.6694444 sd(sw)^2 ## [1] 0.6694444 # See? var = sd^2! 7.7.3 Coefficient of Variation (CV) We could also calculate the coefficient of variation (CV), meaning how great a share of the mean does that average variation constitute? (Also put, how many times does the mean fit into the standard deviation.) sd(sw) / mean(sw) ## [1] 0.1529338 The standard deviation constitutes 15% of the size of the mean seawall height. 7.7.4 Standard Error (SE) But these numbers donâ€™t have much meaning to us, unless we know seawalls really well. Wouldnâ€™t it be nice if we had a kind of uniform measure, that told us how big is the variation in the data, given how big the data is itself? Good news! We do! We can calculate the sample size-adjusted variance like so: var(sw) / length(sw) ## [1] 0.06694444 # or sd(sw)^2 / length(sw) ## [1] 0.06694444 This means we could take this set of seawalls and compare it against samples of coastal infrastructure in Louisiana, in Japan, in Australia, and make meaningful comparisons, having adjusted for sample size. However, sample-size adjusted variance is a little bit of a funky concept, and so itâ€™s much more common for us to use the sample-size adjusted standard deviation, more commonly known as the standard error, or se. $ SE = = = $ # Calculated as: se &lt;- sd(sw) / sqrt(length(sw)) # Or as: se &lt;- sqrt( sd(sw)^2 / length(sw) ) # Or as: se &lt;- sqrt( var(sw) / length(sw)) # See standard error se ## [1] 0.2587362 Learning Check 2 Question Suppose we collected data on 10 randomly selected chunks of cheese from a production line! We measured their moisture in grams (g) in each product We want to make sure weâ€™re making some quality cheesy goodness, so letâ€™s find out how much those moisture (cheesiness) levels vary! The moisture in our cheese weighed 5.52 g, 5.71 g, 5.06 g, 5.10 g, 4.98 g, 5.50 g, 4.81 g, 5.55 g, 4.74 g, &amp; 5.39 g. Please convert the following values into a vector named cheese! What was the average moisture level in the sample? How much did moisture levels vary, on average? We need to compare these levels with cheese produced in Vermont, France, and elsewhere. Whatâ€™s the coefficient of variance and standard error for these moisture levels? [View Answer!] Please convert the following values into a vector named cheese! cheese &lt;- c(5.52, 5.71, 5.06, 5.10, 4.98, 5.50, 4.81, 5.55, 4.74, 5.39) What was the average moisture level in the sample? # Get mean of values mean(cheese) ## [1] 5.236 How much did moisture levels vary, on average? # Get standard deviation of values. # Fun fact: this is how much they varied on average FROM THE AVERAGE sd(cheese) ## [1] 0.3399085 We need to compare these levels with cheese produced in Vermont, France, and elsewhere. Whatâ€™s the coefficient of variance and standard error for these moisture levels? # Coefficient of variation cv &lt;- sd(cheese) / mean(cheese) # Check it! cv ## [1] 0.06491759 # Standard Error se &lt;- sd(cheese) / sqrt(length(cheese)) # Check it se ## [1] 0.1074885 # When you&#39;re finished, remove extra data. remove(cheese, se, cv) 7.8 Shape How then do we describe the shape of a distribution? We can use skewness and kurtosis for this. Thereâ€™s no direct function for skewness or kurtosis in R, but as youâ€™ll see below, we can quickly calculate it using the functions we already know. 7.8.1 Skewness Skewness describes whether the bulk of the distribution sits to the left or right of the center, and its formula are written out below. It is commonly estimated using the formula on the left, while the formula on the right closely approximates it. (Weâ€™re going to use the right-hand formula below, since itâ€™s a little cleaner.) \\[ Skewness = \\frac{ \\sum^{N}_{i=1}{(x - \\bar{x})^{3} / n } }{ [\\sum^{N}_{i=1}{ (x - \\bar{x})^{2} / n }]^{3/2} } \\approx \\frac{ \\sum^{N}_{i=1}{ (x - \\bar{x})^{3} } }{ (n - 1) \\times \\sigma^{3} } \\] When people say that a certain personâ€™s perspective is skewed, they mean, itâ€™s very far from the mean. In this case, we want to know, how skewed are the heights of seawalls overall compared to the mean? To figure this out, weâ€™ll need 4 ingredients: \\(x_{i \\to N}\\): our vector of values (seawall heights! sw) \\(N\\): the length of our vector (how many seawalls? length(sw)) \\(\\bar{x}\\): our mean value: (the mean seawall height? mean(sw)) \\(\\sigma\\): the standard deviation of our vector (how much do the seawall heights vary on average? sd(sw)) Yeah! You just used them a bunch! So letâ€™s calculate skewness! First, we measure diff, how far is each value from the mean? diff &lt;- sw - mean(sw) # Check it out! diff ## [1] -0.85 -0.35 0.15 -0.35 0.15 1.15 1.15 0.65 -0.35 -1.35 diff measures how far / how skewed each of these values (\\(x\\)) are from the mean \\(\\bar{x}\\)). See the visual below! Next, weâ€™re going to cube diff, to emphasize extreme differences from the mean Squaring would turn everything positive, but we care whether those differences are positive or negative, so we cube it instead. diff^3 ## [1] -0.614125 -0.042875 0.003375 -0.042875 0.003375 1.520875 1.520875 ## [8] 0.274625 -0.042875 -2.460375 Then, weâ€™re going to get a few helper values, like: # Get the sample-size # To be conservative, we&#39;ll subtract 1; this happens often in stats n &lt;- length(sw) - 1 # Get the standard deviation sigma &lt;- sw %&gt;% sd() Now, we can calculate, on average, how big are these cubed differences? sum(diff^3) / n ## [1] 0.01333333 Well, thatâ€™s nifty, how do we compare this funky number to other samples? Weâ€™re going to need to put it in terms of a common unit, a â€œstandardâ€ unit - like the standard deviation! Plus, weâ€™ll have to cube the standard deviation, so that itâ€™s in the same terms as our numerator \\(diff^{3}\\). skew &lt;- sum(diff^3) / ( n * sigma^3) # Check it! skew ## [1] 0.0243426 Voila! A standardized measure you can use to compare the skew of our sample of seawalls to any other sample! For comparison, here are a few other values of skew we might possibly get. 7.8.2 Kurtosis Kurtosis describes how tightly bound the distribution is around the mean. Is it extremely pointy, with a narrow distribution (high kurtosis), or does it span wide (low kurtosis)? We can estimate it using the formula on the left, and the formula on the right is approximately the same. \\[ Kurtosis = \\frac{ \\sum^{N}_{i=1}{(x - \\bar{x})^{4} / n } }{ [\\sum^{N}_{i=1}{ (x - \\bar{x})^{2} / n }]^2 } \\approx \\frac{ \\sum^{N}_{i=1}{ (x - \\bar{x})^{4} } }{ (n - 1) \\times \\sigma^{4} } \\] Like skew, we calculate how far each value is from the mean, but we take those differences to the 4th power (\\((x - \\bar{x})^{4}\\)), which hyper-accentuates any extreme deviations and returns only positive values. Then, we calculate the sample-size adjusted average of those differences. Finally, to measure it in a consistent unit comparable across distributions, we divide by the standard deviation taken to the 4th power; the powers in the numerator and denominator then more-or-less cancel each other out. moments::skewness(sw) ## [1] 0.02565935 # 0.2565 x &lt;- sw sum( (x - mean(x))^3 ) / ((length(x) - 1) *sd(x)^3) ## [1] 0.0243426 a &lt;- sum( (x - mean(x))^3 ) / length(x) b &lt;- (sum( (x - mean(x))^2 ) / length(x))^(3/2) a/b ## [1] 0.02565935 # 0.256 # Get the differences again diff &lt;- sw - mean(sw) # And take them to the fourth power diff^4 ## [1] 0.52200625 0.01500625 0.00050625 0.01500625 0.00050625 1.74900625 ## [7] 1.74900625 0.17850625 0.01500625 3.32150625 Theyâ€™re all positive! Next, same as above, weâ€™ll get the conservative estimate of the sample size (n - 1) and the standard deviation. # Get the sample-size # To be conservative, we&#39;ll subtract 1; this happens often in stats n &lt;- length(sw) - 1 # Get the standard deviation sigma &lt;- sw %&gt;% sd() So when we put it all togetherâ€¦ kurt &lt;- sum(diff^4) / ( n * sigma^4) # Check it! kurt ## [1] 1.875851 We can measure kurtosis! A pretty normal bell curve has a kurtosis of about 3, so our data doesnâ€™t demonstrate much kurtosis. Kurtosis ranges from 0 to infinity (it is always positive), and the higher it goes, the pointier the distribution! Finally, just a heads up: As mentioned above, there are a few different formulas floating around there for skewness and kurtosis, so donâ€™t be too surprised if your numbers vary when calculating it in one package versus another versus by hand. (But, if the numbers are extremely different, thatâ€™s probably a sign something is up.) Learning Check 3 A contractor is concerned that the majority of seawalls in her region might skew lower than their regionâ€™s vulnerability to storms requires. Assume (hypothetically) that our sampleâ€™s seawalls are the appropriate height for our level of vulnerability, and that both regions share the same level of vulnerability. The mean seawall in her region is about the same height as in our sample (~5.35), but how do the skewness and kurtosis of her regionâ€™s seawalls compare to our sample? Her region has 12 seawalls! Their height (in meters) are 4.15, 4.35, 4.47, 4.74, 4.92, 5.19, 5.23, 5.35, 5.55, 5.70, 5.78, &amp; 7.16. Question Calculate these statistics and interpret your results in a sentence or two. [View Answer!] # Make a vector of these 12 seawalls x &lt;- c(4.15, 4.35, 4.47, 4.74, 4.92, 5.19, 5.23, 5.35, 5.55, 5.70, 5.78, 7.16) # Calculate skewness skewness &lt;- sum( (x - mean(x))^3) / ((length(x) - 1) * sd(x)^3) # Calculate Kurtosis kurtosis &lt;- sum( (x - mean(x))^4) / ((length(x) - 1) * sd(x)^4) # View them! c(skewness, kurtosis) ## [1] 0.9016585 3.5201492 Her regionâ€™s seawalls are somewhat positively, right skewed, with a skewness of about +0.90. This is much more skewed than our hypothetical areaâ€™s seawalls, which are skewed at just +0.02. But, her regionâ€™s seawallsâ€™ traits are much more closely clustered around the mean than ours, with a kurtosis of 3.52 compared to our 1.88. Since both hypothetical regions have comparable levels of vulnerability to storm surges, her regionâ€™s seawalls do appear to skew low. 7.9 Simulating Distributions Finally, to describe shape, we need some shapes to compare our distributions to. Fortunately, the rnorm(), rbinom(), rpois(), and rgamma() functions allow us to draw the shapes of several common distributions. Table 2 shows the shape of these distributions, and their ranges. (#tab:w2_table2)(#tab:w2_table2)Table 2: Example Distributions Distributions Span Function Parameters Example Resources Normal -Inf to +Inf rnorm() mean, sd .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } .svglite g.glyphgroup path { fill: inherit; stroke: none; } [Wiki](htt Poisson 0, 1, 2, 3â€¦ rpois() lambda (mean) .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } .svglite g.glyphgroup path { fill: inherit; stroke: none; } [Wiki](htt Gamma 0.1, 2.5, 5.5, +Inf rgamma() shape, rate .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } .svglite g.glyphgroup path { fill: inherit; stroke: none; } [Wiki](htt Exponential same rexp() rate .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } .svglite g.glyphgroup path { fill: inherit; stroke: none; } [Wiki](htt Weibull same rweibull() shape, scale .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } .svglite g.glyphgroup path { fill: inherit; stroke: none; } [Wiki](htt Binomial 0 vs.Â 1 rbinom() probability .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } .svglite g.glyphgroup path { fill: inherit; stroke: none; } [Wiki](htt Uniform min to max runif() min, max .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } .svglite g.glyphgroup path { fill: inherit; stroke: none; } [Wiki](htt Note: Wikipedia is actually a pretty fantastic source on distributions. To determine what kind of distribution our vector has, we can visually compare it using simulation. We can compare our real observed distribution against random distributions to determine whether our data matches the shape of a normal vs.Â poisson distribution, for example. 7.10 Finding Parameters for your Distribution To do so, letâ€™s get some statistics from our data to help us visualize what a distribution with those traits would look like. As our raw data, letâ€™s use our vector of seawall heights sw. # Let&#39;s remake again our vector of seawall heights sw &lt;- c(4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4) To simulate, you feed your simulator function (1) n values to draw and (2) any required statistics necessary for computing draws from that distribution. (For example, rnorm() requires the mean and standard deviation.) Fortunately, statisticians have figured out for us 2 ways to figure out what statistics to provide. There are a few equations called method of moments estimators that do a great job of estimating those statistics. Weâ€™ll learn these below. Alternatively, we can ask R to compute the values of those statistics using the MASS packageâ€™s fitdistr(). You can learn more about this optional add-on function in the Appendix. 7.11 Common Distributions 7.11.1 Normal Distribution rnorm() randomly generates for us any number of values randomly sampled from a normal distribution. We just need to supply: (1) n values to draw, (2) the mean of that distribution, and (3) the sd of that distribution. # For example mymean &lt;- sw %&gt;% mean() mysd &lt;- sw %&gt;% sd() # simulate! mynorm &lt;- rnorm(n = 1000, mean = mymean, sd = mysd) # Visualize! mynorm %&gt;% hist() What were our mean and standard deviation? mymean ## [1] 5.35 mysd ## [1] 0.8181958 Neat! 7.11.2 Poisson Distribution rpois() randomly samples integers (eg. 0, 1, 2, 3) from a poisson distribution, based on lambda, the average rate of occurrence. We can approximate that by taking the mean of sw. mypois &lt;- rpois(1000, lambda = mymean) mypois %&gt;% hist() Results in a somewhat skewed distribution, bounded at zero. Whereâ€™s the mean lambda? mymean ## [1] 5.35 Excellent! 7.11.3 Exponential Distribution rexp() randomly simulates positive real numbers over zero from an exponential distribution. Here, the method of moments says: rate \\(\\approx \\frac{ 1 }{ mean })\\) # We&#39;ll name this myrate2! myrate_e &lt;- 1 / mean(sw) # Simulate it! myexp &lt;- rexp(n = 1000, rate = myrate_e) # Visualize it! myexp %&gt;% hist() So whatâ€™s the rate lambda for our data, assuming an exponential distribution? myrate_e ## [1] 0.1869159 Pretty solid! 7.11.4 Gamma Distribution rgamma() randomly samples positive real numbers greater than zero from a gamma distribution. Itâ€™s like the continuous version of rpois(). It requires 2 paramters, shape and rate. You can estimate these a couple of different ways, but a simple one is to use the method of moments, which says that: shape \\(\\approx \\frac{mean^{2}}{ variance}\\). scale \\(\\approx \\frac{variance}{ mean }\\). # For shape, we want the rate of how much greater the mean-squared is than the variance. myshape &lt;- mean(sw)^2 / var(sw) # For rate, we like to get the inverse of the variance divided by the mean. myrate &lt;- 1 / (var(sw) / mean(sw) ) # Simulate it! mygamma &lt;- rgamma(1000, shape = myshape, rate = myrate) ## View it! mygamma %&gt;% hist() So what are the parameter values for this distribution? myshape ## [1] 42.7556 myrate ## [1] 7.991701 Neat! 7.11.5 Weibull Distribution rweibull() randomly samples positive real numbers over zero too, but from a Weibull distribution. It requires a shape and scale parameter, but its method of moments equation is pretty complex. Once we get into Weibull distribution territory, weâ€™ll need to use an advanced concept called maximum likelihood estimation, with the fitdistr() from the MASS package. You can read more about it in the apppendix. # Estimate the shape and scale parameters for a weibull distribution mystats &lt;- sw %&gt;% fitdistr(densfun = &quot;weibull&quot;) # Here, we&#39;re going to extract the estimate for shape myshape_w &lt;- mystats$estimate[1] # and the estimate for scale myscale_w &lt;- mystats$estimate[2] # simulate! myweibull &lt;- rweibull(n = 1000, shape = myshape_w, scale = myscale_w) # View it! myweibull %&gt;% hist() 7.12 Special Distributions 7.12.1 Binomial Distribution Next, the binomial distribution is a bit of a special case, in that itâ€™s mostly only helpful for binary variables (with values 0 and 1). But letâ€™s try an example anyways. rbinom() randomly draws n simulated values from a set of provided values at a given probability (prob). Itâ€™s usually used for drawing binary variables (0 and 1); a coin flip would have prob = 0.5, or a 50-50 chance. rbinom(n = 10, size = 1, prob = 0.5) ## [1] 1 0 1 1 1 1 0 1 0 1 To get a meaningful simulation, maybe we calculate the proportion of values that are greater than the mean. # In how many cases was the observed value greater than the mean? myprob &lt;- sum(sw &gt; mymean) / length(sw) # Sample from binomial distribution with that probability mybinom &lt;- rbinom(1000, size = 1, prob = myprob) # View histogram! mybinom %&gt;% hist() 7.12.2 Uniform Distribution Finally, the uniform distribution is also a special case. The frequency of values in a uniform distribution is more-or-less uniform. It also only spans the length of a specified interval \\(a \\to b\\). A common range is a = 0 to b = 1. So, the frequency of 1.5 in that interval would beâ€¦ zero. # Simulate a uniform distribution ranging from 0 to 1 myunif &lt;- runif(n = 1000, min = 0, max = 1) # View histogram! myunif %&gt;% hist(xlim = c(-0.5,1.5)) 7.13 Comparing Distributions Finally, weâ€™re going to want to outfit those vectors in nice data.frames (skipping rbinom() and runif()), and stack them into 1 data.frame to visualize. We can do this using the bind_rows() function from the dplyr package. # Using bind_rows(), mysim &lt;- bind_rows( # Make a bunch of data.frames, all with the same variable names, data.frame(x = sw, type = &quot;Observed&quot;), # and stack them! data.frame(x = mynorm, type = &quot;Normal&quot;), # and stack it! data.frame(x = mypois, type = &quot;Poisson&quot;), # Stack, stack, stack stack it! data.frame(x = mygamma, type = &quot;Gamma&quot;), # so many stacks! data.frame(x = myexp, type = &quot;Exponential&quot;), # so much data!!!!!! data.frame(x = myweibull, type = &quot;Weibull&quot;) ) Next, we can visualize those distributions using geom_density() in ggplot (or geom_histogram(), really, if that floats your boat). # Let&#39;s write the initial graph and save it as an object g1 &lt;- ggplot(data = mysim, mapping = aes(x = x, fill = type)) + geom_density(alpha = 0.5) + labs(x = &quot;Seawall Height (m)&quot;, y = &quot;Density (Frequency)&quot;, subtitle = &quot;Which distribution fits best?&quot;, fill = &quot;Type&quot;) # Then view it! g1 Personally, I canâ€™t read much out of that, so it would be helpful to narrow in the x-axis a bit. We can do that with xlim(), narrowing to just between values 0 and 10. g1 + xlim(0,10) Beautiful! Wow! It looks like the Normal, Gamma, and Weibull distributions all do a pretty excellent job of matching the observed distribution. &lt;br. Learning Check 4 Question Youâ€™ve been recruited to evaluate the frequency of Corgi sightings in the Ithaca Downtown. A sample of 10 students each reported the number of corgis they saw last Tuesday in town. Using the method of moments (or fitdistr() for Weibull) and ggplot(), find out which type of distribution best matches the observed corgi distribution! Beth saw 5, Javier saw 1, June saw 10(!), Tim saw 3, Melanie saw 4, Mohammad saw 3, Jenny say 6, Yosuke saw 4, Jimena saw 5, and David saw 2. [View Answer!] First, letâ€™s get the stats. # Make distribution of Corgis corgi &lt;- c(5, 1, 10, 3, 4, 3, 6, 4, 5, 2) # Compute statistics for distributions corgi_mean &lt;- mean(corgi) corgi_sd &lt;- sd(corgi) corgi_shape &lt;- mean(corgi)^2 / var(corgi) corgi_rate &lt;- 1 / (var(corgi) / mean(corgi) ) corgi_rate_e &lt;- 1 / mean(corgi) # For Weibull, use fitdistr() from MASS package corgi_stats &lt;- corgi %&gt;% fitdistr(densfun = &quot;weibull&quot;) corgi_shape_w &lt;- corgi_stats$estimate[1] corgi_scale_w &lt;- corgi_stats$estimate[2] Next, letâ€™s bind them together. corgisim &lt;- bind_rows( # Get observed vector data.frame(x = corgi, type = &quot;Observed&quot;), # Get normal dist data.frame(x = rnorm(1000, mean = corgi_mean, sd = corgi_sd), type = &quot;Normal&quot;), # Get poisson data.frame(x = rpois(1000, lambda = corgi_mean), type = &quot;Poisson&quot;), # Get gamma data.frame(x = rgamma(1000, shape = corgi_shape, rate = corgi_rate), type = &quot;Gamma&quot;), # Get exponential data.frame(x = rexp(1000, rate = corgi_rate_e), type = &quot;Exponential&quot;), # Get weibull data.frame(x = rweibull(1000, shape = corgi_shape_w, scale = corgi_scale_w), type = &quot;Weibull&quot;) ) Finally, letâ€™s visualize it! # Visualize! ggplot(data = corgisim, mapping = aes(x = x, fill = type)) + geom_density(alpha = 0.5) + # Narrow it to 0 to 15 to suit plot xlim(0,15) + labs(x = &quot;Corgi Sightings!&quot;, y = &quot;Density (Frequency)&quot;) Neat - looks like the Poisson, Gamma, and Weibull function match well, although the Poisson looks pretty odd! Conclusion So now, you know how to use descriptive statistics in R, how to visualize and evaluate a distribution, and how to simulate several different types of distributions! Youâ€™re well on your way to some serious stats for systems engineering! "],["distributions-and-descriptive-statistics-in-python.html", "8 Distributions and Descriptive Statistics in Python Getting Started 8.1 Our Data 8.2 Size 8.3 Location 8.4 Spread (1) 8.5 Spread (2) 8.6 Shape 8.7 Finding Parameters for Your Distributions 8.8 Common Distributions 8.9 Comparing Distributions Learning Check 1 Conclusion", " 8 Distributions and Descriptive Statistics in Python This tutorial introduces distributions and descriptive statistics in Python using pandas and helper functions that mirror Râ€™s syntax. Getting Started Install and Import Packages %pip install pandas plotnine scipy import os, sys import pandas as p from plotnine import * sys.path.append(os.path.abspath(&#39;functions&#39;)) from functions_distributions import * 8.1 Our Data sw = p.Series([4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4]) sw ## 0 4.5 ## 1 5.0 ## 2 5.5 ## 3 5.0 ## 4 5.5 ## 5 6.5 ## 6 6.5 ## 7 6.0 ## 8 5.0 ## 9 4.0 ## dtype: float64 8.2 Size 8.2.1 Length len(sw) ## 10 8.3 Location 8.3.1 Mean and Median sw.mean() ## 5.35 sw.median() ## 5.25 8.3.2 Mode sw.mode() ## 0 5.0 ## dtype: float64 8.4 Spread (1) 8.4.1 Percentiles sw.quantile(q=0) # min ## 4.0 sw.quantile(q=1) # max ## 6.5 sw.quantile(q=.75) ## 5.875 8.5 Spread (2) 8.5.1 Standard Deviation, Variance, CV, SE # Manual SD (sample) x = ((sw - sw.mean())**2).sum() x = x / (len(sw) - 1) x**0.5 ## 0.8181958472422385 sw.std() ## 0.8181958472422385 sw.var() ## 0.6694444444444444 sw.std()**2 ## 0.6694444444444444 sw.std() / sw.mean() # CV ## 0.15293380322284833 se = sw.std() / (len(sw)**0.5) se ## 0.25873624493766706 8.6 Shape 8.6.1 Skewness and Kurtosis diff = sw - sw.mean() n = len(sw) - 1 sigma = sw.std() sum(diff**3) / (n * sigma**3) ## 0.024342597820882206 sum(diff**4) / (n * sigma**4) ## 1.8758509667533272 # Using helper functions mirroring R skewness(sw) ## 0.024342597820882206 kurtosis(sw) ## 1.8758509667533272 8.7 Finding Parameters for Your Distributions sw = p.Series([4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4]) mymean = sw.mean() mysd = sw.std() 8.8 Common Distributions 8.8.1 Normal mynorm = rnorm(n=1000, mean=mymean, sd=mysd) hist(mynorm) ## &lt;plotnine.ggplot.ggplot object at 0x16ed3d580&gt; 8.8.2 Poisson mypois = rpois(n=1000, mu=mymean) hist(mypois) ## &lt;plotnine.ggplot.ggplot object at 0x308f8d8e0&gt; 8.8.3 Exponential myrate_e = 1 / sw.mean() myexp = rexp(n=1000, rate=myrate_e) hist(myexp) ## &lt;plotnine.ggplot.ggplot object at 0x30b79a420&gt; 8.8.4 Gamma myshape = sw.mean()**2 / sw.var() myrate = 1 / (sw.var() / sw.mean()) mygamma = rgamma(n=1000, shape=myshape, rate=myrate) hist(mygamma) ## &lt;plotnine.ggplot.ggplot object at 0x30b7b0d70&gt; 8.8.5 Weibull from scipy import stats as fitdistr myshape_w, loc, myscale_w = fitdistr.weibull_min.fit(sw, floc=0) myweibull = rweibull(n=1000, shape=myshape_w, scale=myscale_w) hist(myweibull) ## &lt;plotnine.ggplot.ggplot object at 0x30abb1040&gt; 8.9 Comparing Distributions mysim = p.concat([ p.DataFrame({&#39;x&#39;: sw, &#39;type&#39;: &quot;Observed&quot;}), p.DataFrame({&#39;x&#39;: mynorm, &#39;type&#39;: &quot;Normal&quot;}), p.DataFrame({&#39;x&#39;: mypois, &#39;type&#39;: &quot;Poisson&quot;}), p.DataFrame({&#39;x&#39;: mygamma, &#39;type&#39;: &quot;Gamma&quot;}), p.DataFrame({&#39;x&#39;: myexp, &#39;type&#39;: &quot;Exponential&quot;}), p.DataFrame({&#39;x&#39;: myweibull, &#39;type&#39;: &quot;Weibull&quot;}) ]) g1 = (ggplot(mysim, aes(x=&#39;x&#39;, fill=&#39;type&#39;)) + geom_density(alpha=0.5) + labs(x=&#39;Seawall Height (m)&#39;, y=&#39;Density (Frequency)&#39;, subtitle=&#39;Which distribution fits best?&#39;, fill=&#39;Type&#39;)) g1 ## &lt;plotnine.ggplot.ggplot object at 0x30676d8b0&gt; g1 + xlim(0,10) ## &lt;plotnine.ggplot.ggplot object at 0x30b74bc20&gt; Learning Check 1 Question Simulate 1000 draws from a normal distribution using your sw mean and standard deviation. What are the simulated mean and sd? How close are they to swâ€™s? [View Answer!] mymean = sw.mean(); mysd = sw.std() m = rnorm(1000, mean=mymean, sd=mysd) [m.mean(), m.std()] ## [5.31369576645973, 0.8235176440088696] Conclusion You computed size, location, spread, and shape statistics and compared common simulated distributions using helper functions that mirror R. "],["functions-in-r.html", "9 Functions in R Getting Started 9.1 Coding your own function! 9.2 Functions with Default Inputs Conclusion", " 9 Functions in R Weâ€™ve learned how to use built-in R functions like dnorm() and pnorm() to analyze distributions, but sometimes itâ€™s going to be more helpful to be able to (A) do the math by hand or (B) code your function to do it. So, letâ€™s learn how in the world you do that! Getting Started Packages Weâ€™ll be using the tidyverse package, a super-package that auto-loads dplyr, ggplot2, and other common functions. library(tidyverse) 9.1 Coding your own function! Functions are machines that do a specific calculation using an input to produce a specific output. Below, weâ€™ll write an example function, called add(a, b). This function takes two numeric values, a and b, as inputs, and adds them together. Using function(), weâ€™ll tell R that our function contains two inputs, a and b. Then, using { ... }, weâ€™ll put the action we want R to do in there. The function can involve multiple operations inside it. But at the end, you need to print one final output, or put return() around your output. # Make function add &lt;- function(a, b){ # Compute and directly output a + b } add(1, 2) ## [1] 3 # This also works add &lt;- function(a, b){ # Assign output to a temporary object output &lt;- a + b # Return the temporary object &#39;output&#39; return(output) } # add(1, 2) ## [1] 3 9.2 Functions with Default Inputs You can also assign default input values to your function. Below, we write that by default, b = 2. If we supply a different b, the default will get overwritten, but otherwise, we wonâ€™t need to supply b. add = function(a, b = 2){ a + b } Letâ€™s try it! # See? I only need to write &#39;a&#39; now add(1) ## [1] 3 # But if I write &#39;b&#39; too.... add(1, 2) ## [1] 3 # And if I change &#39;b&#39;... add(1, 3) ## [1] 4 # It will adjust accordingly # clear data remove(add) Conclusion Great! Letâ€™s go make some functions! "],["functions-in-python.html", "10 Functions in Python Getting Started 10.1 Coding your own function! 10.2 Functions with Default Inputs Conclusion", " 10 Functions in Python Weâ€™ve learned how to use built-in functions to analyze data, but sometimes itâ€™s more helpful to (A) do the math by hand or (B) write your own function. Letâ€™s learn how! Getting Started Packages Weâ€™ll use base Python here. No extra packages needed. 10.1 Coding your own function! Functions are machines that do a specific calculation using an input to produce a specific output. Below, weâ€™ll write an example function, called add(a, b). This function takes two numeric values, a and b, as inputs, and adds them together. Using def, weâ€™ll tell Python that our function contains two inputs, a and b. The function can involve multiple operations inside it. But at the end, you need to print one final output, or put return before your output. # Make function def add(a, b): # Compute and directly output return a + b add(1, 2) ## 3 # This also works def add(a, b): # Assign output to a temporary object output = a + b # Return the temporary object &#39;output&#39; return output add(1, 2) ## 3 10.2 Functions with Default Inputs You can assign default input values. Below, by default, b = 2. If we supply a different b, the default gets overwritten. def add(a, b = 2): return a + b Letâ€™s try it! # Only provide &#39;a&#39; add(1) ## 3 # Provide both add(1, 2) ## 3 # Change &#39;b&#39; add(1, 3) ## 4 # clear data del add Conclusion Great! Letâ€™s go make some functions! "],["probability-in-r.html", "11 Probability in R Getting Started 11.1 Key Functions 11.2 Probability 11.3 Conditional Probability 11.4 Total Probabilities 11.5 Bayes Rule Conclusion", " 11 Probability in R This tutorial will review probability and how to code joint and conditional probabilistic analyses in R! Getting Started Please open up your Posit.Cloud project. Start a new R script (File &gt;&gt; New &gt;&gt; R Script). Save a new R script. And letâ€™s get started! Load Packages In this tutorial, weâ€™re going to use more of the dplyr package. library(dplyr) 11.1 Key Functions Weâ€™re going to use 3 functions a lot below. This includes bind_rows(), mutate(), and summarize(). So what are they? bind_rows(): stacks 2 or more data.frames on top of each other, matching columns by name. mutate(): creates or edits variables in a data.frame. summarize(): consolidates many rows of data into a single summary statistic (or a set of them.) 11.1.1 bind_rows() How might we use bind_rows()? mycoffee &lt;- bind_rows( # Make first data.frame data.frame( # Containing these vectors style and price style = c(&quot;latte&quot;, &quot;cappuccino&quot;, &quot;americano&quot;), price = c(5, 4, 3)), # Make second data.frame data.frame( # Containing these vectors style, price, and shop style = c(&quot;coffee&quot;, &quot;hot cocoa&quot;), price = c(3, 2), shop = c(&quot;Gimme Coffee&quot;, &quot;Starbucks&quot;)) ) # Notice how they stack, # but in first data.frame values, # shop gets filled with NA, # since it wasn&#39;t in first dataframe mycoffee ## style price shop ## 1 latte 5 &lt;NA&gt; ## 2 cappuccino 4 &lt;NA&gt; ## 3 americano 3 &lt;NA&gt; ## 4 coffee 3 Gimme Coffee ## 5 hot cocoa 2 Starbucks 11.1.2 mutate() How might we use mutate()? mycoffee &lt;- mycoffee %&gt;% # Add a new vector (must be of same length as data.frame) # vector is number of those drinks purchased mutate(purchased = c(5, 4, 10, 2, 1)) 11.1.3 summarize() How might we use summarize()? mycoffee %&gt;% # Summarize data.frame into one row summarize( # Calculate mean price of drinks mean_price = mean(price), # Calculate total drinks purchased total_purchased = sum(purchased)) ## mean_price total_purchased ## 1 3.4 22 Great! Now letâ€™s apply these to probability! 11.2 Probability Probability refers to how often a specify event is expected to occur, given a sufficient number of times. Weâ€™re going to learn (and compute!) several common probability formula in R. 11.3 Conditional Probability Conditional Probability: probability of two events happening together reflects the probability of the first happening, times the probability of the second happening given that the first has already occurred. \\[ P(AB) = P(A) \\times P(B|A)\\] In other words, if two events are interdependent, you multiply. 11.3.1 Example: Twizzlers (#fig:img_twizzlers)Twizzlers vs.Â Red Vines Youâ€™ve been hired by the Hersheyâ€™s Chocolate Company to investigate quality control on their Twizzlers sweets packaging line. At the start of an assembly line, you mixed in 8,000 Red Vines with a sample of 10,000 Twizzlers. Whatâ€™s the probability of a packer picking up a Red Vine on the assembly line? sweets &lt;- data.frame( # We know there are 10,000 twizzlers twizzlers = 10000, # and 8,000 redvines redvines = 8000) %&gt;% # So together, there are 18,000 sweets available # So there&#39;s a 8000-in-18,000 chance of picking a redvine mutate(prob_1 = redvines / (twizzlers + redvines)) # Check it! sweets ## twizzlers redvines prob_1 ## 1 10000 8000 0.4444444 But then, whatâ€™s the probability of a packer picking up 2 Red Vines in a row on the assembly line? sweets %&gt;% # After picking 1 Red Vine, # there&#39;s now 1 less Red Vine in circulation mutate( # Subtract 1 redvine redvines = redvines - 1, # Recalculate total total = twizzlers + redvines) %&gt;% # calculate probability of picking a second red vine now that 1 is gone mutate(prob_2 = redvines / total) %&gt;% # Finally, multiply the first and second probability together # When it&#39;s this AND that, you multiply mutate(prob = prob_1 * prob_2) ## twizzlers redvines prob_1 total prob_2 prob ## 1 10000 7999 0.4444444 17999 0.4444136 0.1975171 Alternatively, if two events are independent (mutually exclusive), meaning they do not affect each other, you add those probabilities together. You dump in a 1000 pieces of Black Licorice. If a packer picks up 2 sweets, whatâ€™s the probability itâ€™s a piece of Black Licorice or Red Vines? sweets %&gt;% # Add a column for black licorice mutate(black_licorice = 1000) %&gt;% # Get total mutate(total = twizzlers + redvines + black_licorice) %&gt;% # Recompute probabilities mutate(prob_1 = redvines / total, prob_2 = black_licorice / total) %&gt;% # When it&#39;s this OR that, you add probabilities mutate(prob_3 = prob_1 + prob_2) ## twizzlers redvines prob_1 black_licorice total prob_2 prob_3 ## 1 10000 8000 0.4210526 1000 19000 0.05263158 0.4736842 remove(mycoffee, sweets) 11.4 Total Probabilities We can also examine total probabilities. Any event \\(A\\) that is mutually exclusive from event \\(E\\) (canâ€™t happen at the same time) has the following probabilityâ€¦ \\[ P(A) = \\sum_{i=1}^{n}{P(A|E_{i}) \\times P(E_{i}) } \\] 11.4.1 Example: Marbles! (#fig:img_marbles)So many marbles! Youâ€™ve got 3 bags (\\(E_{1 \\to 3}\\)), each containing 3 marbles, each with a different split of red vs.Â blue marbles. If we choose a bag at random and sample a marble at random (2 mutually exclusive events), whatâ€™s the probability that marble will be red (\\(P(A)\\))? I like to map these out, so I understand visually what all the possible pathways are. Hereâ€™s a chart I made (using mermaid), where Iâ€™ve diagrammed each possible set of actions, like choosing Bag 1 then Marble a (1 pathway), choosing Bag 1 then Marble b (a second pathway), etc. If we look at the ties to the marbles, youâ€™ll see I labeled each tie to a red marble as 1 and each tie to a blue marble as 0. If we add these pathways up, weâ€™ll get the total probability: 0.67 (aka 2/3). (#fig:img_mermaid)Drawing Probability Diagrams But what if we canâ€™t diagram it out? Perhaps weâ€™re choosing from 100s of marbles, or weâ€™re limited on time! How would we solve this problem mathematically? The key here is knowing that: the blue marbles donâ€™t really matter we need the probability of choosing a bag we need the probability of choosing a red marble in each bag. Hereâ€™s what we know: Thereâ€™s an equal chance of choosing any bag of the 3 bags (because random). (If 1 bag were on a really high shelf, then maybe the probabilities would be different, i.e.Â not random, but letâ€™s assume theyâ€™re random this time.) # there are 3 bags n_bags &lt;- 3 # So.... # In this case, P(Bag1) = P(Bag2) = P(Bag3) # and P(Bag1) + P(Bag2) + P(Bag3) = 100% = 1.0 # 1/3 chance of picking Bag 1 # written P(Bag1) pbag1 &lt;- 1 / n_bags # 1/3 chance of picking Bag 2 # written P(Bag2) pbag2 &lt;- 1 / n_bags # 1/3 chance of picking Bag 3 # written P(Bag3) pbag3 &lt;- 1 / n_bags # Check it! c(pbag1, pbag2, pbag3) ## [1] 0.3333333 0.3333333 0.3333333 There are 3 marbles in each bag. # Total marbles in Bag 1 m_bag1 &lt;- 3 # Total marbles in Bag 2 m_bag2 &lt;- 3 # Total marbles in Bag 3 m_bag3 &lt;- 3 There are 3 red marbles in bag 1, 1 red marbles in bag 2, and 2 red marbles in bag 3. # So, we can calculate the percentages in each bag. # percentage of red marbles in Bag 1 # written P(Red|Bag1) pm_bag1 &lt;- 3 / m_bag1 # percentage of red marbles in Bag 2 # written P(Red|Bag2) pm_bag2 &lt;- 1 / m_bag2 # percentage of red marbles in Bag 3 # written P(Red|Bag3) pm_bag3 &lt;- 2 / m_bag3 # Check it! c(pm_bag1, pm_bag2, pm_bag3) ## [1] 1.0000000 0.3333333 0.6666667 Selecting Bag 1 and then selecting a Red Marble are interdependent events, so we multiply them. # For example # P(Bag1 &amp; Red) = P(Red|Bag1) * P(Bag1) pm_bag1 * pbag1 ## [1] 0.3333333 But each pathway (eg. Bag 1 x Marble A) is distinct and independent of the other pathways, so we can add them together. # P(Bag1 &amp; Red) = P(Red|Bag1) * P(Bag1) pm_bag1 * pbag1 + # P(Bag2 &amp; Red) = P(Red|Bag2) * P(Bag2) pm_bag2 * pbag2 + # P(Bag3 &amp; Red) = P(Red|Bag3) * P(Bag3) pm_bag3 * pbag3 ## [1] 0.6666667 And that gives us the same answer: 0.67 or 2/3. However, that required making a lot of objects in R. Can we do this more succinctly using vectors and data.frames? We could compute a bag-wise data.frame, where each row represents a choice (bag) from event1. bags &lt;- data.frame( bag_id = 1:3, # For each bag, how many do you get to choose? bags = c(1, 1, 1), # For each bag, how many marbles do you get to choose? marbles = c(3, 3, 3), # For each bag, how many marbles are red? red = c(3, 1, 2)) %&gt;% # Then, we can calculate the probability of... mutate( # choosing that bag out of all bags prob_bag = bags / sum(bags), # choosing red out of all marbles in that bag prob_red = red / marbles, # choosing BOTH that bag AND a red marble in that bag prob_bagred = prob_red * prob_bag) Finally, we could just sum the joint probabilities all together. bags %&gt;% summarize(prob_bagred = sum(prob_bagred)) ## prob_bagred ## 1 0.6666667 Much faster! # Let&#39;s remove this now unnecessary data remove(bags, n_bags, m_bag1, m_bag2, m_bag3, pbag1, pbag2, pbag3, pm_bag1, pm_bag2, pm_bag3) 11.5 Bayes Rule A cool trick, called Bayesâ€™ Rule, reveals that we can figure out a probability of interest that depends on other probabilities. Letâ€™s say, we want to know, whatâ€™s the probability of OUTCOME given CONDITION. Bayesâ€™ Rule states that the probability of the OUTCOME occurring given CONDITION is equal to the joint probability of the Outcome and Condition both occurring, divided by the probability of the condition occurring. \\[ P(Outcome = 1| Condition = 1) = \\frac{ P(Outcome = 1\\ \\&amp; \\ Condition = 1) }{ P(Condition = 1)} \\] Thanks to Conditional Probability and Total Probability tricks, we can break that down into quantities we can calculate. \\[ P(Outcome=1 | Condition=0) = \\frac{ P(Condition=1|Outcome=1) \\times P(Outcome=1) }{ \\sum{ P(Condition | Outcome) } \\times P(Outcome)} \\] \\[ = \\frac{ P(Condition=1|Outcome=1) \\times P(Outcome=1) }{ P(Condition=1|Outcome=1) \\times P(Outcome=1) + P(Condition=1|Outcome=0) \\times P(Outcome=0)} \\] Letâ€™s define some terms: posterior: Posterior probability is the probability that the outcome occurs given that the condition occurs. prior: the probability that the outcome occurs, independent of anything else. likelihood: the probability that the condition occurs, given that the outcome occurs. evidence: the total probability that the condition does or does not occur. 11.5.1 Example: Coffee Shop (Incomplete Information) (#fig:img_coffee)Yay Coffee! A local coffee chain needs your help to analyze their supply chain issues. They know that their scones help them sell coffee, but does their coffee help them sell scones? Over the last week, when 7 customers bought scones, 3 went on to buy coffee. When 3 customers didnâ€™t buy scones, just 2 bought coffee. In general, 7 out of 10 of customers ever bought scones. Whatâ€™s the probability that a customer will buy a scone, given that they just bought coffee? # We want to know this p_scone_coffee &lt;- NULL # But we know this! p_coffee_scone &lt;- 3/7 p_coffee_no_scone &lt;- 2/3 p_scone &lt;- 7/10 # AND # If 7 out of 10 customers ever bought scones, # then 3 out of 10 NEVER bought scones p_no_scone &lt;- 3 / 10 Using these 3~4 probabilities, we can deduce the total probability of coffee (the denominator), meaning whether you got coffee OR whether you didnâ€™t get coffee. # Total Prob of Coffee = Getting Cofee + Not getting coffee p_coffee &lt;- p_coffee_scone * p_scone + p_coffee_no_scone * p_no_scone # Check it! p_coffee ## [1] 0.5 So letâ€™s use p_coffee to get the probability of getting a scone given that you got coffee! p_scone_coffee &lt;- p_coffee_scone * p_scone / p_coffee Itâ€™s magic! Bayesâ€™ Rule is helpful when we donâ€™t have complete information, and just have some raw percentages or probabilities. 11.5.2 Example: Coffee Shop (Complete Information) But, if we do have complete information, then we can actually prove Bayesâ€™ Rule quite quickly. For example, say those percentages the shop owner gave us were actually meticulously tabulated by a barista. We talk to the barista, and she explains that she can tell us right away the proportion of folks who got a scone given that they got coffee. She shows us her spreadsheet of orders, listing for each customer, whether they got coffee and whether they got a scone. orders &lt;- tibble( coffee = c(&quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;), scone = c(&quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;)) We can tabulate these quickly using table(), tallying up how many folks did this. orders %&gt;% table() ## scone ## coffee no yes ## no 1 4 ## yes 2 3 # Let&#39;s skip to the end and just calculate the proportion directly! # Out of all people who got coffee, how many got scones? orders %&gt;% summarize(p_scone_coffee = sum(scone == &quot;yes&quot; &amp; coffee == &quot;yes&quot;) / sum(coffee == &quot;yes&quot;) ) ## # A tibble: 1 Ã— 1 ## p_scone_coffee ## &lt;dbl&gt; ## 1 0.6 # The end! # Now that we know this, let&#39;s prove that Bayes works. orders %&gt;% summarize( # The goal (posterior) p_scone_coffee = sum(scone == &quot;yes&quot; &amp; coffee == &quot;yes&quot;) / sum(coffee == &quot;yes&quot;), # The data p_coffee_scone = sum(coffee == &quot;yes&quot; &amp; scone == &quot;yes&quot;) / sum(scone == &quot;yes&quot;), p_coffee_no_scone = sum(coffee == &quot;yes&quot; &amp; scone == &quot;no&quot;) / sum(scone == &quot;no&quot;), p_scone = sum(scone == &quot;yes&quot;) / sum(coffee == &quot;yes&quot; | coffee == &quot;no&quot;), p_no_scone = sum(scone == &quot;no&quot;) / sum(coffee == &quot;yes&quot; | coffee == &quot;no&quot;), # Now recalculate the goal, using the data we have collected. # Does &#39;bayes&#39; equal &#39;p_scone_coffee&#39;? bayes = p_coffee_scone * p_scone / (p_coffee_scone * p_scone + p_coffee_no_scone * p_no_scone)) ## # A tibble: 1 Ã— 6 ## p_scone_coffee p_coffee_scone p_coffee_no_scone p_scone p_no_scone bayes ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.6 0.429 0.667 0.7 0.3 0.6 # It should! And it does! Tada! 11.5.3 Example: Movie Theatre Popularity You are the manager of a movie theatre, and you want to determine the popularity of different genres of movies among your customers. You have collected data on the genres of movies that customers choose to watch Probability of a customer choosing an Action movie, P(Action) = 0.25. Similarly, P(Comedy) = 0.30. P(Drama) = 0.20. P(Sci-Fi) = 0.15. P(Horror) = 0.10. You also have information about the overall popularity of each genre in the market: Out of the customers who chose an Action movie, 60% also bought tickets for Comedy. Out of the customers who chose a Comedy movie, 40% also bought tickets for Drama. Out of the customers who chose a Drama movie, 25% also bought tickets for Sci-Fi. Out of the customers who chose a Science Fiction movie, 70% also bought tickets for Action. Out of the customers who chose a Horror movie, 20% also bought tickets for Drama. You want to calculate the probability that a customer chooses a Drama movie given that they have already purchased a ticket. # We define the prior probabilities of customers choosing each genre based on the given problem statement. P_action &lt;- 0.25 P_comedy &lt;- 0.30 P_drama &lt;- 0.20 P_scifi &lt;- 0.15 P_horror &lt;- 0.10 # We define conditional probabilities that represent the likelihood of crossover purchases. # For example, the probability of choosing Action given that the customer chose Comedy is 60%. conP_action_given_drama &lt;- 0.00 conP_comedy_given_drama &lt;- 0.25 conP_drama_given_drama &lt;- 0.00 conP_scifi_given_drama &lt;- 0.00 conP_horror_given_drama &lt;- 0.20 # Specify the target genre # Our target genre is Drama and we want to calculate the probability that a customer chooses a Drama movie given that they have already purchased a ticket. target_genre &lt;- &quot;Drama&quot; # We can change this to our desired target genre To calculate the probability that a customer chooses the target genre given a ticket purchase, letâ€™s use Bayes Rule! # We will be using Bayes&#39; rule: P(Target Genre | Ticket Purchase) = P(Ticket Purchase | Target Genre) * P(Target Genre) / P(Ticket Purchase) # We calculate P(Ticket Purchase | Target Genre) * P(Target Genre) as probability_given_purchase # If target_genre is &quot;Comedy,&quot; the switch function will select the conditional probability for choosing Comedy given a ticket purchase probability_given_purchase &lt;- switch( target_genre, &quot;Action&quot; = conP_action_given_drama * P_action, &quot;Comedy&quot; = conP_comedy_given_drama * P_comedy, &quot;Drama&quot; = conP_drama_given_drama * P_drama, &quot;SciFi&quot; = conP_scifi_given_drama * P_scifi, &quot;Horror&quot; = conP_horror_given_drama * P_horror ) # View it! probability_given_purchase ## [1] 0 Nextâ€¦ # We will further calculate the total probability of a ticket purchase P(Ticket Purchase) as total_probability_purchase: total_probability_purchase &lt;- sum( conP_action_given_drama * P_action, conP_comedy_given_drama * P_comedy, conP_drama_given_drama * P_drama, conP_scifi_given_drama * P_scifi, conP_horror_given_drama * P_horror ) # View it! total_probability_purchase ## [1] 0.095 We want to calculate the probability that a customer chooses a Drama movie given that they have already purchased a ticket, thus target genre is Drama. # P(Target Genre | Ticket Purchase) &lt;- P(Ticket Purchase | Target Genre) * P(Target Genre) / P(Ticket Purchase) probability_target_genre_given_purchase &lt;- probability_given_purchase / total_probability_purchase # Let&#39;s print that using paste! paste( &quot;Probability that a customer chooses a&quot;, target_genre, &quot;movie given a ticket purchase:&quot;, probability_target_genre_given_purchase) ## [1] &quot;Probability that a customer chooses a Drama movie given a ticket purchase: 0&quot; Conclusion All done! Nice work! "],["probability-functions-in-r.html", "12 Probability Functions in R Getting Started 12.1 Probability Functions 12.2 Hypothetical Probability Functions Learning Check 1 12.3 Observed Probability Functions Learning Check 2 Conclusion", " 12 Probability Functions in R Getting Started Please open up your Posit.Cloud project. Start a new R script (File &gt;&gt; New &gt;&gt; R Script). Save a new R script. And letâ€™s get started! Load Packages In this tutorial, weâ€™re going to use more of the dplyr and ggplot2 packages, plus the broom package and mosaicCalc package. library(dplyr) library(ggplot2) library(broom) library(mosaicCalc) 12.1 Probability Functions How do we use probability in our statistical analyses of risk, performance, and other systems engineering concepts? Probability allows us to measure for any statistic (or parameter) mu, how extreme is that statistic? This is called type II error, measured by a p-value, the probability that a more extreme value occurred than our statistic. Itâ€™s an extremely helpful benchmark. In order to evaluate how extreme it is, we need values to compare it to. We can do this by: assuming the probability function of an unobserved, hypothetical distribution, or; making a probability function curve of the observed distribution. 12.2 Hypothetical Probability Functions Often, our sample of data is just one of the many samples we could have possibly gotten. For example, say we are examining customer behavior at storefronts. Had we looked at a different firm location (by chance), or a different sample of customers come in by chance, we might have gotten slightly different distribution of purchase value made by these customers. The problem is, we almost never can see the distribution of the true population of all observations (eg. all purchases). But, if we can approximately guess what type of distribution that population has, we can very easily compute the probability density functions and cumulative distribution functions of several of the most well known distributions in R (eg. Normal, Poisson, Gamma, etc.) Example: Farmers Market The Ithaca Farmers Market is a vendor-owned cooperative that runs a massive Saturday-and-Sunday morning market for local produce, street food, and hand-made goods, on the waterfront of Cayuga Lake. In markets and street fairs, some stallsâ€™ brands are often better known than others, so businesses new to the market might worry that people wonâ€™t come to their stalls without specific prompting. This past Saturday, a volunteer tracked 500 customers and recorded how many stalls each customers visited during their stay. They calculated the following statistics. The average customer visited a mean of 5.5 stalls and a median of 5 stalls, with a standard deviation of 2.5 stalls. Figure 12.1: Ithaca Farmers Market! Market operators wants to know: Whatâ€™s the probability that customers will stop by 5 stalls? Whatâ€™s the probability that customers will stop by at max 5 stalls? Whatâ€™s the probability that customers will stop by over 5 stalls? How many visits did people usually make? Estimate the interquartile range (25th-75th percentiles). Unfortunately, the wind blew the raw data away into Cayuga Lake before they could finish their analysis. How can we approximate the unobserved distribution of visits and compute these probabilities? Below, we will (1) use these statistics to guess which of several possible archetypal hypothetical distributions it most resembles, and then (2) compute probabilities based off of the shape of that hypothetical distribution. Unobserved Distributions We donâ€™t have the actual data, but we know several basic features of our distribution! Our variable is visits, a count ranging from 0 to infinity. (Canâ€™t have -5 visits, canâ€™t have 1.3 visits.) The median (5) is less than the mean (5.5), so our distribution is right-skewed. This sounds like a classic Poisson distribution! Letâ€™s simulate some poisson-distributed data to demonstrate. # Randomly sample 500 visits from a poisson distribution with a mean of 5.5 visits &lt;- rpois(n = 500, lambda = 5.5) # Check out the distribution! visits %&gt;% hist() Using Hypothetical Probability Functions Much like rpois() randomly generates poisson distributed values, dpois(), ppois(), and qpois() can help you get other quantities of interest from the Poisson distribution. dpois() generates the density of any value on a poisson distribution centered on a given mean (PDF). ppois() returns for any percentile in the distribution the cumulative probability (percentage of the area under the density curve) up until that point (CDF). qpois() returns for any percentile in the distribution the raw value. See the Table below for several examples. Table 12.1: Table 12.2: Table 1: Probability Functions (r, d, p, and q) Meaning Purpose Main Input Normal Poisson Gamma Exponential Random Draws from Distribution Simulate a distribution n = # of simulations rnorm() rpois() rgamma() rexp() Probability Density Function Get Probability of Value in Distribution x = value in distribution dnorm() dpois() dgamma() dexp() Cumulative Distribution Function Get % of Distribution LESS than Value q = a cumulative probability pnorm() ppois() pgamma() pexp() Quantiles Function Get Value of any Percentile in Distribution p = percentile qnorm() qpois() qgamma() qexp() 12.2.1 Density (PDF) So, what percentage of customers stopped by 1 stall? Below, dpois() tells us the density() or frequency of your value, given a distribution where the mean = 5.5. # Get the frequency for 5 visits in the distribution pd5 &lt;- dpois(5, lambda = 5.5) # Check it! pd5 ## [1] 0.1714007 Looks like 17.1% of customers stopped by 5 stalls. [Optional] Validate This! We can validate this using our simulated visits from above, if we use methods for Observed Probabilities, which we learn later in this tutorial. We can calculate the density() function, extract it using approxfun(), and then assign it to dsim(), our own exact probability density function for our data. It works just like dpois(), but you donâ€™t need to specify lambda, because it only works for this exact distribution! # Approximate the PDF of our simulated visits dsim &lt;- visits %&gt;% density() %&gt;% approxfun() # Try our density function for our simulated data! dsim(5) ## [1] 0.1526192 # Pretty close to our results from dpois()! remove(dsim) 12.2.2 Cumulative Probabilities (CDF) What percentage of customers stopped by at max 5 stalls? # Get the cumulative frequency for a value (5) in the distribution cd5 &lt;- ppois(q = 5, lambda = 5.5) # Check it! cd5 ## [1] 0.5289187 Looks like just 52.9% of customers stopped by 1 stall or fewer. What percentage of customers stopped by over 5 stalls? # Get the probability they will NOT stop at 5 or fewer stalls 1 - cd5 ## [1] 0.4710813 [Optional] Validate This! We can validate this using our simulated visits from above, if we use methods for Observed Probabilities, which we learn later in this tutorial. We can validate our results against our simulated distribution. psim &lt;- visits %&gt;% density() %&gt;% tidy() %&gt;% # Get cumulative probability distribution arrange(x) %&gt;% # Get cumulative probabilities mutate(y = cumsum(y) / sum(y)) %&gt;% # Turn it into a literal function! approxfun() # Check it! psim(5) ## [1] 0.4243302 # Pretty close to cdf5! remove(psim) 12.2.3 Quantiles How many visits did people usually make? Estimate the interquartile range (25th-75th percentiles) of the unobserved distribution. q5 &lt;- qpois(p = c(.25, .75), lambda = 5.5) # Check it! q5 ## [1] 4 7 Looks like 50% of folks visited between 4 and 7 stalls [Optional] Validate this! We can validate this using our simulated visits from above, if we use methods for Observed Probabilities, which we learn later in this tutorial. We can compare against our simulated data using quantile(). # Approximate the quantile function of this distribution qsim &lt;- tibble( # Get a vector of percentiles from 0 to 1, in units of 0.001 x = seq(0, 1, by = 0.001), # Using our simulated distribution, # get the quantiles (values) at those points from this distribution y = visits %&gt;% quantile(probs = x)) %&gt;% # Approximate function! approxfun() # Check it! qsim(c(.25, .75)) ## [1] 4 7 remove(qsim) rm(visits, pd5, cd5, q5) Learning Check 1 Question What if we are not certain whether our unobserved vector of visits has a Poisson distribution or not? To give you more practice, please calculate the probability that customers will stop at more than 5 stalls, using appropriate functions for the (1) Normal, (2) Gamma, and (3) Exponential distribution! (See our table above for the list of function names.) [View Answer!] We know there were n = 500 customers, with a mean of 5.5 visits, a median of 5 visits, and a standard deviation of 2.5 visits. For a Normal Distribution: We learned in Workshop 2 that rnorm() requires a mean and sd (standard deviation); we conveniently have both! 1 - pnorm(5, mean = 5.5, sd = 2.5) ## [1] 0.5792597 For a Gamma Distribution: We learned in Workshop 2 that rgamma() requires a shape and scale (or rate); we can calculate these from the mean and sd (standard deviation). # shape = mean^2 / variance = mean^2 / sd^2 shape &lt;- 5.5^2 / 2.5^2 # scale = variance / mean scale &lt;- 2.5^2 / 5.5 # AND # rate = 1 / scale rate &lt;- 1 / scale # So... # Get 1 - cumulative probability up to 5 1 - pgamma(5, shape = shape, scale = scale) ## [1] 0.5211581 # OR (same) 1 - pgamma(5, shape = shape, rate = rate) ## [1] 0.5211581 For an Exponential Distribution: We learned in Workshop 2 that rexp() requires a rate; we can calculate this from the mean. # For exponential distribution, # rate = 1 / mean rate &lt;- 1 / 5.5 # So... # Get 1 - cumulative probability up to 5 1 - pexp(5, rate = rate) ## [1] 0.4028903 12.3 Observed Probability Functions 12.3.1 Example: Observed Distributions (#fig:img_er)Figure 1. Your Local ER For example, a local hospital wants to make their health care services more affordable, given the surge in inflation. They measured n = 15 patients who stayed 1 night over the last 7 days, how much were they charged (before insurance)? Letâ€™s call this vector obs (for â€˜observed dataâ€™). A 16th patient received a bill of $3000 (above the national mean of ~$2500). Weâ€™ll record this as stat below. # Let&#39;s record our vector of 15 patients obs &lt;- c(1126, 1493, 1528, 1713, 1912, 2060, 2541, 2612, 2888, 2915, 3166, 3552, 3692, 3695, 4248) # And let&#39;s get our new patient data point to compare against stat &lt;- 3000 Here, we know the full observed distribution of values (cost), so we can directly compute the p_value from them, using the logical operator &gt;=. # Which values of in vector obs were greater than or equal to stat? obs &gt;= stat ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE ## [13] TRUE TRUE TRUE R interprets TRUE == 1 &amp; FALSE == 0, so we can take the mean() to get the percentage of values in obs greater than or equal to stat. # Get p-value, the probability of getting a value greater than or equal to stat mean(obs &gt;= stat) ## [1] 0.3333333 # This means Total Probability, where probability of each cost is 1/n sum( (obs &gt;= stat) / length(obs) ) ## [1] 0.3333333 Unfortunately, this only takes into account the exact values we observed (eg. $1493), but it canâ€™t tell us anything about values we didnâ€™t observe (eg. $1500). But logically, we know that the probability of getting a bill of $1500 should be pretty similar to a bill of $1493. So how do we fill in the gaps? 12.3.2 Observed PDFs (Probability Density Functions) Above, we calculated the probability of getting a more extreme hospital bill based on a limited sample of points, but for more precise probabilities, we need to fill in the gaps between our observed data points. For a vector x, the probability density function is a curve providing the probability (y) of each value across the range of x. It shows the relative frequency (probability) of each possible value in the range. 12.3.3 density() We can ask R to estimate the probability density function for any observed vector using density(). This returns the density (y) of a bunch of hypothetical values (x) matching our distributionâ€™s curve. We can access those results using the broom package, by tidy()-ing it into a data.frame. obs %&gt;% density() %&gt;% tidy() %&gt;% tail(3) ## # A tibble: 3 Ã— 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5715. 0.000000718 ## 2 5727. 0.000000669 ## 3 5739. 0.000000622 But thatâ€™s data, not a function, right? Functions are equations, machines you can pump an input into to get a specific output. Given a data.frame of 2 vectors, R can actually approximate the function (equation) connecting vector 1 to vector 2 using approxfun(), creating your own function! So cool! # Let&#39;s make dobs(), the probability density function for our observed data. dobs &lt;- obs %&gt;% density() %&gt;% tidy() %&gt;% approxfun() # Now let&#39;s get a sequence (seq()) of costs from 1000 to 3000, in units of 1000.... seq(1000, 3000, by = 1000) ## [1] 1000 2000 3000 # and let&#39;s feed it a range of data to get the frequencies at those costs! seq(1000, 3000, by = 1000) %&gt;% dobs() ## [1] 0.0001503415 0.0003078665 0.0003184267 For now, letâ€™s get the densities for costs ranging from the min to the max observed cost. mypd &lt;- tibble( # Get sequence from min to max, in units of $10 cost = seq(min(obs), max(obs), by = 10), # Get probability densities prob_cost_i = dobs(cost)) %&gt;% # Classify each row as TRUE (1) if cost greater than or equal to stat, or FALSE (0) if not. # This is the probability that each row is extreme (1 or 0) mutate(prob_extreme_i = cost &gt;= stat) # Check it out! mypd %&gt;% head(3) ## # A tibble: 3 Ã— 3 ## cost prob_cost_i prob_extreme_i ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 1126 0.000186 FALSE ## 2 1136 0.000188 FALSE ## 3 1146 0.000191 FALSE Weâ€™ll save it to mypd, naming the x-axis cost and the y-axis prob_cost_i, to show the probability of each cost in row i (eg. $1126, $1136, $1146, â€¦ n). Weâ€™ll also calculate prob_extreme_i, the probability that each ith cost is extreme (greater than or equal to our 16th patientâ€™s bill). Either it is extreme (TRUE == 100% = 1) or it isnâ€™t (FALSE == 0% == 0). Our density function dobs() estimated prob_cost_i (y), the probability/relative frequency of cost (x) occurring, where x represents every possible value of cost. We can visualize mypd using geom_area() or geom_line() in ggplot2! We can add geom_vline() to draw a vertical line at the location of stat on the xintercept. mypd %&gt;% ggplot(mapping = aes(x = cost, y = prob_cost_i, fill = prob_extreme_i)) + # Fill in the area from 0 to y along x geom_area() + # Or just draw curve with line geom_line() + # add vertical line geom_vline(xintercept = stat, color = &quot;red&quot;, size = 3) + # Add theme and labels theme_classic() + labs(x = &quot;Range of Patient Costs (n = 15)&quot;, y = &quot;Probability&quot;, subtitle = &quot;Probability Density Function of Hospital Stay Costs&quot;) + # And let&#39;s add a quick label annotate(&quot;text&quot;, x = 3500, y = 1.5e-4, label = &quot;(%) Area\\nunder\\ncurve??&quot;, size = 5) (#fig:plot_pdf)Figure 2. Visualizing a Probability Density Function! 12.3.4 Using PDFs (Probability Density Functions) Great! We can view the probability density function now above. But how do we translate that into a single probability that measures how extreme Patient 16â€™s bill is? We have the probability prob_cost_i at points cost estimated by the probability density function saved in mypd. We can calculate the total probability or p_value that a value of cost will be greater than our statistic stat, using our total probability formula. We can even restate it, so that it looks a little more like the weighted average it truly is. \\[ P_{\\ Extreme} = \\sum_{i = 1}^{n}{ P (Cost | Extreme_{\\ i}) \\times P (Cost_{\\ i}) } = \\frac{ \\sum_{i = 1}^{n}{ P (Cost_{i}) \\times P(Extreme)_{\\ i} } }{ \\sum_{i = 1}^{n}{ P(Cost_{i}) } } \\] p &lt;- mypd %&gt;% # Calculate the conditional probability of each cost occurring given that condition mutate(prob_cost_extreme_i = prob_cost_i * prob_extreme_i) %&gt;% # Next, let&#39;s summarize these probabilities summarize( # Add up all probabilities of each cost given its condition in row i prob_cost_extreme = sum(prob_cost_extreme_i), # Add up all probabilities of each cost in any row i prob_cost = sum(prob_cost_i), # Calculate the weighted average, or total probability of getting an extreme cost # by dividing these two sums! prob_extreme = prob_cost_extreme / prob_cost) # Check it out! p ## # A tibble: 1 Ã— 3 ## prob_cost_extreme prob_cost prob_extreme ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0317 0.0867 0.365 Very cool! Visually, whatâ€™s happening here? ggplot() + geom_area(data = mypd, mapping = aes(x = cost, y = prob_cost_i, fill = prob_extreme_i)) + geom_vline(xintercept = stat, color = &quot;red&quot;, size = 3) + theme_classic() + labs(x = &quot;Range of Patient Costs (n = 15)&quot;, y = &quot;Probability&quot;, subtitle = &quot;Probability Density Function of Hospital Stay Costs&quot;) + annotate(&quot;text&quot;, x = 3500, y = 1.5e-4, label = paste(&quot;P(Extreme)&quot;, &quot;\\n&quot;, &quot; = &quot;, p$prob_extreme %&gt;% round(2), sep = &quot;&quot;), size = 5) (#fig:plot_pdf_area)Figure 3. PDF with Area Under Curve! 12.3.5 Observed CDFs (Cumulative Distribution Functions) Alternatively, we can calculate that p-value for prob_extreme a different way, by looking at the cumulative probability. To add a values/probabilities in a vector together sequentially, we can use cumsum() (short for cumulative sum). For example: # Normally c(1:5) ## [1] 1 2 3 4 5 # Cumulatively summed c(1:5) %&gt;% cumsum() ## [1] 1 3 6 10 15 # Same as c(1, 2+1, 3+2+1, 4+3+2+1, 5+4+3+2+1) ## [1] 1 3 6 10 15 Every probability density function (PDF) can also be represented as a cumulative distribution function (CDF). Here, we calculate the cumulative total probability of receiving each cost, applying cumsum() to the probability (prob_cost) of each value (cost). In this case, weâ€™re basically saying, weâ€™re interested in all the costs, so donâ€™t discount any. \\[ P_{\\ Extreme} = \\sum_{i = 1}^{n}{ P (Cost | Extreme_{\\ i} = 1) \\times P (Cost_{\\ i}) } = \\frac{ \\sum_{i = 1}^{n}{ P (Cost_{i}) \\times 1 } }{ \\sum_{i = 1}^{n}{ P(Cost_{i}) } } \\] mypd %&gt;% # For instance, we can do the first step here, # taking the cumulative probability of costs i through j.... mutate(prob_cost_cumulative = cumsum(prob_cost_i)) %&gt;% head(3) ## # A tibble: 3 Ã— 4 ## cost prob_cost_i prob_extreme_i prob_cost_cumulative ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 1126 0.000186 FALSE 0.000186 ## 2 1136 0.000188 FALSE 0.000374 ## 3 1146 0.000191 FALSE 0.000565 Our prob_cost_cumulative in row 3 above shows the total probability of n = 3 patients receiving a cost of 1126 OR 1136 OR 1146. But, we want an average estimate for 1 patient. So, like in a weighted average, we can divide by the total probability of all (n) hypothetical patients in the probability density function receiving any of these costs. This gives us our revised prob_cost_cumulative, which ranges from 0 to 1! mycd &lt;- mypd %&gt;% # For instance, we can do the first step here, # taking the cumulative probability of costs i through j.... mutate(prob_cost_cumulative = cumsum(prob_cost_i) / sum(prob_cost_i)) %&gt;% # We can also then identify the segment that is extreme! mutate(prob_extreme = prob_cost_cumulative * prob_extreme_i) # Take a peek at the tail! mycd %&gt;% tail(3) ## # A tibble: 3 Ã— 5 ## cost prob_cost_i prob_extreme_i prob_cost_cumulative prob_extreme ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4226 0.000144 TRUE 0.997 0.997 ## 2 4236 0.000142 TRUE 0.998 0.998 ## 3 4246 0.000140 TRUE 1 1 Letâ€™s visualize mycd, our cumulative probabilities! viz_cd &lt;- ggplot() + # Show the cumulative probability of each cost, # shaded by whether it is &quot;extreme&quot; (cost &gt;= stat) or not geom_area(data = mycd, mapping = aes(x = cost, y = prob_cost_cumulative, fill = prob_extreme_i)) + # Show cumulative probability of getting an extreme cost geom_area(data = mycd, mapping = aes(x = cost, y = prob_extreme, fill = prob_extreme_i)) + # Show the 16th patient&#39;s cost geom_vline(xintercept = stat, color = &quot;red&quot;, size = 3) + # Add formatting theme_classic() + labs(x = &quot;Cost of Hospital Stays (n = 15)&quot;, y = &quot;Cumulative Probability of Cost&quot;, fill = &quot;P(Extreme i)&quot;, title = &quot;Cumulative Distribution Function for Cost of Hospital Stays&quot;, subtitle = &quot;Probability of Cost more Extreme than $3000 = 0.36&quot;) # View it! viz_cd # (Note: I&#39;ve added some more annotation to mine # than your image will have - don&#39;t worry!) (#fig:plot_cdf)Figure 4. Visualizing a Cumulative Distribution Function! But wouldnâ€™t it be handy if we could just make a literal cumulative distribution function, just like we did for the probability density function dobs()? pobs &lt;- obs %&gt;% density() %&gt;% tidy() %&gt;% # Sort from smallest to largest arrange(x) %&gt;% # take cumulative sum, divided by total probability mutate(y = cumsum(y) / sum(y)) %&gt;% # Make cumulative distribution function pobs()! approxfun() # We&#39;ll test it out! 1 - pobs(3000) ## [1] 0.3726148 # Pretty close to our probability we calculated before! # Clear unnecessary data. remove(stat, mycd, p, viz_cd) 12.3.6 Using Calculus! Above we took a computational-approach to the CDF, using R to number-crunch the CDF. To summarize: We took a vector of empirical data obs, We estimated the probability density function (PDF) using density() We calculated the cumulative probability distribution ourselves We connected-the-dots of our CDF into a function with approxfun(). We did this because we started with empirical data, where where the density function is unknown! But sometimes, we do know the density function, perhaps because systems engineers have modeled it for decades! In these cases, we could alternatively use calculus in R to obtain the CDF and make probabilistic assessments. Hereâ€™s how! For example, this equation does a pretty okay job of approximating the shape of our distribution in obs. \\[ f(x) = \\frac{-2}{10^7} + \\frac{25x}{10^8} - \\frac{45x^2}{10^{12}} \\] We can write that up in a function, which we will call pdf. For every x value we supply, it will compute that equation to predict that valueâ€™s relative refequency/probability. # Write out our nice polynomial function pdf = function(x){ -2/10^7 + 25/10^8*x + -45/10^12*x^2 } # Check it! c(2000, 3000) %&gt;% pdf() ## [1] 0.0003198 0.0003448 The figure below demonstrates that it approximates the true density relatively closely. # We&#39;re going to add another column to our mypd dataset, mypd &lt;- mypd %&gt;% # approximating the probability of each cost with our new pdf() mutate(prob_cost_i2 = pdf(cost)) ggplot() + geom_line(data = mypd, mapping = aes(x = cost, y = prob_cost_i, color = &quot;from raw data&quot;)) + geom_line(data = mypd, mapping = aes(x = cost, y = prob_cost_i2, color = &quot;from function&quot;)) + theme_classic() + labs(x = &quot;Cost&quot;, y = &quot;Probability&quot;, color = &quot;Type&quot;) So how do we generate the cumulative density function? The mosaicCalc package can help us with its functions D() and antiD(). D() computes the derivative of a function (Eg. CDF -&gt; PDF) antiD() computes its integral (Eg. PDF -&gt; CDF) # Compute the anti-derivative (integral) of the function pdf(x), solving for x. cdf &lt;- antiD(tilde = pdf(x) ~ x) # It works just the same as our other functions obs %&gt;% head() %&gt;% cdf() ## [1] 0.1368449 0.2284130 0.2380292 0.2910549 0.3517389 0.3989108 # (Note: Our function is not a perfect fit for the data, so probabilities exceed 1!) # Let&#39;s compare our cdf() function made with calculus with pobs(), our computationally-generated CDF function. obs %&gt;% head() %&gt;% pobs() ## [1] 0.07767408 0.16372910 0.17344816 0.22749238 0.28830929 0.33387438 # Pretty similar results. The differences are due the fact that our original function is just an approximation, rather than dobs(), which is a perfect fit for our densities. And we can also take the derivative of our cdf() function with D() to get back our pdf(), which weâ€™ll call pdf2(). pdf2 &lt;- D(tilde = cdf(x) ~ x) # Let&#39;s compare results.... # Our original pdf... obs %&gt;% head() %&gt;% pdf() ## [1] 0.0002242456 0.0002727428 0.0002767347 0.0002960034 0.0003132915 ## [6] 0.0003238380 # Our pdf dervied from cdf... obs %&gt;% head() %&gt;% pdf2() ## [1] 0.0002242456 0.0002727428 0.0002767347 0.0002960034 0.0003132915 ## [6] 0.0003238379 # They&#39;re the same! Tada! You can do calculus in R! remove(mypd, pdf, pdf2, cdf, obs) Learning Check 2 Question A month has gone by, and our hospital has now billed 30 patients. Youâ€™ve heard that hospital bills at or above $3000 a day may somewhat deter people from seeking future medical care, while bills at or above $4000 may greatly deter people from seeking future care. (These numbers are hypothetical.) Using the vectors below, please calculate the following, using a PDF or CDF. Whatâ€™s the probability that a bill might somewhat deter a patient from going to the hospital? Whatâ€™s the probability that a bill might greatly deter a patient from going to the hospital? Whatâ€™s the probability that a patient might be somewhat deterred but not greatly deterred from going to the hospital? Note: Assume that the PDF matches the range of observed patients. # Let&#39;s record our vector of 30 patients patients &lt;- c(1126, 1493, 1528, 1713, 1912, 2060, 2541, 2612, 2888, 2915, 3166, 3552, 3692, 3695, 4248, 3000, 3104, 3071, 2953, 2934, 2976, 2902, 2998, 3040, 3030, 3021, 3028, 2952, 3013, 3047) # And let&#39;s get our statistics to compare against somewhat &lt;- 3000 greatly &lt;- 4000 [View Answer!] # Get the probability density function for our new data dobs2 &lt;- patients %&gt;% density() %&gt;% tidy() %&gt;% approxfun() # Get the probability densities mypd2 &lt;- tibble( cost = seq(min(patients), max(patients), by = 10), prob_cost_i = cost %&gt;% dobs2()) %&gt;% # Calculate probability of being somewhat deterred mutate( prob_somewhat_i = cost &gt;= somewhat, prob_greatly_i = cost &gt;= greatly, prob_somewhat_not_greatly_i = cost &gt;= somewhat &amp; cost &lt; greatly) To calculate these probabilities straight from the probability densities, do like so: mypd2 %&gt;% summarize( # Calculate total probability of a cost somewhat deterring medical care prob_somewhat = sum(prob_cost_i * prob_somewhat_i) / sum(prob_cost_i), # Calculate total probability of a cost greatly deterring medical care prob_greatly = sum(prob_cost_i * prob_greatly_i) / sum(prob_cost_i), # Calculate total probability of a cost somewhat-but-not-greatly deterring medical care prob_somewhat_not_greatly = sum(prob_cost_i * prob_somewhat_not_greatly_i) / sum(prob_cost_i)) ## # A tibble: 1 Ã— 3 ## prob_somewhat prob_greatly prob_somewhat_not_greatly ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.431 0.0172 0.414 To calculate these probabilities from the cumulative distribution functions, we can do the following: # Get cumulative probabilities of each mycd2 &lt;- mypd2 %&gt;% mutate( # Calculate total probability of a cost somewhat deterring medical care prob_somewhat = cumsum(prob_cost_i) * prob_somewhat_i / sum(prob_cost_i), # Calculate total probability of a cost greatly deterring medical care prob_greatly = cumsum(prob_cost_i) * prob_greatly_i / sum(prob_cost_i), # Calculate total probability of a cost somewhat-but-not-greatly deterring medical care prob_somewhat_not_greatly = cumsum(prob_cost_i) * prob_somewhat_not_greatly_i / sum(prob_cost_i)) # Check it! mycd2 %&gt;% tail(3) ## # A tibble: 3 Ã— 8 ## cost prob_cost_i prob_somewhat_i prob_greatly_i prob_somewhat_not_greatly_i ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 4226 0.000106 TRUE TRUE FALSE ## 2 4236 0.000107 TRUE TRUE FALSE ## 3 4246 0.000107 TRUE TRUE FALSE ## # â„¹ 3 more variables: prob_somewhat &lt;dbl&gt;, prob_greatly &lt;dbl&gt;, ## # prob_somewhat_not_greatly &lt;dbl&gt; Finally, if you want to visualize them, hereâ€™s how you would do it! ggplot() + # Get cumulative probability generally geom_area(data = mycd2, mapping = aes(x = cost, y = cumsum(prob_cost_i) / sum(prob_cost_i), fill = &quot;Very Little&quot;)) + # Get cumulative probability if somewhat but not greatly geom_area(data = mycd2, mapping = aes(x = cost, y = prob_somewhat_not_greatly, fill = &quot;Somewhat-not-Greatly&quot;)) + # Get cumulative probability if greatly geom_area(data = mycd2, mapping = aes(x = cost, y = prob_greatly, fill = &quot;Greatly&quot;)) + theme_classic() + labs(x = &quot;Cost of Hospital Stay (n = 30)&quot;, y = &quot;Cumulative Probability&quot;, subtitle = &quot;Probability of Hospital Bill Deterring Future Hospital Visits&quot;, fill = &quot;Deter Future Visits&quot;) Conclusion And thatâ€™s a wrap! Nice work! You can now figure out a lot of things about the world if you (a) can guess their distribution and (b) have one or two statistics about that distribution. Here we go! "],["system-reliability.html", "13 System Reliability Getting Started 13.1 Concepts Learning Check 1 13.2 Joint Probabilities 13.3 Table of Failure-Related Functions 13.4 Hazard Rate Function 13.5 Accumulative Hazard Function 13.6 Average Failure Rate 13.7 Units Learning Check 2 13.8 System Reliability 13.9 Renewal Rates via Bayes Rule", " 13 System Reliability In this tutorial, weâ€™re going to learn how to conduct Reliability Analysis (also known as Survival Analysis) in R. (#fig:img_epic_fail)Figure 1. Why We Do Reliability Analysis Getting Started Packages library(tidyverse) library(broom) library(DiagrammeR) 13.1 Concepts In Reliability/Survival Analysis, our quantity of interest is the amount of time it takes to reach a particular outcome (eg. time to failure, time to death, time to market saturation, etc.) Letâ€™s learn how to approximate them in R! 13.1.1 Life Distributions All technologies, operations, etc. have a â€˜lifetime distributionâ€™. If you took a sample of, say, cars in New York, you could measure how long each car functioned properly (its life-span), and build a Lifetime Distribution from that vector. # Let&#39;s imagine a normally distributed lifespan for these cars... lifespan &lt;- rnorm(100, mean = 5, sd = 1) The lifetime distribution is the probability density function telling us how frequently each potential lifespan is expected to occur. # We can build ourself the PDF of our lifetime distribution here dlife &lt;- lifespan %&gt;% density() %&gt;% tidy() %&gt;% approxfun() In contrast, the Cumulative Distribution Function (CDF) for a lifetime distribution tells us, for any time \\(t\\), the probability that a car will fail by time \\(t\\). # And we can build the CDF here plife &lt;- lifespan %&gt;% density() %&gt;% tidy() %&gt;% mutate(y = cumsum(y) / sum(y)) %&gt;% approxfun() Having built these functions for our cars, we can generate the probability (PDF) and cumulative probability (CDF) of failure across our observed vector of car lifespans, from ~2.82 to ~8.17. Reliability or Survival Analysis is concerned with the probability that a unit (our car) will still be operating by a specific time \\(t\\), representing the percentage of all cars that will survive to that point in time. So letâ€™s also calculate 1 - cumulative probability of failure. mycars &lt;- tibble( time = seq(min(lifespan), max(lifespan), by = 0.1), # Get probability of failing at time time prob = time %&gt;% dlife(), # Get probability of failing at or before time t prob_cumulative = time %&gt;% plife(), # Get probability of surving past time t # (NOT failing at or before time t) prob_survival = 1 - time %&gt;% plife()) Letâ€™s plot our three curves! ggplot() + # Make one area plot for Cumulative Probability (CDF) geom_area(data = mycars, mapping = aes(x = time, y = prob_cumulative, fill = &quot;Cumulative Probability&quot;), alpha = 0.5) + # Make one area plot for Relibability geom_area(data = mycars, mapping = aes(x = time, y = prob_survival, fill = &quot;Reliability (Survival)&quot;), alpha = 0.5) + # Make one area plot for Probability (PDF) geom_area(data = mycars, mapping = aes(x = time, y = prob, fill = &quot;Probability&quot;), alpha = 0.5) + theme_classic() + theme(legend.position = &quot;bottom&quot;) + labs(x = &quot;Lifespan of Car&quot;, y = &quot;Probability&quot;, subtitle = &quot;Example Life Distributions&quot;) Figure 13.1: Figure 2. Life Distributions of a Fleet of Cars This new reliability function allows us to calculate 2 quantities of interest: expected (average) number of cars that fail up to time \\(t\\). total cars expected to still operate after time \\(t\\). 13.1.2 Example: Airplane Propeller Failure! Suppose Lockheed Martin purchases 800 new airplane propellers. When asked about the failure rate, the seller reports that every 1500 days, 2 of these propellers are expected to break. Using this, we can calculate \\(m\\), the mean time to fail! \\[ \\lambda \\ t = t_{days} \\times \\frac{2 \\ units}{1500 \\ days} \\ \\ \\ and \\ \\ \\ m = \\frac{1500}{2} = 750 \\ days \\] This lets us generate the failure rate \\(F(t)\\), also known as the Cumulative Distribution Function, and we can write it up like this. \\[ CDF(t) = F(t) = 1 - e^{-(2t/1500)} = 1 - e^{-t/750} \\] Whatâ€™s pretty cool is, we can tell R to make a matching function fplane(), using the function() command. # For any value `t` we supply, do the following to that `t` value. fplane = function(t){ 1 - exp( -1*(t / 750)) } Letâ€™s use our function fplane() to answer our Lockheed engineersâ€™ questions. Whatâ€™s the probability a propeller will fail by t = 600 days? By t = 5000 days? fplane(t = c(600, 5000)) ## [1] 0.5506710 0.9987274 Looks like 55% will fail by 600 days, and 99% fail by 5000 days. Whatâ€™s the probability a propeller will fail between 600 and 5000 days? fplane(t = 5000) - fplane(t = 600) ## [1] 0.4480563 ~45% more will fail between this period. What percentage of new propellers will work more than 6000 days? 1 - fplane(t = 6000) ## [1] 0.0003354626 0.03% will survive past 6000 days. If Lockheed uses 300 propellers, how many will fail in 1 year? In 3 years? # Given a sample of 300 propellers, n &lt;- 300 # We project n * fplane(t = 362.25) will fail in 1 year (365.25 days) # that&#39;s ~115 propellers. n*fplane(t = 365.25) ## [1] 115.6599 # We also prject that n * fplane(t = 3 * 362.25) will fail in 3 years # that&#39;s ~230 propellers! n*fplane(t = 3*365.25) ## [1] 230.3988 Pretty powerful! Learning Check 1 Question (#fig:img_phone)Figure 3. Exploding Phones! (True Story) Hypothetical: Samsung is releasing a new Galaxy phone. But after the 2016 debacle of exploding phones, they want to estimate how likely it is a phone will explode (versus stay intact). Their pre-sale trials suggest that every 500 days, 5 phones are expected to explode. What percentage of phones are expected to work after more than 6 months? 1 year? [View Answer!] Using the information above, we can calculate the mean time to fail m, the rate of how many days it takes for an average unit to fail. days &lt;- 500 units &lt;- 5 m &lt;- days / units # Check it! m ## [1] 100 We can use m to make our explosion function fexplode(), which in this case, is our (catastrophic) failure function \\(f(t)\\)! \\[ CDF(days) = Explode(days) = 1 - e^{-(days \\times \\frac{1 \\ unit}{100 \\ days})} = 1 - e^{-0.01 \\times days} \\] fexplode = function(days){ 1 - exp(-1*days*0.01) } Then, we can calculate \\(r(t)\\), the cumulative probability that a phone will not explode after \\(t\\) days. Letâ€™s answer our questions! What percentage of phone are expected to survive 6 months? # What percent 1 - fexplode(365.25 / 2) ## [1] 0.1610162 What percentage of phone are expected to survive 1 year? 1 - fexplode(365.25) ## [1] 0.02592623 13.2 Joint Probabilities Two extra rules of probability can help us understand system reliability. 13.2.1 Multiplication Rule probability that \\(n\\) units with a reliability function \\(R(t)\\) survive past time \\(t\\) is multiplied, because of conditional probability, to equal \\(R(t)^{n}\\).. Figure 13.2: Figure 4. Oops. For example, thereâ€™s a 50% chance that 1 coffee cup breaks at local coffeeshop Coffee Please! every 6 months (180 days). Thus, the mean number of days to cup failure is \\(m = \\frac{180 \\ days}{ 1 \\ cup \\times 0.50 \\ chance} = 360 \\ days\\), while the relative frequency (probability) that a cup will break is $ = $. fcup = function(days){ 1 - exp( -1*(days/360))} # So the probability that 1 breaks within 100 days is XX percent fcup(100) ## [1] 0.2425349 And letâ€™s write out a reliability function too, based on our function for the failure function. # Notice how we can reference an earlier function fcup in our later function? Always have to define functions in order. rcup = function(days){ 1 - fcup(days) } # So the probability that 1 *doesn&#39;t* break within 100 days is XX perecent rcup(100) ## [1] 0.7574651 But the probability that two break within 100 days isâ€¦ fcup(100) * fcup(100) ## [1] 0.05882316 And the probability that 5 break within 100 days isâ€¦ very small! fcup(100)^5 ## [1] 0.0008392106 13.2.2 Compliment Rule The probability that at least 1 of \\(n\\) units fails by time \\(t\\) is \\(1 - R(t)^{n}\\). So, if Coffee Please! buys 2 new cups for their store, the probability that at least 1 unit breaks within a year isâ€¦ 1 - rcup(days = 365.25)^2 ## [1] 0.868555 While if they buy 5 new cups for their store, the chance at least 1 cup breaks within a year isâ€¦ 1 - rcup(days = 365.25)^5 ## [1] 0.9937359 13.3 Table of Failure-Related Functions (#tab:tab_functions)(#tab:tab_functions)Table 1. Failure and Reliability Functions Function Name Formula Equivalency Meaning \\(F(t)\\) Failure Function \\(1 - e^{-\\lambda t}\\) \\(1 - e^{-H(t)}\\) Cumulative Distribution Function (CDF) of Lifespans \\(R(t)\\) Reliability Distribution \\(e^{-\\lambda t}\\) \\(e^{-H(t)}\\) Remainder of CDF for Lifespans \\(f(t)\\) Change in Failure Function \\(\\frac{F(t + \\Delta t) - F(t)}{\\Delta t}\\) \\(-R&#39;(t)\\) Change in CDF at time \\(t_{2}\\) \\(-\\) at \\(t_{1}\\), per extra timestep \\(z(t)\\) Failure Rate (Hazard Rate) \\(\\frac{f(t)}{R(t)}\\) \\(\\lambda = \\frac{1}{m}\\) Mean Failure Rate \\(\\lambda\\); Inverse of Mean Time to Fail \\(m\\) \\(H(t)\\) Accumulative Hazard Rate \\(-log( R(t) )\\) \\(\\lambda t\\) Total Risk of Failure gained from time 0 to \\(t\\) \\(AFR(t_1,t_2)\\) Average Failure Rate \\(\\frac{H(t_{2}) - H(t_{1})}{t_{2} - t_{1}}\\) \\(\\frac{log(R(t_{1})) -log(R(t_{2})}{t_{2} - t_{1}}\\) Average failures per timestep between times \\(t_{1}\\) and \\(t_{2}\\) 13.4 Hazard Rate Function But if a unit has survived up until now, shouldnâ€™t its odds of failing change? We can express this as: \\[ P(Fail \\ Tomorrow | Survive \\ until \\ Today) = \\frac{ F(days + \\Delta days) - F(days) }{ \\Delta days \\times R(days)} = \\frac{ F(days + 1 ) - F(days) }{ 1 \\times R(days)} \\] Local coffeeshop Coffee Please! also has a lucky mug, which has stayed intact for 5 years, despite being dropped numerous times by patrons. Coffee Please!â€™s failure rate suggests they had a 99.3% chance of it breaking to date. # we call this the Hazard Rate Z zcup = function(days, plus = 1){ ( fcup(days + plus) - fcup(days) ) / (plus * rcup(days) ) } 13.5 Accumulative Hazard Function \\(H(t)\\): total accumulated risk of experiencing the event of interest that has been gained by progressing from 0 to time \\(t\\). the (instantaneous) hazard rate \\(h(t)\\) can grow or shrink over time, but the cumulative hazard rate only increases or stays constant. hcup = function(days){ -1*log( rcup(days) ) } # This captures the accumulative probability of a hazard (failure) occurring given the number of days past. hcup(100) ## [1] 0.2777778 13.6 Average Failure Rate The hazard rate \\(z(t)\\) varies over time, so letâ€™s generate a single statistic to summarize the distribution of hazard rates that \\(z(t)\\) can provide us between times \\(t_{a} \\to t_{z}\\). Weâ€™ll call this the Average Failure Rate \\(AFR(T)\\). afrcup = function(t1,t2){ (hcup(t2) - hcup(t1) ) / (t2 - t1)} afrcup(0, 5) ## [1] 0.002777778 # Or write it as.... afrcup = function(t1,t2){ (log(rcup(t1)) - log(rcup(t2)) ) / (t2 - t1)} afrcup(0, 5) ## [1] 0.002777778 # And if we&#39;re going from 0 to time t, # it simplifies to... afrcup = function(days){ hcup(days) / days } afrcup(5) ## [1] 0.002777778 # or to this afrcup = function(days){ -1*log(rcup(days)) / days } afrcup(5) ## [1] 0.002777778 When the probability for a time \\(t\\) is less than 0.10, \\(AFR = F(t) / T\\). This means that \\(F(t) = 1 - e^{-T \\times AFR(T)} \\approx T \\times AFR(T) \\ \\ when \\ F(T) &lt; 0.10\\). afrcup = function(days){ fcup(days) / days } afrcup(5) ## [1] 0.002758577 # and this is approximately.... 13.7 Units Units can be tough with failure rates, because they get tiny really quickly. Here are some common units: Percent per thousand hours, where \\(\\% / K = 10^5 \\times z(t)\\) Failure in Time (FIT) per thousand hours, also known as Parts per Million per Thousand Hours, written \\(PPM/K = 10^9 \\times z(t)\\). This equals \\(10^4 \\times Failure \\ Rate \\ in \\ \\% / K\\). For this lifetime function \\(F(t) = 1 - e^{-(t/2000)^{0.5}}\\), whatâ€™s the failure rate at t = 10, t = 1000, and t = 10,000 hours? Convert them into \\(\\%/K\\) and \\(PPM/K\\). 13.7.1 Failure Functions First, letâ€™s write the failure function f(t). # Write failure rate f = function(t){ 1 - exp(-(t/2000)^0.5) } Second, letâ€™s write the hazard rate z(t), for a 1 unit change in t. # Write hazard rate z = function(t, change = 1){ # Often I like to break up my functions into multiple lines; # it makes it much clearer to me. # To help, we can make &#39;temporary&#39; objects; # they only exist within the function. # Get change in failure function deltaf &lt;- (f(t+change) - f(t) ) / change # Get reliability function r &lt;- 1 - f(t) # Get hazard rate deltaf / r } Third, letâ€™s write the average hazard rate afr(t1,t2). afr = function(t1,t2){ # Let&#39;s get the survival rate r(t) r1 = 1 - f(t1) r2 = 1 - f(t2) # Let&#39;s get the accumulative hazard rate h(t) h1 = -log(r1) h2 = -log(r2) # And let&#39;s calculate the averge failure rate! afr = (h2 - h1) / (t2 - t1) # And return it! afr } 13.7.2 Conversion Functions Fourth, letâ€™s write some functions to convert our results into %/K and PPM/K, so we can be lazy! Weâ€™ll call our functions pk() and ppmk(). # % per 1000 hours pk = function(rate){ rate * 100 * 10^3 } # PPM/1000 hours ppmk = function(rate){ rate * 10^9 } 13.7.3 Converting Estimates Letâ€™s compare our hazard rates when t = 10, per hour, in % per 1000 hours, and in PPM per 1000 hours. # Per hour... Ew. Not very readable. z(t = 10) ## [1] 0.003445358 # % per 1000 hours.... Wheee! Much more legible z(t = 10) %&gt;% pk() ## [1] 344.5358 # PPM per 1000 hours.... Who! Big numbers! z(t = 10) %&gt;% ppmk() ## [1] 3445358 Finally, letâ€™s calculate the Average Failure Rate between 1000 and 10000 hours, in %/K. # Tada! Average Failure Rate from 1000 to 10000 hours, in % of units per 1000 hours afr(1000, 10000) %&gt;% pk() ## [1] 16.98846 # And in ppmk! afr(1000, 10000) %&gt;% ppmk() ## [1] 169884.6 Learning Check 2 Question (#fig:img_ramen)Figure 5. Does Instant Ramen ever go bad? A food safety inspector is investigating the average shelf life of instant ramen noodles. A company estimates the average shelf life of a package of ramen noodles at ~240 days per package. In a moment of poor judgement, she hires a team of hungry college students to taste-test old packages of that companyâ€™s ramen noodles, randomly sampled from a warehouse. When a student comes down with food poisoning, she records that product as having gone bad after XX days. She treats the record of ramen food poisonings as a sample of the lifespan of ramen products. ramen &lt;- c(163, 309, 215, 211, 246, 198, 281, 180, 317, 291, 238, 281, 215, 208, 212, 300, 231, 240, 285, 232, 252, 261, 310, 226, 282, 140, 208, 280, 237, 270, 185, 409, 293, 164, 231, 237, 269, 233, 246, 287, 187, 232, 180, 227, 215, 260, 236, 229, 263, 220) Using this data, please calculateâ€¦ Whatâ€™s the cumulative probability of a pack of ramen going bad within 8 months (240 days)? Are the companyâ€™s predictions accurate? Whatâ€™s the average failure rate (\\(\\lambda\\)) for the period between 8 months (240 days) to 1 year? Whatâ€™s the mean time to fail (\\(m\\)) for the period between 8 months to 1 year? [View Answer!] First, we take her ramen lifespan data, estimate the PDF with density(), and make the failure function (CDF), which Iâ€™ve called framen() below. # Get failure function f(t) = CDF of ramen failure framen &lt;- ramen %&gt;% density() %&gt;% tidy() %&gt;% # Now compute CDF mutate(y = cumsum(y) / sum(y)) %&gt;% approxfun() Second, we calculate the reliability function rramen(). # Get survival function r(t) = 1 - f(t) rramen &lt;- function(days){ 1 - framen(days) } Third, we can shortcut to the average failure rate, called afrramen() below, by using the reliability function rramen() to make our hazard rates at time 1 (h1) and time 2 (h2). # Get average failure rate from time 1 to time 2 afrramen &lt;- function(days1, days2){ h1 &lt;- -1*log(rramen(days1)) h2 &lt;- -1*log(rramen(days2)) (h2 - h1) / (days2 - days1) } Whatâ€™s the cumulative probability of a pack of ramen going bad within 8 months (240 days)? Are the companyâ€™s predictions accurate? framen(240) ## [1] 0.508274 Yes! ~50% of packages will go bad within 8 months. Pretty accurate! Whatâ€™s the average failure rate (\\(\\lambda\\)) for the period between 8 months (240 days) to 1 year? lambda &lt;- afrramen(240, 365) # Check it! lambda ## [1] 0.02563947 On average, between 8 months to 1 year, ramen packages go bad at a rate of ~0.026 units per day. Whatâ€™s the mean time to fail (\\(m\\)) for the period between 8 months to 1 year? # Calculate the inverse of lambda! m &lt;- 1 / lambda # check it! m ## [1] 39.00237 39 days per package. In other words, during this period post-expiration, this data suggests 1 package will go bad every 39 days. 13.8 System Reliability Reliability rates become extremely useful when we look at an entire system! This is where system reliability analysis can really improve the lives of ordinary people, decision-makers, and day-to-day users, because we can give them the knowledge they need to make decisions. So what knowledge do users usually need? How likely is the system as a whole to survive (or fail) over time? 13.8.1 Series Systems (#fig:fig_series)Dominos: A Series System In a series system, we have a set of \\(n\\) components (sometimes called nodes in a network), which get utilized sequentially. A domino train, for example, is a series system. It only takes 1 component to fail to stop the entire system (causing system failure). The overall reliability of a series system is defined as the success of every individual component (A AND B AND C). We write it using the formula below. \\[ Series \\ System \\ Reliability = R_{S} = \\prod^{n}_{i = 1}R_{i} = R_1 \\times R_2 \\times ... \\times R_n \\] We can also visualize it below, where each labelled node is a component. (#fig:mermaid_series)Figure 6. Example Series System 13.8.2 Parallel Systems (#fig:fig_parallel)Spoons: A Parallel System In a parallel system (a.k.a. redundant system), we have a set of \\(n\\) components, but only 1 component needs to function in order for the system to function. The overall reliability of a series system is defined as the success of any individual component (A OR B OR [A AND B]). A silverware drawer is an simple example of a parallel system. You probably just need 1 spoon for yourself at any time, but you have a stock of several spoons available in case you need them. We write it using the formula below, where each component \\(i\\) has a reliability rate of \\(R_{i}\\) and a failure rate of \\(F_{i}\\). \\[ Parallel \\ System \\ Reliability = R_{P} = 1 - \\prod^{n}_{i = 1}(1 - R_{i}) = 1 - \\prod^{n}_{i = 1}F_{i} = 1 - (F_1 \\times F_2 \\times ... \\times F_n) \\] Or we can represent it visually, where each labelled node is a component. (Unlabelled nodes are just junctions for relationships, so they donâ€™t get a probability. Some diagrams will not have these; you need them in mermaid.) (#fig:mermaid_parallel)Figure 7. Example Parallel System 13.8.3 Combined Systems Most systems actually involve combining the probabilities of several subsystems. When combining configurations, we calculate the probabilities of each subsystem, then then calculate the overall probability of the final system. Series System with Nested Parallel System: Figure 13.3: Figure 8. Series System with Nested Parallel System In the Figure above, we calculate the reliability rate for the parallel system, then calculate the reliability rate for the whole series; the parallel systemâ€™s rate becomes just one probability in the overall series system: \\(0.80 \\times (1 - (1 - 0.98) \\times (1 - 0.99) \\times (1 - 0.90)) \\times 0.95 \\approx 0.76\\). Parallel System with Nested Series Systems (Right Diagram): Figure 13.4: Figure 9. Parallel System with Nested Series Systems In the figure above, we calculate the reliability rate for each series system first, then calculate the reliability rate for the whole parallel system; each seriesâ€™ rate becomes one probability in the overall parallel system. \\(1 - ((1 - 0.80 \\times 0.90) \\times (1 - 0.95 \\times 0.99)) \\approx 0.98\\) The key is identifying exactly which system is nested in which system! 13.8.4 Example: Business Reliability Local businesses deal heavily with series system reliability, even if they donâ€™t regularly model it. Youâ€™ve been hired to analyze the system reliability of our local coffee shop Coffee Please! Our coffee shopâ€™s ability to serve cold brew coffee relies on 5 components, each with its own constant chance of failure. Water: Access to clean tap water. (Water outages occur ~ 3 days a year.) Coffee Grounds: Sufficient coffee grounds supply. (Ran out of stock 5 days in the last year). Refrigerator: Refrigerate coffee for 12 hours. (1% fail in a year.) Dishwasher: Run dishwasher on used cups (failed 2 times in last 60 days). Register: Use Cash Register to process transaction and give change (Failed 5 times in the 3 months). We can represent this in a system diagram below. (#fig:coffee_series)Figure 10. Example Series System in a Coffeeshop We can extract the average daily failure rate \\(lambda\\) for each of these components. # Water outage occrred 3 days in last 365 days lambda_water &lt;- 3 / 365 # Ran out of stock 5 days in last 365 days lambda_grounds &lt;- 5 / 365 # 1% of refrigerators fail within a 365 days lambda_refrigerator &lt;- 0.01 / 365 # Failed 2 times in last 60 days lambda_dishwasher &lt;- 2 / 60 # Failed 5 times in last 90 days lambda_cash &lt;- 5 / 90 Assuming a constant chance of failure, we can write ourselves a quick failure function f and reliability function r for an exponential distribution. # Write a reliability function r = function(t, lambda){ exp(-1*t*lambda) } And we can calculate the overall reliability of this coffeeshopâ€™s series system in 1 day by multiplying these reliability rates together. r(1, lambda_water) * r(1, lambda_grounds) * r(1, lambda_refrigerator) * r(1, lambda_dishwasher) * r(1, lambda_cash) ## [1] 0.8950872 This means thereâ€™s an 89.5% chance of this system fully functioning on a given day! 13.9 Renewal Rates via Bayes Rule We would hope that failed parts often get replaced, so we might want to adjust our functions accordingly. Renewal Rate: \\(r(t)\\) reflects the mean number of failures per unit at time \\(t\\). Example: Letâ€™s say thatâ€¦ For 10 units, we calculated how many days post production they lasted till failure (failure) as well as how many days post production till they were replaced (replace). Using this, we can calculate the lag-time, or the time taken for renewal. units &lt;- tibble( id = 1:15, failure = c(10, 200, 250, 300, 350, 375, 525, 525, 525, 525, 600, 650, 650, 675, 725), replace = c(100, 250, 350, 440, 550, 390, 600, 625, 660, 605, 700, 700, 700, 725, 750), renewal = replace - failure ) # Let&#39;s get the failure funciont... # by calculating the CDF of failure # first, we calculate the PDF of failure f &lt;- units$failure %&gt;% density() %&gt;% tidy() %&gt;% # Get CDF mutate(y = cumsum(y) / sum(y)) %&gt;% # turn into function approxfun() # Let&#39;s also calculate the CDF of replacement fr &lt;- units$replace %&gt;% density() %&gt;% tidy() %&gt;% # Get CDF mutate(y = cumsum(y) / sum(y)) %&gt;% # Get function approxfun() Above, we made the function fr(), which represents the cumulative probability of replacement, unrelated to failure. But what we really want to know is a conditional probability, specifically: how likely is a unit to get replaced at time \\(b\\), given that it failed at time \\(a\\)? Fortunately, we can use Bayesâ€™ Rule to deduce this. First, letâ€™s make a function f_fr(), meaning the cumulative probability of failure given replacement. This should (probably) be the same probability of failure as usual, but we need to restart the clock after replacement, so weâ€™ll set the time as \\(time_{failure} - time_{replacement}\\). # Probability of failure given replacement f_fr = function(time, time_replacement){ f(time - time_replacement) } Next, weâ€™ll use Bayes Rule to get the cumulative probability of replacement given failure, estimated in a function fr_f(). # Probability of replacement given failure fr_f = function(time, time_replacement){ # Estimate conditional probability of Failure given Replacement times Replacement top &lt;- f_fr(time, time_replacement) * fr(time_replacement) # Estimate total probability of Failure bottom &lt;- f_fr(time, time_replacement) * fr(time_replacement) + (1 - f_fr(time, time_replacement)) * (1 - fr(time_replacement)) # Divide them, and voila! top/bottom } Finally, what do these functions actually look like? Letâ€™s simulate failure and replacement over time, in a dataset of fakeunits. fakeunits &lt;- tibble( # As time increases from 0 to 1100, time = seq(0, 1100, by = 1), # Let&#39;s get the probability of failure at that time, prob_f = time %&gt;% f(), # Let&#39;s get the probability of replacement at time t + 10 given failure at time t prob_fr_f_10 = fr_f(time = time, time_replacement = time + 10), # How about t + 50? prob_fr_f_50 = fr_f(time = time, time_replacement = time + 50) ) # Check it! fakeunits %&gt;% head(3) ## # A tibble: 3 Ã— 4 ## time prob_f prob_fr_f_10 prob_fr_f_50 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.0341 0.000435 0.000484 ## 2 1 0.0344 0.000441 0.000490 ## 3 2 0.0347 0.000448 0.000496 And letâ€™s visualize it! (#fig:img_cumulative)Figure 11. Renewal Rates From this point on, the math for \\(z(t)\\), \\(H(t)\\), and \\(AFR(t)\\) for renewal rates gets a little tricky, so weâ€™ll save that for another day. But youâ€™re well on your way to working with life distributions! "],["system-reliability-in-python.html", "14 System Reliability in Python Getting Started 14.1 Concepts 14.2 Hazard Rate Function 14.3 Accumulative Hazard Function 14.4 Average Failure Rate 14.5 Units and Conversions Learning Check 1 14.6 Units and Conversions Learning Check 2 14.7 System Reliability Conclusion", " 14 System Reliability in Python In this tutorial, we learn core Reliability/Survival Analysis concepts in Python by mirroring the R workflow. Getting Started Packages # Remember to install these packages using a terminal, if you haven&#39;t already! !pip install pandas plotnine sympy scipy import pandas as pd from plotnine import * import sympy as sp Custom Functions This workshop uses custom reliability functions from the functions_distributions.py module. To use these functions, you need to download them from the repository: Option 1: Clone the entire repository git clone https://github.com/timothyfraser/sigma.git cd sigma Option 2: Download just the functions file # Download the functions file directly curl -O https://raw.githubusercontent.com/timothyfraser/sigma/main/functions/functions_distributions.py Option 3: Add the functions directory to your Python path import sys import os # Add the functions directory to Python path sys.path.append(&#39;functions&#39;) # or path to wherever you placed the functions folder Once you have the functions available, you can import them: from functions_distributions import rnorm, density, tidy_density, approxfun 14.1 Concepts In Reliability/Survival Analysis, our quantity of interest is the amount of time it takes to reach a particular outcome (eg. time to failure, time to death, time to market saturation, etc.) Letâ€™s learn how to approximate them in Python! 14.1.1 Life Distributions All technologies, operations, etc. have a â€˜lifetime distributionâ€™. If you took a sample of, say, cars in New York, you could measure how long each car functioned properly (its life-span), and build a Lifetime Distribution from that vector. # Add functions directory to path if not already there import sys if &#39;functions&#39; not in sys.path: sys.path.append(&#39;functions&#39;) from functions_distributions import rnorm, density, tidy_density, approxfun # Let&#39;s imagine a normally distributed lifespan for these cars... lifespan = rnorm(100, mean=5, sd=1) The lifetime distribution is the probability density function telling us how frequently each potential lifespan is expected to occur. # We can build ourself the PDF of our lifetime distribution here dlife = density(lifespan) dlife = tidy_density(dlife) dlife_fn = approxfun(dlife) In contrast, the Cumulative Distribution Function (CDF) for a lifetime distribution tells us, for any time \\(t\\), the probability that a car will fail by time \\(t\\). # And we can build the CDF here plife_df = tidy_density(density(lifespan)) plife_df = plife_df.sort_values(&#39;x&#39;) plife_df[&#39;y&#39;] = plife_df.y.cumsum() / plife_df.y.sum() plife = approxfun(plife_df) Having built these functions for our cars, we can generate the probability (PDF) and cumulative probability (CDF) of failure across our observed vector of car lifespans. Reliability or Survival Analysis is concerned with the probability that a unit (our car) will still be operating by a specific time \\(t\\), representing the percentage of all cars that will survive to that point in time. So letâ€™s also calculate 1 - cumulative probability of failure. import numpy as np time = np.arange(lifespan.min(), lifespan.max(), 0.1) mycars = pd.DataFrame({ &#39;time&#39;: time, &#39;prob&#39;: dlife_fn(time), &#39;prob_cumulative&#39;: plife(time) }) mycars[&#39;prob_survival&#39;] = 1 - mycars[&#39;prob_cumulative&#39;] Letâ€™s plot our three curves! (ggplot(mycars, aes(x=&#39;time&#39;)) + geom_area(aes(y=&#39;prob_cumulative&#39;, fill=&#39;&quot;Cumulative Probability&quot;&#39;), alpha=0.5) + geom_area(aes(y=&#39;prob_survival&#39;, fill=&#39;&quot;Reliability (Survival)&quot;&#39;), alpha=0.5) + geom_area(aes(y=&#39;prob&#39;, fill=&#39;&quot;Probability&quot;&#39;), alpha=0.5) + theme_classic() + theme(legend_position=&#39;bottom&#39;) + labs(x=&#39;Lifespan of Car&#39;, y=&#39;Probability&#39;, subtitle=&#39;Example Life Distributions&#39;)) ## &lt;plotnine.ggplot.ggplot object at 0x31445a3f0&gt; This new reliability function allows us to calculate 2 quantities of interest: expected (average) number of cars that fail up to time \\(t\\). total cars expected to still operate after time \\(t\\). 14.1.2 Example: Airplane Propeller Failure! Suppose Lockheed Martin purchases 800 new airplane propellers. When asked about the failure rate, the seller reports that every 1500 days, 2 of these propellers are expected to break. Using this, we can calculate \\(m\\), the mean time to fail! \\[ \\lambda \\ t = t_{days} \\times \\frac{2 \\ units}{1500 \\ days} \\ \\ \\ and \\ \\ \\ m = \\frac{1500}{2} = 750 \\ days \\] This lets us generate the failure rate \\(F(t)\\), also known as the Cumulative Distribution Function, and we can write it up like this. \\[ CDF(t) = F(t) = 1 - e^{-(2t/1500)} = 1 - e^{-t/750} \\] Whatâ€™s pretty cool is, we can tell Python to make a matching function fplane(), using the def command. import math # For any value `t` we supply, do the following to that `t` value. def fplane(t): return 1 - math.exp(-(t/750)) Letâ€™s use our function fplane() to answer our Lockheed engineersâ€™ questions. Whatâ€™s the probability a propeller will fail by t = 600 days? By t = 5000 days? print(&quot;fplane(600):&quot;, fplane(600)) ## fplane(600): 0.5506710358827784 print(&quot;fplane(5000):&quot;, fplane(5000)) ## fplane(5000): 0.9987273661986602 Looks like 55% will fail by 600 days, and 99% fail by 5000 days. Whatâ€™s the probability a propeller will fail between 600 and 5000 days? print(&quot;Probability of failure between 600 and 5000 days:&quot;, fplane(5000) - fplane(600)) ## Probability of failure between 600 and 5000 days: 0.44805633031588177 ~45% more will fail between this period. What percentage of new propellers will work more than 6000 days? print(&quot;Percentage surviving past 6000 days:&quot;, 1 - fplane(6000)) ## Percentage surviving past 6000 days: 0.00033546262790251635 0.03% will survive past 6000 days. If Lockheed uses 300 propellers, how many will fail in 1 year? In 3 years? # Given a sample of 300 propellers, n = 300 # We project n * fplane(t = 365.25) will fail in 1 year (365.25 days) # that&#39;s ~115 propellers. print(&quot;Failures in 1 year:&quot;, n * fplane(365.25)) ## Failures in 1 year: 115.65989011661077 # We also project that n * fplane(t = 3 * 365.25) will fail in 3 years # that&#39;s ~230 propellers! print(&quot;Failures in 3 years:&quot;, n * fplane(3*365.25)) ## Failures in 3 years: 230.398753639659 Pretty powerful! 14.1.3 Joint Probabilities Two extra rules of probability can help us understand system reliability. 14.1.3.1 Multiplication Rule probability that \\(n\\) units with a reliability function \\(R(t)\\) survive past time \\(t\\) is multiplied, because of conditional probability, to equal \\(R(t)^{n}\\). For example, thereâ€™s a 50% chance that 1 coffee cup breaks at local coffeeshop Coffee Please! every 6 months (180 days). Thus, the mean number of days to cup failure is \\(m = \\frac{180 \\ days}{ 1 \\ cup \\times 0.50 \\ chance} = 360 \\ days\\), while the relative frequency (probability) that a cup will break is \\(\\lambda = \\frac{1 \\ cup}{360 \\ days}\\). def fcup(days): return 1 - math.exp(-(days/360)) # So the probability that 1 breaks within 100 days is XX percent print(&quot;Probability 1 cup breaks in 100 days:&quot;, fcup(100)) ## Probability 1 cup breaks in 100 days: 0.24253487160303355 And letâ€™s write out a reliability function too, based on our function for the failure function. # Notice how we can reference an earlier function fcup in our later function? Always have to define functions in order. def rcup(days): return 1 - fcup(days) # So the probability that 1 *doesn&#39;t* break within 100 days is XX percent print(&quot;Probability 1 cup survives 100 days:&quot;, rcup(100)) ## Probability 1 cup survives 100 days: 0.7574651283969664 But the probability that two break within 100 days isâ€¦ print(&quot;Probability 2 cups break in 100 days:&quot;, fcup(100) * fcup(100)) ## Probability 2 cups break in 100 days: 0.05882316394349997 And the probability that 5 break within 100 days isâ€¦ very small! print(&quot;Probability 5 cups break in 100 days:&quot;, fcup(100)**5) ## Probability 5 cups break in 100 days: 0.0008392105809454709 14.1.3.2 Compliment Rule The probability that at least 1 of \\(n\\) units fails by time \\(t\\) is \\(1 - R(t)^{n}\\). So, if Coffee Please! buys 2 new cups for their store, the probability that at least 1 unit breaks within a year isâ€¦ print(&quot;Probability at least 1 of 2 cups breaks in a year:&quot;, 1 - rcup(365.25)**2) ## Probability at least 1 of 2 cups breaks in a year: 0.8685549869686017 While if they buy 5 new cups for their store, the chance at least 1 cup breaks within a year isâ€¦ print(&quot;Probability at least 1 of 5 cups breaks in a year:&quot;, 1 - rcup(365.25)**5) ## Probability at least 1 of 5 cups breaks in a year: 0.9937358768884709 14.2 Hazard Rate Function But if a unit has survived up until now, shouldnâ€™t its odds of failing change? We can express this as: \\[ P(Fail \\ Tomorrow | Survive \\ until \\ Today) = \\frac{ F(days + \\Delta days) - F(days) }{ \\Delta days \\times R(days)} = \\frac{ F(days + 1 ) - F(days) }{ 1 \\times R(days)} \\] Local coffeeshop Coffee Please! also has a lucky mug, which has stayed intact for 5 years, despite being dropped numerous times by patrons. Coffee Please!â€™s failure rate suggests they had a 99.3% chance of it breaking to date. # we call this the Hazard Rate Z def zcup(days, plus=1): return (fcup(days + plus) - fcup(days)) / (plus * rcup(days)) 14.3 Accumulative Hazard Function \\(H(t)\\): total accumulated risk of experiencing the event of interest that has been gained by progressing from 0 to time \\(t\\). the (instantaneous) hazard rate \\(h(t)\\) can grow or shrink over time, but the cumulative hazard rate only increases or stays constant. def hcup(days): return -1*math.log(rcup(days)) # This captures the accumulative probability of a hazard (failure) occurring given the number of days past. print(&quot;Accumulative hazard at 100 days:&quot;, hcup(100)) ## Accumulative hazard at 100 days: 0.27777777777777773 14.4 Average Failure Rate The hazard rate \\(z(t)\\) varies over time, so letâ€™s generate a single statistic to summarize the distribution of hazard rates that \\(z(t)\\) can provide us between times \\(t_{a} \\to t_{z}\\). Weâ€™ll call this the Average Failure Rate \\(AFR(T)\\). def afrcup(t1, t2): return (hcup(t2) - hcup(t1)) / (t2 - t1) print(&quot;Average failure rate from 0 to 5 days:&quot;, afrcup(0, 5)) ## Average failure rate from 0 to 5 days: 0.00277777777777777 When the probability for a time \\(t\\) is less than 0.10, \\(AFR = F(t) / T\\). This means that \\(F(t) = 1 - e^{-T \\times AFR(T)} \\approx T \\times AFR(T) \\ \\ when \\ F(T) &lt; 0.10\\). def afrcup_approx(days): return fcup(days) / days print(&quot;Approximate AFR at 5 days:&quot;, afrcup_approx(5)) ## Approximate AFR at 5 days: 0.0027585766512167485 14.5 Units and Conversions Units can be tough with failure rates, because they get tiny really quickly. Here are some common units: Percent per thousand hours, where \\(\\% / K = 10^5 \\times z(t)\\) Failure in Time (FIT) per thousand hours, also known as Parts per Million per Thousand Hours, written \\(PPM/K = 10^9 \\times z(t)\\). This equals \\(10^4 \\times Failure \\ Rate \\ in \\ \\% / K\\). For this lifetime function \\(F(t) = 1 - e^{-(t/2000)^{0.5}}\\), whatâ€™s the failure rate at t = 10, t = 1000, and t = 10,000 hours? Convert them into \\(\\%/K\\) and \\(PPM/K\\). 14.5.1 Failure Functions First, letâ€™s write the failure function f(t). # Write failure rate def f(t): return 1 - math.exp(-(t/2000)**0.5) Second, letâ€™s write the hazard rate z(t), for a 1 unit change in t. # Write hazard rate def z(t, change=1): # Often I like to break up my functions into multiple lines; # it makes it much clearer to me. # To help, we can make &#39;temporary&#39; objects; # they only exist within the function. # Get change in failure function deltaf = (f(t+change) - f(t)) / change # Get reliability function r = 1 - f(t) # Get hazard rate return deltaf / r Third, letâ€™s write the average hazard rate afr(t1,t2). def afr(t1, t2): # Let&#39;s get the survival rate r(t) r1 = 1 - f(t1) r2 = 1 - f(t2) # Let&#39;s get the accumulative hazard rate h(t) h1 = -math.log(r1) h2 = -math.log(r2) # And let&#39;s calculate the average failure rate! return (h2 - h1) / (t2 - t1) 14.5.2 Conversion Functions Fourth, letâ€™s write some functions to convert our results into %/K and PPM/K, so we can be lazy! Weâ€™ll call our functions pk() and ppmk(). # % per 1000 hours def pk(rate): return rate * 100 * 10**3 # PPM/1000 hours def ppmk(rate): return rate * 10**9 14.5.3 Converting Estimates Letâ€™s compare our hazard rates when t = 10, per hour, in % per 1000 hours, and in PPM per 1000 hours. # Per hour... Ew. Not very readable. print(&quot;Hazard rate per hour at t=10:&quot;, z(10)) ## Hazard rate per hour at t=10: 0.0034453578389621797 # % per 1000 hours.... Wheee! Much more legible print(&quot;Hazard rate %/K at t=10:&quot;, pk(z(10))) ## Hazard rate %/K at t=10: 344.535783896218 # PPM per 1000 hours.... Whoa! Big numbers! print(&quot;Hazard rate PPM/K at t=10:&quot;, ppmk(z(10))) ## Hazard rate PPM/K at t=10: 3445357.8389621796 Finally, letâ€™s calculate the Average Failure Rate between 1000 and 10000 hours, in %/K. # Tada! Average Failure Rate from 1000 to 10000 hours, in % of units per 1000 hours print(&quot;AFR %/K from 1000 to 10000 hours:&quot;, pk(afr(1000, 10000))) ## AFR %/K from 1000 to 10000 hours: 16.9884577368138 # And in ppmk! print(&quot;AFR PPM/K from 1000 to 10000 hours:&quot;, ppmk(afr(1000, 10000))) ## AFR PPM/K from 1000 to 10000 hours: 169884.577368138 Learning Check 1 Question ðŸ“±ðŸ’¥Exploding Phones! Hypothetical: Samsung is releasing a new Galaxy phone. But after the 2016 debacle of exploding phones, they want to estimate how likely it is a phone will explode (versus stay intact). Their pre-sale trials suggest that every 500 days, 5 phones are expected to explode. What percentage of phones are expected to work after more than 6 months? 1 year? [View Answer!] Using the information above, we can calculate the mean time to fail m, the rate of how many days it takes for an average unit to fail. days = 500 units = 5 m = days / units # Check it! print(&quot;Mean time to fail (days):&quot;, m) ## Mean time to fail (days): 100.0 We can use m to make our explosion function fexplode(), which in this case, is our (catastrophic) failure function \\(f(t)\\)! \\[ CDF(days) = Explode(days) = 1 - e^{-(days \\times \\frac{1 \\ unit}{100 \\ days})} = 1 - e^{-0.01 \\times days} \\] import math def fexplode(days): return 1 - math.exp(-0.01*days) Then, we can calculate \\(r(t)\\), the cumulative probability that a phone will not explode after \\(t\\) days. Letâ€™s answer our questions! What percentage of phones are expected to survive 6 months? # What percent print(&quot;6 months survival:&quot;, 1 - fexplode(365.25 / 2)) ## 6 months survival: 0.16101624797343572 What percentage of phones are expected to survive 1 year? print(&quot;1 year survival:&quot;, 1 - fexplode(365.25)) ## 1 year survival: 0.02592623211144296 14.6 Units and Conversions def pk(rate): return rate * 100 * 10**3 def ppmk(rate): return rate * 10**9 def f(t): return 1 - math.exp(-((t/2000)**0.5)) def z(t, change=1): deltaf = (f(t+change) - f(t)) / change r = 1 - f(t) return deltaf / r def afr(t1, t2): r1 = 1 - f(t1) r2 = 1 - f(t2) h1 = -math.log(r1) h2 = -math.log(r2) return (h2 - h1) / (t2 - t1) # Hazard rate at t=10 z(10) ## 0.0034453578389621797 # Hazard rate per 1000 hours pk(z(10)) ## 344.535783896218 # PPM per 1000 hours ppmk(z(10)) ## 3445357.8389621796 # Average failure rate from 1000 to 10000 hours afr(1000, 10000) ## 0.000169884577368138 # Average failure rate in % per 1000 hours pk(afr(1000, 10000)) ## 16.9884577368138 # Average failure rate in PPM per 1000 hours ppmk(afr(1000, 10000)) ## 169884.577368138 Learning Check 2 Question ðŸœ Does Instant Ramen ever go bad? A food safety inspector is investigating the average shelf life of instant ramen noodles. A company estimates the average shelf life of a package of ramen noodles at ~240 days per package. In a moment of poor judgement, she hires a team of hungry college students to taste-test old packages of that companyâ€™s ramen noodles, randomly sampled from a warehouse. When a student comes down with food poisoning, she records that product as having gone bad after XX days. She treats the record of ramen food poisonings as a sample of the lifespan of ramen products. ramen = [163, 309, 215, 211, 246, 198, 281, 180, 317, 291, 238, 281, 215, 208, 212, 300, 231, 240, 285, 232, 252, 261, 310, 226, 282, 140, 208, 280, 237, 270, 185, 409, 293, 164, 231, 237, 269, 233, 246, 287, 187, 232, 180, 227, 215, 260, 236, 229, 263, 220] Using this data, please calculateâ€¦ Whatâ€™s the cumulative probability of a pack of ramen going bad within 8 months (240 days)? Are the companyâ€™s predictions accurate? Whatâ€™s the average failure rate (\\(\\lambda\\)) for the period between 8 months (240 days) to 1 year? Whatâ€™s the mean time to fail (\\(m\\)) for the period between 8 months to 1 year? [View Answer!] First, we take her ramen lifespan data, estimate the PDF with density(), and make the failure function (CDF), which Iâ€™ve called framen() below. import numpy as np import pandas as pd from scipy.stats import gaussian_kde # Get failure function f(t) = CDF of ramen failure # Create a density estimate kde = gaussian_kde(ramen) x_range = np.linspace(min(ramen), max(ramen), 1000) density_values = kde(x_range) # Create CDF by integrating the density cdf_values = np.cumsum(density_values) / np.sum(density_values) # Create interpolation function for CDF from scipy.interpolate import interp1d framen = interp1d(x_range, cdf_values, kind=&#39;linear&#39;, bounds_error=False, fill_value=(0, 1)) Second, we calculate the reliability function rramen(). # Get survival function r(t) = 1 - f(t) def rramen(days): return 1 - framen(days) Third, we can shortcut to the average failure rate, called afrramen() below, by using the reliability function rramen() to make our hazard rates at time 1 (h1) and time 2 (h2). # Get average failure rate from time 1 to time 2 def afrramen(days1, days2): h1 = -1*np.log(rramen(days1)) h2 = -1*np.log(rramen(days2)) return (h2 - h1) / (days2 - days1) Whatâ€™s the cumulative probability of a pack of ramen going bad within 8 months (240 days)? Are the companyâ€™s predictions accurate? print(&quot;Cumulative probability of going bad within 240 days:&quot;, framen(240)) ## Cumulative probability of going bad within 240 days: 0.49920812311668555 Yes! ~50% of packages will go bad within 8 months. Pretty accurate! Whatâ€™s the average failure rate (\\(\\lambda\\)) for the period between 8 months (240 days) to 1 year? lambda_val = afrramen(240, 365) # Check it! print(&quot;Average failure rate (lambda):&quot;, lambda_val) ## Average failure rate (lambda): 0.030972058387967075 On average, between 8 months to 1 year, ramen packages go bad at a rate of ~0.026 units per day. Whatâ€™s the mean time to fail (\\(m\\)) for the period between 8 months to 1 year? # Calculate the inverse of lambda! m = 1 / lambda_val # check it! print(&quot;Mean time to fail (days):&quot;, m) ## Mean time to fail (days): 32.28716630563079 39 days per package. In other words, during this period post-expiration, this data suggests 1 package will go bad every 39 days. 14.7 System Reliability Reliability rates become extremely useful when we look at an entire system! This is where system reliability analysis can really improve the lives of ordinary people, decision-makers, and day-to-day users, because we can give them the knowledge they need to make decisions. So what knowledge do users usually need? How likely is the system as a whole to survive (or fail) over time? 14.7.1 Series Systems In a series system, we have a set of \\(n\\) components (sometimes called nodes in a network), which get utilized sequentially. A domino train, for example, is a series system. It only takes 1 component to fail to stop the entire system (causing system failure). The overall reliability of a series system is defined as the success of every individual component (A AND B AND C). We write it using the formula below. \\[ Series \\ System \\ Reliability = R_{S} = \\prod^{n}_{i = 1}R_{i} = R_1 \\times R_2 \\times ... \\times R_n \\] 14.7.2 Parallel Systems In a parallel system (a.k.a. redundant system), we have a set of \\(n\\) components, but only 1 component needs to function in order for the system to function. The overall reliability of a parallel system is defined as the success of any individual component (A OR B OR [A AND B]). A silverware drawer is an simple example of a parallel system. You probably just need 1 spoon for yourself at any time, but you have a stock of several spoons available in case you need them. We write it using the formula below, where each component \\(i\\) has a reliability rate of \\(R_{i}\\) and a failure rate of \\(F_{i}\\). \\[ Parallel \\ System \\ Reliability = R_{P} = 1 - \\prod^{n}_{i = 1}(1 - R_{i}) = 1 - \\prod^{n}_{i = 1}F_{i} = 1 - (F_1 \\times F_2 \\times ... \\times F_n) \\] 14.7.3 Combined Systems Most systems actually involve combining the probabilities of several subsystems. When combining configurations, we calculate the probabilities of each subsystem, then then calculate the overall probability of the final system. For systems in series, multiply component reliabilities. For parallel, use complements. def r_exp(t, mean_time): import math return math.exp(-t/mean_time) def series_reliability(t, means): r = 1.0 for m in means: r *= r_exp(t, m) return r def parallel_reliability(t, means): import math prod_fail = 1.0 for m in means: prod_fail *= (1 - r_exp(t, m)) return 1 - prod_fail print(&quot;Series reliability:&quot;, series_reliability(1000, [750, 900, 1200])) ## Series reliability: 0.0377119681337726 print(&quot;Parallel reliability:&quot;, parallel_reliability(1000, [750, 900, 1200])) ## Parallel reliability: 0.720700446343458 The key is identifying exactly which system is nested in which system! Conclusion You translated reliability concepts to Python: failure, reliability, hazard, cumulative hazard, and AFR with simple functions, plus basic series/parallel reasoning. "],["statistical-process-control-in-r.html", "15 Statistical Process Control in R Getting Started 15.1 Visualizing Quality Control Learning Check 1 15.2 Average and Standard Deviation Graphs Learning Check 2 15.3 Moving Range Charts 15.4 Constants Learning Check 3", " 15 Statistical Process Control in R Figure 2.1: Statistical Process Control! In this workshop, we will learn how to perform statistical process control in R, using statistical tools and ggplot visualizations! Statistical Process Control refers to using statistics to (1) measure variation in product quality over time and (2) identify benchmarks to know when intervention is needed. Letâ€™s get started! Getting Started Packages Weâ€™ll be using the tidyverse package for visualization, viridis for color palletes, moments for descriptive statistics, plus ggpubr for some add-on functions in ggplot. library(tidyverse) library(viridis) # you&#39;ll probably need to install these packages! # install.packages(c(&quot;ggpubr&quot;, &quot;moments&quot;)) library(ggpubr) library(moments) Our Case Figure 4.1: Obanazawa City, Yamagata Prefecture - A Hot Springs Economy. Photo credit and more here. For todayâ€™s workshop, weâ€™re going to think about why quality control matters in a local economy, by examining the case of the Japanese Hot Springs bath economy! Hot springs, or onsen, are a major source of tourism and recreation for families in Japan, bringing residents from across the country every year to often rural communities where the right geological conditions have brought on naturally occurring hot springs. Restaurants, taxi and bus companies, and many service sector firms rely on their local onsen to bring in a steady stream (pun intended) of tourists to the local economy. So, itâ€™s often in the best interest of onsen operators to keep an eye on the temperature, minerals, or other aspects of their hot springs baths to ensure quality control, to keep up their firm (and townâ€™s!) reputation for quality rest and relaxation! Onsen-goers often seek out specific types of hot springs, so itâ€™s important for an onsen to actually provide what it advertises! Serbulea and Payyappallimana (2012) describe some of these benchmarks. Temperature: Onsen are divided into â€œExtra Hot Springsâ€ (&gt;42 degreesC), â€œHot Springsâ€ (41~34 degreesC), and â€œWarm Springsâ€ (33~25 degreesC). pH: Onsen are classified into â€œAcidicâ€ (pH &lt; 3), â€œMildly Acidicâ€ (pH 3~6), â€œNeutralâ€ (pH 6~7.5), â€œMildly alkalineâ€ (ph 7.5~8.5), and â€œAklalineâ€ (pH &gt; 8.5). Sulfur: Sulfur onsen typically have about 2mg of sulfur per 1kg of hot spring water; sulfur levels must exceed 1 mg to count as a Sulfur onsen. (It smells like rotten eggs!) These are decent examples of quality control metrics that onsen operators might want to keep tabs on! Figure 15.1: Monkeys are even fans of onsen! Read more here! Our Data Youâ€™ve been hired to evaluate quality control at a local onsen in sunny Kagoshima prefecture! Every month, for 15 months, you systematically took 20 random samples of hot spring water and recorded its temperature, pH, and sulfur levels. How might you determine if this onsen is at risk of slipping out of one sector of the market (eg. Extra Hot!) and into another (just normal Hot Springs?). Letâ€™s read in our data from workshops/onsen.csv! # Let&#39;s import our samples of bathwater over time! water = read_csv(&quot;workshops/onsen.csv&quot;) # Take a peek! water %&gt;% glimpse() ## Rows: 160 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, â€¦ ## $ time &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, â€¦ ## $ temp &lt;dbl&gt; 43.2, 45.3, 45.5, 43.9, 45.9, 45.0, 42.3, 44.2, 42.2, 43.4, 46.â€¦ ## $ ph &lt;dbl&gt; 5.1, 4.8, 6.2, 6.4, 5.1, 5.6, 5.5, 5.3, 5.2, 5.9, 5.8, 5.3, 5.9â€¦ ## $ sulfur &lt;dbl&gt; 0.0, 0.4, 0.9, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.7, 1.1, 0.1â€¦ Our dataset contains: id: unique identifer for each sample of onsen water. time: categorical variable describing date of sample (month 1, month 3, â€¦ month 15). temp: temperature in celsius. ph: pH (0 to 14) sulfur: milligrams of sulfur ions. 15.1 Visualizing Quality Control Letâ€™s learn some key techniques for visualizing quality control! 15.1.1 theme_set() First, when youâ€™re about to make a bunch of ggplot visuals, it can help to set a common theme across them all with theme_set(). # By running theme_set() theme_set( # we tell ggplot to give EVERY plot this theme theme_classic(base_size = 14) + # With these theme traits, including theme( # Putting the legend on the bottom, if applicable legend.position = &quot;bottom&quot;, # horizontally justify plot subtitle and caption in center plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0.5), # Getting rid of busy axis ticks axis.ticks = element_blank(), # Getting rid of busy axis lines axis.line = element_blank(), # Surrounding the plot in a nice grey border panel.border = element_rect(fill = NA, color = &quot;grey&quot;), # Remove the right margin, for easy joining of plots plot.margin = margin(r = 0) ) ) 15.1.2 Process Descriptive Statistics First, letâ€™s describe our process, using favorite description statistics. Weâ€™re going to want to do this a bunch, so why donâ€™t we just write a function for it? Letâ€™s write describe(), which will take a vector x and calculate the mean(), sd(), skewness(), and kurtosis(), and then paste() together a nice caption describing them. (I encourage you to write your own functions like this to help expedite your coding! Start simple!) describe = function(x){ # Put our vector x in a tibble tibble(x) %&gt;% # Calculate summary statistics summarize( mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE), # We&#39;ll use the moments package for these two skew = skewness(x, na.rm = TRUE), kurtosis = kurtosis(x, na.rm = TRUE)) %&gt;% # Let&#39;s add a caption, that compiles all these statistics mutate( # We&#39;ll paste() the following together caption = paste( # Listing the name of each stat, then reporting its value and rounding it, then separating with &quot; | &quot; &quot;Process Mean: &quot;, mean %&gt;% round(2), &quot; | &quot;, &quot;SD: &quot;, sd %&gt;% round(2), &quot; | &quot;, &quot;Skewness: &quot;, skew %&gt;% round(2), &quot; | &quot;, &quot;Kurtosis: &quot;, kurtosis %&gt;% round(2), # Then make sure no extra spaces separate each item sep = &quot;&quot;)) %&gt;% return() } # Run descriptives! tab = water$temp %&gt;% describe() # Check it out! tab ## # A tibble: 1 Ã— 5 ## mean sd skew kurtosis caption ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 44.8 1.99 0.0849 3.62 Process Mean: 44.85 | SD: 1.99 | Skewness: 0.08 |â€¦ 15.1.3 Process Overview Visual Your first step should always be to look at the data overall! geom_jitter(), geom_boxplot(), and ggMarginal() can help you do this. geom_jitter() is a jittered scatterplot, jittering points a little to help with visibility. Since we want to be really precise on our quality control metrics, we could jitter the width a little (width = 0.25), but hold the y-axis (quality metric) constant at height = 0. These are in your x and y axis units, so decide based on your data each time. geom_boxplot() makes a boxplot for each group (time) in our data, showing the interquartile range (25th, 50th, and 75th percentiles) of our y variable for each group. Helpful way to view distributions. (Alternatively, you can try geom_violin(), which works the same way.) geom_hline() makes a horizontal line at the yintercept; we can tell it to show the mean() of y (in this case, temp). geom_histogram() is a histogram! We can use coord_flip() to turn it vertical to match the y-axis. ggarrange() from the ggpubr package binds two plots together into one, giving each a specific proportional width (eg. c(0.25, 0.75) percent, or c(5, 1) would be 5/6 and 1/6.) # Make the initial boxplot... g1 = water %&gt;% ggplot(mapping = aes(x = time, y = temp, group = time)) + # Plot grand mean geom_hline(mapping = aes(yintercept = mean(temp)), color = &quot;lightgrey&quot;, size = 3) + # Plot points and boxplots geom_jitter(height = 0, width = 0.25) + geom_boxplot() + labs(x = &quot;Time (Subgroup)&quot;, y = &quot;Temperature (Celsius)&quot;, subtitle = &quot;Process Overview&quot;, # Add our descriptive stats in the caption! caption = tab$caption) # Part 1 of plot g1 # Make the histogram, but tilt it on its side g2 = water %&gt;% ggplot(mapping = aes(x = temp)) + geom_histogram(bins = 15, color = &quot;white&quot;, fill = &quot;grey&quot;) + theme_void() + # Clear the them coord_flip() # tilt on its side # Part 2 of plot g2 # Then bind them together into 1 plot, &#39;h&#39;orizontally aligned. p1 = ggarrange(g1,g2, widths = c(5,1), align = &quot;h&quot;) # Check it out! p1 We can tell from this visual several things! Side Histogram: Our overall distribution is pretty centered. Descriptive Statistics: Our distribution has little skew (~0) and has slightly higher-than-average kurtosis (&lt;3) (very centered) (Review Skewness and Kurtosis here.) Line vs.Â Boxplots: Over time, our samples sure do seem to be getting slightly further from the mean! Learning Check 1 Question We analyzed temperature variation above, but our hot springs owner wants to know about variation in pH too! Write a function to produce a process overview plot given any 2 vectors (water$time and water$pH, in this case), and visualize the process overview for pH! (You can do it!) [View Answer!] ggprocess = function(x, y, xlab = &quot;Subgroup&quot;, ylab = &quot;Metric&quot;){ # Get descriptive statistics tab = describe(y) # Make the initial boxplot... g1 = ggplot(mapping = aes(x = x, y = y, group = x)) + # Plot grand mean geom_hline(mapping = aes(yintercept = mean(y)), color = &quot;lightgrey&quot;, size = 3) + # Plot points and boxplots geom_jitter(height = 0, width = 0.25) + geom_boxplot() + labs(x = xlab, y = ylab, subtitle = &quot;Process Overview&quot;, # Add our descriptive stats in the caption! caption = tab$caption) # Make the histogram, but tilt it on its side g2 = ggplot(mapping = aes(x = y)) + geom_histogram(bins = 15, color = &quot;white&quot;, fill = &quot;grey&quot;) + theme_void() + # Clear the them coord_flip() # tilt on its side # Then bind them together into 1 plot, &#39;h&#39;orizontally aligned. p1 = ggarrange(g1,g2, widths = c(5,1), align = &quot;h&quot;) return(p1) } # Visualize it! ggprocess(x = water$time, y = water$ph) In comparison to temp, pH is a much more controlled process. I encourage you to use this ggprocess() function you just created! 15.2 Average and Standard Deviation Graphs 15.2.1 Key Statistics Next, to analyze these processes more in depth, we need to assemble statistics at 2 levels: within-group statistics measure quantities of interest within each subgroup (eg. each monthly slice time in our onsen data). between-group statistics measure total quantities of interest for the overall process (eg. the overall â€œgrandâ€ mean, overall standard deviation in our onsen data). 15.2.2 Subgroup (Within-Group) Statistics Letâ€™s apply these to our onsen data to get statistics describing each subgroupâ€™s distribution, a.k.a. short-term or within-group statistics. # Calculate short-term statistics within each group stat_s = water %&gt;% # For each timestpe group_by(time) %&gt;% # Calculate these statistics of interest! summarize( # within-group mean xbar = mean(temp), # within-group range r = max(temp) - min(temp), # within-group standard deviation sd = sd(temp), # within-group sample size nw = n(), # Degrees of freedom within groups df = nw - 1) %&gt;% # Last, we&#39;ll calculate sigma_short (within-group variance) # We&#39;re going to calculate the short-term variation parameter sigma_s (sigma_short) # by taking the square root of the average of the standard deviation # Essentially, we&#39;re weakening the impact of any special cause variation # so that our sigma is mostly representative of common cause (within-group) variation mutate( # these are equivalent sigma_s = sqrt( sum(df * sd^2) / sum(df) ), sigma_s = sqrt(mean(sd^2)), # And get standard error (in a way that retains each subgroup&#39;s sample size!) se = sigma_s / sqrt(nw), # Calculate 6-sigma control limits! upper = mean(xbar) + 3*se, lower = mean(xbar) - 3*se) # Check it! stat_s %&gt;% head(3) ## # A tibble: 3 Ã— 10 ## time xbar r sd nw df sigma_s se upper lower ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 44.6 4.20 1.34 20 19 1.99 0.444 46.2 43.5 ## 2 3 45.3 7.9 2.00 20 19 1.99 0.444 46.2 43.5 ## 3 5 44.8 5.90 1.63 20 19 1.99 0.444 46.2 43.5 15.2.3 Total Statistics (Between Groups) # To get between-group estimates.... stat_t = stat_s %&gt;% summarize( xbbar = mean(xbar), rbar = mean(r), sdbar = mean(sd), # We can also recalculate sigma_short here too sigma_s = sqrt( mean(sd^2) ), # Or we can calculate overall standard deviation sigma_t = sd(water$temp) ) So, now that we have estimated within-group, common cause variation via \\(\\sigma_{short}\\) (sigma_s) and the standard error (se), what can we say about our process? 15.2.4 Average and Standard Deviation Charts The preferred method for measuring within-group variability is the standard deviation, rather than the range, so we generally recommend (a) Average (\\(\\bar{X}\\)) and Standard Deviation (\\(S\\)) charts over (b) Average (\\(\\bar{X}\\)) and Range (\\(R\\)) charts. # Let&#39;s extract some labels labels = stat_s %&gt;% summarize( time = max(time), type = c(&quot;xbbar&quot;, &quot;upper&quot;, &quot;lower&quot;), name = c(&quot;mean&quot;, &quot;+3 s&quot;, &quot;-3 s&quot;), value = c(mean(xbar), unique(upper), unique(lower)), value = round(value, 2), text = paste(name, value, sep = &quot; = &quot;)) stat_s %&gt;% ggplot(mapping = aes(x = time, y = xbar)) + geom_hline(mapping = aes(yintercept = mean(xbar)), color = &quot;lightgrey&quot;, size = 3) + geom_ribbon(mapping = aes(ymin = lower, ymax = upper), fill = &quot;steelblue&quot;, alpha = 0.2) + geom_line(size = 1) + geom_point(size = 5) + # Plot labels geom_label(data = labels, mapping = aes(x = time, y = value, label = text), hjust = 1) + labs(x = &quot;Time (Subgroups)&quot;, y = &quot;Average&quot;, subtitle = &quot;Average and Standard Deviation Chart&quot;) This tells us that excitingly, our onsen temperatures are quite firmly within range. While the average varies quite a bit, it remains comfortably within 3 standard deviations of the mean. Learning Check 2 Question Well, that was nifty, but can we do it all over again for pH? Make some rad upper and lower confidence intervals for \\(\\bar{X}\\) for pH! [View Answer!] # Get the within-group stats for ph! ph_s = water %&gt;% group_by(time) %&gt;% summarize( xbar = mean(ph), r = max(ph) - min(ph), sd = sd(ph), nw = n(), df = nw - 1) %&gt;% mutate( sigma_s = sqrt(mean(sd^2)), se = sigma_s / sqrt(nw), upper = mean(xbar) + 3*se, lower = mean(xbar) - 3*se) # Let&#39;s extract some labels labels = ph_s %&gt;% summarize( time = max(time), type = c(&quot;xbbar&quot;, &quot;upper&quot;, &quot;lower&quot;), name = c(&quot;mean&quot;, &quot;-3 s&quot;, &quot;+3 s&quot;), value = c(mean(xbar), unique(upper), unique(lower)), value = round(value, 2), text = paste(name, value, sep = &quot; = &quot;)) # and let&#39;s visualize it! ph_s %&gt;% ggplot(mapping = aes(x = time, y = xbar)) + geom_hline(mapping = aes(yintercept = mean(xbar)), color = &quot;lightgrey&quot;, size = 3) + geom_ribbon(mapping = aes(ymin = lower, ymax = upper), fill = &quot;steelblue&quot;, alpha = 0.2) + geom_line(size = 1) + geom_point(size = 5) + # Plot labels geom_label(data = labels, mapping = aes(x = time, y = value, label = text), hjust = 1) + labs(x = &quot;Time (Subgroups)&quot;, y = &quot;Average pH&quot;, subtitle = &quot;Average and Standard Deviation Chart&quot;) 15.3 Moving Range Charts 15.3.1 Individual and Moving Range Charts Suppose we only had 1 observation per subgroup! Thereâ€™s no way to calculate standard deviation for that - after all, thereâ€™s no variation within each subgroup! Instead, we can generate an individual and moving range chart. # Suppose we sample just the first out of each our months. indiv = water %&gt;% filter(id %in% c(1, 21, 41, 61, 81, 101, 121, 141)) The average moving range \\(m\\bar{R}\\) aptly refers to the average of the moving Range \\(mR\\), the difference in values over time. We can calculate the moving range using the diff() function on a vector like temp, shown below. abs() converts each value to be positive, since ranges are always 0 to infinity. # Let&#39;s see our original values indiv$temp ## [1] 43.2 46.0 46.6 42.1 44.4 46.8 43.7 45.9 # diff() gets range between second and first, third and second, and so on indiv$temp %&gt;% diff() %&gt;% abs() ## [1] 2.8 0.6 4.5 2.3 2.4 3.1 2.2 Just like all statistics, \\(m\\bar{R}\\) too has its own distribution, containing a range of slightly higher and lower \\(m\\bar{R}\\) statistics we might have gotten had our sample been just slightly different due to chance. As a result, we will want to estimate a confidence interval around \\(m\\bar{R}\\), but how are we to do that if we have no statistic like \\(\\sigma\\) to capture that variation? Well, good news: We can approximate \\(\\sigma_s\\) by taking the ratio of \\(m\\bar{R}\\) over a factor called \\(d_{2}\\). What is \\(d_{2}\\)? Iâ€™m glad you asked! \\[\\sigma_{short} \\approx \\frac{m\\bar{R}}{d_{2}} \\] 15.3.2 Factors \\(d_{2}\\) and Friends As discussed above, any statistics has a latent distribution of other values you might have gotten for your statistic had your sample been just slightly different due to random error. Fun fact: we can actually see those distributions pretty easily thanks to simulation! Suppose we have a subgroup of size n = 1, so we calculate a moving range of length 1. This subgroup and its moving range is just one of the possible subgroups we could have encountered by chance, so we can think of it as a random draw from an archetypal normal distribution (mean = 0 and sd = 1). If we take enough samples of moving ranges from that distribution, we can plot the distribution. Below, we take n = 10000 samples with rnorm() and plot the vector with hist(). We find a beautiful distribution of moving range statistics for n=1 size subgroups. mrsim = rnorm(n = 10000, mean = 0, sd = 1) %&gt;% diff() %&gt;% abs() mrsim %&gt;% hist() Much like \\(k\\) factors in the exponential distribution, we can use this distribution of \\(mR\\) stats to produce a series of factors that can help us estimate any upper or lower confidence interval in a moving range distribution. For example, we can calculate: \\(d_{2}\\), the mean of this archetypal \\(mR\\) distribution. \\(d_{3}\\), the standard deviation of this \\(mR\\) distribution. Technically, \\(d_{2}\\) is a ratio, which says that in a distribution with a standard deviation of 1, the mean mR is \\(d_{2}\\). In other words, \\(d_{2} = \\frac{m\\bar{R}_{normal, n = 1} }{\\sigma_{normal,n=1} }\\). So, if we have observed a real life average moving range \\(m\\bar{R}_{observed,n=1}\\), we can use this \\(d_{2}\\) factor to convert out of units of \\(1 \\sigma_{normal,n=1}\\) into units the \\(\\sigma_{short}\\) of our observed data! # For example, the mean of our vector mrsim, for subgroup size n = 1, # says that d2 (mean of these mR stats) is... mrsim %&gt;% mean() ## [1] 1.116105 # While d3 (standard deviation is...) mrsim %&gt;% sd() ## [1] 0.8297099 But why stop there? We can calculate loads of other interesting statistics! # For example, these statistics # estimate the median, upper 90, and upper 95% of the distribution! mrsim %&gt;% quantile(probs = c(0.50, 0.90, .95)) %&gt;% round(3) ## 50% 90% 95% ## 0.954 2.294 2.707 15.3.3 Estimating \\(\\sigma_{short}\\) for Moving Range Statistics Letâ€™s apply our new knowledge about \\(d_{2}\\) to calculate some upper bounds (\\(+3 \\sigma\\)) for our average moving range estimates! istat_s = indiv %&gt;% summarize( time = time[-1], # get moving range mr = temp %&gt;% diff() %&gt;% abs(), # Get average moving range mrbar = mean(mr), # Get total sample size! d2 = rnorm(n = 10000, mean = 0, sd = 1) %&gt;% diff() %&gt;% abs() %&gt;% mean(), # If we approximate sigma_s.... # pretty good! sigma_s = mrbar / d2, # Our subgroup size was 1, right? n = 1, # so this means sigma_s just equals the standard error here se = sigma_s / sqrt(n), # compute upper 3-se bound upper = mrbar + 3 * se, # and lower ALWAYS equals 0 for moving range lower = 0) Why stop there? Letâ€™s visualize it! # Let&#39;s get our labels! labels = istat_s %&gt;% summarize( time = max(time), type = c(&quot;mean&quot;, &quot;+3 s&quot;, &quot;lower&quot;), value = c(mrbar[1], upper[1], lower[1]) %&gt;% round(2), name = paste(type, value, sep = &quot; = &quot;)) # Now make the plot! istat_s %&gt;% ggplot(mapping = aes(x = time, y = mr)) + # Plot the confidence intervals geom_ribbon(mapping = aes(x = time, ymin = lower, ymax = upper), fill = &quot;steelblue&quot;, alpha = 0.25) + # Plot mrbar geom_hline(mapping = aes(yintercept = mean(mr)), size = 3, color = &quot;darkgrey&quot;) + # Plot moving range geom_line(size = 1) + geom_point(size = 5) + geom_label(data = labels, mapping = aes(x = time, y = value, label = name), hjust = 1) + labs(x = &quot;Time (Subgroup)&quot;, y = &quot;Moving Range&quot;, subtitle = &quot;Moving Range Chart&quot;) Again, this process looks pretty sound, firmly within range. 15.4 Constants 15.4.1 Find any \\(d_x\\) Factor While our book writes extensively about \\(d_{2}\\) and other \\(d_{whatever}\\) factors, itâ€™s not strictly necessary to calculate them unless you need them. Usually, we do this when we canâ€™t calculate the standard deviation normally (eg. when we have only moving range statistics or only a subgroup sample size of n=1). If youâ€™re working with full data from your process though, you can easily calculate \\(\\sigma_{short}\\) right from the empirical data, without ever needing to use \\(d_x\\) factors. But letâ€™s say you did need a \\(d_x\\) factor for a subgroup range of a given sample size n = 1, 2, 3.... n. Could we calculate some kind of function to give us it? Funny you should ask! Iâ€™ve written a little helper function you can use. # Let&#39;s calculate our own d function dn = function(n, reps = 1e4){ # For 10,0000 reps tibble(rep = 1:reps) %&gt;% # For each rep, group_by(rep) %&gt;% # Simulate the ranges of n values summarize(r = rnorm(n = n, mean = 0, sd = 1) %&gt;% range() %&gt;% diff() %&gt;% abs()) %&gt;% ungroup() %&gt;% # And calculate... summarize( # Mean range d2 = mean(r), # standard deviation of ranges d3 = sd(r), # and constants for obtaining lower and upper ci for rbar D3 = 1 - 3*(d3/d2), # sometimes written D3 D4 = 1 + 3*(d3/d2), # sometimes written D4 # Sometimes D3 goes negative; we need to bound it at zero D3 = if_else(D3 &lt; 0, true = 0, false = D3) ) %&gt;% return() } # Let&#39;s try it, where subgroup size is n = 2 dn(n = 2) ## # A tibble: 1 Ã— 4 ## d2 d3 D3 D4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.12 0.850 0 3.28 # Let&#39;s get the constants we need too. # Each of our samples has a sample size of 20 d = dn(n = 20) # Check it! d ## # A tibble: 1 Ã— 4 ## d2 d3 D3 D4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.74 0.734 0.412 1.59 15.4.2 Using \\(d_x\\) factors Using d_n(), we can make a quick approximation for the upper and lower control limits for the range \\(\\bar{R}\\) as well (as opposed to \\(m\\bar{R}\\))! # Let&#39;s get within group range for temperature... stat_w = water %&gt;% group_by(time) %&gt;% summarize(r = temp %&gt;% range() %&gt;% diff() %&gt;% abs(), n_w = n()) # get subgroup size # Let&#39;s get average within group range for temperature... stat = stat_w %&gt;% summarize(rbar = mean(r), # get Rbar n_w = unique(n_w)) # assuming constant subgroup size... # Check it! stat ## # A tibble: 1 Ã— 2 ## rbar n_w ## &lt;dbl&gt; &lt;int&gt; ## 1 7.26 20 # We find that dn() gives us constants D3 and D4... mydstat = dn(n = stat$n_w) mydstat ## # A tibble: 1 Ã— 4 ## d2 d3 D3 D4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.72 0.724 0.417 1.58 And use these constants to estimate the upper and lower CI for \\(\\bar{r}\\)! stat %&gt;% mutate(rbar_lower = rbar * mydstat$D3, rbar_upper = rbar * mydstat$D4) %&gt;% select(rbar, rbar_lower, rbar_upper) ## # A tibble: 1 Ã— 3 ## rbar rbar_lower rbar_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.26 3.02 11.5 So quick! You could use these values to make a range chart now. remove(stat, stat_w) 15.4.3 Finding any \\(b_x\\) Factor. We might also want to know how much the standard deviation could possible vary due to sampling error. To figure this out, weâ€™ll simulate many many standard deviations from a normal distribution, like in dn(), for a given subgroup size n. Then, we can calculate some quantities of interest like \\(C_{4}\\) (the mean standard deviation from an archetypal normal distribution), \\(B_{3}\\) (a multiplier for getting the lower control limit for 3 sigmas), and \\(B_{4}\\) (a multiplier for getting the upper control limit for 3 sigmas.) # Let&#39;s write a function bn() to calculate our B3 and B4 statistics for any subgroup size n bn = function(n, reps = 1e4){ tibble(rep = 1:reps) %&gt;% group_by(rep) %&gt;% summarize(s = rnorm(n, mean = 0, sd = 1) %&gt;% sd()) %&gt;% summarize(b2 = mean(s), b3 = sd(s), C4 = b2, # this is sometimes called C4 A3 = 3 / (b2 * sqrt( n )), B3 = 1 - 3 * b3/b2, B4 = 1 + 3 * b3/b2, # bound B3 at 0, since we can&#39;t have a standard deviation below 0 B3 = if_else(B3 &lt; 0, true = 0, false = B3)) %&gt;% return() } Letâ€™s apply this to our temp vector. First, weâ€™ll calculate the standard deviation within each subgroup, saved in stat_w under s. # Let&#39;s get within group standard deviation for temperature... stat_w = water %&gt;% group_by(time) %&gt;% summarize(s = temp %&gt;% sd(), n_w = n()) # get subgroup size stat_w ## # A tibble: 8 Ã— 3 ## time s n_w ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 1.34 20 ## 2 3 2.00 20 ## 3 5 1.63 20 ## 4 7 2.66 20 ## 5 9 2.57 20 ## 6 11 2.02 20 ## 7 13 1.65 20 ## 8 15 1.61 20 Second, weâ€™ll calculate the average standard deviation across subgroups, saved in stat under sbar. # Let&#39;s get average within group range for temperature... stat = stat_w %&gt;% summarize(sbar = mean(s), # get Rbar n_w = unique(n_w)) # assuming constant subgroup size... # Check it! stat ## # A tibble: 1 Ã— 2 ## sbar n_w ## &lt;dbl&gt; &lt;int&gt; ## 1 1.94 20 Third, weâ€™ll get our constants \\(B_{3}\\) and \\(B_{4}\\)! # For a subgroup size of 20... stat$n_w ## [1] 20 # Let&#39;s get our B constants! mybstat = bn(n = stat$n_w) # Check it out! mybstat ## # A tibble: 1 Ã— 6 ## b2 b3 C4 A3 B3 B4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.985 0.161 0.985 0.681 0.509 1.49 Finally, letâ€™s calculate our control limits! stat = stat %&gt;% # Add our constants to the data.frame... mutate(mybstat) %&gt;% # Calculate 3 sigma control limits mutate(sbar_lower = sbar * B3, sbar_upper = sbar * B4) # Check it out! stat %&gt;% select(sbar, sbar_lower, sbar_upper) ## # A tibble: 1 Ã— 3 ## sbar sbar_lower sbar_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.94 0.985 2.89 Now youâ€™re all ready to make a control chart showing variation in the standard deviation! Learning Check 3 Question Using our dn() function above, compile for yourself a short table of the \\(d_{2}\\) and \\(d_{3}\\) factors for subgroups sized 2 to 10. [View Answer!] # Let&#39;s bind them together! dx = bind_rows( dn(2), dn(3), dn(4), dn(5), dn(6), dn(7), dn(8), dn(9), dn(10)) %&gt;% mutate(n = 2:10) %&gt;% select(n, d2, d3) # Look at that cool table! dx ## # A tibble: 9 Ã— 3 ## n d2 d3 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 1.13 0.850 ## 2 3 1.70 0.885 ## 3 4 2.04 0.870 ## 4 5 2.33 0.867 ## 5 6 2.54 0.840 ## 6 7 2.69 0.843 ## 7 8 2.84 0.821 ## 8 9 2.97 0.813 ## 9 10 3.07 0.797 All done! Great work! "],["statistical-process-control-in-python.html", "16 Statistical Process Control in Python Getting Started 16.1 Process Descriptive Statistics 16.2 Process Overview Visual 16.3 Subgroup (Within-Group) Statistics Learning Check 1 16.4 Moving Range Charts (n=1) Conclusion", " 16 Statistical Process Control in Python Figure 2.1: Statistical Process Control! In this workshop, we will learn how to perform statistical process control in Python, using statistical tools and plotnine visualizations! Statistical Process Control refers to using statistics to (1) measure variation in product quality over time and (2) identify benchmarks to know when intervention is needed. Letâ€™s get started! Getting Started Packages # Remember to install these packages using a terminal, if you haven&#39;t already! !pip install pandas plotnine scipy Weâ€™ll be using pandas for data manipulation, plotnine for visualization, and scipy for statistical functions. import pandas as pd from plotnine import * Custom Functions This workshop uses custom functions from the functions/ directory. You may need both: - functions_distributions.py - for reliability and distribution functions - functions_process_control.py - for statistical process control functions To use these functions, you need to acquire them from the repository at github.com/timothyfraser/sigma/tree/main/functions. Add the functions directory to your Python path import sys import os # Add the functions directory to Python path sys.path.append(&#39;functions&#39;) # or path to wherever you placed the functions folder Once you have the functions available, you can import them: from functions_distributions import density, tidy_density, approxfun # from functions_process_control import ggprocess, ggsubgroup, ggmoving, ggcapability # if needed Our Case Figure 12.1: Obanazawa City, Yamagata Prefecture - A Hot Springs Economy. Photo credit and more here. For todayâ€™s workshop, weâ€™re going to think about why quality control matters in a local economy, by examining the case of the Japanese Hot Springs bath economy! Hot springs, or onsen, are a major source of tourism and recreation for families in Japan, bringing residents from across the country every year to often rural communities where the right geological conditions have brought on naturally occurring hot springs. Restaurants, taxi and bus companies, and many service sector firms rely on their local onsen to bring in a steady stream (pun intended) of tourists to the local economy. So, itâ€™s often in the best interest of onsen operators to keep an eye on the temperature, minerals, or other aspects of their hot springs baths to ensure quality control, to keep up their firm (and townâ€™s!) reputation for quality rest and relaxation! Onsen-goers often seek out specific types of hot springs, so itâ€™s important for an onsen to actually provide what it advertises! Serbulea and Payyappallimana (2012) describe some of these benchmarks. Temperature: Onsen are divided into â€œExtra Hot Springsâ€ (&gt;42Â°C), â€œHot Springsâ€ (41~34Â°C), and â€œWarm Springsâ€ (33~25Â°C). pH: Onsen are classified into â€œAcidicâ€ (pH &lt; 3), â€œMildly Acidicâ€ (pH 3~6), â€œNeutralâ€ (pH 6~7.5), â€œMildly alkalineâ€ (pH 7.5~8.5), and â€œAlkalineâ€ (pH &gt; 8.5). Sulfur: Sulfur onsen typically have about 2mg of sulfur per 1kg of hot spring water; sulfur levels must exceed 1 mg to count as a Sulfur onsen. (It smells like rotten eggs!) These are decent examples of quality control metrics that onsen operators might want to keep tabs on! Figure 4.1: Monkeys are even fans of onsen! Read more here! Our Data Youâ€™ve been hired to evaluate quality control at a local onsen in sunny Kagoshima prefecture! Every month, for 15 months, you systematically took 20 random samples of hot spring water and recorded its temperature, pH, and sulfur levels. How might you determine if this onsen is at risk of slipping out of one sector of the market (eg. Extra Hot!) and into another (just normal Hot Springs?). Letâ€™s read in our data from workshops/onsen.csv! # Add functions directory to path if not already there import sys if &#39;functions&#39; not in sys.path: sys.path.append(&#39;functions&#39;) from functions_distributions import density, tidy_density, approxfun water = pd.read_csv(&#39;workshops/onsen.csv&#39;) water.head(3) ## id time temp ph sulfur ## 0 1 1 43.2 5.1 0.0 ## 1 2 1 45.3 4.8 0.4 ## 2 3 1 45.5 6.2 0.9 16.1 Process Descriptive Statistics First, letâ€™s get a sense of our process by calculating some basic descriptive statistics. Weâ€™ll create a simple function to calculate the mean and standard deviation, which are fundamental to evaluating process variation. from pandas import Series def describe(x: Series): x = Series(x) out = pd.DataFrame({ &#39;mean&#39;: [x.mean()], &#39;sd&#39;: [x.std()], }) out[&#39;caption&#39;] = (&quot;Process Mean: &quot; + out[&#39;mean&#39;].round(2).astype(str) + &quot; | SD: &quot; + out[&#39;sd&#39;].round(2).astype(str)) return out tab = describe(water[&#39;temp&#39;]) tab ## mean sd caption ## 0 44.85 1.989501 Process Mean: 44.85 | SD: 1.99 Now letâ€™s apply this to our temperature data to see the overall process mean and variation. 16.2 Process Overview Visual The process overview chart is one of the most important tools in SPC. It shows us how our process behaves over time, helping us identify patterns, trends, and potential issues. Weâ€™ll create a visualization that shows individual measurements, subgroup means, and the overall process average. g1 = (ggplot(water, aes(x=&#39;time&#39;, y=&#39;temp&#39;, group=&#39;time&#39;)) + geom_hline(aes(yintercept=water[&#39;temp&#39;].mean()), color=&#39;lightgrey&#39;, size=3) + geom_jitter(height=0, width=0.25) + geom_boxplot() + labs(x=&#39;Time (Subgroup)&#39;, y=&#39;Temperature (Celsius)&#39;, subtitle=&#39;Process Overview&#39;, caption=tab[&#39;caption&#39;][0])) # Save the plot g1.save(&#39;images/05_process_overview.png&#39;, width=8, height=6, dpi=100) g2 = (ggplot(water, aes(x=&#39;temp&#39;)) + geom_histogram(bins=15, color=&#39;white&#39;, fill=&#39;grey&#39;) + theme_void() + coord_flip()) # Save the plot g2.save(&#39;images/05_process_histogram.png&#39;, width=8, height=6, dpi=100) The histogram shows us the distribution of all temperature measurements, giving us insight into the overall process variation. This helps us understand if our process is centered and how much variation weâ€™re seeing. 16.3 Subgroup (Within-Group) Statistics In SPC, we often work with subgroups - small samples taken at regular intervals. This allows us to distinguish between common cause variation (inherent to the process) and special cause variation (due to specific events). Letâ€™s calculate statistics for each subgroup to see how the process behaves over time. stat_s = (water.groupby(&#39;time&#39;).apply(lambda d: pd.Series({ &#39;xbar&#39;: d[&#39;temp&#39;].mean(), &#39;r&#39;: d[&#39;temp&#39;].max() - d[&#39;temp&#39;].min(), &#39;sd&#39;: d[&#39;temp&#39;].std(), &#39;nw&#39;: len(d) })).reset_index()) stat_s[&#39;df&#39;] = stat_s[&#39;nw&#39;] - 1 stat_s[&#39;sigma_s&#39;] = ( (stat_s[&#39;df&#39;] * (stat_s[&#39;sd&#39;]**2)).sum() / stat_s[&#39;df&#39;].sum() )**0.5 stat_s[&#39;se&#39;] = stat_s[&#39;sigma_s&#39;] / (stat_s[&#39;nw&#39;]**0.5) stat_s[&#39;upper&#39;] = stat_s[&#39;xbar&#39;].mean() + 3*stat_s[&#39;se&#39;] stat_s[&#39;lower&#39;] = stat_s[&#39;xbar&#39;].mean() - 3*stat_s[&#39;se&#39;] stat_s.head(3) ## time xbar r sd nw df sigma_s se upper lower ## 0 1 44.635 4.2 1.342533 20.0 19.0 1.986174 0.444122 46.182366 43.517634 ## 1 3 45.305 7.9 2.001440 20.0 19.0 1.986174 0.444122 46.182366 43.517634 ## 2 5 44.765 5.9 1.628133 20.0 19.0 1.986174 0.444122 46.182366 43.517634 Here weâ€™ve calculated key statistics for each subgroup: xbar: The mean of each subgroup r: The range (max - min) within each subgroup sd: The standard deviation within each subgroup sigma_s: The pooled within-subgroup standard deviation se: The standard error for each subgroup mean 16.3.1 Total Statistics (Between Groups) Now letâ€™s calculate the overall process statistics that summarize the behavior across all subgroups: stat_t = pd.DataFrame({ &#39;xbbar&#39;: [stat_s[&#39;xbar&#39;].mean()], &#39;rbar&#39;: [stat_s[&#39;r&#39;].mean()], &#39;sdbar&#39;: [stat_s[&#39;sd&#39;].mean()], &#39;sigma_s&#39;: [(stat_s[&#39;sd&#39;]**2).mean()**0.5], &#39;sigma_t&#39;: [water[&#39;temp&#39;].std()] }) stat_t ## xbbar rbar sdbar sigma_s sigma_t ## 0 44.85 7.2625 1.93619 1.986174 1.989501 These statistics give us: xbbar: The grand mean (average of all subgroup means) rbar: The average range across subgroups sdbar: The average standard deviation across subgroups sigma_s: The pooled within-subgroup standard deviation sigma_t: The total process standard deviation 16.3.2 Average and Standard Deviation Charts Control charts are the heart of SPC. They help us monitor process stability over time and detect when the process is out of control. Weâ€™ll create charts for both the subgroup means (X-bar chart) and standard deviations (S chart). labels = pd.DataFrame({ &#39;time&#39;: [stat_s[&#39;time&#39;].max()]*3, &#39;type&#39;: [&#39;xbbar&#39;,&#39;upper&#39;,&#39;lower&#39;], &#39;name&#39;: [&#39;mean&#39;,&#39;+3 s&#39;,&#39;-3 s&#39;], &#39;value&#39;: [stat_s[&#39;xbar&#39;].mean(), stat_s[&#39;upper&#39;].iloc[0], stat_s[&#39;lower&#39;].iloc[0]] }) control_chart = (ggplot(stat_s, aes(x=&#39;time&#39;, y=&#39;xbar&#39;)) + geom_hline(aes(yintercept=stat_s[&#39;xbar&#39;].mean()), color=&#39;lightgrey&#39;, size=3) + geom_ribbon(aes(ymin=&#39;lower&#39;, ymax=&#39;upper&#39;), fill=&#39;steelblue&#39;, alpha=0.2) + geom_line(size=1) + geom_point(size=5) + geom_label(data=labels, mapping=aes(x=&#39;time&#39;, y=&#39;value&#39;, label=&#39;name&#39;), ha=&#39;right&#39;) + labs(x=&#39;Time (Subgroups)&#39;, y=&#39;Average&#39;, subtitle=&#39;Average and Standard Deviation Chart&#39;)) # Save the plot control_chart.save(&#39;images/05_control_chart.png&#39;, width=8, height=6, dpi=100) This control chart shows: Center line: The grand mean (xbbar) Control limits: Upper and lower 3-sigma limits based on the standard error Individual points: Each subgroup mean plotted over time Shaded area: The control limits region Points outside the control limits or showing non-random patterns indicate the process may be out of control and requires investigation. Learning Check 1 Question Produce the same process overview chart for pH. [View Answer!] def ggprocess(x, y, xlab=&#39;Subgroup&#39;, ylab=&#39;Metric&#39;): import pandas as pd from plotnine import ggplot, aes, geom_hline, geom_jitter, geom_boxplot, labs d = pd.DataFrame({&#39;x&#39;: x, &#39;y&#39;: y}) g = (ggplot(d, aes(x=&#39;x&#39;, y=&#39;y&#39;, group=&#39;x&#39;)) + geom_hline(aes(yintercept=d[&#39;y&#39;].mean()), color=&#39;lightgrey&#39;, size=3) + geom_jitter(height=0, width=0.25) + geom_boxplot() + labs(x=xlab, y=ylab, subtitle=&#39;Process Overview&#39;)) return g ph_chart = ggprocess(water[&#39;time&#39;], water[&#39;ph&#39;]) # Save the plot ph_chart.save(&#39;images/05_ph_chart.png&#39;, width=8, height=6, dpi=100) 16.4 Moving Range Charts (n=1) When we have individual measurements rather than subgroups, we use moving range charts. The moving range is the absolute difference between consecutive measurements, which helps us estimate process variation when we canâ€™t calculate within-subgroup statistics. indiv = water.iloc[[0,20,40,60,80,100,120,140]] mr = (indiv[&#39;temp&#39;].diff().abs().dropna()) mrbar = mr.mean() import numpy as np d2 = np.mean(np.abs(np.diff(np.random.normal(0,1,10000)))) sigma_s = mrbar / d2 se = sigma_s / (1**0.5) upper = mrbar + 3*se lower = 0 istat = pd.DataFrame({&#39;time&#39;: indiv[&#39;time&#39;].iloc[1:], &#39;mr&#39;: mr, &#39;mrbar&#39;: mrbar, &#39;upper&#39;: upper, &#39;lower&#39;: lower}) mr_chart = (ggplot(istat, aes(x=&#39;time&#39;, y=&#39;mr&#39;)) + geom_ribbon(aes(ymin=&#39;lower&#39;, ymax=&#39;upper&#39;), fill=&#39;steelblue&#39;, alpha=0.25) + geom_hline(aes(yintercept=mr.mean()), size=3, color=&#39;darkgrey&#39;) + geom_line(size=1) + geom_point(size=5) + labs(x=&#39;Time (Subgroup)&#39;, y=&#39;Moving Range&#39;, subtitle=&#39;Moving Range Chart&#39;)) # Save the plot mr_chart.save(&#39;images/05_moving_range_chart.png&#39;, width=8, height=6, dpi=100) The moving range chart shows: Center line: The average moving range (mrbar) Upper control limit: Based on the estimated process standard deviation Lower control limit: Set to 0 (moving ranges canâ€™t be negative) Individual points: Each moving range value This chart helps us monitor process variation when we have individual measurements rather than subgroups. Conclusion Youâ€™ve successfully produced SPC visuals and statistics in Python: process overviews, subgroup statistics, and moving range logic. These tools help us understand process behavior, identify when processes are in or out of control, and make data-driven decisions about process improvement. "],["indices-and-confidence-intervals-for-statistical-process-control-in-r.html", "17 Indices and Confidence Intervals for Statistical Process Control in R Getting Started 17.1 Process Capability vs.Â Stability 17.2 Index Functions Learning Check 1 17.3 Confidence Intervals 17.4 CIs for any Index! Conclusion", " 17 Indices and Confidence Intervals for Statistical Process Control in R Figure 2.1: Bootstrapping Sampling Distributions for Statistics!! This workshop extends our toolkit developed in Workshop 9, discussing Process Capability and Stability Indices, and introducing means to calculate confidence intervals for these indices. Getting Started Packages Weâ€™ll be using the tidyverse package for visualization, viridis for color palletes, moments for descriptive statistics, plus ggpubr for some add-on functions in ggplot. library(tidyverse) library(viridis) # you&#39;ll probably need to install these packages! # install.packages(c(&quot;ggpubr&quot;, &quot;moments&quot;)) Our Data Weâ€™ll be continuing to analyze our quality control data from a local hot springs inn (onsen) in sunny Kagoshima Prefecture, Japan. Every month, for 15 months, you systematically took 20 random samples of hot spring water and recorded its temperature, pH, and sulfur levels. How might you determine if this onsen is at risk of slipping out of one sector of the market (eg. Extra Hot!) and into another (just normal Hot Springs?). Letâ€™s read in our data from workshops/onsen.csv! # Let&#39;s import our samples of bathwater over time! water = read_csv(&quot;workshops/onsen.csv&quot;) # Take a peek! water %&gt;% glimpse() ## Rows: 160 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, â€¦ ## $ time &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, â€¦ ## $ temp &lt;dbl&gt; 43.2, 45.3, 45.5, 43.9, 45.9, 45.0, 42.3, 44.2, 42.2, 43.4, 46.â€¦ ## $ ph &lt;dbl&gt; 5.1, 4.8, 6.2, 6.4, 5.1, 5.6, 5.5, 5.3, 5.2, 5.9, 5.8, 5.3, 5.9â€¦ ## $ sulfur &lt;dbl&gt; 0.0, 0.4, 0.9, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.7, 1.1, 0.1â€¦ Our dataset contains: id: unique identifer for each sample of onsen water. time: categorical variable describing date of sample (month 1, month 3, â€¦ month 15). temp: temperature in celsius. ph: pH (0 to 14) sulfur: milligrams of sulfur ions. 17.1 Process Capability vs.Â Stability 17.1.1 Definitions Production processes can be categorized in terms of Capability (does it meet required specifications?) and Stability (is production consistent and predictable?) Both are vital. A capable process delivers goods that can actually perform their function, like a 20-foot ladder that is actually 20 feet! A stable process delivers products with consistent and predictable traits (regardless of whether those traits are good). We need to maximize both process capability and stability to make an effective process, be it in manufacturing, health care, local businesses, or social life! Depending on the shape and stability of our data, we should choose one of the 4 statistics to evaluate our data. 17.1.2 Table of Indices These statistics rely on some combination of (1) the mean \\(\\mu\\), (2) the standard deviation \\(\\sigma\\), and (3) the upper and lower â€œspecification limitsâ€; the specification limits are our expected values \\(E_{upper}\\) and \\(E_{lower}\\), as compared to our actual observed values, summarized by \\(\\mu\\) and \\(\\sigma\\). Index Shape Sided Stability Formula Meaning Capability Indices (How \\(\\textit{could}\\) it perform, if stable?) \\(C_{p}\\) Centered 2-sided Stable \\(\\frac{E_{upper} - E_{lower}}{6 \\sigma_{short}}\\) How many times wider is the expected range than the observed range, assuming it is stable? \\(C_{pk}\\) Uncentered 1-sided Stable \\(\\frac{\\mid E_{limit} - \\mu \\mid}{3 \\sigma_{short}}\\) How many times wider is the expected vs.Â observed range for the left/right side, assuming it is stable? Process Performance Indices (How is it performing, stable or not?) \\(P_{p}\\) Centered 2-sided Unstable \\(\\frac{E_{upper} - E_{lower}}{6 \\sigma_{total}}\\) How many times wider is the expected vs.Â observed range, stable or not? \\(P_{pk}\\) Uncentered 1-sided Unstable \\(\\frac{\\mid E_{limit} - \\mu \\mid}{3 \\sigma_{total}}\\) How many times wider is the expected vs.Â observed range for the left/right side, stable or not? 17.2 Index Functions Letâ€™s do ourselves a favor and write up some simple functions for these. # Capability Index (for centered, normal data) cp = function(sigma_s, upper, lower){ abs(upper - lower) / (6*sigma_s) } # Process Performance Index (for centered, normal data) pp = function(sigma_t, upper, lower){ abs(upper - lower) / (6*sigma_t) } # Capability Index (for skewed, uncentered data) cpk = function(mu, sigma_s, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_s) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_s) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } # Process Performance Index (for skewed, uncentered data) ppk = function(mu, sigma_t, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_t) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_t) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } How might we use these indices to describe our process data? For example, recall that our onsen operator needs to be sure that their hot springs water is consistently falling into the temperature bins for Extra Hot Springs, which start at 42 degrees Celsius (and go as high as 80). Letâ€™s use those as our specification limits (pretty easy-going limits, I might add). Letâ€™s start by calculating our quantities of interest. stat = water %&gt;% group_by(time) %&gt;% summarize( xbar = mean(temp), # Get within group mean sd = sd(temp), # Get within group sigma n_w = n() # Get within subgroup size ) %&gt;% summarize( xbbar = mean(xbar), # Get grand mean sigma_s = sqrt(mean(sd^2)), # Get sigma_short sigma_t = sd(water$temp), # get sigma_total n = sum(n_w), # Get total observations n_w = unique(n_w), # get size of subgroups k = n()) # Get number of subgroups k # Check it! stat ## # A tibble: 1 Ã— 6 ## xbbar sigma_s sigma_t n n_w k ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 44.8 1.99 1.99 160 20 8 17.2.1 Capacity Index \\(C_{p}\\) Our \\(C_{p}\\) Capacity index says, assuming that the distribution is centered and stable, how many times wider are our limits than our approximate observed distribution (\\(6 \\sigma_{short}\\))? mycp = cp(sigma_s = stat$sigma_s, lower = 42, upper = 80) # Check it! mycp ## [1] 3.18871 Great! This says, our observed variation is many times (3.1887098 times) narrower than the expected specification limits. 17.2.2 Process Performance Index \\(P_{p}\\) Our \\(P_{p}\\) Process Performance Index asks, even if the distribution is not stable (meaning it varies not just due to common causes), how many times wider are our specification limits than our approximate observed distribution (\\(6 \\sigma_{total}\\))? mypp = pp(sigma_t = stat$sigma_t, lower = 42, upper = 80) mypp ## [1] 3.183378 Much like before, the specification limit range remains quite bigger than the observed distribution. 17.2.3 Capacity Index \\(C_{pk}\\) Our \\(C_{pk}\\) Capacity index says, assuming the distribution is pretty stable across subgroups, how many times wider is (a) the distance from the tail of interest to the mean than (b) our approximate observed tail (3 sigma)? This always looks at the shorter tail. mycpk = cpk(sigma_s = stat$sigma_s, mu = stat$xbbar, lower = 42, upper = 80) mycpk ## [1] 0.4783065 If we only care about one of the tails, eg. the lower specification limit of 42, which is much closer than the upper limit of 80, we can just write the lower limit only. cpk(sigma_s = stat$sigma_s, mu = stat$xbbar, lower = 42) ## [1] 0.4783065 This says, our observed variation is much wider than the lower specification limit, since \\(C_{pk}\\) is far from 1, which which show equality. 17.2.4 Process Performance Index \\(P_{pk}\\) Our \\(P_{pk}\\) process performance index says, even if the distribution is neither stable nor centered, how much wider is the observed variation \\(3 \\sigma_{total}\\) than the distance from the tail of interest to the mean? We use \\(\\sigma_{total}\\) here to account for instability (considerable variation between subgroups) and one-tailed testing to account for the uncentered distribution. myppk = ppk(sigma_t = stat$sigma_t, mu = stat$xbbar, lower = 42, upper = 80) myppk ## [1] 0.4775067 17.2.5 Equality A final reason why these quantities are neat is that these 4 indices are related; if you know 3 of them, we can always calculate the 4th! See the identity below: \\[P_{p} \\times C_{pk} = P_{pk} \\times C_{p}\\] # whaaaaaat? They&#39;re equal!!!! mypp * mycpk == myppk * mycp ## [1] TRUE Learning Check 1 Question Letâ€™s apply this to some tasty examples! A manufacturer of granola bars is aiming for a weight of 2 ounces (oz), plus or minus 0.5 oz. Suppose the standard deviation of our granola bar machine is 0.02 oz, and the mean weight is 2.01 oz. Whatâ€™s the process capability index? (I.e. How many times greater is the expected variation than the observed variation?) [View Answer!] lower = 2 + 0.05 upper = 2 - 0.05 sigma = 0.02 mu = 2.01 cp(sigma = 0.02, upper = 2.05, lower = 1.95) ## [1] 0.8333333 cpk(mu = 2.01, sigma = 0.02, lower = 1.95, upper = 2.05) ## [1] 0.6666667 17.3 Confidence Intervals Any statistic is really just 1 of the many possible values of statistics you could have gotten from your sample, had you taken just a slightly different random sample. So, when we calculate our indices, we should be prepared that our indices might vary due to chance (sampling error), so we should build in confidence intervals. This helps us benchmark how trustworthy any given index is. 17.3.1 Confidence Intervals show us Sampling Distributions Letâ€™s quickly go over what confidence intervals are trying to show us! Suppose we take a statistic like the mean \\(\\mu\\) to describe our vector temp. water %&gt;% summarize(mean = mean(temp)) ## # A tibble: 1 Ã— 1 ## mean ## &lt;dbl&gt; ## 1 44.8 We might have gotten a slightly different statistic had we had a slightly different sample. We can approximate what slightly different sample might look like by using bootstrapped resamples. This means, randomly sampling a bunch of observations from our dataframe water, sometimes taking the same observation multiple times, sometimes leaving out some observations by chance. We can use the sample(x, size = ..., replace = TRUE) function to take a bootstrapped sample. water %&gt;% # Grab n() randomly sampled temperatures, with replacement, # we&#39;ll call those &#39;boot&#39;, since they were &#39;bootstrapped&#39; summarize(boot = sample(temp, size = n(), replace = TRUE)) %&gt;% # and take the mean! summarize(mean = mean(boot)) ## # A tibble: 1 Ã— 1 ## mean ## &lt;dbl&gt; ## 1 44.9 Our bootstrapped mean is very, very close to the original mean - just slightly off due to sampling error! Bootstrapping is a very powerful tool, as it lets us circumvent many long formulas, as long as you take enough samples. Letâ€™s take 1000 resamples below: # Get a vector of ids from 1 to 1000 myboot = tibble(rep = 1:1000) %&gt;% # For each repetition, group_by(rep) %&gt;% # Get a random bootstrapped sample of temperatures summarize(boot = water$temp %&gt;% sample(size = n(), replace = TRUE)) %&gt;% # For that repetition, group_by(rep) %&gt;% # Calculate the mean of the bootstrapped samples! summarize(mean = mean(boot)) # Let&#39;s view them! myboot$mean %&gt;% hist() We can see above the latent distribution of 1000 statistics we could have gotten due to random sampling error. This is called a sampling distribution. Whenever we make confidence intervals, we are always drawing from a sampling distribution. 17.3.2 Bootstrapped or Theoretical Sampling Distributions? The question is, are you relying on a bootstrapped sampling distribution or a theoretical sampling distribution? If we assume a perfectly normal distribution, then weâ€™re relying on a theoretical sampling distribution, and we need formulas to calculate our confidence intervals. This is a big assumption! If we are comfortable with computing 1000 or more replicates, then we can bootstrap those confidence intervals instead, gaining accuracy at the expense of computational power. Letâ€™s learn how to make confidence intervals for our indices from (1) a theoretical sampling distribution, and then weâ€™ll learn to make them from (2) a bootstrapped sampling distribution. 17.3.3 Confidence Intervals with Theoretical Sampling Distributions Suppose our lower and upper specification limits - the expectations of the market and/or regulators - are that our onsenâ€™s temperature should be between 42 and 50 degrees Celsius if we advertise ourselves as an Extra Hot Onsen. For any index, youâ€™ll need to get the ingredients needed to calculate the index and to calculate its standard error (the standard deviation of the sampling distribution youâ€™re trying to approximate). So, letâ€™s first get our ingredientsâ€¦ stat = water %&gt;% group_by(time) %&gt;% summarize(xbar = mean(temp), s = sd(temp), n_w = n()) %&gt;% summarize( xbbar = mean(xbar), # grand mean x-double-bar sigma_s = sqrt(mean(s^2)), # sigma_short sigma_t = water$temp %&gt;% sd(), # sigma_total! n = sum(n_w), # or just n = n() # Total sample size n_w = unique(n_w), k = time %&gt;% unique() %&gt;% length()) # Get total subgroups # Check it! stat ## # A tibble: 1 Ã— 6 ## xbbar sigma_s sigma_t n n_w k ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 44.8 1.99 1.99 160 20 8 Now, letâ€™s calculate our Capability Index \\(C_{p}\\), which assumes a process centered between the upper and lower specification limits and a stable process. # Capability Index (for centered, normal data) cp = function(sigma_s, upper, lower){ abs(upper - lower) / (6*sigma_s) } stat %&gt;% summarize( limit_lower = 42, limit_upper = 50, # index estimate = cp(sigma_s = sigma_s, lower = limit_lower, upper = limit_upper)) ## # A tibble: 1 Ã— 3 ## limit_lower limit_upper estimate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 42 50 0.671 That was surprisingly painless! Now, letâ€™s estimate the two-sided, 95% confidence interval of our sampling distribution for the statistic cp, assuming that this sampling distribution has a normal shape. Weâ€™re getting the interval that spans 95%, so itâ€™s got to start at 2.5% and end at 97.5%, covering the 95% most frequently occurring statistics in the sampling distribution. bands = stat %&gt;% summarize( limit_lower = 42, limit_upper = 50, # index estimate = cp(sigma_s = sigma_s, lower = limit_lower, upper = limit_upper), # Get our extra quantities of interest v_short = k*(n_w - 1), # get degrees of freedom # Get standard error for estimate se = estimate * sqrt(1 / (2*v_short)), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Now if z, the 97.5th percentile, # is 1.96 standard deviations from the mean in the normal, # Then so too is the 2.5th percentile in the normal. # Give me 1.96 standard deviations above cp # in the sampling distribution of cp! # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) bands ## # A tibble: 1 Ã— 8 ## limit_lower limit_upper estimate v_short se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 42 50 0.671 152 0.0385 1.96 0.596 0.747 17.3.4 Visualizing Confidence Intervals Were we to visualize this, it might look likeâ€¦ bands %&gt;% ggplot(mapping = aes(x = &quot;Cp Index&quot;, y = estimate, ymin = lower, ymax = upper)) + # Get draw us some benchmarks to make our chart meaningful geom_hline(yintercept = c(0,1,2), color = c(&quot;grey&quot;, &quot;black&quot;, &quot;grey&quot;)) + # Draw the points! geom_point() + geom_linerange() + # Add theming theme_classic(base_size = 14) + coord_flip() + labs(y = &quot;Index Value&quot;, x = NULL) Itâ€™s not the most exciting plot, but it does show very clearly that the value of \\(C_{p}\\) and its 95% confidence interval are nowhere even close to 1.0, the key threshold. This means we can say with 95% confidence that the true value of \\(C_{p}\\) is less than 1. 17.3.5 Bootstrapping \\(C_{p}\\) How might we estimate this using the bootstrap? Well, we couldâ€¦ myboot = tibble(rep = 1:1000) %&gt;% # For each rep, group_by(rep) %&gt;% # Give me the data.frame water, summarize(water) %&gt;% # And give me a random sample of observations sample_n(size = n(), replace = TRUE) myboot %&gt;% glimpse() ## Rows: 160,000 ## Columns: 6 ## Groups: rep [1,000] ## $ rep &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â€¦ ## $ id &lt;dbl&gt; 60, 83, 122, 150, 147, 135, 104, 36, 43, 97, 77, 131, 110, 128,â€¦ ## $ time &lt;dbl&gt; 5, 9, 13, 15, 15, 13, 11, 3, 5, 9, 7, 13, 11, 13, 1, 5, 3, 11, â€¦ ## $ temp &lt;dbl&gt; 44.1, 45.7, 45.3, 41.5, 47.4, 44.0, 43.1, 43.2, 44.6, 44.4, 44.â€¦ ## $ ph &lt;dbl&gt; 4.9, 4.4, 4.9, 4.6, 5.6, 5.0, 5.8, 5.2, 5.3, 5.9, 5.1, 6.0, 5.6â€¦ ## $ sulfur &lt;dbl&gt; 0.4, 0.0, 0.8, 1.2, 0.9, 0.7, 0.9, 0.2, 0.0, 0.3, 0.1, -0.2, 4.â€¦ This produces a very, very big data.frame! Letâ€™s now, for each rep, calculate our statistics from before! mybootstat = myboot %&gt;% # For each rep, and each subgroup... group_by(rep, time) %&gt;% summarize( xbar = mean(temp), # Get within group mean sigma_w = sd(temp), # Get within group sigma n_w = n() # Get within subgroup size ) %&gt;% # For each rep... group_by(rep) %&gt;% summarize( limit_upper = 42, limit_lower = 50, xbbar = mean(xbar), # Get grand mean sigma_s = sqrt(mean(sigma_w^2)), # Get sigma_short n = sum(n_w), # Get total observations n_w = unique(n_w)[1], k = n(), # Get number of subgroups k limit_lower = 42, limit_upper = 50, estimate = cp(sigma_s, upper = limit_upper, lower = limit_lower)) So cool! Weâ€™ve now generated the sampling distributions for xbbar, sigma_s, and cp! We can even visualize the raw distributions now! Look at those wicked cool bootstrapped sampling distributions!!! g = mybootstat %&gt;% # For each rep, group_by(rep) %&gt;% # Stack our values atop each other... summarize( # Get these names, and repeat them each n times type = c(&quot;xbbar&quot;, &quot;sigma_s&quot;, &quot;cp&quot;) %&gt;% rep(each = n()), value = c(xbbar, sigma_s, estimate)) %&gt;% ggplot(mapping = aes(x = value, fill = type)) + geom_histogram() + facet_wrap(~type, scales = &quot;free&quot;) + guides(fill = &quot;none&quot;) # View it! g So last, letâ€™s take our boostrapped \\(C_{p}\\) statistics in mybootstat$cp and estimate a confidence interval and standard error for this sampling distribution. Because we have the entire distribution, we can extract values at specific percentiles in the distribution using quantiles(), rather than qnorm() or such theoretical distributions. # We&#39;ll save it as &#39;myqi&#39;, for quantities of interest myqi = mybootstat %&gt;% summarize( # Let&#39;s grab the original cp statistic cp = bands$estimate, # Get the lower and upper 95% confidence intervals lower = quantile(estimate, probs = 0.025), upper = quantile(estimate, probs = 0.975), # We can even get the standard error, # which is *literally* the standard deviation of this sampling distribution se = sd(estimate)) # Check it out! myqi ## # A tibble: 1 Ã— 4 ## cp lower upper se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.671 0.617 0.779 0.0434 This suggests a wider confidence interval that our normal distribution assumes by default - interesting! We can perform bootstrapping to estimate confidence intervals for any statistic, including \\(C_{p}\\), \\(C_{pk}\\), \\(P_{p}\\), or \\(P_{pk}\\). The only limit is your computational power! Wheee! Note: Whenever you bootstrap, itâ€™s important that you clear out your R environment to keep things running quickly, because you tend to accumulate a lot of really big data.frames. You can use remove() to do this. remove(myboot, mybootstat) 17.4 CIs for any Index! Letâ€™s practice calculating confidence intervals (CIs) for each of these indices. 17.4.1 CIs for \\(P_p\\) Now that we have our ingredients, letâ€™s get our index and its confidence intervals! # Capability Index (for centered, normal data) cp = function(sigma_s, upper, lower){ abs(upper - lower) / (6*sigma_s) } stat %&gt;% summarize( # index estimate = cp(sigma_s = sigma_s, lower = 42, upper = 50), # Get our extra quantities of interest v_short = k*(n_w - 1), # get degrees of freedom # Get standard error for cpk se = estimate * sqrt(1 / (2*v_short)), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 Ã— 6 ## estimate v_short se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.671 152 0.0385 1.96 0.596 0.747 17.4.2 CIs for \\(C_{pk}\\) Write the function and generate the confidence interval for \\(P_{pk}\\)! # Capability Index (for skewed, uncentered data) cpk = function(mu, sigma_s, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_s) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_s) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } stat %&gt;% summarize( # index estimate = cpk(mu = xbbar, sigma_s = sigma_s, lower = 42, upper = 50), # Get our extra quantities of interest v_short = k*(n_w - 1), # get degrees of freedom # Get standard error for ppk se = estimate * sqrt( 1 / (2*v_short) + 1 / (9 * n * estimate^2) ), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 Ã— 6 ## estimate v_short se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.478 152 0.0380 1.96 0.404 0.553 17.4.3 CIs for \\(P_p\\) Now that we have our ingredients, letâ€™s get our index and its confidence intervals! # Suppose we&#39;re looking at the entire process! # Process Performance Index (for centered, normal data) pp = function(sigma_t, upper, lower){ abs(upper - lower) / (6*sigma_t) } stat %&gt;% summarize( # index estimate = pp(sigma_t = sigma_t, lower = 42, upper = 50), # Get our extra quantities of interest v_total = n_w*k - 1, # get degrees of freedom # Get standard error for cpk se = estimate * sqrt(1 / (2*v_total)), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 Ã— 6 ## estimate v_total se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.670 159 0.0376 1.96 0.597 0.744 17.4.4 CIs for \\(P_{pk}\\) Write the function and generate the confidence interval for \\(P_{pk}\\)! # Process Performance Index (for skewed, uncentered data) ppk = function(mu, sigma_t, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_t) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_t) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } stat %&gt;% summarize( # index estimate = ppk(mu = xbbar, sigma_t = sigma_t, lower = 42, upper = 50), # Get our extra quantities of interest v_total = n_w*k - 1, # get degrees of freedom # Get standard error for cpk se = estimate * sqrt( 1 / (2*v_total) + 1 / (9 * n * estimate^2) ), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 Ã— 6 ## estimate v_total se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.478 159 0.0376 1.96 0.404 0.551 Conclusion Alright! You are now a confidence interval wizard! Go forth and make confidence intervals! "],["indices-and-confidence-intervals-for-statistical-process-control-in-python.html", "18 Indices and Confidence Intervals for Statistical Process Control in Python Getting Started 18.1 Process Capability vs.Â Stability 18.2 Confidence Intervals (Normal Approximation) 18.3 Bootstrapping Cp (Example) Conclusion", " 18 Indices and Confidence Intervals for Statistical Process Control in Python Figure 2.1: Bootstrapping Sampling Distributions for Statistics!! This workshop extends our toolkit developed earlier, discussing Process Capability and Stability Indices, and introducing means to calculate confidence intervals for these indices using Python. Weâ€™ll calculate Cp, Cpk, Pp, and Ppk, then construct normal-approximation confidence intervals and a simple bootstrap, paralleling the R chapter. Getting Started Packages Weâ€™ll be using pandas for data manipulation, plotnine for visualization, scipy for statistical functions, and custom functions from the functions_distributions module. # Remember to install these packages using a terminal, if you haven&#39;t already! !pip install pandas plotnine scipy # Load our packages import pandas as pd from plotnine import * import numpy as np from scipy import stats Custom Functions This workshop uses custom functions from the functions_distributions.py module. To use these functions, you need to acquire them from the repository at github.com/timothyfraser/sigma/tree/main/functions. Add the functions directory to your Python path import sys import os # Add the functions directory to Python path sys.path.append(&#39;functions&#39;) # or path to wherever you placed the functions folder Once you have the functions available, you can import them: from functions_distributions import * Our Data Weâ€™ll be continuing to analyze our quality control data from a local hot springs inn (onsen) in sunny Kagoshima Prefecture, Japan. Every month, for 15 months, you systematically took 20 random samples of hot spring water and recorded its temperature, pH, and sulfur levels. How might you determine if this onsen is at risk of slipping out of one sector of the market (eg. Extra Hot!) and into another (just normal Hot Springs?). Letâ€™s read in our data from workshops/onsen.csv! # Let&#39;s import our samples of bathwater over time! water = pd.read_csv(&#39;workshops/onsen.csv&#39;) # Take a peek! print(water.head(3)) ## id time temp ph sulfur ## 0 1 1 43.2 5.1 0.0 ## 1 2 1 45.3 4.8 0.4 ## 2 3 1 45.5 6.2 0.9 print(f&quot;Dataset shape: {water.shape}&quot;) ## Dataset shape: (160, 5) print(f&quot;Columns: {list(water.columns)}&quot;) ## Columns: [&#39;id&#39;, &#39;time&#39;, &#39;temp&#39;, &#39;ph&#39;, &#39;sulfur&#39;] Our dataset contains: id: unique identifier for each sample of onsen water. time: categorical variable describing date of sample (month 1, month 3, â€¦ month 15). temp: temperature of the hot spring water in Celsius. ph: pH level of the hot spring water. sulfur: sulfur content in mg per kg of water. 18.0.1 Process Overview Letâ€™s get a visual overview of our temperature data across time to understand the process: # Create a process overview plot g = (ggplot(water, aes(x=&#39;time&#39;, y=&#39;temp&#39;)) + geom_point(alpha=0.6, size=2) + geom_hline(yintercept=[42, 50], color=[&#39;red&#39;, &#39;red&#39;], linetype=&#39;dashed&#39;) + theme_classic(base_size=14) + labs(x=&#39;Time (Month)&#39;, y=&#39;Temperature (Â°C)&#39;, title=&#39;Hot Spring Temperature Over Time&#39;, subtitle=&#39;Red dashed lines show specification limits (42-50Â°C)&#39;) + theme(plot_title=element_text(hjust=0.5), plot_subtitle=element_text(hjust=0.5))) # Save the plot g.save(&#39;images/06_process_overview.png&#39;, width=10, height=6, dpi=100) This plot shows our temperature measurements over time, with the specification limits for â€œExtra Hot Springsâ€ (42-50Â°C) marked as red dashed lines. We can see the process variation and how it relates to our target specifications. 18.1 Process Capability vs.Â Stability 18.1.1 Definitions Production processes can be categorized in terms of Capability (does it meet required specifications?) and Stability (is production consistent and predictable?) Both are vital. A capable process delivers goods that can actually perform their function, like a 20-foot ladder that is actually 20 feet! A stable process delivers products with consistent and predictable traits (regardless of whether those traits are good). We need to maximize both process capability and stability to make an effective process, be it in manufacturing, health care, local businesses, or social life! Depending on the shape and stability of our data, we should choose one of the 4 statistics to evaluate our data. 18.1.2 Table of Indices These statistics rely on some combination of (1) the mean \\(\\mu\\), (2) the standard deviation \\(\\sigma\\), and (3) the upper and lower â€œspecification limitsâ€; the specification limits are our expected values \\(E_{upper}\\) and \\(E_{lower}\\), as compared to our actual observed values, summarized by \\(\\mu\\) and \\(\\sigma\\). Index Shape Sided Stability Formula Meaning \\(C_{p}\\) Centered 2-sided Stable \\(\\frac{E_{upper} - E_{lower}}{6 \\sigma_{short}}\\) How many times wider is the expected range than the observed range, assuming it is stable? \\(C_{pk}\\) Uncentered 1-sided Stable \\(\\frac{\\mid E_{limit} - \\mu \\mid}{3 \\sigma_{short}}\\) How many times wider is the expected vs.Â observed range for the left/right side, assuming it is stable? \\(P_{p}\\) Centered 2-sided Unstable \\(\\frac{E_{upper} - E_{lower}}{6 \\sigma_{total}}\\) How many times wider is the expected vs.Â observed range, stable or not? \\(P_{pk}\\) Uncentered 1-sided Unstable \\(\\frac{\\mid E_{limit} - \\mu \\mid}{3 \\sigma_{total}}\\) How many times wider is the expected vs.Â observed range for the left/right side, stable or not? Capability Indices (How could it perform, if stable?): \\(C_p\\), \\(C_{pk}\\) Process Performance Indices (How is it performing, stable or not?): \\(P_p\\), \\(P_{pk}\\) 18.1.3 Index Functions Letâ€™s do ourselves a favor and write up some simple functions for these. def cp(sigma_s, upper, lower): &quot;&quot;&quot;Capability index for centered, stable processes&quot;&quot;&quot; return abs(upper - lower) / (6*sigma_s) def pp(sigma_t, upper, lower): &quot;&quot;&quot;Performance index for centered, unstable processes&quot;&quot;&quot; return abs(upper - lower) / (6*sigma_t) def cpk(mu, sigma_s, lower=None, upper=None): &quot;&quot;&quot;Capability index for uncentered, stable processes&quot;&quot;&quot; a = None; b = None if lower is not None: a = abs(mu - lower) / (3*sigma_s) if upper is not None: b = abs(upper - mu) / (3*sigma_s) if (lower is not None) and (upper is not None): return min(a,b) return a if upper is None else b def ppk(mu, sigma_t, lower=None, upper=None): &quot;&quot;&quot;Performance index for uncentered, unstable processes&quot;&quot;&quot; a = None; b = None if lower is not None: a = abs(mu - lower) / (3*sigma_t) if upper is not None: b = abs(upper - mu) / (3*sigma_t) if (lower is not None) and (upper is not None): return min(a,b) return a if upper is None else b 18.1.4 Ingredients Now we need to calculate the key statistics that feed into our capability indices. Weâ€™ll calculate both within-subgroup statistics (for capability indices) and total statistics (for performance indices). stat_s = (water.groupby(&#39;time&#39;).apply(lambda d: pd.Series({ &#39;xbar&#39;: d[&#39;temp&#39;].mean(), &#39;s&#39;: d[&#39;temp&#39;].std(), &#39;n_w&#39;: len(d) })).reset_index()) stat = pd.DataFrame({ &#39;xbbar&#39;: [stat_s[&#39;xbar&#39;].mean()], &#39;sigma_s&#39;: [(stat_s[&#39;s&#39;]**2).mean()**0.5], &#39;sigma_t&#39;: [water[&#39;temp&#39;].std()], &#39;n&#39;: [stat_s[&#39;n_w&#39;].sum()], &#39;n_w&#39;: [stat_s[&#39;n_w&#39;].iloc[0]], &#39;k&#39;: [len(stat_s)] }) stat ## xbbar sigma_s sigma_t n n_w k ## 0 44.85 1.986174 1.989501 160.0 20.0 8 These statistics give us: xbbar: The grand mean across all subgroups sigma_s: The pooled within-subgroup standard deviation (for capability indices) sigma_t: The total process standard deviation (for performance indices) n: Total number of observations n_w: Number of observations per subgroup k: Number of subgroups 18.1.5 Cp and Pp Now letâ€™s calculate the capability and performance indices. Weâ€™ll use specification limits for â€œExtra Hot Springsâ€ (42-50Â°C) as our target range. limit_lower = 42; limit_upper = 50 estimate_cp = cp(stat[&#39;sigma_s&#39;][0], upper=limit_upper, lower=limit_lower) estimate_pp = pp(stat[&#39;sigma_t&#39;][0], upper=limit_upper, lower=limit_lower) print(f&quot;Cp (capability): {estimate_cp:.3f}&quot;) ## Cp (capability): 0.671 print(f&quot;Pp (performance): {estimate_pp:.3f}&quot;) ## Pp (performance): 0.670 18.1.6 Cpk and Ppk Now letâ€™s calculate the uncentered indices that account for process centering: estimate_cpk = cpk(mu=stat[&#39;xbbar&#39;][0], sigma_s=stat[&#39;sigma_s&#39;][0], lower=limit_lower, upper=limit_upper) estimate_ppk = ppk(mu=stat[&#39;xbbar&#39;][0], sigma_t=stat[&#39;sigma_t&#39;][0], lower=limit_lower, upper=limit_upper) print(f&quot;Cpk (capability, uncentered): {estimate_cpk:.3f}&quot;) ## Cpk (capability, uncentered): 0.478 print(f&quot;Ppk (performance, uncentered): {estimate_ppk:.3f}&quot;) ## Ppk (performance, uncentered): 0.478 18.1.7 Equality Thereâ€™s an interesting mathematical relationship between these indices: equality_check = (estimate_pp * estimate_cpk) == (estimate_ppk * estimate_cp) print(f&quot;Pp Ã— Cpk = Ppk Ã— Cp: {equality_check}&quot;) ## Pp Ã— Cpk = Ppk Ã— Cp: True print(f&quot;Pp Ã— Cpk = {estimate_pp * estimate_cpk:.6f}&quot;) ## Pp Ã— Cpk = 0.320554 print(f&quot;Ppk Ã— Cp = {estimate_ppk * estimate_cp:.6f}&quot;) ## Ppk Ã— Cp = 0.320554 18.2 Confidence Intervals (Normal Approximation) Now letâ€™s construct confidence intervals for our capability indices using the normal approximation method. This gives us a range of plausible values for the true population parameters. 18.2.1 Cp Confidence Interval import math v_short = stat[&#39;k&#39;][0]*(stat[&#39;n_w&#39;][0] - 1) se_cp = estimate_cp * math.sqrt(1 / (2*v_short)) z = 1.959963984540054 # 95% confidence level ci_cp = (estimate_cp - z*se_cp, estimate_cp + z*se_cp) print(f&quot;Cp estimate: {estimate_cp:.3f}&quot;) ## Cp estimate: 0.671 print(f&quot;Standard error: {se_cp:.3f}&quot;) ## Standard error: 0.039 print(f&quot;95% Confidence interval: ({ci_cp[0]:.3f}, {ci_cp[1]:.3f})&quot;) ## 95% Confidence interval: (0.596, 0.747) 18.2.2 Cpk Confidence Interval se_cpk = estimate_cpk * math.sqrt(1 / (2*v_short) + 1 / (9*stat[&#39;n&#39;][0]*(estimate_cpk**2))) ci_cpk = (estimate_cpk - z*se_cpk, estimate_cpk + z*se_cpk) print(f&quot;Cpk estimate: {estimate_cpk:.3f}&quot;) ## Cpk estimate: 0.478 print(f&quot;Standard error: {se_cpk:.3f}&quot;) ## Standard error: 0.038 print(f&quot;95% Confidence interval: ({ci_cpk[0]:.3f}, {ci_cpk[1]:.3f})&quot;) ## 95% Confidence interval: (0.404, 0.553) 18.2.3 Pp and Ppk Confidence Intervals v_total = stat[&#39;n_w&#39;][0]*stat[&#39;k&#39;][0] - 1 se_pp = estimate_pp * math.sqrt(1 / (2*v_total)) ci_pp = (estimate_pp - z*se_pp, estimate_pp + z*se_pp) print(f&quot;Pp estimate: {estimate_pp:.3f}&quot;) ## Pp estimate: 0.670 print(f&quot;Standard error: {se_pp:.3f}&quot;) ## Standard error: 0.038 print(f&quot;95% Confidence interval: ({ci_pp[0]:.3f}, {ci_pp[1]:.3f})&quot;) ## 95% Confidence interval: (0.597, 0.744) se_ppk = estimate_ppk * math.sqrt(1 / (2*v_total) + 1 / (9*stat[&#39;n&#39;][0]*(estimate_ppk**2))) ci_ppk = (estimate_ppk - z*se_ppk, estimate_ppk + z*se_ppk) print(f&quot;\\nPpk estimate: {estimate_ppk:.3f}&quot;) ## ## Ppk estimate: 0.478 print(f&quot;Standard error: {se_ppk:.3f}&quot;) ## Standard error: 0.038 print(f&quot;95% Confidence interval: ({ci_ppk[0]:.3f}, {ci_ppk[1]:.3f})&quot;) ## 95% Confidence interval: (0.404, 0.551) 18.2.4 Visualizing Confidence Intervals Letâ€™s create a visualization to show our Cp estimate with its confidence interval and important benchmark lines: # Create a DataFrame for plotting bands_data = pd.DataFrame({ &#39;index&#39;: [&#39;Cp Index&#39;], &#39;estimate&#39;: [estimate_cp], &#39;lower&#39;: [ci_cp[0]], &#39;upper&#39;: [ci_cp[1]] }) # Create the plot g = (ggplot(bands_data, aes(x=&#39;index&#39;, y=&#39;estimate&#39;, ymin=&#39;lower&#39;, ymax=&#39;upper&#39;)) + geom_hline(yintercept=[0, 1, 2], color=[&#39;grey&#39;, &#39;black&#39;, &#39;grey&#39;]) + geom_point(size=3) + geom_linerange(size=1) + theme_classic(base_size=14) + coord_flip() + labs(y=&#39;Index Value&#39;, x=None) + theme(axis_text_y=element_blank(), axis_ticks_y=element_blank())) # Save the plot g.save(&#39;images/06_confidence_interval_plot.png&#39;, width=8, height=4, dpi=100) Itâ€™s not the most exciting plot, but it does show very clearly that the value of \\(C_{p}\\) and its 95% confidence interval are nowhere even close to 1.0, the key threshold. This means we can say with 95% confidence that the true value of \\(C_{p}\\) is less than 1. 18.3 Bootstrapping Cp (Example) The normal approximation method assumes certain distributional properties. As an alternative, we can use bootstrapping to estimate confidence intervals by resampling our data. This method is more robust to distributional assumptions. import numpy as np reps = 500 def boot_cp(seed=1): np.random.seed(seed) vals = [] for r in range(reps): sample = water.sample(n=len(water), replace=True) stat_s_b = (sample.groupby(&#39;time&#39;).apply(lambda d: pd.Series({&#39;xbar&#39;: d[&#39;temp&#39;].mean(), &#39;s&#39;: d[&#39;temp&#39;].std(), &#39;n_w&#39;: len(d)})).reset_index()) sigma_s_b = (stat_s_b[&#39;s&#39;]**2).mean()**0.5 vals.append(cp(sigma_s_b, upper=limit_upper, lower=limit_lower)) s = pd.Series(vals) return pd.DataFrame({&#39;cp&#39;: [estimate_cp], &#39;lower&#39;: [s.quantile(0.025)], &#39;upper&#39;: [s.quantile(0.975)], &#39;se&#39;: [s.std()]}) bootstrap_results = boot_cp() # Print out the results boot_cp() ## cp lower upper se ## 0 0.671307 0.620466 0.785899 0.04336 18.3.1 Bootstrap Sampling Distributions Visualization So cool! Weâ€™ve now generated the sampling distributions for our bootstrap statistics! Letâ€™s visualize the raw distributions to see those wicked cool bootstrapped sampling distributions: # Create a more comprehensive bootstrap function that returns all statistics def boot_comprehensive(seed=1): np.random.seed(seed) results = [] for r in range(reps): sample = water.sample(n=len(water), replace=True) stat_s_b = (sample.groupby(&#39;time&#39;).apply(lambda d: pd.Series({ &#39;xbar&#39;: d[&#39;temp&#39;].mean(), &#39;s&#39;: d[&#39;temp&#39;].std(), &#39;n_w&#39;: len(d) })).reset_index()) xbbar_b = stat_s_b[&#39;xbar&#39;].mean() sigma_s_b = (stat_s_b[&#39;s&#39;]**2).mean()**0.5 cp_b = cp(sigma_s_b, upper=limit_upper, lower=limit_lower) results.append({ &#39;rep&#39;: r, &#39;xbbar&#39;: xbbar_b, &#39;sigma_s&#39;: sigma_s_b, &#39;cp&#39;: cp_b }) return pd.DataFrame(results) # Generate bootstrap data bootstrap_data = boot_comprehensive() # Reshape for plotting plot_data = [] for _, row in bootstrap_data.iterrows(): plot_data.extend([ {&#39;type&#39;: &#39;xbbar&#39;, &#39;value&#39;: row[&#39;xbbar&#39;]}, {&#39;type&#39;: &#39;sigma_s&#39;, &#39;value&#39;: row[&#39;sigma_s&#39;]}, {&#39;type&#39;: &#39;cp&#39;, &#39;value&#39;: row[&#39;cp&#39;]} ]) plot_df = pd.DataFrame(plot_data) # Create the plot g = (ggplot(plot_df, aes(x=&#39;value&#39;, fill=&#39;type&#39;)) + geom_histogram(bins=30, alpha=0.7) + facet_wrap(&#39;~type&#39;, scales=&#39;free&#39;) + theme_classic(base_size=12) + labs(x=&#39;Value&#39;, y=&#39;Frequency&#39;) + theme(legend_position=&#39;none&#39;)) # Save the plot g.save(&#39;images/06_bootstrap_distributions.png&#39;, width=10, height=6, dpi=100) 18.3.2 Bootstrap Confidence Intervals Now letâ€™s take our bootstrapped \\(C_{p}\\) statistics and estimate a confidence interval and standard error for this sampling distribution. Because we have the entire distribution, we can extract values at specific percentiles in the distribution using quantile(), rather than theoretical distributions. print(&quot;Bootstrap Results for Cp:&quot;) ## Bootstrap Results for Cp: print(f&quot;Original estimate: {bootstrap_results[&#39;cp&#39;][0]:.3f}&quot;) ## Original estimate: 0.671 print(f&quot;Bootstrap 95% CI: ({bootstrap_results[&#39;lower&#39;][0]:.3f}, {bootstrap_results[&#39;upper&#39;][0]:.3f})&quot;) ## Bootstrap 95% CI: (0.620, 0.786) print(f&quot;Bootstrap standard error: {bootstrap_results[&#39;se&#39;][0]:.3f}&quot;) ## Bootstrap standard error: 0.043 This suggests a wider confidence interval than our normal distribution assumes by default - interesting! Conclusion Youâ€™ve successfully computed capability and performance indices (Cp, Cpk, Pp, Ppk) and their confidence intervals using both normal approximation and bootstrap methods in Python. These tools help us assess process capability and performance, providing insights into whether our processes meet specifications and how consistently they perform. "],["useful-life-distributions-exponential.html", "19 Useful Life Distributions (Exponential) 19.1 Getting Started 19.2 Quantities of Interest Learning Check 1 Learning Check 2 19.3 Quantities of Interest (continued) Learning Check 3 19.4 System Failure with Independent Failure Rates 19.5 Conclusion", " 19 Useful Life Distributions (Exponential) In this workshop, weâ€™re going to learn some R functions for working with common life distributions, namely the Exponential distributions. 19.1 Getting Started 19.1.1 Load Packages Letâ€™s start by loading the tidyverse package, which will let us mutate(), filter(), and summarize() data quickly. Weâ€™ll also load mosaicCalc, for taking derivatives and integrals (eg. D() and antiD()). # Load packages library(tidyverse) library(mosaicCalc) 19.1.2 Key Concepts In this lesson, weâ€™ll be building on several key concepts from prior lessons. Iâ€™ve defined them below as a helpful review. life distribution: the distribution of a vector of \\(n\\) products, whose values recording the amount of time it took for each product to fail. In other words, its lifespan. probability density function (PDF): the function describing the probability (relative frequency) of any value in a distribution. cumulative distribution function (CDF): the function describing the cumulative probability of each successive value in a distribution. Spans from 0 to 1. Have questions? I strongly recommend you review Workshops 2, 3, and 4 before this one! It will help it all fit together! 19.1.3 Our Data In this workshop, weâ€™re going to work with a data.frame called masks. An extremely annoying moment in the COVID-era is when a part of your mask snaps, requiring you to get a fresh mask. Letâ€™s examine a (hypothetical) sample of n = 50 masks produced by Company X to explore how often this happens! Please import the masks.csv data.frame below. Each row is a mask, with its own unique id. Columns describe how many hours it took for the left_earloop to snap, the right_earloop to snap, the nose wire to snap, and the fabric of the mask to tear. masks &lt;- read_csv(&quot;workshops/masks.csv&quot;) # Let&#39;s glimpse() its contents! masks %&gt;% glimpse() ## Rows: 50 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1â€¦ ## $ left_earloop &lt;dbl&gt; 6, 16, 46, 4, 1, 5, 32, 35, 27, 3, 4, 7, 1, 20, 22, 17, â€¦ ## $ right_earloop &lt;dbl&gt; 12, 1, 17, 14, 19, 8, 18, 14, 5, 8, 8, 2, 12, 7, 20, 4, â€¦ ## $ wire &lt;dbl&gt; 4, 1, 8, 29, 23, 8, 10, 38, 11, 31, 7, 4, 3, 33, 13, 2, â€¦ ## $ fabric &lt;dbl&gt; 177, 462, 65, 405, 2483, 1064, 287, 2819, 1072, 288, 863â€¦ 19.2 Quantities of Interest When we work with life distributions, we often want to find several useful quantities of interest (a.k.a. parameters) about them. Letâ€™s find out how to do that with an exponential distribution! 19.2.1 Lack of Memory Exponential distributions are famous for a key trait. The failure rate $ $ remains constant in an exponential distribution. The probability that a product fails in the next hour of use is the same at t = 0, t = 100, or t = infinity! It doesnâ€™t worsen with time. (Itâ€™s the literal meaning of the saying, â€œif it is not broke, donâ€™t fix it!â€) 19.2.2 Mean Time to Fail The Mean Time to Fail describes the mean of a lifespan distribution. For example, letâ€™s calculate the mean time to fail (in hours) for a maskâ€™s left_earloop in our sample. stat &lt;- masks %&gt;% summarize( # We can take the mean of this vector of time to fail in hours mttf = mean(left_earloop), # Lambda is the reciprocal of the MTTF lambda = 1 / mttf) # Check out the contents! stat ## # A tibble: 1 Ã— 2 ## mttf lambda ## &lt;dbl&gt; &lt;dbl&gt; ## 1 13.4 0.0749 In an exponential distribution, the MTTF always has a cumulative probability of 1 - 1 / e = 0.632. (This can be coded in R like so:) 1 - 1 / exp(1) ## [1] 0.6321206 Letâ€™s assume our sampleâ€™s left earloops have an exponential lifespan distribution, and use pexp() to calculate the cumulative probability of getting an MTTF of 13.36. Weâ€™ll need to supply pexp() the benchmark in the distribution in question (mttf), plus the the rate parameter \\(\\lambda\\), which we always need when simulating an exponential distribution. prob &lt;- pexp(stat$mttf, rate = stat$lambda) prob ## [1] 0.6321206 Indeed, the figure below compares the observed PDF function (made using density()) to the assumed exponential PDF (made using dexp()), and we can see that that 63% of the distribution has failed by the Mean Time to Fail. 19.2.3 Mean Time to Fail via Integration Mean Time to Fail (MTTF) can be number-crunched empirically as the mean of observed lifespans, assuming an exponential distribution. But it is also equal to the integral of the reliability function: $MTTF = _{0}^{}{R(t)dx} $. So, letâ€™s make ourselves a nice reliability function to help us calculate the MTTF. We know the reliability function can be stated as \\(R(t) = 1 - F(t)\\). Assuming an exponential distribution, the failure rate \\(F(t) = 1 - e^{-\\lambda t}\\). We know \\(\\lambda = \\frac{1}{MTTF}\\), and above, we found that lambda = 0.0748502994011976 for a left-earloop. # Reliability Function for exponential distribution r = function(t, lambda){ exp(-1*t*lambda)} We can calculate it belowâ€¦ # Use mosaicCalc&#39;s antiD function # To get integral of r(t, lambda) as x goes from 0 to infinity mttf = antiD(tilde = r(t, lambda) ~ t) Great! We have developed our own mttf function for an exponential distribution! If we feed t a suitably large value, like 1000 (approaching infinity), we will reach the original observed/estimated mttf. mttf(t = 1000, lambda = stat$lambda) ## [1] 13.36 mttf(t = Inf, lambda = stat$lambda) ## [1] 13.36 This only helps you if you know lambda, the inverse of the MTTF, or have the reliability function but not the MTTF. 19.2.4 Median Time to Fail \\(T_{50}\\) We might also want to know the median time to failure (\\(T_{50}\\)), the value on the x axis that splits the area under the curve in half at 50% and 50%. We can calculate this asâ€¦ \\(F(T_{50}) = 50\\% = 0.5 = 1 - e^{-\\lambda T_{50}}\\) where: \\(T_{50} = \\frac{log(2)}{ \\lambda } = \\frac{0.693}{\\lambda}\\) # Let&#39;s update stat to include the observed &#39;median&#39; # and &#39;t50&#39;, the median assuming an exponential distribution stat &lt;- masks %&gt;% summarize( # The literal mean time to fail # in our observed distribution is this mttf = mean(left_earloop), # And lambda is this... lambda = 1 / mttf, # The observed median is this.... median = median(left_earloop), # But if we assume it&#39;s an exponential distribution # and calculate the median from lambda, # we get t50, which is very close. t50 = log(2) / lambda) # Check it out! stat ## # A tibble: 1 Ã— 4 ## mttf lambda median t50 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 13.4 0.0749 8 9.26 19.2.5 Modal Time to Fail Finally, the modal time to fail is pretty easy to calculate. Its the most common time to fail, also known as the max probability in a PDF. masks %&gt;% summarize( # Let&#39;s get lambda, the reciprocal of the MTTF lambda = 1 / mean(left_earloop), # And let&#39;s estimate the PDF... t = 1:max(left_earloop), prob = dexp(t, rate = lambda)) %&gt;% # And let&#39;s sort the data.frame from highest to lowest arrange(desc(prob)) %&gt;% # Grab first 3 rows, for brevity head(3) ## # A tibble: 3 Ã— 3 ## lambda t prob ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.0749 1 0.0695 ## 2 0.0749 2 0.0644 ## 3 0.0749 3 0.0598 # This reveals that t = 1 is our mode Learning Check 1 Question A competing mask manufacturer made a mask whose earloops fail at a constant failure rate of 0.08. What is the probability that 1 fails before 20 hours of use? What is the probability that 2 fail before 20 hours of use? After how long should we expect 1% failures? [View Answer!] What is the probability that 1 fails before 20 hours of use? # Let&#39;s generate an expondential failure function, # because **constant** rate of failure f = function(t, lambda){1 - exp(-1*t*lambda)} # Use failure function (CDF) to get area under curve BEFORE 20 hours. f(t = 20, lambda = 0.08) ## [1] 0.7981035 # There&#39;s a 79% chance 1 fails within 20 hours What is the probability that 2 fail before 20 hours of use? # For n failures, take F(t) to the nth power f(t = 20, lambda = 0.08)^2 ## [1] 0.6369692 # There&#39;s a 63% chance 2 fail within 20 hours. After how long should we expect 1% failures? # We can solve this using the failure function # f(t) = 1 - e^{-t*lambda} # But we need to invert it, # to solve for t # f(t) = 1 - e^{-t*lambda} # e^{-t*lambda} = 1 - f(t) # log(1 - f(t)) = -t*lambda # -log(1 - f(t)) / lambda = t # So if we set f(t) = 1%, and lambda = 0.08, # this will tell us at what time F(t) will equal 1% -log(1 - 0.01) / 0.08 ## [1] 0.1256292 # Looks like that time of interest is t = 0.1256 hours. Learning Check 2 Question Above, we examined a sample of surgical masks, checking how often their left_earloop snapped. How does that compare with the right_earloop? Calculate the mean time to fail for the right earloop, and \\(\\lamba\\), the mean failure rate. Is the right earloop more or less reliable than the left earloop? [View Answer!] MTTF and Lambda compare &lt;- masks %&gt;% summarize(mttf_right = mean(right_earloop), mttf_left = mean(left_earloop), lambda_right = 1 / mttf_right, lambda_left = 1 / mttf_left) # Check it compare ## # A tibble: 1 Ã— 4 ## mttf_right mttf_left lambda_right lambda_left ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.6 13.4 0.0943 0.0749 # Looks like the left earloop fails less often. 19.3 Quantities of Interest (continued) 19.3.1 Conditional Reliability (Survival) Function We may also want to know, after age t, whatâ€™s the probability that a product survives an additional x years to age t + x? We can restate this in terms of \\(T_{Fail}\\), the time at which the product finally fails. We want to know, whatâ€™s the probability that \\(T_{Fail}\\) is greater than \\(t + x\\), given that we already know \\(T_{Fail}\\) must be greater than \\(t\\) (since it hasnâ€™t failed yet as of time \\(t\\))? Fortunately, this can be simplified in terms of the reliability functions. As long as we can calculate \\(R(x + t)\\) and \\(R(t)\\), we can find \\(R(x|t)\\), the conditional survival function. \\[ R(x | t) = \\frac{ R(x + t) }{ R(t) } = \\frac{ P(T_{Fail} &gt; x + t) }{P(T_{Fail} &gt; t)} \\] So, letâ€™s use our nice reliability function from before to help us calculate the conditional reliability function. # Reliability Function for exponential distribution r = function(t, lambda){ exp(-1*t*lambda)} So, whatâ€™s the probability that a left-earloop that has lasted 10 hours will last another 5 hours? r(t = 10 + 5, lambda = stat$lambda) / r(t = 10, lambda = stat$lambda) ## [1] 0.6878039 Looks like thereâ€™s a 69% chance it will last another 5 hours, given that it has already lasted 10 hours. Letâ€™s finish up by building ourselves a nice Conditional Reliability function cr, which which calculates the conditional probability of any item surviving x more hours given that it survived t hours and a mean failure rate of lambda. cr = function(t, x, lambda){ # We can actually nest functions inside each other, # to make them easier to write r = function(t, lambda){ exp(-1*t*lambda)} # Calculate R(x + t) / R(t) output &lt;- r(t = t + x, lambda) / r(t = t, lambda) # and return the result! return(output) } # Let&#39;s compare our result to above! It&#39;s the same! cr(t = 10, x = 5, lambda = stat$lambda) ## [1] 0.6878039 If we were to visualize our conditional reliability function cr below as x ranges from 1 to 50, it would produce the following curve. 19.3.2 \\(\\mu(t)\\): Mean Residual Life The Conditional Reliability Function \\(R(x|t)\\) also allows us to calculate the Mean Residual Life (MRL, a.k.a. \\(\\mu\\)) at time \\(t\\). The MRL at time \\(t\\) refers to the average number of years the product is expected to survive after time \\(t\\). We can calculate it using the distribution below. \\[MRL(t) = \\mu(t) = \\int_{0}^{\\infty}{ R(x|t)dx} = \\frac{1}{R(t)} \\int_{0}^{\\infty}{ R(x) dx} \\] It shows the mean expected remaining life years after \\(t\\). We can formalize this as function mu(t, lambda). (Since the Greek letter \\(\\mu\\) is pronounced mu.) # Conditional Reliability library(mosaicCalc) library(dplyr) # Calculate Mean Residual Life mu = function(t = 5, lambda = 0.001){ #t = 5 #lambda = 0.001 # Get the Reliability Function for exponential distribution r = function(t, lambda){ exp(-1*t*lambda)} # Get the integral of R(x), the time remaining (mttf) integral = antiD(tilde = r(x, lambda) ~ x, lower.bound = 0) # Now calculate mu(), the Mean Residual Life function # as of time t output &lt;- integral(x = Inf, lambda = lambda) * 1 / r(t = t, lambda = lambda) return(output) } Letâ€™s test it out! # Mean residual life given it&#39;s lasted 500 hours mu(t = 500, lambda = 0.001) ## [1] 1648.721 # Mean residual life given it&#39;s lasted 2000 hours mu(t = 2000, lambda = 0.001) ## [1] 7389.056 # Mean residual life given it&#39;s lasted 5000 hours mu(t = 5000, lambda = 0.001) ## [1] 148413.2 # [Note: mu is NOT vectorized] Notice also that when t = 0, we have a mu(0) which equals the MTTF. # Mean residual life given it&#39;s lasted 0 hours... mu(t = 0, lambda = 0.001) # same as MTTF! ## [1] 1000 # Get mean time to failure function... mttf = antiD(tilde = r(t, lambda) ~ t) mttf(t = Inf, lambda = 0.001) ## [1] 1000 Works perfectly! This can also be applied to other life distributions, which we explore in later chapters. For example, hereâ€™s a exmaple using the Weibullâ€™s reliability function r(t, m, c). muw = function(t = 5, m = 2, c = 20000){ #t = 5; m = 2; c = 20000 # Get the Reliability Function for exponential distribution r = function(t, m, c){ exp(-1*(t/c)^m) } # Get the integral of R(x), from 0 to infinity integral = antiD(tilde = r(x, m, c) ~ x, lower.bound = 0) # Now calculate mu(), the Mean Residual Life function at time t output &lt;- integral(x = Inf, m = m, c = c) * 1 / r(t = t, m = m, c= c) return(output) } # Try it! # see how this mean residual time to failure gets a little bigger as we move forward? muw(t = 500, m = 2, c = 20000) ## [1] 17735.62 muw(t = 501, m = 2, c = 20000) ## [1] 17735.66 muw(t = 502, m = 2, c = 20000) ## [1] 17735.71 Learning Check 3 Question A competing firm produced a mask with a wire that fails at a constant rate of 1 failure per 240 hours. The probability that the wire survives 1 week (t = 168 hours) in continuous use isâ€¦ ( You buy the mask, and it works without failure for 2 weeks (t = 336 hours) The probability the wire will snap during the next week (t = 504 hours) isâ€¦ [View Answer!] The probability that the wire survives 1 week (t = 168 hours) in continuous use isâ€¦ ( # Write the reliability function r = function(t, lambda){ exp(-1*t*lambda)} # Probability it survives 1 month (760 hours) is... r(t = 168, lambda = 1 / 240) ## [1] 0.4965853 # ~ 50% You buy the mask, and it works without failure for 2 weeks (t = 336 hours) The probability the wire will snap during the next week (t = 504 hours) isâ€¦ # We can calculate it 2 ways. # First, we can take # R(t + x) / R(t) r(t = 504, lambda = 1 / 240) / r(t = 336, lambda = 1 / 240) ## [1] 0.4965853 # Or, we can calculate the failure function f = function(t, lambda){1 - exp(-1*t*lambda)} # And just calculate the rate of F(t = x) # Because lambda is a constant failure rate f(t = 168, lambda = 1 / 240) ## [1] 0.5034147 19.4 System Failure with Independent Failure Rates Consider a mask with a left and right earloop, which have independent failure rates \\(\\lambda_{left}\\) and \\(\\lambda_{right}\\). Whatâ€™s the probability that the left loop fails before the right loop? We can express this as: \\[ P(j \\ fails \\ first) = \\frac{ \\lambda_{j}}{ \\sum_{i=1}^{n}{ \\lambda{i}} }\\] In other words, the probability that component \\(j\\) fails first reflects how big \\(\\lambda_{j}\\) is relative to all the other failure rates in total. Letâ€™s test this out with our masks dataset. masks %&gt;% summarize( # Calculate failure rates of left and right loops lambda_left = 1 / mean(left_earloop), lambda_right = 1 / mean(right_earloop), # Calculate the total probability of either loop failing lambda_sum = lambda_left + lambda_right, # Calculate probability the left loop fails first prob_left_first = lambda_left / lambda_sum) ## # A tibble: 1 Ã— 4 ## lambda_left lambda_right lambda_sum prob_left_first ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0749 0.0943 0.169 0.442 # Looks like a probability of about 44% that left loop fails first. 19.4.1 Reliability Functions with Multiple Inputs Suppose a fraction of our masks are shipped in from manufacturer 1, while some are shipped from manufacturer 2. A fraction \\(p\\) is coming from manufacturer 1, while \\(1-p\\) is coming from manufacturer 2. We can use the rules of total probability to calculate the reliability function and other quantities for this mask: \\[R(t) = \\sum_{i=1}^{n}{ ( \\ p_{i} \\times R_i(t) \\ ) } = \\sum_{i=1}^{n}{ ( \\ p_{i} \\times e^{-\\lambda_{i} t} ) }\\] \\[MTTF = \\sum_{i=1}^{n}{ \\frac{p_i}{\\lambda_{i}}}\\] \\[f(t) = \\frac{-d}{dt}R(t) = \\sum_{i=1}^{n}{p\\lambda_{i}e^{-\\lambda_{i}t}}\\] \\[z(t) = \\frac{ f(t) }{ R(t)} = \\frac{ \\sum_{i=1}^{n}{p\\lambda_{i}e^{-\\lambda_{i}t}} }{ \\sum_{i=1}^{n}{ ( \\ p_{i} \\times R_i(t) \\ ) } } \\] Itâ€™s pretty messy, but powerful! Suppose we order 75% of our stock from a manufacturer with a failure rate of 1 fabric tear per 50 hours, but 25% from our stock from a manufacturer with a failure rate of 1 tear per 100 hours. What is the (1) overall mean time to failure and (2) overall failure rate at 100 hours for any random mask in your supply? We can tally this up in a stock data.frame. stock &lt;- data.frame( prob = c(0.75, 0.25), lambda = c(1 / 50, 1 / 100)) To calculate the MTTF, we just take the sum of the fraction of each proportion and each failure rate lambda. stock %&gt;% summarize(mttf = sum(prob / lambda)) ## mttf ## 1 62.5 To calculate the overall failure rate, we will generate the reliability function, take its derivative to get the PDF. # Let&#39;s write an exponential reliability function r = function(t, lambda){ exp(-1*t*lambda) } # Let&#39;s derive an exponential PDF f(t), using mosaicCalc&#39;s D() f = D(-1*r(t, lambda) ~ t) Then, we use the PDFs \\(f_i(t)\\) and the Reliability functions \\(R_i(t)\\) to get calculate the overall failure rate \\(z(t)\\). stock %&gt;% summarize( total_f = sum(prob * f(t = 100, lambda = lambda)), total_r = sum(prob * r(t = 100, lambda = lambda)), z = total_f / total_r) ## total_f total_r z ## 1 0.002949728 0.1934713 0.01524633 We could even write it as a function, where p and lambda are equal length vectors for plant 1, plant 2, plant 3, â€¦ plant \\(n\\). z = function(t){ # Set the input percentages of products from plants 1 and 2 p = c(0.75, 0.25) # Set the failure rates for each lambda = c(0.02, 0.01) # Let&#39;s write an exponential reliability function r = function(t, lambda){ exp(-1*t*lambda) } # Let&#39;s derive an exponential PDF f(t), f = D(-1*r(t, lambda) ~ t) # Calculate total probability total_f = sum( p * f(t, lambda) ) # Calculate total reliability total_r = sum( p * r(t, lambda) ) # Calculate overall failure rate z = total_f / total_r return(z) } # Try it out! z(t = 1) ## [1] 0.0174812 z(t = 10) ## [1] 0.01730786 z(t = 100) ## [1] 0.01524633 19.4.2 Phase-Type Distributions One way to more accurately model the lifespan of a product is to accept that its failure rate may remain constant, but it might change between failure rates as it passes through phases. Letâ€™s model that! We can write the time \\(T\\) to critical failure (via either overstress OR degraded failure) as \\(T = min(T_c, T_d+T_{dc})\\). The total probability of a product being in any phase \\(a_{i \\to n}\\) equals 1. \\(a_c = 25\\%\\) \\(a_d = 40\\%\\) \\(a_dc = 10\\%\\) We can represent it in a dataframe, like so: myphase &lt;- data.frame( id = 1:4, phase = c(&quot;c&quot;, &quot;d&quot;, &quot;c&quot;, &quot;dc&quot;), alpha = c(0.25, 0.40, 0.25, 0.10), lambda = c(0.50, 1, 0.50, 3) ) And, using the same tricks from the preceding section, we can calculate z(t), the overall hazard rate of a phase-type exponential distribution, by taking the sum of the weighted probabilities. r = function(t, lambda){ exp(-1*t*lambda) } # Let&#39;s derive an exponential PDF f(t), f = D(-1*r(t, lambda) ~ t) z = function(t, data){ output &lt;- data %&gt;% summarize( prob_f = sum(alpha * f(t = t, lambda)), prob_r = sum(alpha * r(t = t, lambda)), ratio_z = prob_f / prob_r) output$ratio_z %&gt;% return() } # Take a peek! z(t = 1, data = myphase) ## [1] 0.6888965 z(t = 2, data = myphase) ## [1] 0.6161738 z(t = 5, data = myphase) ## [1] 0.5308124 z(t = 10, data = myphase) ## [1] 0.5026807 19.5 Conclusion Congrats! You made it! Youâ€™ve just picked up some of the key techniques for evaluating product lifespans in R! "],["statistical-techniques-for-exponential-distributions.html", "20 Statistical Techniques for Exponential Distributions 20.1 Getting Started 20.2 Factors in R 20.3 Crosstabulation Learning Check 1 20.4 Estimating Lambda Learning Check 2 Learning Check 3 20.5 Planning Experiments 20.6 Chi-squared Learning Check 4 20.7 Conclusion", " 20 Statistical Techniques for Exponential Distributions 20.1 Getting Started In this workshop, weâ€™re going to examine several key tools that will help you (1) cross-tabulate failure data over time, (2) use statistics to determine whether an archetypal distribution (eg. exponential) fits your data sufficiently, (3) how to estimate the failure rate \\(\\lambda\\) from tabulated data, and (4) how to plan experiments for product testing. Here we go! 20.1.1 Load Packages Letâ€™s start by loading the dplyr package, which will let us mutate(), filter(), and summarize() data quickly. Weâ€™ll also load mosaicCalc, for taking derivatives and integrals (eg. D() and antiD()). # Load packages library(dplyr) library(readr) library(ggplot2) library(mosaicCalc) 20.1.2 Our Data In this workshop, weâ€™re going to continue working with a data.frame called masks. Letâ€™s examine a hypothetical sample of n = 50 masks produced by Company X to measure how many hours it took for each part of the mask to break. Please import the masks.csv data.frame below. Each row is a mask, with its own unique id. Columns describe how many hours it took for the left_earloop to snap, the right_earloop to snap, the nose wire to snap, and the fabric of the mask to tear. masks &lt;- read_csv(&quot;workshops/masks.csv&quot;) # Let&#39;s glimpse() its contents! masks %&gt;% glimpse() ## Rows: 50 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1â€¦ ## $ left_earloop &lt;dbl&gt; 6, 16, 46, 4, 1, 5, 32, 35, 27, 3, 4, 7, 1, 20, 22, 17, â€¦ ## $ right_earloop &lt;dbl&gt; 12, 1, 17, 14, 19, 8, 18, 14, 5, 8, 8, 2, 12, 7, 20, 4, â€¦ ## $ wire &lt;dbl&gt; 4, 1, 8, 29, 23, 8, 10, 38, 11, 31, 7, 4, 3, 33, 13, 2, â€¦ ## $ fabric &lt;dbl&gt; 177, 462, 65, 405, 2483, 1064, 287, 2819, 1072, 288, 863â€¦ 20.2 Factors in R For the next section, youâ€™ll need to understand factors. Factors are ordered vectors. They are helpful in ggplot and elsewhere for telling R what order to interpret things in. You can make your own vector into a factor using factor(vector, levels = c(\"first\", \"second\", \"etc\")). # Make character vector myvector &lt;- c(&quot;surgical&quot;, &quot;KN-95&quot;, &quot;KN-95&quot;, &quot;N-95&quot;, &quot;surgical&quot;) # Turn it into a factor myfactor &lt;- factor(myvector, levels = c(&quot;N-95&quot;, &quot;KN-95&quot;, &quot;surgical&quot;)) # check it myfactor ## [1] surgical KN-95 KN-95 N-95 surgical ## Levels: N-95 KN-95 surgical factors can be reduced to numeric vectors using as.numeric(). This returns the level for each value in the factor. # Turn the factor numeric mynum &lt;- myfactor %&gt;% as.numeric() # Compare data.frame(myfactor, mynum) ## myfactor mynum ## 1 surgical 3 ## 2 KN-95 2 ## 3 KN-95 2 ## 4 N-95 1 ## 5 surgical 3 # for example, N-95, which was ranked first in the factor, receives a 1 anytime the value N-95 appears 20.3 Crosstabulation Sometimes, we might want to tabulate failures in terms of meaningful units of time, counting the total failures every 5 hours, every 24 hours, etc. Letâ€™s learn how! d1 &lt;- data.frame(t = masks$left_earloop) %&gt;% # classify each value into 5-point width bins (0 to 5, 6-10, 11-15, etc.) # then convert it to a numeric ranking of categories from 1 to n bins mutate(label = cut_interval(t, length = 5)) # Let&#39;s take a peek d1 %&gt;% glimpse() ## Rows: 50 ## Columns: 2 ## $ t &lt;dbl&gt; 6, 16, 46, 4, 1, 5, 32, 35, 27, 3, 4, 7, 1, 20, 22, 17, 8, 18, 1â€¦ ## $ label &lt;fct&gt; &quot;(5,10]&quot;, &quot;(15,20]&quot;, &quot;(45,50]&quot;, &quot;[0,5]&quot;, &quot;[0,5]&quot;, &quot;[0,5]&quot;, &quot;(30,â€¦ Step 2: Tabulate Observations per Bin. d2 &lt;- d1 %&gt;% # For each bin label group_by(label, .drop = FALSE) %&gt;% # Get total observed rows in each bin # .drop = FALSE records factor levels in label that have 0 cases summarize(r_obs = n()) d2 %&gt;% glimpse() ## Rows: 11 ## Columns: 2 ## $ label &lt;fct&gt; &quot;[0,5]&quot;, &quot;(5,10]&quot;, &quot;(10,15]&quot;, &quot;(15,20]&quot;, &quot;(20,25]&quot;, &quot;(25,30]&quot;, &quot;â€¦ ## $ r_obs &lt;int&gt; 17, 13, 3, 7, 2, 2, 3, 0, 0, 1, 2 Step 3: Get bounds and midpoint of Bins Last, we might need the bounds (upper and lower value), or the midpoint. Hereâ€™s how! d3 &lt;- d2 %&gt;% # Get bin ranking, lower and upper bounds, and midpoint mutate( bin = as.numeric(label), lower = (bin - 1) * 5, upper = bin * 5, midpoint = (lower + upper) / 2) # Check it! d3 ## # A tibble: 11 Ã— 6 ## label r_obs bin lower upper midpoint ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [0,5] 17 1 0 5 2.5 ## 2 (5,10] 13 2 5 10 7.5 ## 3 (10,15] 3 3 10 15 12.5 ## 4 (15,20] 7 4 15 20 17.5 ## 5 (20,25] 2 5 20 25 22.5 ## 6 (25,30] 2 6 25 30 27.5 ## 7 (30,35] 3 7 30 35 32.5 ## 8 (35,40] 0 8 35 40 37.5 ## 9 (40,45] 0 9 40 45 42.5 ## 10 (45,50] 1 10 45 50 47.5 ## 11 (50,55] 2 11 50 55 52.5 Learning Check 1 Question A small start-up is product testing a new super-effective mask. They product tested 25 masks over 60 days. They contract you to analyze the masksâ€™ lifespan data, recorded below as the number of days to failure. # Lifespan in days supermasks &lt;- c(1, 2, 2, 2, 3, 3, 4, 4, 5, 9, 13, 15, 17, 19, 20, 21, 23, 24, 24, 24, 32, 33, 33, 34, 54) Cross-tabulate the lifespan distribution in intervals of 7 days. Whatâ€™s the last 7-day interval? How many masks are expected to fail within that interval? What percentage of masks are expected to fail within 28 days? [View Answer!] Cross-tabulate the lifespan distribution in intervals of 7 days. ## # A tibble: 8 Ã— 6 ## label r_obs bin lower upper midpoint ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [0,7] 9 1 0 7 3.5 ## 2 (7,14] 2 2 7 14 10.5 ## 3 (14,21] 5 3 14 21 17.5 ## 4 (21,28] 4 4 21 28 24.5 ## 5 (28,35] 4 5 28 35 31.5 ## 6 (35,42] 0 6 35 42 38.5 ## 7 (42,49] 0 7 42 49 45.5 ## 8 (49,56] 1 8 49 56 52.5 Whatâ€™s the last 7-day interval? # Find the bin with the highest bin id... a %&gt;% filter(bin == max(bin)) ## # A tibble: 1 Ã— 6 ## label r_obs bin lower upper midpoint ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (49,56] 1 8 49 56 52.5 # or a %&gt;% tail(1) ## # A tibble: 1 Ã— 6 ## label r_obs bin lower upper midpoint ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (49,56] 1 8 49 56 52.5 # or # Find the bin with the highest midpoint a %&gt;% filter(midpoint == max(midpoint)) ## # A tibble: 1 Ã— 6 ## label r_obs bin lower upper midpoint ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (49,56] 1 8 49 56 52.5 How many masks are expected to fail within that interval? # Find the count of masks to fail `r_obs` within that interval a %&gt;% filter(bin == max(bin)) %&gt;% select(label, r_obs) ## # A tibble: 1 Ã— 2 ## label r_obs ## &lt;fct&gt; &lt;int&gt; ## 1 (49,56] 1 What percentage of masks are expected to fail within 28 days? a %&gt;% # Calculate cumulative failures... mutate(r_cumulative = cumsum(r_obs)) %&gt;% # Calculate percent of cumulative failures, divided by total failures mutate(percent = cumsum(r_obs) / sum(r_obs)) %&gt;% # Filter to just our failures within 28 days filter(midpoint &lt;= 28) ## # A tibble: 4 Ã— 8 ## label r_obs bin lower upper midpoint r_cumulative percent ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 [0,7] 9 1 0 7 3.5 9 0.36 ## 2 (7,14] 2 2 7 14 10.5 11 0.44 ## 3 (14,21] 5 3 14 21 17.5 16 0.64 ## 4 (21,28] 4 4 21 28 24.5 20 0.8 20.4 Estimating Lambda Sometimes, we donâ€™t have access to the full raw data of times to failure for a product. Often, we only have access to cross-tabulated data released by companies. How then are we supposed to analyze product failure and reliability, if we cannot compute it directly? Even if we do not have raw data, we can still estimate the failure rate \\(\\lambda\\) for a component. Hereâ€™s a few ways! 20.4.1 Lambda from Complete Sample If we have a complete sample of data (eg. not censored), then we can just calculate: $ = $. # Eg. 1 / mean(masks$left_earloop) ## [1] 0.0748503 20.4.2 \\(\\hat{\\lambda}\\) from Cross-Tabulation Alternatively, if our data is censored or pre-tabulated into groups, we may need to use the midpoint and count of failures in each bin to calculate the failure rate \\(\\lambda\\). But, because this is an estimate, subject to error, we call it \\(\\hat{\\lambda}\\) (said â€œlambda-hatâ€). Here are the 3 kinds of cross-tabulated samples you might encounter. Tabulated, No Censoring: tallied up in equally sized bins. All products eventually fail, yielding complete sample of times to failure. Tabulated, Type I Censoring: experiment stops when time \\(t\\) reaches \\(limit\\). Tabulated, Type II Censoring: experiment stops when number of units failed \\(n_{failed}\\) reaches \\(limit\\). For example, if we receive just the d3 data.frame we made above, how do we estimate \\(\\lambda\\) from it? We will need: \\(r\\): total failures (sum(r_obs)) \\(n\\): total observations (failed or not) (provided; otherwise, \\(n = r\\)) \\(\\sum_{i=1}^{r}{t_i}\\): total unit-hours (sum(midpoint*r_obs)) \\(T\\): timestep of (1) last failure observed (sometimes written \\(t_r\\)) or (2) last timestep recorded; usually obtained by max(midpoint). Unless some cases do not fail, \\((n - r)t_z\\) will cancel out as 0. \\[ \\hat{\\lambda} = \\frac{ r }{ \\sum_{i=1}^{r}{t_i} + (n - r)t_z} \\] # Let&#39;s calculate it! d4 &lt;- d3 %&gt;% summarize( r = sum(r_obs), # total failures hours = sum(midpoint*r_obs), # total failure-hours n = r, # in this case, total failures = total obs tz = max(midpoint), # end of study period # Calculate lambda hat! lambda_hat = r / (hours + (n - r)*tz)) # Let&#39;s compare 1 / mean(masks$left_earloop) ## [1] 0.0748503 Learning Check 2 Question The same startup testing that super-effective mask needs to estimate their failure rate, but they only have crosstabulated data. Theyâ€™ve provided it to you below, using a tribble() (another dplyr way to write a data.frame). Estimate \\(\\hat{\\lambda}\\) from the cross-tabulated data, knowing that they tested 25 masks over 60 days. a = tribble( ~bin, ~label, ~r_obs, ~lower, ~upper, 1, &#39;[0,7]&#39;, 9, 0, 7, 2, &#39;(7,14]&#39;, 2, 7, 14, 3, &#39;(14,21]&#39;, 5, 14, 21, 4, &#39;(21,28]&#39;, 4, 28, 35, 5, &#39;(28,35]&#39;, 4, 28, 35, 6, &#39;(35,42]&#39;, 0, 35, 42, 7, &#39;(42,49]&#39;, 0, 42, 49, 8, &#39;(49,56]&#39;, 1, 49, 56 ) a ## # A tibble: 8 Ã— 5 ## bin label r_obs lower upper ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 [0,7] 9 0 7 ## 2 2 (7,14] 2 7 14 ## 3 3 (14,21] 5 14 21 ## 4 4 (21,28] 4 28 35 ## 5 5 (28,35] 4 28 35 ## 6 6 (35,42] 0 35 42 ## 7 7 (42,49] 0 42 49 ## 8 8 (49,56] 1 49 56 [View Answer!] Estimate \\(\\hat{\\lambda}\\) from the cross-tabulated data, knowing that they tested 25 masks over 60 days. ## # A tibble: 1 Ã— 5 ## r days n tz lambda_hat ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 444. 25 52.5 0.0562 20.4.3 Confidence Intervals for \\(\\hat{\\lambda}\\) Also, our estimate of lambda might be slightly off due to random sampling error. (Every time you take a random sample, thereâ€™s a little change, right?) Thatâ€™s why we call it \\(\\hat{\\lambda}\\). So, letâ€™s build a confidence interval around our estimate. 20.4.3.1 \\(k\\) factor We can build confidence intervals around our point estimate \\(\\hat{\\lambda}\\) using something called a \\(k\\) factor. We will weight \\(\\hat{\\lambda}\\) by a factor \\(k\\), to get a slightly higher/slightly lower estimated failure rate that shows our confidence interval. This factor \\(k\\) may be greater depending on (1) the total number of failures \\(r\\) and (2) our level of acceptable error (\\(alpha\\)). Like any statistic, our estimate \\(\\hat{\\lambda}\\) has a full sampling distribution of \\(\\hat{\\lambda}\\) values we could get due to random sampling error, with some of them occurring more frequently than others. We want to find the upper and lower bounds around the 90%, 95%, or perhaps 99% most common (middle-most) values in that sampling distribution. So, if \\(alpha = 0.10\\), weâ€™re going to get a 90% confidence interval (dubbed \\(interval\\)) spanning from the 5%~95% of that sampling distribution. # To calculate a &#39;one-tailed&#39; 90% CI (eg. we only care if above 90%) alpha = 0.10 ci = 1 - alpha # To adjust this to be &#39;two-tailed&#39; 90% CI (eg. we care if below 5% or above 95%)... 0.5 + (ci / 2) ## [1] 0.95 0.5 - ci / 2 ## [1] 0.05 20.4.3.2 \\(k\\) factor by Type of Data No Censoring: If we have complete data (all observations failed!), using this formula to calculate factor \\(k\\): # For example, let&#39;s say r = 50 r = 50 k = qchisq(ci, df = 2*r) / (2*r) Time-Censoring: If we only record up to a specific time (eg. planning an experiment), use this formula to calculate factor \\(k\\), setting as the degrees of freedom \\(df = 2(r+1)\\). # For example, let&#39;s say r = 50 r = 50 k = qchisq(ci, df = 2*(r+1)) / (2*r) # Clear these remove(r, k) So, since we do not have time censoring in our d4 dataset, we can go ahead an calculate the df = 2*r and compute the 90% lower and upper confidence intervals. d4 %&gt;% summarize( lambda_hat = lambda_hat, r = r, k_upper = qchisq(0.95, df = 2*r) / (2*r), k_lower = qchisq(0.05, df = 2*r) / (2*r), lower = lambda_hat * k_lower, upper = lambda_hat * k_upper) ## # A tibble: 1 Ã— 6 ## lambda_hat r k_upper k_lower lower upper ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0769 50 1.24 0.779 0.0599 0.0956 20.4.3.3 \\(k\\) factor functions ðŸ§® Here are some helper functions that make it easy to calculate and visualize k-factors used in confidence intervals for failure rates! This qk function is useful when you need the upper and lower k-factor for a given probability p and number of failures r. #&#39; @name qk #&#39; @title k-factor Quantiles #&#39; @description #&#39; Function to return quantiles for k-factors. #&#39; Intended for estimating confidence intervals for failure rates. #&#39; @param p:[dbl] vector of probabilities / percentile(s) #&#39; @param r:int number of failures (non-negative integers; can include zero) #&#39; @param .time:logical logical; is case time-censored data? #&#39; @param .failure:logical logical; is case failure-censored data? #&#39; #&#39; @importFrom dplyr `case_when` qk = function(p, r, .time = FALSE, .failure = FALSE){ # Testing values # p = 0.95; r = 20; .time = FALSE; .failure = FALSE # Input error handling stopifnot(is.numeric(p) &amp; p &gt;= 0 &amp; p &lt;= 1) stopifnot(is.numeric(as.integer(r))) stopifnot(is.logical(.time)) stopifnot(is.logical(.failure)) stopifnot( (.time == TRUE &amp; .failure == FALSE) | (.time == FALSE &amp; .failure == FALSE) | (.time == FALSE &amp; .failure == TRUE) ) stopifnot( # R should either be greater than 0 (r &gt; 0) | # OR # zero and the data should be time censored (r == 0 &amp; .time == TRUE) ) # Evaluate if p is in the upper or lower tail .upper = p &gt; 0.5 # Does r == 0? .zerofailures = r == 0 k = case_when( # 1+ failures AND complete data AND UPPER tail --&gt; Get k-factor for r as normal .zerofailures == FALSE &amp; .time == FALSE &amp; .failure == FALSE &amp; .upper == TRUE ~ qchisq(p, df = 2*r) / (2*r), # 1+ failures AND complete data AND LOWER tail --&gt; Get k-factor for r as normal .zerofailures == FALSE &amp; .time == FALSE &amp; .failure == FALSE &amp; .upper == FALSE ~ qchisq(p, df = 2*r) / (2*r), # 1+ failures AND time-censored data AND UPPER tail --&gt; Get k-factor for r+1 .zerofailures == FALSE &amp; .time == TRUE &amp; .failure == FALSE &amp; .upper == TRUE ~ qchisq(p, df = 2*(r + 1) ) / (2*r), # 1+ failures AND time-censored data AND LOWER tail --&gt; Get k-factor for r as normal .zerofailures == FALSE &amp; .time == TRUE &amp; .failure == FALSE &amp; .upper == FALSE ~ qchisq(p, df = 2*(r) ) / (2*r), # 1+ failures AND time-censored data AND LOWER tail --&gt; Get k-factor with adjustment .zerofailures == FALSE &amp; .time == FALSE &amp; .failure == TRUE &amp; .upper == TRUE ~ qchisq(p, df = 2*( (r-1) + 1)) / (2 * (r-1)) * (r-1)/r, # 1+ failures AND time-censored data AND LOWER tail --&gt; Get k-factor with r as normal .zerofailures == FALSE &amp; .time == FALSE &amp; .failure == TRUE &amp; .upper == FALSE ~ qchisq(p, df = 2*r) / (2*r), # If zero failures --&gt; then time-censored --&gt; time = TRUE, and upper/lower distinction doesn&#39;t matter. .zerofailures == TRUE &amp; .time == TRUE &amp; .failure == FALSE ~ -log(1-p), # Otherwise, return NA. TRUE ~ NA_real_ ) if(any(is.na(k))){ message(&quot;At least 1 k-factor could not be calculated, due to improper inputs. Review the rules for time-censored, failure-censored, and zero-failure data.&quot;)} return(k) } # Let&#39;s try an example of 95th percentile k-factor for a sample with 20 failures qk(p = 0.95, r = 20) ## [1] 1.393962 This function below estimates the probability densities for a given ser of k-factor values. It is useful to visualize and compare distributions under different conditions. #&#39; @name dk #&#39; @title k-factor Probability Density Function #&#39; @description #&#39; Function to return probability densities given a supplied k-factor quantile `q`. #&#39; Intended for visualizing sampling distributions of failure rates. #&#39; @param x:[dbl] vector of quantiles (k-factors) #&#39; @param r:int number of failures (non-negative integers; can include zero) #&#39; @param .time:logical logical; is case time-censored data? #&#39; @param .failure:logical logical; is case failure-censored data? dk = function(x, r, .time = FALSE, .failure = FALSE){ # Testing values # x = 2; r = 20; .time = FALSE; .failure = FALSE # Construct a range of quantiles corresponding to a range of cumulative probabilities by = 0.001 p_range = c(by/10000, by/1000, by/100, by/10, seq(from = 0, to = 1, by = 0.001), 1 - by/10, 1 - by/100, 1 - by/1000, 1 - by/10000) p_range = sort(p_range) # Get the quantiles for that range q_range = qk(p = p_range, r = r, .time = .time, .failure = .failure) # Fit a density curve to that quantile data, truncated at 0. curve = density(q_range, cut = c(0)) # Approximate a function using the density curve&#39;s x and y values f = approxfun(x = curve$x, y = curve$y, method = &quot;linear&quot;, rule = 2, na.rm = TRUE) # Estimate density d = f(x) return(d) } # Example dk(x = c(0, 1, 2, 3), r = 21, .time = TRUE, .failure = FALSE) ## [1] 0.006787618 1.218508013 0.002907306 0.006853889 If you already have the k-factor value and want to know what percentile it is, use this pk function to find out! #&#39; @name pk #&#39; @title k-factor Cumulative Distribution Function #&#39; @description #&#39; Function to return cumulative probabilities / percentiles given a supplied k-factor quantile `q`. #&#39; Intended for confidence intervals for failure rates. #&#39; @param q:[dbl] vector of quantiles (k-factors) #&#39; @param r:int number of failures (non-negative integers; can include zero) #&#39; @param .time:logical logical; is case time-censored data? #&#39; @param .failure:logical logical; is case failure-censored data? pk = function(q, r, .time = FALSE, .failure = FALSE){ # Testing values # q = 2; r = 20; .time = FALSE; .failure = FALSE # We just need to map the function... # Construct an approximation function f, which gives the inverse of the Quantile Function, # such that you use linear interpolation to return a Probability for any Quantile supplied. by = 0.001 p_range = c(by/10000, by/1000, by/100, by/10, seq(from = 0, to = 1, by = 0.001), 1 - by/10, 1 - by/100, 1 - by/1000, 1 - by/10000) p_range = sort(p_range) # Get the quantiles for that range q_range = qk(p = p_range, r = r, .time = .time, .failure = .failure) # Get the inverse quantile function f = approxfun(x = q_range, y = p_range, method = &quot;linear&quot;, rule = 2, na.rm = TRUE) # Return the expected CDF for that quantile p = f(q) return(p) } # Probability that k-factor value &lt;= 2 with 20 failures pk(q = 2, r = 20) ## [1] 0.9996857 This rk function generates random samples of k-factor values. Itâ€™s useful when you want to run simulations and make plots to visualize the spread. #&#39; @name rk #&#39; @title k-factor Random Deviates #&#39; @description #&#39; Get a random sample of k-factor values for simulating sampling distributions of failure rates. #&#39; @param n:int number of observations. #&#39; @param r:int number of failures (non-negative integers; can include zero) #&#39; @param .time:logical logical; is case time-censored data? #&#39; @param .failure:logical logical; is case failure-censored data? rk = function(n, r, .time = FALSE, .failure = FALSE){ # Testing values # n = 100; r = 20; .time = FALSE; .failure = FALSE # Input error handling stopifnot(is.integer(as.integer(n)) &amp; as.integer(n) &gt; 0) stopifnot(is.logical(.time)) stopifnot(is.logical(.failure)) stopifnot( (.time == TRUE &amp; .failure == FALSE) | (.time == FALSE &amp; .failure == FALSE) | (.time == FALSE &amp; .failure == TRUE) ) # Does r == 0? .zerofailures = r == 0 # Generate a uniform distribution of percentiles p p_uniform = runif(n = n, min = 0, max = 1) # Return quantiles for the random percentiles k = qk(p = p_uniform, r = r, .time = .time, .failure = .failure) return(k) } # Visualize 1000 random k-factors rk(n = 1000, r = 20) %&gt;% hist() Learning Check 3 Question A small start-up is product testing a new super-effective mask. They product tested 25 masks over 60 days. They contracted you to analyze the cross-tabulated data, tabulated in intervals of 7 days. You have an estimate for \\(\\hat{\\lambda}\\), provided below. # Your estimate b = tibble(r = 25, days = 416.5, n = 25, tz = 52.5, lambda_hat = 0.06002401 ) Estimate a 95% confidence interval for \\(\\hat{\\lambda}\\). [View Answer!] Estimate a 95% confidence interval for \\(\\hat{\\lambda}\\). # Get data... b = tibble(r = 25, days = 416.5, n = 25, tz = 52.5, lambda_hat = 0.06002401 ) # And estimate lambda hat! c &lt;- b %&gt;% summarize( lambda_hat = lambda_hat, r = r, k_upper = qchisq(0.975, df = 2*r) / (2*r), k_lower = qchisq(0.025, df = 2*r) / (2*r), lower = lambda_hat * k_lower, upper = lambda_hat * k_upper) # Check it c ## # A tibble: 1 Ã— 6 ## lambda_hat r k_upper k_lower lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0600 25 1.43 0.647 0.0388 0.0857 20.5 Planning Experiments When developing a new product, you often need to make that product meet regulatory standards, eg. meet a specific \\(\\lambda\\) failure rate requirement. But as we just learned, estimated failure rates are dependent on the sample size of products and time range evaluated, and are subject to error. Even if we have a product whose true failure rate should be within range, our experiment might yield a \\(\\hat{\\lambda}\\) that varies from it by chance and exceeds regulatory requirements. Further, (most) all experiments cost time and money! So we probably want to keep our experiment as short as possible (timely results) and use as few products as possible (less expensive). We often need to answer the following questions: Whatâ€™s the minimum time we should run our test to produce a failure rate \\(\\hat{\\lambda}\\) that we can say with confidence is within limits? Whatâ€™s the minimum sample size we can run our test on to produce a failure rate \\(\\hat{\\lambda}\\) that we can say with confidence is within limits? How confident are we that our product has a failure rate within a specific limit \\(\\hat{\\lambda}\\)? When planning an experiment, we can use the following formula to determine the necessary values of \\(r\\), \\(t\\), \\(lambda\\), \\(k\\), or \\(n\\) for our experiment to function: \\[ \\frac{r}{n \\ t} \\times k_{r, 1 - \\alpha} = \\hat{\\lambda} \\] Note: Experiment planning by definition makes time-censored datasets, so youâ€™ll need your adjusted \\(k\\) formula so that \\(df = 2(r+1)\\). 20.5.1 Example: Minimum Sample Size Letâ€™s try an example. Imagine a company wants to test a new line of masks. Your company can budget 500 hours to test these masksâ€™ elastic bands, and they accept that up to 10 of these masks may break in the process. Company policy anticipates a failure rate of just 0.00002 masks per hour, and we want to be 90% confidence that the failure rate will not exceed this level. (But itâ€™s fine if it goes below that level.) If we aim to fit within these constraints, how many masks need to be tested in this product trial? # Max failures r = 10 # Number of hours t = 500 # Max acceptable failure rate lambda = 0.00002 # Calcuate a one-tailed confidence interval alpha = 0.10 ci = 1 - alpha # Use these ingredients to calculate the factor k, using time-censored rule df = 2*(r+1) k = qchisq(ci, df = 2*(r+1)) / (2*r) # Reformat # r / (t*n) * k = lambda n = k * r / (lambda * t) # Check it! n ## [1] 1540.664 Looks like youâ€™ll need a sample of about 1541 masks to be able to fit those constraints. 20.6 Chi-squared Chi-squared (\\(\\chi^{2}\\)) is a special statistic used frequently to evaluate the relationship between two categorical variables. In this case, the first variable is the type of data (observed vs.Â model), while the second variable is bin. 20.6.1 Compute Chi-squared from scratch Back in Workshop 2, we visually compared several distributions to an observed distribution, to determine which fits best. But can we do that statistically? One way to do this is to chop our observed data into bins of length equal width using cut_interval() from ggplot2. We must specify: Step 1 Crosstabulate Observed Values into Bins. # Let&#39;s repeat our process from before! c1 &lt;- data.frame(t = masks$left_earloop) %&gt;% # Part 1.1: Split into bins mutate(interval = cut_interval(t, length = 5)) %&gt;% # Part 1.2: Tally up observed failures &#39;r_obs&#39; by bin group_by(interval, .drop = FALSE) %&gt;% summarize(r_obs = n()) %&gt;% mutate( bin = 1:n(), # give each bin a numeric id from 1 to inf # bin = as.numeric(interval), # you could alternatively turn the factor numeric lower = (bin - 1) * 5, upper = bin * 5, midpoint = (lower + upper) / 2) Step 2: Calculate Observed and Expected Values per Bin. Sometimes you might only receive tabulated data, meaning a table of bins, not the original vector. In that case, start from Step 2! # Get any parameters you need (might need to be provided if only tabulated data) mystat = masks %&gt;% summarize( # failure rate lambda = 1 / mean(left_earloop), # total number of units under test n = n()) # Get your &#39;model&#39; function f = function(t, lambda){ 1 - exp(-1*lambda*t) } # Note: pexp(t, rate) is equivalent to f(t, lambda) for exponential distribution # Now calculate expected units to fail per interval, r_exp c2 = c1 %&gt;% mutate( # Get probability of failure by time t = upper bound p_upper = f(t = upper, lambda = mystat$lambda), # Get probability of failure by time t = lower bound p_lower = f(t = lower, lambda = mystat$lambda), # Get probability of failure during the interval, # i.e. between these thresholds p_fail = p_upper - p_lower, # Add in total units under test n_total = mystat$n, # Calculate expected units to fail in that interval r_exp = n_total * p_fail) # Check it! c2 ## # A tibble: 11 Ã— 11 ## interval r_obs bin lower upper midpoint p_upper p_lower p_fail n_total ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 [0,5] 17 1 0 5 2.5 0.312 0 0.312 50 ## 2 (5,10] 13 2 5 10 7.5 0.527 0.312 0.215 50 ## 3 (10,15] 3 3 10 15 12.5 0.675 0.527 0.148 50 ## 4 (15,20] 7 4 15 20 17.5 0.776 0.675 0.102 50 ## 5 (20,25] 2 5 20 25 22.5 0.846 0.776 0.0699 50 ## 6 (25,30] 2 6 25 30 27.5 0.894 0.846 0.0481 50 ## 7 (30,35] 3 7 30 35 32.5 0.927 0.894 0.0331 50 ## 8 (35,40] 0 8 35 40 37.5 0.950 0.927 0.0227 50 ## 9 (40,45] 0 9 40 45 42.5 0.966 0.950 0.0156 50 ## 10 (45,50] 1 10 45 50 47.5 0.976 0.966 0.0108 50 ## 11 (50,55] 2 11 50 55 52.5 0.984 0.976 0.00740 50 ## # â„¹ 1 more variable: r_exp &lt;dbl&gt; # We only need a few of these columns; let&#39;s look at them: c2 %&gt;% # interval: get time interval in that bin # r_obs: Get OBSERVED failures in that bin # r_exp: Get EXPECTED failures in that bin # n_total: Get TOTAL units, failed or not, overall select(interval, r_obs, r_exp, n_total) ## # A tibble: 11 Ã— 4 ## interval r_obs r_exp n_total ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 [0,5] 17 15.6 50 ## 2 (5,10] 13 10.7 50 ## 3 (10,15] 3 7.38 50 ## 4 (15,20] 7 5.08 50 ## 5 (20,25] 2 3.49 50 ## 6 (25,30] 2 2.40 50 ## 7 (30,35] 3 1.65 50 ## 8 (35,40] 0 1.14 50 ## 9 (40,45] 0 0.782 50 ## 10 (45,50] 1 0.538 50 ## 11 (50,55] 2 0.370 50 Step 3: Calculate Chi-squared Statistic Chi-squared here represents the sum of ratios for each bin. Each ratio is the (1) squared difference between the observed and expected value over (2) the expected value. Ranges from 0 to infinity. The bigger (more positive) the statistic, the greater difference between the observed and expected data. Degrees of Freedom (df) is used as the standard deviation in the chi-squared distribution, to help evaluate how extreme is our chi-squared statistic? Chi-squared Distribution is a distribution of squared deviations from a normal distribution centered at 0. This means it only has positive values. c3 &lt;- c2 %&gt;% summarize( # Calculate Chi-squared statistic chisq = sum((r_obs - r_exp)^2 / r_exp), # Calculate number of bins (rows) nbin = n(), # Record number of parameters used (jjust lambda) np = 1, # Calculate degree of freedom df = nbin - np - 1) # Check it! c3 ## # A tibble: 1 Ã— 4 ## chisq nbin np df ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.2 11 1 9 Step 4: Calculate p-values and confidence intervals Last, letâ€™s use the pchisq() function to evaluate the CDF of the Chi-squared distribution. Weâ€™ll find out: p_value: whatâ€™s the probability of getting a value greater than or equal to (more extreme than) our observed chi-squared statistic? â€œstatistically significantâ€: if our observed statistic is more extreme than most possible chi-squared statistics (eg. &gt;95% of the distribution), itâ€™s probably not due to chance! We call it â€˜statistically significant.â€™ # Calculate area remaining under the curve c4 &lt;- c3 %&gt;% mutate(p_value = 1 - pchisq(q = chisq, df = df)) # Check c4 ## # A tibble: 1 Ã— 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.2 11 1 9 0.0847 For a visual representation: 20.6.2 Building a Chi-squared function That was a lot of work! It might be more helpful for us to build our own function doing all those steps. Hereâ€™s a function I built, which should work pretty flexibly. You can improve it, tweak it, or use it for your own purposes. Our function get_chisq() is going to need several kinds of information. Our exponential failure function f Our parameters, eg. failure rate lambda Total number of parameters (if exponential, 1) Our total number of units under test n Then, weâ€™ll need a observed vector of times to failure t, plus a constant binwidth (previously, 5). Or, if our data is pre-crosstabulated, we can ignore t and binwidth and just supply a data.frame data of crosstabulated vectors lower, upper, and r_obs. For example, these inputs might look like this: # Get your &#39;model&#39; function f = function(t, lambda){ 1 - exp(-1*lambda*t) } # Parameters # lambda = 1 / mean(masks$left_earloop) # number of parameters # np = 1 # total number of units (sometimes provided, if the data is time-censored) # n_total = length(masks$left_earloop) # AND # Raw observed data + binwidth # t = masks$left_earloop # binwidth = 5 # OR # Crosstabulated data (c1 is an example we made before) # data = c1 %&gt;% select(lower, upper, r_obs) Then, we could write out the function like this! Iâ€™ve added some fancy @ tags below just for notation, but you can ditch them if you prefer. This is a fairly complex function! Iâ€™ve shared it with you as an example to help you build your own for your projects. #&#39; @name get_chisq #&#39; @title Function to Get Chi-Squared! #&#39; @name Tim Fraser #&#39; If observed vector... #&#39; @param t a vector of times to failure #&#39; @param binwidth size of intervals (eg. 5 hours) (Only if t is provided) #&#39; If cross-tabulated data... #&#39; @param data a data.frame with the vectors `lower`, `upper`, and `r_obs` #&#39; Common Parameters: #&#39; @param n_total total number of units. #&#39; @param f specific failure function, such as `f = f(t, lambda)` #&#39; @param np total number of parameters in your function (eg. if exponential, 1 (lambda)) #&#39; @param ... fill in here any named parameters you need, like `lambda = 2.4` or `rate = 2.3` or `mean = 0, sd = 2` get_chisq = function(t = NULL, binwidth = 5, data = NULL, n_total, f, np = 1, ...){ # If vector `t` is NOT NULL # Do the raw data route if(!is.null(t)){ # Make a tibble called &#39;tab&#39; tab = tibble(t = t) %&gt;% # Part 1.1: Split into bins mutate(interval = cut_interval(t, length = binwidth)) %&gt;% # Part 1.2: Tally up observed failures &#39;r_obs&#39; by bin group_by(interval, .drop = FALSE) %&gt;% summarize(r_obs = n()) %&gt;% # Let&#39;s repeat our process from before! mutate( bin = 1:n(), lower = (bin - 1) * binwidth, upper = bin * binwidth, midpoint = (lower + upper) / 2) # Otherwise, if data.frame `data` is NOT NULL # Do the cross-tabulated data route }else if(!is.null(data)){ tab = data %&gt;% mutate(bin = 1:n(), midpoint = (lower + upper) / 2) } # Part 2. Calculate probabilities by interval output = tab %&gt;% mutate( p_upper = f(upper, ...), # supplied parameters p_lower = f(lower, ...), # supplied parameters p_fail = p_upper - p_lower, n_total = n_total, r_exp = n_total * p_fail) %&gt;% # Part 3-4: Calculate Chi-Squared statistic and p-value summarize( chisq = sum((r_obs - r_exp)^2 / r_exp), nbin = n(), np = np, df = nbin - np - 1, p_value = 1 - pchisq(q = chisq, df = df) ) return(output) } Finally, letâ€™s try using our function! Using a raw observed vector t: get_chisq( t = masks$left_earloop, binwidth = 5, n_total = 50, f = f, np = 1, lambda = mystat$lambda) ## # A tibble: 1 Ã— 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.2 11 1 9 0.0847 Or using crosstabulated data: get_chisq( data = c1, n_total = 50, f = f, np = 1, lambda = mystat$lambda) ## # A tibble: 1 Ã— 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.2 11 1 9 0.0847 Using our pexp function instead of our homemade f function: get_chisq( data = c1, n_total = 50, f = pexp, np = 1, rate = mystat$lambda) ## # A tibble: 1 Ã— 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.2 11 1 9 0.0847 Or using a different function that is not exponential! get_chisq( data = c1, n_total = 50, f = pweibull, np = 2, shape = 0.2, scale = 0.5) ## # A tibble: 1 Ã— 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 171. 11 2 8 0 Learning Check 4 Question A small start-up is product testing a new super-effective mask. They product tested 25 masks over 60 days. They contracted you to analyze the masksâ€™ lifespan data, recorded below as the number of days to failure. If you were to make projections from this data, youâ€™d want to be sure that you know the lifespan distribution of this data with confidence. Do these masksâ€™ lifespan distribution fit an exponential distribution? Follow the steps below to find out. # Lifespan in days supermasks &lt;- c(1, 2, 2, 2, 3, 3, 4, 4, 5, 9, 13, 15, 17, 19, 20, 21, 23, 24, 24, 24, 32, 33, 33, 34, 54) Cross-tabulate the lifespan distribution in intervals of 7 days. Estimate \\(\\hat{\\lambda}\\) from the cross-tabulated data, knowing that they tested 25 masks over 60 days. Using the cross-tabulated data, do these masksâ€™ lifespan distribution fit an exponential distribution, or does their distribution differ to a statistically significant degree from the exponential? How much? (eg. statistic and p-value). [View Answer!] Cross-tabulate the lifespan distribution in intervals of 7 days. # Days to failure a &lt;- data.frame(t = supermasks) %&gt;% # Step 1: Split into bins mutate(label = cut_interval(t, length = 7)) %&gt;% # Step 2: Tally up by bin group_by(label, .drop = FALSE) %&gt;% summarize(r_obs = n()) %&gt;% mutate( bin = 1:n(), lower = (bin - 1) * 7, upper = bin * 7, midpoint = (lower + upper) / 2) a ## # A tibble: 8 Ã— 6 ## label r_obs bin lower upper midpoint ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [0,7] 9 1 0 7 3.5 ## 2 (7,14] 2 2 7 14 10.5 ## 3 (14,21] 5 3 14 21 17.5 ## 4 (21,28] 4 4 21 28 24.5 ## 5 (28,35] 4 5 28 35 31.5 ## 6 (35,42] 0 6 35 42 38.5 ## 7 (42,49] 0 7 42 49 45.5 ## 8 (49,56] 1 8 49 56 52.5 Estimate \\(\\hat{\\lambda}\\) from the cross-tabulated data, knowing that they tested 25 masks over 60 days. # Let&#39;s estimate lambda-hat! b &lt;- a %&gt;% # Get the midpoint... mutate(midpoint = (upper - lower)/2 + lower) %&gt;% # Estimate lambda summarize( r = sum(r_obs), # total failures days = sum(midpoint*r_obs), # total failure-days n = r, # in this case, total failures = total obs tz = max(midpoint), # end of study period # Calculate lambda hat! lambda_hat = r / (days + (n - r)*tz)) # view it b ## # A tibble: 1 Ã— 5 ## r days n tz lambda_hat ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 416. 25 52.5 0.0600 Using the cross-tabulated data, do these masksâ€™ lifespan distribution fit an exponential distribution, or does their distribution differ to a statistically significant degree from the exponential? How much? (eg. statistic and p-value). # Get your &#39;model&#39; function f = function(t, lambda){ 1 - exp(-1*lambda*t) } # Get lambda-hat from b$lambda_hat # Step 3a: Calculate Observed vs. Expected get_chisq(data = a, n_total = 25, np = 1, f = f, lambda = b$lambda_hat) ## # A tibble: 1 Ã— 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.85 8 1 6 0.131 # Step 3b: Calculate Chi-squared if we had received the vector supermasks all along, assuming that were the whole population # Get lambda from directly from the data f = function(t, lambda){ 1 - exp(-1*lambda*t) } get_chisq(t = supermasks, binwidth = 7, n_total = 25, np = 1, f = f, lambda = 1 / mean(supermasks) ) ## # A tibble: 1 Ã— 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.72 8 1 6 0.137 The slight difference in results is due to b$lambda_hat being slightly different from 1 / mean(supermasks), the true empirical failure rate. 20.7 Conclusion All done! Youâ€™ve just picked up some powerful statistical techniques for estimating product lifespans in R. "],["useful-life-distributions-weibull-gamma-lognormal.html", "21 Useful Life Distributions (Weibull, Gamma, &amp; Lognormal) Getting Started 21.1 Maximum Likelihood Estimation (MLE) Learning Check 1 21.2 Gamma Distribution Learning Check 2 21.3 Weibull Distribution Learning Check 3 Learning Check 4 21.4 Lognormal Distribution Learning Check 5 Learning Check 6 21.5 Conclusion", " 21 Useful Life Distributions (Weibull, Gamma, &amp; Lognormal) In this workshop, weâ€™re going to continue learning some R functions for working with common life distributions, specifically the Weibull, Gamma, and Log-Normal distributions. Getting Started Load Packages Letâ€™s start by loading the tidyverse package. Weâ€™ll also load mosaicCalc, for taking derivatives and integrals (eg. D() and antiD()). # Load packages library(tidyverse) library(mosaicCalc) Load Data In this workshop, weâ€™re going to use a dataset of crop lifetimes! (Agriculture needs statistics too!) For each crop (id), we measured its lifespan in days from the day its seed is planted to the time it is ripe (usually about 90 days). # Import crop lifespan data! crops &lt;- read_csv(&quot;workshops/crops.csv&quot;) 21.1 Maximum Likelihood Estimation (MLE) In our last few workshops, we learned that the exponential distribution does a pretty good job of approximating many life distributions, and that we can evaluate the fit of a distribution using a chi-squared test. In this workshop, weâ€™ll learn several alternative, related distributions that might fit your data even better depending on the scenario. But modeling any data using a distribution requires that we know the parameters that best match the observed data. This can be really hard, as we found out with \\(\\hat{\\lambda}\\) in the exponential distribution. Below, weâ€™ll learn a computational approach called Maximum Likelihood Estimation (MLE), that will help us find the true (most likely) values for any parameter in any dataset. Itâ€™s pretty robust if the number of failures is â€˜largeâ€™ enough, where â€˜largeâ€™ counts as &gt;10 (which is really small!). 21.1.1 Likelihood Function MLE involves 3 ingredients: sample: a vector of raw empirical data probability density function (PDF): tells us probability (relative frequency) of each value in a sample occurring. likelihood function: tells us probability of getting this EXACT sample, given the PDF values for each observed data point. We can get the â€œprobabilityâ€ of any sample by multiplying the density function f(t) at each data point \\(t_{i}\\), for r data points. Multiplication indicates joint probability, so this product really means how likely is is to get this specific sample. We call it the likelihood of that sample. \\[ LIK = \\prod_{i = 1}^{r}{ f(t_i) } = f(t_1) \\times f(t_2) \\times ... f(t_n) \\] # Let&#39;s write a basic pdf function d = function(t, lambda){lambda * exp(-t*lambda) } # Let&#39;s imagine we already knew lambda... mylambda = 0.01 crops %&gt;% # Calculate probability density function for observed data mutate(prob = d(days, lambda = mylambda)) %&gt;% summarize( # Let&#39;s calculate the likelihood likelihood = prob %&gt;% prod(), # Since that&#39;s a really tiny number, # let&#39;s take its log to get log-Likelihood loglik = prob %&gt;% prod() %&gt;% log()) ## # A tibble: 1 Ã— 2 ## likelihood loglik ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7.55e-116 -265. Technically, our likelihood above shows the joint probability of each of these observed values occurring together supposing that: they have an exponential probability density function (PDF), andâ€¦ that the parameter lambda in that PDF equals 0.01. So we can get different (higher/lower) likelihoods of these values occuring together if we supply (1) different parameter values and therefore (2) a different hypothesized PDF. The most accurate parameters will be the ones that maximize the likelihood. In practice though, the likelihood is teeny-tiny, and multiplying tiny numbers is hard! Fortunatelyâ€¦ the log of a tiny value makes it bigger and easier to read and compute. the log of a product of values actually equals the sum of the log of those values (see equation below!). \\[ log ( \\ L(t) \\ ) = \\log\\prod_{i=1}^{n}{ f(t_i) = \\sum_{i=1}^{n}\\log( \\ f(t_i) \\ ) } \\] ll = function(data, lambda){ # Calculate Log-Likelihood, # by summing the log d(t = data, lambda) %&gt;% log() %&gt;% sum() } 21.1.2 Maximizing with optim() We can use the optim() function from base R to: run our function (fn = â€¦) many times, varying the value of lambda (or any parameter). supply any other parameters like data = crops$days to each run. identify the log-likelihood that is greatest (if control = list(fnscale = -1)); by default, optim() actually minimizes otherwise. return the corresponding value for lambda (or any other parameter). optim() will output a $par (our parameter estimate(s)) and $value (the maximum log-likelihood). # Maximize the log-likelihood! optim(par = c(0.01), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.014375 ## ## $value ## [1] -262.167 ## ## $counts ## function gradient ## 18 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 21.1.3 Under the Hood Wondering whatâ€™s happening inside optim()? Good news: itâ€™s not too tough. We can actually code and visualize it ourselves! First, letâ€™s get all the log-likelihoods in that interval. # Let&#39;s manyll &lt;- data.frame(parameter = seq(from = 0.00001, to = 1, by = 0.001)) %&gt;% # For each parameter, get the loglikelihood group_by(parameter) %&gt;% summarize(loglik = ll(data = crops$days, lambda = parameter)) # Check a few manyll %&gt;% head(3) ## # A tibble: 3 Ã— 2 ## parameter loglik ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00001 -576. ## 2 0.00101 -348. ## 3 0.00201 -317. Letâ€™s maximize the log-likelihood manuallyâ€¦ # Now find the parameter that has the greatest log-likelihood output &lt;- manyll %&gt;% filter(loglik == max(loglik)) #check it output ## # A tibble: 1 Ã— 2 ## parameter loglik ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0140 -262. Now, letâ€™s visualize it! ggplot() + geom_line(data = manyll, mapping = aes(x = parameter, y = loglik), color = &quot;steelblue&quot;) + geom_vline(xintercept = output$parameter, linetype = &quot;dashed&quot;) + theme_classic(base_size = 14) + labs(x = &quot;parameter (lambda)&quot;, y = &quot;loglik (Log-Likelihood)&quot;, subtitle = &quot;Maximizing the Log-Likelihood (Visually)&quot;) + # We can actually adust the x-axis to work better with log-scales here scale_x_log10() + # We can also annnotate our visuals like so. annotate(&quot;text&quot;, x = 0.1, y = -2000, label = output$parameter) 21.1.4 Multi-parameter optimization Multi-parameter optimization works pretty similarly. We can use optim() for this too! The key here is that optim() only varies whatever values we supply to optim(par = ...). So if we have multiple parameters, like the mean and sd for the normal distribution, we need to put both parameters into one vector in our input par, like so: # Let&#39;s write a new function ll = function(data, par){ # Our parameters input is now going to be vector of 2 values # par[1] gives the first value, the mean # par[2] gives the second value, the standard deviation dnorm(data, mean = par[1], sd = par[2]) %&gt;% log() %&gt;% sum() } Now, letâ€™s maximize! # Let&#39;s optimize it! # put in some reasonable starting values for &#39;par&#39; and it optim() will do the rest. optim(par = c(90, 15), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 69.65196 47.56853 ## ## $value ## [1] -264.0527 ## ## $counts ## function gradient ## 51 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL That was ridiculously easy! Letâ€™s compare to their known values: crops %&gt;% summarize(mu = mean(days), sigma = sd(days)) ## # A tibble: 1 Ã— 2 ## mu sigma ## &lt;dbl&gt; &lt;dbl&gt; ## 1 69.6 48.0 Almost spot on! Pretty good for an estimation technique! 21.1.5 MLE with Censored Data But what do we do if we need to estimate parameters with MLE, but our data is censored (eg. time censored, or cross-tabulated?) Well, fortunately, same general rules apply! \\[ LIK = k \\times ( \\Pi_{i = 1}^{r}{f(t_i) \\ } \\times [1 - F(T_{max})]^{n-r} ) \\approx \\prod_{i=1}^{r}{ f(t_i) } \\times R(T_{max})^{n-r} \\] We can re-write the likelihood function as the joint probability of each time to failure \\(f(t_i)\\) for \\(r\\) total failures that did occur, times the cumulative probability that they did not fail by the end of the study period \\(T_{max}\\) for \\(n - r\\) potential failures that did not occur. Just like when estimating lambda though, we know that thereâ€™s room for error, so we can calculate a \\(k\\) factor like before to upweight or downweight our likelihood statistic, giving us upper and lower confidence intervals. Most often, we are looking for the precise estimate of parameters, not upper and lower confidence intervals for them, so in those cases, \\(k\\) can be omitted, simplifying down to the right-side equation above. Letâ€™s try a few applications of this formula below. 21.1.5.1 Time Censored Data Suppose we actually had \\(n = 75\\) crops, but we only ever saw \\(r = 50\\) of them fail, and we stopped recording after 200 days. How would we estimate the maximum likelihood to get our parameter of interest \\(\\lambda\\)? First, letâ€™s write our functions. # Let&#39;s write a basic pdf function &#39;d&#39; and cdf function &#39;f&#39; d = function(t, lambda){ lambda * exp(-t*lambda) } f = function(t, lambda){ 1 - exp(-t*lambda)} Now, using our observed (but incomplete data in crops), letâ€™s calculate the log-likelihood if \\(\\lambda = 0.01\\). crops %&gt;% summarize( # Get total failures observed r = n(), # Get total sample size, n = 75, # Get last timestep tmax = 200, # Take the product of the PDF at each timestep prob_d = d(t = days, lambda = 0.01) %&gt;% prod(), # Get probability of survival by the last time step, # for as many observations as did not fail prob_r = (1 - f(t = tmax, lambda = 0.01))^(n - r), # Get likelihood lik = prob_d * prob_r, # Get loglikelihood loglik = log(lik)) ## # A tibble: 1 Ã— 7 ## r n tmax prob_d prob_r lik loglik ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 50 75 200 7.55e-116 1.93e-22 1.46e-137 -315. Great! Letâ€™s formalize that in a function, for any value of \\(\\lambda\\). # We&#39;ll write a log-likelihood function &#39;ll&#39; # that takes a data input and a single numeric parameter par ll = function(data, par){ output &lt;- data %&gt;% summarize( r = n(), n = 75, tmax = 200, prob_d = d(t = days, lambda = par) %&gt;% prod(), prob_r = (1 - f(t = tmax, lambda = par))^(n - r), loglik = log(prob_d * prob_r)) # Return the output output$loglik } # Last, we can run &#39;optim&#39; to get the MLE, with a starter guess of lambda at 0.05 optim(par = c(0.01), data = crops, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.005898438 ## ## $value ## [1] -306.6839 ## ## $counts ## function gradient ## 22 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 21.1.5.2 Cross-Tabulated, Time-Censored Data Alternatively, what if our data were not only time-censored, but had been cross-tabulated!? The crosstab data.frame below encodes days to failure for our crops, tallied up in 40 day intervals, supposing a total sample of 75 sampled crops evaluated for 200 days. We can still estimate parameters with MLE using the formula above! crosstab &lt;- data.frame( label = c(&quot;[0,40]&quot;, &quot;(40,80]&quot;, &quot;(80,120]&quot;, &quot;(120,160]&quot;, &quot;(160,200]&quot;), t = c(20, 60, 100, 140, 180), count = c(18, 14, 10, 5, 3)) We can write our log-likelihood function very similarly to above. The main change is that we add up the counts to get r, the total observed failures. To estimate our probabilities, we just use the interval midpoints as our days to failure t. ll = function(data, par){ output &lt;- data %&gt;% summarize( # Get total failures observed (sum of all tallies) r = sum(count), # Get total sample size, n = 75, # Get last timestep tmax = 200, # Get PDF at each timestep; take product for each failure observed prob_d = d(t = t, lambda = par)^count %&gt;% prod(), # Get probability of survival by the last time step, # for as many n-r observations that did not fail prob_r = (1 - f(t = tmax, lambda = par))^(n - r), # Get log-likelihood loglik = log(prob_d * prob_r)) # Return the output output$loglik } # Last, we can run &#39;optim&#39; to get the MLE, with a starter guess of lambda at 0.05 optim(par = c(0.01), data = crosstab, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.005925781 ## ## $value ## [1] -306.4357 ## ## $counts ## function gradient ## 24 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL The estimate provided is a little different than the one above, but itâ€™s still in a very similar ballpark. When working with cross-tabulations but estimating complex parameters, this estimation strategy can be a life-saver (literally). Learning Check 1 Question Weâ€™ve learned several probability density functions for different distributions, including dexp(), dgamma(), dpois(), and dweibull(). Use optim() to maximize the likelihood of each of these PDFsâ€™ likelihood, using our crops data. [View Answer!] For the exponential distributionâ€¦ ll = function(data, par){ dexp(data, rate = par) %&gt;% log() %&gt;% sum() } optim(par = c(0.01), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.014375 ## ## $value ## [1] -262.167 ## ## $counts ## function gradient ## 18 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL For the gamma distributionâ€¦ ll = function(data, par){ dgamma(data, shape = par[1], scale = par[2]) %&gt;% log() %&gt;% sum() } optim(par = c(1, 1), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 1.921441 36.239762 ## ## $value ## [1] -256.9295 ## ## $counts ## function gradient ## 83 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL For the Poisson distributionâ€¦ ll = function(data, lambda){ dpois(data, lambda) %&gt;% log() %&gt;% sum() } # Optimize! optim(par = 90, data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 69.63574 ## ## $value ## [1] -942.9518 ## ## $counts ## function gradient ## 26 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL For the Weibull distributionâ€¦ ll = function(data, par){ dweibull(data, shape = par[1], scale = par[2]) %&gt;% log() %&gt;% sum() } # Optimize! optim(par = c(1,1), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 1.492828 77.106302 ## ## $value ## [1] -256.775 ## ## $counts ## function gradient ## 107 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL remove(ll, d, output, mylambda, manyll) 21.2 Gamma Distribution Alternatively, the Gamma distribution is well suited to modeling products exposed to a series of shocks over time at a given rate over time. 21.2.1 \\(\\Gamma\\) function gamma() In the gamma distribution, events are exposed to \\(k\\) shocks. We often will use something called a gamma function \\(\\Gamma\\) of \\(k\\) to show the total ways a series of shocks could have occurred. Gamma functions are written \\((k - 1)!\\), where \\(!\\) means â€œfactorialâ€, or taking the product of k by every prior integer. Since, \\((k - 1)! = \\Gamma(k)\\), then if \\(k = 4\\), then \\(\\Gamma(k = 4) = (4 - 1) \\times (3 - 1) \\times (2 - 1) = 3 \\ \\times 2 \\times 1 = 6\\). We can quickly code in this in r using the gamma() function. For example, in the prior example, gamma(4) renders 6. Pretty quick, right? We can also code this directly using the factorial() function. For example, if k = 4, we can write factorial(4 - 1). # Let&#39;s try out the gamma function. gamma(4) ## [1] 6 # Produces same output as factorial(k - 1) factorial(4 - 1) ## [1] 6 21.2.2 \\(f(t)\\) or d(t)(Probability Density Function, aka PDF) Excitingly, we can reapply most of the same rules we learned for exponential distributions; we just have to update our functions. In the gamma distribution, events are exposed to \\(k\\) shocks that occur at rate \\(\\lambda\\) over time \\(t_1, t_2, ... t_n\\). We can model the time to failure for such a function like so! \\[f(t) = \\frac{\\lambda}{(k - 1)!}(\\lambda t)^{k-1}e^{-\\lambda t}\\] We can also write \\((k - 1)!\\) above as \\(\\Gamma(k)\\). As mentioned above, this is called a \"Gamma function of k\", which is where the distributionâ€™s name comes from. \\(k\\) effectively controls the shape of the function. # Let&#39;s write our new PDF function, d(t); written as f(t) above d = function(t, k, lambda){ lambda / factorial(k - 1) * (lambda*t)^(k-1) * exp(-t*lambda) } # Try it out! d(t = 1, k = 1, lambda = 1) ## [1] 0.3678794 # Compare with our dgamma() function! dgamma(x = 1, shape = 1, rate = 1) ## [1] 0.3678794 # They&#39;re the same! 21.2.3 \\(F(t)\\) Failure Function and \\(R(t)\\) Reliability Function The failure and reliability function are always closely related, no matter the distribution, where \\(R(t) = 1 - F(t)\\). We can write the failure function \\(F(t)\\) (a.k.a. the CDF) as: \\[ F(t) = 1 - \\sum_{n = 0}^{k - 1}{\\frac{(\\lambda t)^n }{n!}e^{-\\lambda t}}\\] Therefore, we can also write the reliability function \\(R(t)\\) as: \\[ R(t) = \\sum_{n = 0}^{k - 1}{\\frac{(\\lambda t)^n }{n!}e^{-\\lambda t}}\\] # Write the failure function for Gamma distribution f = function(t, k, lambda){ # Make a vector of values from 0 to k-1 n = seq(from = 0, to = k - 1) # Now compute the failure function 1 - sum( (lambda*t)^n / factorial(n) * exp(-lambda*t) ) } # We can also integrate the PDF to get the CDF, alternatively, using mosaicCalc fc = antiD(tilde = d(t, k, lambda) ~ t) # Let&#39;s compare! # Using pgamma() pgamma(q = 10,shape = 3,rate = 1) ## [1] 0.9972306 # Using our calculate-based fc() fc(t = 10, k = 3, lambda = 1) ## [1] 0.9972306 # Using our direct function f() f(t = 10, k = 3, lambda = 1) ## [1] 0.9972306 # All the same! Correspondingly, the reliability function \\(R(t)\\) would beâ€¦ # Write the reliability function r = function(t, k, lambda){ # Make a vector of values from 0 to k-1 n = seq(from = 0, to = k - 1) # Now compute the reliability function sum( (lambda*t)^n / factorial(n) * exp(-lambda*t) ) } # Get reliability function via calculus... rc = function(t,k,lambda){ # compute CDF fc = antiD(tilde = d(t, k, lambda) ~ t) # return 1 - CDF 1 - fc(t = t, k = k, lambda = lambda) } # Compare outputs! # with pgamma() 1 - pgamma(q = 10, shape = 3, rate = 1) ## [1] 0.002769396 # with rc() rc(t = 10, k = 3, lambda = 1) ## [1] 0.002769396 # with r() r(t = 10, k = 3, lambda = 1) ## [1] 0.002769396 # They&#39;re the same! \\(z(t)\\) Failure Rate function The failure rate remains \\(z(t) = \\frac{f(t)}{R(t)}\\). Further, in the gamma distribution, the rate remains constant, although the size parameter causes a result quite different from the exponential! # Let&#39;s write the failure rate function z = function(t,k,lambda){ # We&#39;ll break it up into parts to help readability # Compute the PDF d(t) (or f(t)) d_of_t = lambda / factorial(k - 1) * (lambda*t)^(k-1) * exp(-t*lambda) # Make a vector of values from 0 to k-1 n = seq(from = 0, to = k - 1) # Now compute the reliability function R(t) r_of_t = sum( (lambda*t)^n / factorial(n) * exp(-lambda*t) ) # Divide the two to get the failure rate! d_of_t / r_of_t } # Or, using calculus.... zc = function(t, k, lambda){ # Get CDF via integration fc = antiD(tilde = d(t, k, lambda) ~ t) # Get r(t) using 1 - f(t) # Take PDF / (1 - CDF) d(t = t, k = k, lambda = lambda) / (1 - fc(t = t, k = k, lambda = lambda) ) } # Try it out! # Using dgamma() and pgamma() dgamma(1,shape = 1, rate = 0.1) / (1 - pgamma(1,shape = 1, rate = 0.1)) ## [1] 0.1 # Using our calculus based function zc(t = 1, k = 1, lambda = 0.1) ## [1] 0.1 # Using our home cooked function z(t = 1, k = 1, lambda = 0.1) ## [1] 0.1 21.2.4 MTTF and Variance We can also compute the mean time to fail (MTTF) and variance using the following formulas. \\[ MTTF = \\int_{0}^{\\infty}{ t \\ f(t) dt} = \\frac{k}{\\lambda} \\] \\[ Var(t) = \\frac{k}{ \\lambda^2} \\] We can encode these as functions as follows. # Mean (aka mean time to fail) mttf = function(k, lambda){ k / lambda } # Check it! mttf(k = 1, lambda = 0.1) ## [1] 10 # Variance variance = function(k, lambda){ k / lambda^2 } # Try it! variance(k = 1, lambda = 0.1) ## [1] 100 Learning Check 2 Question A car bumper has demonstrated a gamma distribution with a failure rate of \\(\\lambda = 0.005\\) per day given \\(k = 3\\) potential shocks. Determine the reliability of this bumper over a 30, 300, and 600 day period. [View Answer!] # Compute a reliability function using pgamma() r = function(t,k,lambda){ 1 - pgamma(t,shape = k, rate = lambda) } # For 24 hours r(t = c(30, 300, 600), k = 3, lambda = 0.005) ## [1] 0.9994971 0.8088468 0.4231901 21.3 Weibull Distribution Another popular distribution is the Weibull distribution. The exponential is a special case of Weibull distribution where the failure rate \\(\\lambda\\) is held constant such that \\(m = 1\\). But in a usual Weibull distribution, the failure rate can change over time! This makes it very flexible, able to take on the shape of an exponential, gamma, or normal distribution depending on the parameters. We can show this easily using the Weibullâ€™s cumulative hazard function \\(H(t) = (\\lambda t)^m\\). If \\(m = 1\\), as in the exponential distribution, then \\(H(t) = \\lambda t\\), as in the exponential distribution. But if \\(m \\neq 1\\), then the accumulative hazard rate increases to the \\(m\\) power with ever passing hour. We can use this to derive the failure function \\(F(t)\\), reliability function \\(R(t)\\), and failure rate \\(z(t)\\). (Similarly, we can use all the same tricks from before to get the \\(AFR(t_1, t_2)\\), like using the accumulative hazard function \\(\\frac{H(t_2) - H(t_1)}{t_2 - t_1}\\).) 21.3.1 \\(F(t)\\) Failure Function and \\(R(t)\\) Reliability Function To derive \\(F(t)\\), we can sub in the new formula for the accumulative hazard function into the formula for the failure function. The Weibull failure function is also commonly written replacing \\(\\lambda\\) with \\(c = \\frac{1}{\\lambda}\\), the characteristic life. \\[ F(t) = 1 - e^{-H(t)} = 1 - e^{-(\\lambda t)^m} = 1 - e^{-(t/c)^m}\\] We can code it like so: f = function(t, m, c){ 1 - exp(-1*(t/c)^m) } # Compare! # Using pweibull() pweibull(1, shape = 1, scale = 1) ## [1] 0.6321206 # Using our home-made function! f(t = 1, m = 1, c = 1) ## [1] 0.6321206 Similarly, we can write the reliability function asâ€¦ r = function(t, m, c){ exp(-1*(t/c)^m) } # Try it with pweibull() 1 - pweibull(1, shape = 1, scale = 1) ## [1] 0.3678794 # Using our home-made function! r(t = 1, m = 1, c = 1) ## [1] 0.3678794 21.3.2 \\(f(t)\\) or d(t) (PDF) While not nearly as straightforward to derive, we can literally take the derivative of the Failure function to get the PDF. \\[ f(t) = \\frac{m}{t} \\times (\\frac{t}{c})^m \\times e^{-(t/c)^m}\\] d = function(t, m, c){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) # alternatively written using calculus: # D(f(t,m,c) ~ t) } # Compare results! dweibull(1, shape = 1, scale = 1) ## [1] 0.3678794 d(t = 1, m = 1, c = 1) ## [1] 0.3678794 21.3.3 \\(z(t)\\) Failure Rate We can similarly derive the failure rate \\(z(t)\\) by taking the derivative of the accumulative hazard rate, which gives us: \\[z(t) = m \\lambda (\\lambda t)^{m-1} = (m/c) \\times (t/c)^{m-1} \\]/â€™ z = function(t, m, c){ (m/c) * (t/c)^(m-1) } # try it! z(1, m = 1, c = 0.1) ## [1] 10 # using dweibull() and pweibull() dweibull(1, shape = 1, scale = 0.1) / (1 - pweibull(1, shape = 1, scale = 0.1)) ## [1] 10 21.3.4 \\(m\\) and \\(c\\) Fortunately, if any 3 of the 4 parameters (\\(F(t), m, t, c\\) are known about a Weibull distribution, the remaining parameter can be calculated. Since \\(F(t) = 1 - e^{-(t/c)^m}\\), we can say: \\[ t = c \\times ( -log( 1 - F ))^{1/m} \\] \\[m = \\frac{log( -log( 1 - F) )}{log(t/c)}\\] \\[c = \\frac{t}{( -log(1 - F) )^{1/m}}\\] Characteristic life \\(c\\), in this case, really means the time at which 63.2% of units will consistently have failed. Shape parameter \\(m\\) is used to describe several different types of Weibull distributions. \\(m = 1\\) is an exponential; \\(m = 2\\) is a Reyleigh distribution, where the failure rate increases linearly. When \\(m &lt; 1\\), it looks like the left-end of a bath-tub. When \\(m &gt; 1\\), it looks like the right-end of a bath-tub. 21.3.5 Weibull Series Systems In a series system of independent components, which are each Weibull distributed with the same shape parameter \\(m\\), that system has a Weibull distribution, as follows: \\[ c_{series} = (\\sum_{i=1}^{n}{ \\frac{1}{c_i^m}})^{-\\frac{1}{m}} \\] 21.3.6 MTTF and Variance Finally, we could code the mean time to fail and variance as follows. \\[ MTTF = c \\Gamma(1 + \\frac{1}{m}) = c \\times (k - 1)! \\times (1 + \\frac{1}{m}) \\] \\[ Variance = c^2 \\Gamma(1 + \\frac{2}{m} ) - [ c \\Gamma(1 + \\frac{1}{m})]^{2} \\] # We could code these up simply like so mttf = function(c, m){ c * gamma( 1 + 1/m) } # Though we know that mttf = integral of R(t), so we could also write it like this mttf = antiD((1 - pweibull(t, shape = m, scale = c)) ~ t) # We could code these up simply like so variance = function(c, m){ c^2 * gamma( 1 + 2/m ) - ( c * gamma( 1 + 1/m) )^2 } Learning Check 3 Question Youâ€™ve done this a bunch now - for this learning check, write your own function to find \\(t\\), \\(m\\), and \\(c\\) in a Weibull distribution. [View Answer!] # Write a function to get t get_t = function(f,m,c){ log(-log(1 - f) )/log(t/c) } # Write a function to get m get_m = function(t,f,c){ log(-log(1 - f) )/log(t/c) } # Write a function to get c get_c = function(t,f,m){ t / ( (-log(1 - f))^(1/m) ) } Learning Check 4 Question Find the characteristic life necessary for 10% of failures to occur by 168 hours, if the shape parameter \\(m = 2\\). Then, using that characteristic life, plot the probability of failure when \\(m = 1\\), \\(m = 2\\), and \\(m = 3\\). [View Answer!] # Write our function to find c get_c = function(t, f, m){ t / ( (-log(1 - f))^(1/m) ) } get_c(t = 168, f = 0.10, m = 2) ## [1] 517.5715 # Looks like we expect a characteristic life of &gt;500 hours. # Write the failure function f = function(t, c, m){ 1 - exp(-1*(t/c)^m) } m1 &lt;- data.frame(hours = 1:100) %&gt;% mutate(prob = f(t = hours, c = 517.5715, m = 1), m = &quot;m = 1&quot;) m2 &lt;- data.frame(hours = 1:100) %&gt;% mutate(prob = f(t = hours, c = 517.5715, m = 2), m = &quot;m = 2&quot;) m3 &lt;- data.frame(hours = 1:100) %&gt;% mutate(prob = f(t = hours, c = 517.5715, m = 3), m = &quot;m = 3&quot;) bind_rows(m1,m2,m3) %&gt;% ggplot(mapping = aes(x = hours, y = prob, color = m)) + geom_line() 21.4 Lognormal Distribution The log-normal distribution can be very useful, often in modeling semi-conductors, among other product. To understand the log-normal distribution, letâ€™s outline the normal distribution PDF, and compare it to the log-normal distribution PDF. 21.4.1 \\(f(t)\\) or d(t) (PDF) The normal distribution contains the parameters \\(\\mu\\) (mean) and \\(\\sigma\\) (standard deviation). \\[f(t) = \\frac{1}{\\sigma \\sqrt{2\\pi} } \\times e^{-(t - \\mu)^2 / 2\\sigma^2 } \\] The log-normal distribution is quite similar. Here, \\(t\\) becomes \\(e^t\\) and median \\(T_{50} = e^{\\mu}\\). \\(\\sigma\\) is really more of a shape parameter here than the standard deviation as we usually think of it. \\[ f(t) = \\frac{1}{\\sigma t \\sqrt{2\\pi}} \\times e^{-(1/2\\sigma^2) \\times (log(t) - log(T_{50} ))^2}\\] 21.4.2 \\(\\Phi\\) Interestingly, the CDF of the log-normal relies on the CDF of the normal distribution, sometimes written as \\(\\Phi\\). Both distributionsâ€™ CDFs are written below. \\[ Normal \\ F(t) = \\Phi ( \\frac{t - \\mu}{ \\sigma } ) \\] \\[ Log-Normal \\ \\ F(t) = \\Phi (\\frac{log(t / T_{50})}{\\sigma}) \\] We can write the normal distributionâ€™s CDF, \\(\\Phi\\), as the cumulative probability of a lifespan \\(t\\) that is \\(\\frac{t - \\mu}{\\sigma}\\) standard deviations away from the mean. In the log-normal CDF, we transform our value \\(t\\) into a point measured in standard deviations from the mean. These points are called z-scores. We can feed that value to pnorm() to find out the cumulative probability of reaching that point on a normal distribution, with a mean of 0 and a standard deviation of 1. # Suppose... t = 50 t50 = 100 sigma = 2 # Get the cumulative probability of failure at time t p &lt;- pnorm( log(t/t50) / sigma ) # Check it! p ## [1] 0.3644558 Similarly, if we have a cumulative probability like p, we can solve for the z-score that made it using qnorm(). qnorm() is sometimes referred to as inverse-phi (\\(\\Phi^{-1}\\)). # Get the z-score for F(t) = 0.36! z &lt;- qnorm(p) # Check it! z ## [1] -0.3465736 Now that we know the z-score, we can easily solve for t, t50, or sigma with algebra if we know the other parameters, since \\(z = \\frac{log(t/T_{50})}{\\sigma}\\)! To get tâ€¦ # t = exp(z * sigma) * t50 exp(z * sigma) * t50 ## [1] 50 To get t50â€¦ # t50 = t / exp(z * sigma) t / exp(z * sigma) ## [1] 100 To get sigmaâ€¦ # sigma = log(t/t50) / z log(t/t50) / z ## [1] 2 Pretty quick, right? 21.4.3 Key Functions This oneâ€™s pretty messy. (One can use the dlnorm() function, but it contains 2 parameters, meanlog and sdlog, which are NOT \\(T_{50}\\) (median) and \\(\\sigma\\) (shape). For six-sigma, it tends to be more helpful for us to write the log-normal functions in terms of \\(T_{50}\\) and \\(\\sigma\\) instead.) # Write the pdf of the log-normal, f(t) or d(t)! d = function(t, t50, sigma){ 1 / (sigma * t * sqrt(2*pi)) * exp( -(1/ 2 * sigma^2 )*(log(t) - log(t50))^2) } # Write the failure function of the log-normal F(t)! f = function(t, t50, sigma){ pnorm( log(t / t50) / sigma ) } # Or with mosaicCalc via derivation... # dc = D(f(t,t50, sigma)~t) # Write the reliability function R(t)! r = function(t, t50, sigma){ 1 - pnorm( log(t / t50) / sigma ) } # Write the failure rate z(t)! z = function(t, t50, sigma){ d(t, t50, sigma) / r(t, t50, sigma) } # Write the accumulative hazard function H(t)! h = function(t, t50, sigma){ -log(r(t, t50, sigma)) } # Write the average failure rate function AFR(t) afr = function(t1, t2, t50, sigma){ (h(t2) - h(t1) ) / (t2 - t1) } Wow! That was surprisingly easy! In this way, as we pick up more and more distributions in this course, our tools for generating failure and reliability functions get easier and easier too! 21.4.4 MTTF and Variance Finally, we can quickly calculate these quantities of interest too. \\[ MTTF = T_{50} \\times e^{\\sigma^2 / 2} \\] \\[ Variance = (T_{50})^{2} \\times e^{\\sigma^2} \\times (e^{\\sigma^2} - 1)\\] # We could code these up simply like so mttf = function(median, sigma){ median * exp(sigma^2 / 2) } # We could code these up simply like so variance = function(median, sigma){ median^2 * exp(sigma^2) * (exp(sigma^2) - 1) } 21.4.5 Using Other Parameters Itâ€™s important to note that there are other ways to specify the log-normal distribution in R too. For example, these ways of writing the mean time to failure (MTTF) are all equivalent. # Using the median... mttf = function(median, sigma){ median * exp(sigma^2 / 2) } mttf(median = 1, sigma = 3) ## [1] 90.01713 # Using the meanlog, which is equal to the log of the median mttf = function(meanlog, sigma){ exp(meanlog) * exp(sigma^2 / 2) } mttf(meanlog = log(1), sigma = 3) ## [1] 90.01713 # Using the meanlog, written a different way mttf = function(meanlog, sigma){ exp(meanlog + sigma^2 / 2) } mttf(meanlog = log(1), sigma = 3) ## [1] 90.01713 There are also multiple ways to calculate the variance(). # Requires the median, as well as sigma variance1 = function(median, sigma){ median^2 * exp( sigma^2 ) * (exp(sigma^2) - 1) } variance1(median = 2, sigma = 3) ## [1] 262607464 # Requires a statistic called the meanlog, as well as sigma variance2 = function(meanlog, sigma){ (exp(sigma^2) - 1) * exp(2*meanlog + sigma^2) } variance2(meanlog = log(2), sigma = 3) ## [1] 262607464 You could also calculate your own sigma() function if you chose. # Let&#39;s write our own sigma function... sigma = function(t, t50, prob){ # Let&#39;s derive it... #prob = pnorm(log(t/t50)/sigma) #qnorm(prob) = log(t/t50) / sigma sigma = log(t/t50) / qnorm(prob) return(sigma) } Learning Check 5 Question A crop tends to grow to a median height of 2 feet tall. We know from past data that this crop has a lognormal distribution, with a shape parameter \\(\\sigma\\) of about 0.2 feet. Market standards prefer crops between 1.5 and 8 feet tall. Given these standards, what percentage of crops are not eligible for market? [View Answer!] # Get percent under 1.5 feet below &lt;- f(1.5, t50 = 2, sigma = 0.2) # Get percentage over 8 feet above &lt;- 1 - f(8, t50 = 2, sigma = 0.2) # Get total percentage outside of these bounds below + above ## [1] 0.07515883 Learning Check 6 Question A log normal distribution has a median time to failure equal to 50,000 hours and shape parameter \\(\\sigma\\) equal to 0.8. What is the mean time to failure and true standard deviation in this distribution? [View Answer!] # Let&#39;s write a mean time to failure function mttf = function(median, sigma){ median * exp(sigma^2 / 2) } mttf(median = 50000, sigma = 0.8) ## [1] 68856.39 # Looks like the MTTF is ~69,000 hours # Let&#39;s write the variance function variance = function(median, sigma){ median^2 * exp(sigma^2 / 2) * (exp(sigma^2) - 1) } # Now calculate variance, and take square root to get sd variance(median = 50000, sigma = 0.8) %&gt;% sqrt() ## [1] 55555.57 # with a very wide standard deviation 21.5 Conclusion Great work! Youâ€™ve learned to work with the Exponential, Gamma, Weibull, and Log-normal distribution. Youâ€™re ready to start coding some cool systems reliability analyses! Next week, weâ€™ll learn some new techniques to help! "],["fault-tree-analysis-in-r.html", "22 Fault Tree Analysis in R Getting Started 22.1 Simulating Fault Trees 22.2 Example: Superman Fault Tree 22.3 Using Raw Probabilities Learning Check 1 22.4 Using Failure Rates at Time t Learning Check 2 22.5 Simulating Uncertainty in Probabilities 22.6 Simulating Uncertainty in Failure Rates 22.7 Example: Contagion Learning Check 3", " 22 Fault Tree Analysis in R Fault trees are visual representation of boolean probability equations, typically depicting the sets of necessary events leading to system failure. Weâ€™re going to learn how to make calculations about fault trees in R! Getting Started Prerequisites As a prerequisite for this workshop, be sure to have read our class reading on Fault Trees! Load Packages library(dplyr) library(readr) library(ggplot2) 22.1 Simulating Fault Trees Fault Trees are a powerful way of modeling the probability of a top event, eg. a big prominent failure, like a nuclear disaster, a missile launch, etc. It supposes that a series of different chains of events can all lead to that top event. Each event in the chain has a probability, and these all can be represented graphically and numerically using boolean logic to create functions! All fault trees can be represented as a function - an equation to estimate the probability of the top event. Fortunately, fault trees are extremely flexible. You can apply all of our past simulation approaches, probability rules, lifespan distribution concepts, and reliability analysis tools to them. 22.2 Example: Superman Fault Tree A favorite simple fault tree example of mine is the â€˜Superman Turns Evilâ€™ fault tree. It consists of a few events: T: Superman turns evil M: Superman movies do poorly at the box office C: Boring Childhood in Kansas D: Lois Lane Dumps Superman K: Steps on Kryptonite Lego in the middle of the night In this fault tree, it looks like Superman could turn evil if (A) Superman Movies do poorly at the box office OR (B) if all 3 other conditions happen (boring childhood, dumped by Lois Lane, steps on Kryptonite Lego). 22.3 Using Raw Probabilities We could represent that using boolean logic. So if we know the probability of events m, c, d, and k, we can calculate the probability of the top event top. f1 = function(m, c, d, k){ top = m + c * d * k return(top) } # For example... given these probabilities, the chance Superman turns evil is highly contigent on event m - the success of superman movies at the box office. probs1 = f1(m = 0.50, c = 0.99, d = 0.25, k = 0.01) # view it! probs1 ## [1] 0.502475 Learning Check 1 Question You are tasked with assessing the risk of a widespread outbreak of a contagious disease in a population. The top event is defined as the widespread outbreak of a contagious disease in a population. This top event T can occur (G1) if any of the 3 following crises occur (G2, G3, or G4). G2: Insufficient Vaccination Coverage: All of the following must occur for this crisis to happen: Causes and Probabilities: - A. Lack of public awareness: P(Lack of public awareness) = 0.2 (20%) - B. Limited access to vaccines: P(Limited access) = 0.15 (15%) - C. Vaccine hesitancy due to misinformation: P(Vaccine hesitancy) = 0.3 (30%) G3: Ineffective Quarantine Measures: Any of the following can lead to this crisis: Causes and Probabilities: - D. Inadequate quarantine protocols: P(Inadequate protocols) = 0.1 (10%) - E. Non-compliance with quarantine rules: P(Non-compliance) = 0.15 (15%) - F. Inefficient monitoring of quarantined individuals: P(Inefficient monitoring) = 0.2 (20%) G4: Mutation of the Pathogen: Both events must occur for this crisis to occur. Causes and Probabilities: - G. High mutation rate of the pathogen: P(High mutation rate) = 0.05 (5%) - H. Inadequate monitoring of the pathogen mutations: P(Inadequate monitoring) = 0.1 (10%) [View Answer!] Use these eventsâ€™ probabilities to construct a boolean equation and its function in R! f = function(a = 0.2, b = 0.15, c = 0.3, d = 0.1, e = 0.15, f = 0.2, g = 0.05, h = 0.1){ # G2. Probability of Insufficient Vaccination Coverage: # All of the sub-events must occur for this intermediate event to happen: p_insufficient_vaccine_coverage = a * b * c # Probabilities for AND logic # G3. Probability of Ineffective Quarantine Measures: # Any of the sub-event event can lead to this intermediate event: p_quarantine_measures = d + e + f # Probabilities for OR logic # G4. Probability of Mutation of the Pathogen: # Both conditions need to be present in order for pathogen to mutate: p_pathogen_mutation &lt;- g * h # Probabilities for AND logic # If EITHER insufficient vaccine coverage # OR no quarantine measures # OR pathogen mutation occur, # the outbreak occurs. # T = OR Gate G1 = G2 + G3 + G4 p_top = p_insufficient_vaccine_coverage + p_quarantine_measures + p_pathogen_mutation return(p_top) } Letâ€™s see what the probability of widespread outbreak, in this hypothetical scenario, turns out to be: f(a = 0.2, b = 0.15, c = 0.3, d = 0.1, e = 0.15, f = 0.2, g = 0.05, h = 0.1) ## [1] 0.464 22.4 Using Failure Rates at Time t But if we knew the failure rate of each event, we could calculate the probability of the top event at any time t! f2 = function(t, lambda_m, lambda_c, lambda_d, lambda_k){ # Get probability at time t... prob_m = pexp(t, rate = lambda_m) prob_c = pexp(t, rate = lambda_c) prob_d = pexp(t, rate = lambda_d) prob_k = pexp(t, rate = lambda_k) # Use boolean equation to calculate top event prob_top = prob_m + (prob_c * prob_d * prob_k) return(prob_top) } # Then we could simulate the probability of the top event over time! probs2 = tibble(t = 1:100) %&gt;% mutate(prob = f2(t = t, lambda_m = 0.01, lambda_c = 0.001, lambda_d = 0.025, lambda_k = 0.00005)) # Check out the first few rows! probs2 %&gt;% head(3) ## # A tibble: 3 Ã— 2 ## t prob ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.00995 ## 2 2 0.0198 ## 3 3 0.0296 # Let&#39;s visualize it! ggplot() + geom_area(data = probs2, mapping = aes(x = t, y = prob)) Learning Check 2 Question Suppose we have a small fault tree, where the top event T depends on either [A and B] OR [B and C] occuring. What is the probability of the top event after 1 hour vs.Â after 10 hours, given the following parameters? lambda_a = 0.05 lambda_b = 0.03 lambda_c = 0.02 [View Answer!] # Let&#39;s create a function calculate probability of the top event using failure rates and time period f = function( t = 1, # Time period we want to analyze the events for lambda_a = 0.05, # Failure rate for event A lambda_b = 0.03, # Failure rate for event B lambda_c = 0.02 # Failure rate for event C ){ # As these events follow exponential distribution, we need to calculate intermediate probabilities accoridingly prob_a = pexp(t, rate = lambda_a) prob_b = pexp(t, rate = lambda_b) prob_c = pexp(t, rate = lambda_c) # Get probability of top event prob_top = (prob_a * prob_b) + (prob_b * prob_c) return(prob_top) } # Probability of failure after 1 hour f(t = 1, lambda_a = 0.05, lambda_b = 0.03, lambda_c = 0.02) ## [1] 0.002026606 # Probability of failure after 10 hours f(t = 10, lambda_a = 0.05, lambda_b = 0.03, lambda_c = 0.02) ## [1] 0.1489618 22.5 Simulating Uncertainty in Probabilities We can use simulation to answer several important questions about variation. For example, we can simulate how the probability of the top event would change if the probability of each event varies ever so slightly across 1000 simulations. When simulating variation in probabilities, you can use random draws from the binomial distribution, randomly sampling 1s or 0s at a given probability prob a total of n times. probs3 = tibble( n = 1000, prob_m = rbinom(n = n, size = 1, prob = 0.50), prob_c = rbinom(n = n, size = 1, prob = 0.99), prob_d = rbinom(n = n, size = 1, prob = 0.25), prob_k = rbinom(n = n, size = 1, prob = 0.01), # Calculate probability of top event for each simulation prob_top = f1(m = prob_m, c = prob_c, d = prob_d, k = prob_k) ) # Let&#39;s get some descriptive statistics - the average will be particularly informative probs3 %&gt;% summarize( mu_top = mean(prob_top), sigma_top = sd(prob_top)) ## # A tibble: 1 Ã— 2 ## mu_top sigma_top ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.499 0.500 22.6 Simulating Uncertainty in Failure Rates Usually, we go a few steps further, saying, suppose the probability of each failure \\(/lambda\\) might vary slightly, according to a normal distribution. What would be the average probability of failure for the top event? How much should we expect that average to vary, on average? Suppose each failure rate has a specific standard error: for m, 0.0001; for c, 0.00001, for d and k, 0.000002. f4 = function(t, lambda_m, lambda_c, lambda_d, lambda_k){ sim_lambda_m = rnorm(n = 1, mean = lambda_m, sd = 0.0001) sim_lambda_c = rnorm(n = 1, mean = lambda_c, sd = 0.00001) sim_lambda_d = rnorm(n = 1, mean = lambda_d, sd = 0.000002) sim_lambda_k = rnorm(n = 1, mean = lambda_k, sd = 0.000002) # Get probability at time t... sim_prob_m = pexp(t, rate = sim_lambda_m) sim_prob_c = pexp(t, rate = sim_lambda_c) sim_prob_d = pexp(t, rate = sim_lambda_d) sim_prob_k = pexp(t, rate = sim_lambda_k) # Use boolean equation to calculate top event prob_top = sim_prob_m + (sim_prob_c * sim_prob_d * sim_prob_k) return(prob_top) } # Then we could simulate the probability of the top event over time, probs4 = tibble(t = 1:100) %&gt;% # This would give us 1 random simulation per time period mutate(prob = f4(t = 1:100, lambda_m = 0.01, lambda_c = 0.001, lambda_d = 0.025, lambda_k = 0.00005)) # But we really probably want MANY random simulations per time period. probs5 = tibble(reps = 1:1000) %&gt;% group_by(reps) %&gt;% # We can use `reframe()`, a version of summarize() # used when you want to return MANY rows per group reframe( t = 1:100, prob = f4(t = t, lambda_m = 0.01, lambda_c = 0.001, lambda_d = 0.025, lambda_k = 0.00005)) probs5 %&gt;% head(3) ## # A tibble: 3 Ã— 3 ## reps t prob ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0.00997 ## 2 1 2 0.0198 ## 3 1 3 0.0296 # And then we could get quantities of interest for each time period! probs6 = probs5 %&gt;% group_by(t) %&gt;% summarize( mu = mean(prob), sigma = sd(prob), # Exact lower and upper 95% simulated confidence intervals lower = quantile(prob, probs = 0.025), upper = quantile(prob, probs = 0.975), # Approximated lower and upper 95% confidence intervals lower_approx = mu - qnorm(0.025) * sigma, upper_approx = mu + qnorm(0.975) * sigma) probs6 %&gt;% head(3) ## # A tibble: 3 Ã— 7 ## t mu sigma lower upper lower_approx upper_approx ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.00995 0.0000954 0.00976 0.0101 0.0101 0.0101 ## 2 2 0.0198 0.000189 0.0194 0.0202 0.0202 0.0202 ## 3 3 0.0296 0.000281 0.0290 0.0301 0.0301 0.0301 Letâ€™s visualize that confidence interval over time! ggplot() + geom_ribbon(data = probs6, mapping = aes(x = t, ymin = lower, ymax = upper)) The sky is the limit! Happy fault tree simulating! 22.7 Example: Contagion You are tasked with assessing the risk of a widespread outbreak of a contagious disease in a population. The top event is defined as the â€œWidespread outbreak of a contagious disease in a population.â€ This top event can occur based on the following intermediate events: Insufficient Vaccination Coverage: All of the following must occur for this event to happen. Causes and Probabilities: Lack of public awareness: P(Lack of public awareness) = 0.2 (20%) Limited access to vaccines: P(Limited access) = 0.15 (15%) Vaccine hesitancy due to misinformation: P(Vaccine hesitancy) = 0.3 (30%) Ineffective Quarantine Measures: Any of the following can lead to this event Causes and Probabilities: Inadequate quarantine protocols: P(Inadequate protocols) = 0.1 (10%) Non-compliance with quarantine rules: P(Non-compliance) = 0.15 (15%) Inefficient monitoring of quarantined individuals: P(Inefficient monitoring) = 0.2 (20%) Mutation of the Pathogen: This event itself can lead to the top event Causes and Probabilities: High mutation rate of the pathogen: P(High mutation rate) = 0.05 (5%) Inadequate monitoring of the pathogen mutations: P(Inadequate monitoring) = 0.1 (10%) # Calculating probabilities of intermediate events using probabilities based on available data: # Probability of Insufficient Vaccination Coverage: # All of the sub-events must occur for this intermediate event to happen: p_event1&lt;- c(0.2, 0.15, 0.3) # Probabilities for AND logic p_insufficient_vaccine_coverage &lt;- prod(p_event1) p_insufficient_vaccine_coverage ## [1] 0.009 # Probability of Ineffective Quarantine Measures: # Any of the sub-event event can lead to this intermediate event: p_event2 &lt;- c(0.1, 0.15, 0.2) # Probabilities for OR logic p_quarantine_measures &lt;- sum(p_event2) p_quarantine_measures ## [1] 0.45 # Probability of Mutation of the Pathogen: # Both conditions need to be present in order for pathogen to mutate: p_event3 &lt;- c(0.05, 0.1) # Probabilities for AND logic logic_mutation &lt;- &quot;AND&quot; p_pathogen_mutation &lt;- prod(p_event3) p_pathogen_mutation ## [1] 0.005 # Calculating the probability of widespread outbreak (top event) using the probabilities of intermediate events # Let&#39;s create a function that can calculate the probability of the top event using any number of intermediate events and any logic gate top_event &lt;- function(intermediate_probability, logic) { if (logic == &quot;AND&quot;) { return(prod(intermediate_probability)) } else if (logic == &quot;OR&quot;) { return(1 - prod(1 - intermediate_probability)) } else { stop(&quot;Unsupported logic operator. Use &#39;AND&#39; or &#39;OR&#39;.&quot;) } } # Define probabilities and logic gates as vector intermediate_probability&lt;- c(p_insufficient_vaccine_coverage,p_quarantine_measures,p_pathogen_mutation) # Probabilities for the intermediate events logic &lt;- &quot;OR&quot; # Logic gate for the top event (&quot;AND&quot; or &quot;OR&quot;) # Calculate the probability of the top event - Widespread outbreak of disease: p_widespread_outbreak &lt;- top_event(intermediate_probability, logic) # Print the result cat(&quot;Probability of the top event:&quot;, p_widespread_outbreak, &quot;\\n&quot;) ## Probability of the top event: 0.4576752 Learning Check 3 Suppose we have some fault tree involving 3 key events, A, B, and C. Suppose these events have the following failure rates: lambda_a = 0.05, lambda_b = 0.03, and lambda_c = 0.02. Simulate the 95% confidence interval for the probability of each event, for each hour from 0 to 100 hours, assuming a standard error for each of 0.001. Then visualize these confidence intervals as a ribbon plot. [View Answer!] # Firstly, let&#39;s load necessary libraries for plotting library(ggplot2) library(dplyr) stat = tibble( reps = 1:1000, # Simulate 1000 lambdas! lambda_a = rnorm(n = 1000, mean = 0.05, sd = 0.001), lambda_b = rnorm(n = 1000, mean = 0.03, sd = 0.001), lambda_c = rnorm(n = 1000, mean = 0.02, sd = 0.001) ) %&gt;% # For each simulation... group_by(reps) %&gt;% reframe( # Get a time frame from 1 to 100 hours t = 1:100, # Get the probabilities for that time frame given each simulated lambda prob_a = pexp(t, rate = lambda_a), prob_b = pexp(t, rate = lambda_b), prob_c = pexp(t, rate = lambda_c) ) %&gt;% # For each time period, get the lower and upper confidence interval group_by(t) %&gt;% summarize( # 95% CI for prob a lower_a = quantile(prob_a, probs = 0.025), upper_a = quantile(prob_a, probs = 0.975), # 95% CI for prob b lower_b = quantile(prob_b, probs = 0.025), upper_b = quantile(prob_b, probs = 0.975), # 95% CI for prob c lower_c = quantile(prob_c, probs = 0.025), upper_c = quantile(prob_c, probs = 0.975)) # Let&#39;s view it stat %&gt;% head(3) ## # A tibble: 3 Ã— 7 ## t lower_a upper_a lower_b upper_b lower_c upper_c ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.0469 0.0507 0.0276 0.0314 0.0178 0.0218 ## 2 2 0.0917 0.0988 0.0545 0.0619 0.0353 0.0431 ## 3 3 0.134 0.144 0.0807 0.0914 0.0525 0.0640 # Let&#39;s make the plot! ggplot() + # Plot bands for event a geom_ribbon( data = stat, mapping = aes(x = t, ymin = lower_a, ymax = upper_a, fill = &quot;A&quot;), alpha = 0.25) + # Plot bands for event b geom_ribbon( data = stat, mapping = aes(x = t, ymin = lower_b, ymax = upper_b, fill = &quot;B&quot;), alpha = 0.25) + # Plot bands for event c geom_ribbon( data = stat, mapping = aes(x = t, ymin = lower_c, ymax = upper_c, fill = &quot;C&quot;), alpha = 0.25) + theme_classic() + labs(title = &quot;Probability of Events over 100 hours&quot;, x = &quot;Hours&quot;, y = &quot;Probability (95% CI)&quot;, fill = &quot;Event Type&quot;) "],["physical-acceleration-models.html", "23 Physical Acceleration Models Getting Started 23.1 Acceleration Factor Learning Check 1 Learning Check 2 23.2 Modeling Normal Use Lifespan 23.3 Eyring Model (Multiple Stressors) 23.4 Degradation Model (Time Trends) 23.5 Burn-in Periods 23.6 Maximum Likelihood Estimation (MLE) for Physical Acceleration Models 23.7 Conclusion", " 23 Physical Acceleration Models In this workshop, weâ€™ll learn how to use physical acceleration models to convert results from reliability tests done under laboratory conditions (which may be slightly unrealistic) into estimates that match real outcomes in the field! Figure 23.1: Crash Testing with LEGOs: a very safe prototype Getting Started Load Packages Letâ€™s start by loading the tidyverse and broom packages! Also, sometimes select gets overruled by other packages, so it can help to load it directly. # Load packages library(tidyverse) library(broom) # Load specific function to environment select = dplyr::select Helpful Functions Weâ€™ll be using the tibble() function; it works identically to the data.frame() function, but allows you to reference any vector that came before. For example: # This doesn&#39;t work... data.frame(x = c(1,2), y = x + 2) # But this does. tibble(x = c(1,2), y = x + 2) ## # A tibble: 2 Ã— 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 ## 2 2 4 23.1 Acceleration Factor 23.1.1 Stress vs.Â Usage Conditions Imagine: You work in Dearborn, Michigan, testing airbag failure rates with car crash dummies every day. Your work helps people stay safe on the road! Unfortunately, lab conditions tend to be a little more extreme than the real world. Moisture levels, temperature, speed, friction, etc. are very hard (nay impossible) to perfectly match to real driving conditions. A product life distribution measured under lab â€œstressâ€ conditions will always be slightly off from the real, â€œuseâ€-case life distribution by a specific factor. What if we could measure and approximate that factor? Suppose researchers are stress testing airbags in the lab! Their empirical data reveals the airbag lifespans are Weibull distributed, with a characteristic life of c = 4100 hours and a shape parameter of m = 1.25. They write the Weibull density function (PDF) d() below, and use it to calculate and visualize the probability the airbags fail every hour from 1 to 8000 hours. They can visualize the blue density curve of the stress-tested airbags shown below (\\(s\\)), called \\(f_s(t)\\). But what they really want to know is the red dashed curve of the airbags under normal use conditions (\\(u\\)), called \\(f_u(t)\\)! But they donâ€™t know it! Fortunately, they know that in past on-road and off-road studies, the median airbagâ€™s time to failure was 250% times greater under normal usage as when under stress condition. We write 250% here the Acceleration Factor \\(AF\\) (or \\(A\\)), because it describes how stress-testing accelerates the failure by a factor of 2.5. We can write it like: \\[f_u(t) = f_s(t) \\times AF = f_s(t) \\times 2.5 \\] # Let&#39;s write ourselves a speedy weibull density function &#39;d()&#39; d = function(t, m, c){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) # or dweibull(t, scale = c, shape = m) } airbags &lt;- data.frame(t = seq(1, 12000, by = 20)) %&gt;% mutate(d_stress = d(t, c = 4100, m = 1.25)) %&gt;% # Suppose the lifespans under normal usage are just off by a factor of ~2.5 # then we could project the PDF under normal usage like: mutate(d_usage = d(t = t / 2.5, c = 4100, m = 1.25) / 2.5) ggplot() + # Plot the PDF under stress conditions geom_line(data = airbags, mapping = aes(x = t, y = d_stress, color = &quot;Stress&quot;), size = 1.5) + # Plot the PDF under normal usage conditions geom_line(data = airbags, mapping = aes(x = t, y = d_usage, color = &quot;Normal Usage&quot;), size = 1, linetype = &quot;dashed&quot;) + # Add a nice theme and clear labels theme_classic(base_size = 14) + labs(x = &quot;Airbag Lifespan in hours (t)&quot;, y = &quot;Probability Density d(t)&quot;, color = &quot;Conditions&quot;, subtitle = &quot;We want Lifespan under Normal Usage,\\nbut can only get Lifespan under Stress&quot;) 23.1.2 Acceleration as a Function \\(\\frac{f_{s}(t)}{f_{u}(t)}\\) In reality, \\(f_{u}(t)\\) probably doesnâ€™t have exactly a constant relationship with \\(f_{s}(t)\\). In a perfect world, we could collect raw data for both lifetimes under normal conditions \\(t_u\\) and under stress-testing \\(t_s\\), and then estimate their density functions \\(f_u(t)\\) and \\(f_s(t)\\). We could then calculate \\(AF\\) exactly as a function af() that relates them. For example: # Let&#39;s write an acceleration factor function! af = function(t){ # Find density under stress ds &lt;- d(t, m = 1.25, c = 4100) # Find density function under normal conditions du &lt;- d(t, m = 1.5, c = 4500) # Since f_u(t) = f_s(t) x AF, # then AF = f_s(t) / f_u(t) = ds / du ds / du } Were we to plot it, we can see below AF is not constant here, but varies over time, because \\(f_u(t)\\) and \\(f_s(t)\\) vary over time. So, when we pick an AF, weâ€™re usually picking the AF corresponding to a specific parameter, like the characteristic life or median of a distribution. data.frame( time = 1:1000, af = af(1:1000) ) %&gt;% # Visualize it! ggplot(mapping = aes(x = time, y = af)) + geom_line() + geom_point() + labs(subtitle = &quot;Acceleration Factor for Airbag Lifespan&quot;, y = &quot;Acceleration Factor function af()&quot;, x = &quot;Time in hours (t)&quot;) Supposing that \\(t_{u} = t_{s} \\times AF\\), we can say several more things: \\[ Failure \\ Function = F_{u}(t) = F_{s}(t/AF) \\] \\[ Density \\ Function = f_{u}(t) = \\frac{f_{s}(t/AF)}{AF} \\] \\[ Failure \\ Rate = z_{u}(t) = \\frac{z_{s}(t/AF)}{AF} \\] 23.1.3 Linear Acceleration However, itâ€™s usually very difficult to obtain the density functions for both usage and stress conditions. Thatâ€™s why we want acceleration factors (AF) - because theyâ€™ll let use estimate \\(f_u(t)\\) when we only have \\(f_s(t)\\). So in practice, we usually assume \\(AF\\) shows a constant, linear relationship between \\(f_{u}(t)\\) and \\(f_{s}(t)\\), like when \\(AF = 2.5\\). This is called linear acceleration. Linear acceleration requires us to choose a constant value of \\(AF\\) from just 1 time-step from the plot above. How can we choose!? We should probably choose a fairly representative lifespan, based off a parameter like the median time to fail \\(T_{50}\\) (or the mean time to fail \\(m\\), characteristic life \\(c\\), etc.). Even if we donâ€™t have access to all the raw data, if we know the median lifespan under stress \\(T_{50_{s}}\\) and under normal conditions \\(T_{50_{u}}\\), we can estimate \\(AF\\) by taking \\(AF = \\frac{T_{50_{u}}}{T_{50_{s}}}\\). For example: # Let&#39;s write a weibull quantile function q = function(p, c, m){ qweibull(p, scale = c, shape = m) } # Get median under stress median_s &lt;- q(0.5, c = 4100, m = 1.25) # Get median under normal conditions median_u &lt;- q(0.5, c = 4500, m = 1.5) # Calculate it! af = median_u / median_s # Check the Acceleration Factor! af ## [1] 1.152529 Letâ€™s clear this data. remove(af, median_s, median_u, q, d) Learning Check 1 Question A kitchen mixer has an exponential distributed lifespan. Labs stress tested the mixer at 125 degrees Celsius, yielding a mean time to fail of 4500 hours. But, it is usually heats to 32 degrees Celsius, and has an acceleration factor of 35. What proportion of mixers do we expect will fail by 40,000 hours under normal use? [View Answer!] # Given an acceleration factor of 35 af &lt;- 35 # Write a failure function f = function(t, lambda){ 1 - exp(-t*lambda) } # Method 1: # Failure Function = fu(t) = fs(t/AF) f(t = 40000 / af, lambda = 1 / 4500) ## [1] 0.2242836 # Method 2: # We know that lambda = zu(t) = zs(t/AF) / AF # so... plus that in for lambda f(t = 40000, lambda = 1 / (4500 * af) ) ## [1] 0.2242836 We expect ~22% of mixers will fail after 40,000 hours when running at 32 degrees Celsius. remove(af, f) Learning Check 2 Question Suppose we want no more than 10% of components to fail after 40,000 hours at normal usage conditions. So, we redesign the mixer to operate at a lower temperature. What (linear) acceleration factor is needed to project from our 125 degree conditions in lab to the normal use temperature of 32 degrees? [View Answer!] # Write failure function f = function(t, lambda){ 1 - exp(-t*lambda) } # Failure Function = fu(t) = fs(t/AF) # 0.10 = fu(t = 40000) = 1 - exp(-40000*lambda) # log(1 - 0.10) = -40000*lambda lambda_u = -log(0.90) / 40000 lambda_s = 1/4500 # Get ratio between these parameters # lambda_u = 1 / af * lambda_s # so... af = lambda_s / lambda_u # Check acceleration factor! af ## [1] 84.36641 We would need an acceleration factor af of ~84.4. remove(lambda_u, lambda_s, af, f) 23.2 Modeling Normal Use Lifespan 23.2.1 Models and Prediction A model is an equation approximating the relationship between two or more vectors of data. Itâ€™s not the real thing - itâ€™s an approximation! Why use models? Usually we know quite a lot about the conditions under which a product was tested in lab, but we canâ€™t actually observe the characteristic lifespan of that product under normal use conditions. But if we can make a really good model of the relationship between the outcome and those conditions, then we can predict for any set of conditions what the outcome - the characteristic lifespan of a product - would be under those conditions. Fortunately, R was built to make statistical models! To make a model of lifespans, we need 4 ingredients. an outcome vector, saved in a data.frame. 1 (or more) vectors of condition/predictors, saved in the same data.frame. the lm() function in R, which makes a linear model drawing the line of best fit between each value of the predictor(s) and outcome. The model is the equation of that line! the outcome vector needs to be log-transformed, because (a) most useful life distributions involve exponential processes and (b) lifespans are by nature non-negative, right-skewed variables. Taking log(outcome) adjusts for that and lets us plot a straight line of best fit. Note: Weâ€™ll learn more in later weeks about how* lm() makes models, but for now, weâ€™ll assume itâ€™s magic!* For example, letâ€™s think about a car alternator, the device that converts mechanical energy into electrical energy in a car. Weâ€™ll make a few tiny example models! Suppose we tested 5 samples of 100 alternators each, and recorded the results in our alt data.frame below. We calculated the characteristic life in each of our 5 samples in the c, but our samples were all subjected to different stress conditions: different temperatures in Celsius (temp), which were converted into units of temperature factor scores (tf); different voltage levels in volts; at different points in the devicesâ€™ lifespan (time, in hours); and with different average performance ratings (rating) in amps. alt &lt;- tibble( # Characteristic life in hours c = c(1400, 1450, 1500, 1550, 1650), # Temperature in Celsius temp = c(160, 155, 152, 147, 144), # Temperature Factor (a standardized unit) tf = 1 / (1 / 11605 * (temp + 273.15)), # Voltage, in volts volts = c(17, 16.5, 14.5, 14, 13), # Hours of life spent by time of test time = c(1200, 1000, 950, 600, 500), # Performance Rating, in Amps rating = c(60, 70, 80, 90, 100)) Letâ€™s use this data to explore some of the different classic physical acceleration models, including the Arrhenius Model, Eyring Model, and Degradation Model. 23.2.2 Arrhenius Model (Temperature) The Arrhenius Model is a simple equation that models the impact of temperature (tf) on the log() of lifespan (c). Letâ€™s explore it visually using ggplot(), and then estimate it using lm(). 23.2.3 Visualizing the Arrhenius Model We could visualize the relationship between each of these conditions and the log() of the characteristic lifespan c usingâ€¦ geom_point() which makes a scatterplot between x and y coordinates in the aes(). geom_smooth(method = \"lm\"), which finds the line of best fit. g &lt;- alt %&gt;% ggplot(mapping = aes(x = tf, y = log(c) )) + geom_point(size = 5) + # Add scatterplot points geom_smooth(method = &quot;lm&quot;, se = FALSE) # Make line of best fit, using lm() - a linear model # we can write &#39;se = FALSE&#39; (standard error = FALSE) to get rid of the confidence interval We can also add in the equation of this line of best fit (which weâ€™ll calculate below), plus other labels. g + # Add theme theme_classic(base_size = 14) + # Add labels labs(title = &quot;Arrhenius Model, Visualized&quot;, subtitle = &quot;Model Equation: log(c) = 3.1701 + 0.1518 TF \\nModel Fit: 96%&quot;, # We can add a line-break in the subtitle by writing \\n x = &quot;Temperature Factor (TF)&quot;, y = &quot;Characteristic Lifespan log(c)&quot;) 23.2.4 Estimating the Arrhenius Model So how did R calculate that line of best fit? It used the lm() function to make a linear model - an equation, which fits the data with a specific accuracy rate (eg. 96%). Letâ€™s make a model m1 to predict the log() of characteristic lifespan c, based on our temperature factor vector tf in alt! First, letâ€™s make the model with lm(), piping our vectors from alt. m1 &lt;- alt %&gt;% lm(formula = log(c) ~ tf) Second, letâ€™s inspect the modelâ€™s fit with glance() from the broom package, and select() the r.squared statistic. 0% means terrible model fit. 100% means the model equation perfectly predicts every value of log(c) in our alt data.frame. We aim for excellent predictive power where possible. 96% is excellent! m1 %&gt;% glance() %&gt;% select(r.squared) ## # A tibble: 1 Ã— 1 ## r.squared ## &lt;dbl&gt; ## 1 0.962 Third, we can now read the model equation for our line of best fit. m1 ## ## Call: ## lm(formula = log(c) ~ tf, data = .) ## ## Coefficients: ## (Intercept) tf ## 3.1701 0.1518 The lm() function estimated our model equation m1, including 2 constant coefficients named (Intercept) and tf. These coefficients show the y-intercept ((Intercept)), called \\(\\alpha\\) and the slope/effect/rate of change called \\(\\beta\\) for every 1 unit increase in the temperature factor tf. We can write this model equation formally as: \\[ \\begin{align*} log(c) =&amp; \\ Intercept + Slope \\ \\times TF \\\\ or:&amp; \\\\ log(c) =&amp; \\ \\alpha + \\beta \\times TF, \\\\ &amp; where \\ \\alpha = 3.1701 \\ and \\ \\beta = 0.1518, \\\\ so:&amp; \\\\ log(c) =&amp; \\ 3.1701 + 0.1518 \\ TF \\\\ so:&amp; \\\\ c =&amp; e^{\\alpha + \\beta \\times TF} \\\\ or:&amp; \\\\ c =&amp; \\ e^\\alpha \\times e^{\\beta \\times TF} \\\\ or:&amp; \\\\ c =&amp; e^{3.1701} + e^{0.1518 \\ TF} \\\\ or:&amp;\\\\ Arrhenius \\ Model:&amp; \\\\ c =&amp; A + e^{\\Delta H \\times TF} \\ \\\\ &amp;\\ where \\ A = e^{Intercept} = e^{\\beta} \\ \\ and \\ \\Delta H = Slope = \\beta \\\\ also:&amp; \\\\c =&amp; A + e^{\\Delta H \\times (1 / (k T))} \\\\ &amp;where \\ TF = 1 / (k \\times T_{Kelvin}) \\ and \\ k = 1 / 11605 \\end{align*} \\] As you can see above, there are many ways to write the Arrhenius model, but it boils down to this: any linear model of the log-characteristic life will involve: a y-intercept constant called \\(\\alpha\\) (\\(log(A)\\) in the Arrhenius model). \\(\\alpha\\) describes how much log(c) we get independent of any other factors (ie: if tf = 0). Interpreting our model: If the temperature factor \\(TF = 0\\), our model predicts that \\(log(c) = 3.1701\\). a slope constant called \\(\\beta\\) describing the effect of your variable (\\(TF\\) or tf), for every 1 unit increase in your variable. In the Arrhenius model, \\(\\beta\\) is written as \\(\\Delta H\\), the â€˜activation energyâ€™ rate at which a temperature factor increase of 1 unit affects log(c). Interpreting our model: If the temperature factor \\(TF\\) increases by 1, our model predicts that \\(log(c)\\) will change by \\(\\beta = \\Delta H = 0.1518\\). And thatâ€™s how we read any statistical model with two variables! &lt;br 23.2.5 Prediction Now that we have our model equation (and, importantly, a good fitting one), we can feed it values of our predictor \\(TF\\) (generically called \\(X\\)) to calculate our predicted log of the characteristic life \\(log(\\hat{c})\\) (generically called \\(log(\\hat{Y})\\)). We can do this two ways: (1) by writing a function or (2) using the predict() function in R. Writing our own function works the same way as writing our d(), f(), or r() function. Weâ€™ll call it c_hat(). # For any value of tf, we can now calculate c_hat. # We write the model equation for log(c), then exponentiate it with exp()! c_hat = function(tf){ exp( 3.1701 + 0.1518*tf) } # Test it! c_hat(tf = 28) ## [1] 1669.868 It works! The characteristic life for tf = 28 is ~1670. # Or better yet, let&#39;s calculate temperature factor &#39;tf&#39; too, # so we only have to supply a temperature in Celsius tf = function(temp){ k = 1 / 11605 # Get Boltzmann&#39;s constant 1 / (k * (temp + 273.15)) # Get TF! } # Now predict c_hat for 30, 60, and 90 degrees celsius! c_hat = function(temp){ exp( 3.1701 + 0.1518*tf(temp)) } c(30, 60, 90) %&gt;% c_hat() ## [1] 7952.275 4712.271 3044.511 So cool! Wouldnâ€™t it be nice though, if we could simplify that process? The predict() function in R can help! The predict function will run your model equation on any newdata that you feed it, calculating the predicted outcomes. newdata must be formatted as a data.frame, containing vectors named to match each predictor from your original data (which we named alt). Letâ€™s make a data.frame of fakedata with tibble(), varying temperature from 0 to 200 degrees Celsius, and then transform that into a temperature factor tf with our tf() function. fakedata &lt;- tibble( temp = seq(0, 200, by = 10), tf = tf(temp)) # Check the first 3 rows! fakedata %&gt;% head(3) ## # A tibble: 3 Ã— 2 ## temp tf ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 42.5 ## 2 10 41.0 ## 3 20 39.6 Then, weâ€™ll feet our 21 rows of fakedata and our model m1 to the predict() function, which will output 21 predictions for log(c). m1 %&gt;% predict(newdata = fakedata) ## 1 2 3 4 5 6 7 8 ## 9.619376 9.391606 9.179376 8.981148 8.795580 8.621497 8.457864 8.303769 ## 9 10 11 12 13 14 15 16 ## 8.158401 8.021038 7.891038 7.767824 7.650877 7.539733 7.433969 7.333203 ## 17 18 19 20 21 ## 7.237091 7.145316 7.057591 6.973655 6.893267 But we can exponentiate it with exp() to get c_hat! m1 %&gt;% predict(newdata = fakedata) %&gt;% exp() ## 1 2 3 4 5 6 7 ## 15053.6495 11987.3387 9695.1035 7951.7546 6604.9840 5549.6862 4711.9837 ## 8 9 10 11 12 13 14 ## 4039.0668 3492.5958 3044.3360 2673.2172 2363.3224 2102.4897 1881.3274 ## 15 16 17 18 19 20 21 ## 1692.5112 1530.2759 1390.0440 1268.1516 1161.6437 1068.1196 985.6159 We could even write the whole thing inside a tibble() function: fakedata &lt;- tibble( temp = seq(0, 200, by = 10), tf = tf(temp), # Predict c_hat c_hat = predict(m1, newdata = tibble(tf)) %&gt;% exp()) # View the first 3 rows! fakedata %&gt;% head(3) ## # A tibble: 3 Ã— 3 ## temp tf c_hat ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 42.5 15054. ## 2 10 41.0 11987. ## 3 20 39.6 9695. And now, we can plot the line of best fit between temp and c_hat, which are the quantities we actually care about. g2 &lt;- fakedata %&gt;% ggplot(mapping = aes(x = temp, y = c_hat)) + geom_line() + geom_point() # add some labels! g2 + labs(x = &quot;Temperature in Celsius (T)&quot;, y = &quot;Predicted Characteristic Life (c-hat)&quot;, title = &quot;Arrhenius Model of Car Alternator Characteristic Life&quot;, # Here&#39;s a little trick for cleanly adding linebreaks in subtitles: # Just write a paste() function subtitle = paste( &quot;Equation: c = e^(3.1701 + 0.1518 TF) = e^3.1701 * e^(0.1518 * (1 / kT))&quot;, &quot;Model: log(c) = 3.1701 + 0.1518 TF&quot;, sep = &quot;\\n&quot;)) 23.3 Eyring Model (Multiple Stressors) But in our alt data, multiple conditions varied, including tf but also volts and time. These might each have independent effects on log(c)! So letâ€™s estimate their effect by adding some more coefficients to our model! The Eyring Model provides an equation derived from chemical reaction theory and quantum mechanics, which supposes that we can predict lifespan parameters pretty well if we know temperature and any other stresses, such as voltage. The general form of the Eyring Model can also be distilled into a multivariate regression equation. This means instead of drawing a line of best fit approximating the relationship between 2 vectors (log(c) and tf), we can approximate the relationship between 3 vectors (log(c), tf, and volts), making a plane. \\[ \\begin{align*} c =&amp; AT^{\\alpha} \\times e^{\\frac{\\Delta H}{kT}} \\times e^{(B + C/T) \\times S} \\\\ &amp;where: \\ T = Temperature \\ in \\ Kelvin, \\\\ &amp;so: \\ if \\ \\alpha = 0, \\ T^{\\alpha} = 1 \\\\ &amp;also:\\\\ &amp;where: \\ S = Stressor \\ S, \\\\ &amp;and: \\ B + C/T = \\ temperature \\ contigent \\ effect \\ of \\ S\\\\ \\\\ &amp;simplifies \\ to: \\\\ c=&amp;e^{intercept} \\times T^{\\alpha} + e^{\\Delta H \\times (1 / (kT)) \\ + \\ (B + C/T)^S } \\\\ \\\\ &amp; so \\ assuming \\ \\ T^{\\alpha} = 1: \\\\ log(c) =&amp; intercept \\ + \\ \\Delta H \\times (1 / (kT)) + (B + C/T)\\times S \\\\ &amp; can \\ be \\ rewritten \\ as: \\\\ log(c) =&amp; \\alpha \\ + \\ \\beta_1 X_1 + B_2 X_2 + ... \\\\ &amp; where \\ \\alpha = intercept, \\\\ &amp;X_1 = Temperature \\ Factor = TF = 1 / (kT), \\\\ &amp;\\beta_1 = effect \\ of X_1 \\ [Temperature \\ Factor], \\\\ &amp;X_2 = Stressor \\ S, \\\\ &amp;B_2 = net \\ effect \\ of \\ X_2 \\ [Stressor \\ S] \\\\ &amp;... = any \\ other \\ stressors \\end{align*}\\] This is a monster of an equation, but we can still model it relatively simply using a linear model lm() of the log(c). Often, we care about voltage, which can be written as: \\[ \\beta_{2}X_{2} = B \\times (-log(V)), \\ where \\ \\beta_2 = B = effect \\ of \\ -log(Voltage) \\] We can write model this as m2, like so: m2 &lt;- alt %&gt;% lm(formula = log(c) ~ tf + log(volts) ) # See our model equation! m2 ## ## Call: ## lm(formula = log(c) ~ tf + log(volts), data = .) ## ## Coefficients: ## (Intercept) tf log(volts) ## 5.0086 0.1027 -0.1837 Then, we can just supply predict() with any values of tf and volts to predict log(c_hat), and exponeniate the result. fakedata &lt;- tibble( # Hold temperature constant temp = 30, tf = tf(temp), # But vary volts volts = seq(from = 1, to = 30, by = 1), # Predict c_hat c_hat = predict(m2, newdata = tibble(tf, volts)) %&gt;% exp()) And we can visualize our fakedata to see the impact of changing volts on c_hat as temp and tf were held constant. fakedata %&gt;% ggplot(mapping = aes(x = volts, y = c_hat)) + geom_line() + geom_point() + theme_classic(base_size = 14) + labs(title = &quot;Eyring Model of Effect of Voltage on Lifespan (30 Deg. Celsius)&quot;, subtitle = &quot;Equation: c = e^(5.0086 + 0.1027 TF - 0.1837 * log(volts))&quot;, x = &quot;Voltage (volts)&quot;, y = &quot;Predicted Characteristic Life (c-hat)&quot;) 23.4 Degradation Model (Time Trends) One final common impact on lifespan is simple degradation in functioning over time. With each passing day, the product is exposed to a stressor an additional time, bringing it a little closer to whatever we define as failure, and in turn, impacting the estimation of performance across our samples. We can model this asâ€¦ \\[ \\begin{align*} Q_t =&amp; \\ Q_0 \\times e^{-R(S)t} \\\\ &amp;where: \\\\&amp; Q_t = performance \\ metric \\ at \\ time \\ t, \\\\&amp; Q_0 = starting \\ value \\ at \\ t = 0, \\ and \\\\&amp; R(S) = effect \\ of \\ stressor \\ S \\ \\\\ \\\\&amp; modeled \\ as: \\\\ log(Y_t) =&amp; \\ \\alpha + \\beta X \\times t \\\\ &amp;where: \\\\&amp;log(Y_t) = log(Q_t), \\\\ &amp; \\alpha = intercept = log(Q_0), \\\\&amp; \\beta = -R = degradation \\ effect \\ of \\ S,\\\\&amp; X = stressor \\ S \\ (sometimes \\ excluded), \\ and \\\\ &amp;t = time \\ t \\\\&amp;or: \\\\ log(Y_t) =&amp; \\ \\alpha + \\beta \\times t \\\\&amp; where: \\beta = overall \\ degradation \\ effect \\ of \\ time \\ t \\end{align*} \\] Letâ€™s try this out using our alt data! We might expect the power rating of an alternator might decline over time. We could model the overall degradation effect on the log(rating) by regressing a single vector time against our model. This produces a great fit of ~94%. alt %&gt;% lm(formula = log(rating) ~ time) %&gt;% glance() %&gt;% select(r.squared) ## # A tibble: 1 Ã— 1 ## r.squared ## &lt;dbl&gt; ## 1 0.940 Alternatively, if the degradation effect depends on another condition, like how much voltage was run on it, we could write an interaction effect using I(volts * time). This forces our model to be written as \\(log(Y) = \\alpha + \\beta X \\times t\\) instead of just \\(log(Y) = \\alpha + \\beta \\times t\\). alt %&gt;% lm(formula = log(rating) ~ I(volts * time) ) ## ## Call: ## lm(formula = log(rating) ~ I(volts * time), data = .) ## ## Coefficients: ## (Intercept) I(volts * time) ## 4.825e+00 -3.497e-05 We could even apply these effects right into our estimation of characteristic life \\(\\hat{c}\\) for our alternators! Perhaps we think the characteristic life would tend to be lower if the sample were measured later in time, after being exposed to higher volts. Letâ€™s estimate this model as m3! m3 &lt;- alt %&gt;% lm(formula = log(c) ~ tf + log(volts) + I(volts * time)) # Really good fit! m3 %&gt;% glance() ## # A tibble: 1 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.981 0.924 0.0174 17.3 0.174 3 17.2 -24.4 -26.3 ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; As you can see, we can build nearly as complex models as you can imagine! A good rule of thumb is to seek to build the most parsimonious (simplest) model that explains the most variation (maximizes the r.squared) Figure 23.2: Wasnâ€™t that fun? 23.5 Burn-in Periods Sometimes, manufacturers will deliberately test-run their products for several hundred hours before selling them, just to â€˜burn-inâ€™ to their lifespan distributions. The idea is, if they can force the defective products to fail early on, then the products that remain are less likely to fail. Acceleration Factors can help us identify the ideal burn in period. We can say: \\(z(t)\\) = failure rate curve with no burn-in. \\(z_b(t)\\) = failure rate curve after a burn-in period of \\(t_b\\) hours. \\(a\\) = acceleration factor between normal use vs.Â stress (burn-in), so: \\(z_b(t = 0) = z(a \\times t_b)\\): failure rate after 0 hours of burn-in \\(z_b(t = 0)\\) should equal the normal failure rate at a time \\(a\\) times greater than the burn-in period of \\(t_b\\) hours. In other words, we can use the acceleration factor \\(a\\) to project the conditional probability of failure after surviving burn-in. We can say, hey, whatâ€™s the probability of failure under normal use \\(t\\) hours after burn-in \\(F_b(t)\\), given that we know it survived up through the burn-in period \\(t_b\\) and its use-to-stress relationship is characterized by an acceleration factor of \\(a\\)? # Let&#39;s write the Weibull density and failure function, as always... d = function(t, c, m){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) } f = function(t, c, m){ 1 - exp(-1*((t/c)^m)) } Since we can calculate \\(F(t)\\) as f(t,c,m) and \\(f(t)\\) as d(t,c,m), we can write the conditional probability of failure post-burn-in as: \\[ F_b(t) = \\frac{ F(t + a t_b) - F(at_b)}{ 1 - F(at_b) } \\] And we can code it as: fb = function(t, tb, a, c, m){ # Change in probability of failure delta_failure &lt;- f(t = t + a*tb, c, m) - f(t = a*tb, c, m) # Reliability after burn-in period reliability &lt;- 1 - f(t = a*tb, c, m) # conditional probability of failure delta_failure / reliability } Letâ€™s try it! # 1000 hours after burn-in # with a burn-in period of 100 hours # an acceleration factor of 20 # characteristic life c = 2000 hours # and # shape parameter m = 1.5 fb(t = 1000, tb = 100, a = 20, c = 2000, m = 1.5) ## [1] 0.5670432 Likewise, the conditional density function \\(f_b(t)\\) can be written as: \\[ f_b(t) = \\frac{ f(t + at_b)}{ 1 - F(at_b)} \\] # And we&#39;ll write the condition db = function(t, tb, a, c, m){ density &lt;- d(t = t + a*tb, c, m) reliability &lt;- 1 - f(t = a*tb, c, m) # get conditional density density / reliability } Letâ€™s try it! Whatâ€™s the conditional probability of having a lifespan of 1000 hours, given that you had a burn-in period of 100 hours? Letâ€™s assume an acceleration factor of 20, characteristic life of 2000 hours, and a shape parameter of 1.5, like before. db(t = 1000, tb = 100, a = 20, c = 2000, m = 1.5) ## [1] 0.0003976962 23.6 Maximum Likelihood Estimation (MLE) for Physical Acceleration Models But how in the world did we get all these estimates of c in the first place? Often, we end up calculating it via maximum likelihood estimation (MLE) from cross-tabulated readout data. Suppose we have several cross-tabulations of readout data available to us about the occurrence of failure for wheels by temperature. We know these are Weibull distributed, but we donâ€™t know the distributionsâ€™ parameters for each temperature level, let alone the \\(\\Delta H\\) or the Acceleration Factor \\(AF\\)! The examples below will focus on samples of car wheels, each with a Weibull distribution, but they are equally applicable to other distributions. # Let&#39;s write Weibull density, failure, and reliability functions d = function(t, c, m){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) } f = function(t, c, m){ 1 - exp(-1*((t/c)^m)) } r = function(t, c, m){ 1 - f(t,c,m) } 23.6.1 MLE for an Example Arrhenius Model # Let&#39;s load in our crosstable wheels &lt;- tibble( label = c(&quot;[0,1000]&quot;, &quot;(1000,2000]&quot;, &quot;(2000,3000]&quot;, &quot;(3000,4000]&quot;, &quot;(4000,5000]&quot;), t = c(500, 1500, 2500, 3500, 4500), temp_100 = c(106, 66, 22, 4, 2), # out of 200 wheels temp_150 = c(125, 25, 25, 15, 10), # out of 175 wheels temp_200 = c(140, 30, 15, 10, 15)) # out of 300 wheels # Check it! wheels ## # A tibble: 5 Ã— 5 ## label t temp_100 temp_150 temp_200 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [0,1000] 500 106 125 140 ## 2 (1000,2000] 1500 66 25 30 ## 3 (2000,3000] 2500 22 25 15 ## 4 (3000,4000] 3500 4 15 10 ## 5 (4000,5000] 4500 2 10 15 We can write a maximum likelihood estimation function to find the most likely parameters for our products stress tested at 100 degrees Celsius, using optim() to perform MLE. # Let&#39;s write our crosstable&#39;s likelihood function ll = function(t, x, par){ r = sum(x) # Get total failures n = 200 # Record total sampel size tmax = max(t) # Record last time step # Get the product of the log-densities at each time step, for all failures then prob_d = ((d(t, c = par[1], m = par[2]) %&gt;% log()) * x) %&gt;% sum() # For the last time step, get the probability of each remaining unit surviving prob_r = r(t = tmax, c = par[1], m = par[2])^(n - r) %&gt;% log() # Get joint log-likelihood prob_d + prob_r } # And let&#39;s run MLE! mle100 &lt;- optim(par = c(1000, 1), t = wheels$t, x = wheels$temp_100, fn = ll, control = list(fnscale = -1)) # Our characteristic life and shape parameter m! mle100$par ## [1] 1287.316800 1.511327 But doesnâ€™t it seem like a waste that we have all this data at multiple temperature readouts, but weâ€™re just relying on one temperature to estimate parameters? We can do better! Letâ€™s write a maximum likelihood estimator that maximizes more parameters! Weâ€™ll assume that the shape parameter \\(m\\) is the same for each distribution, but the characteristic life varies (a common assumption in physical acceleration models). # Let&#39;s write our crosstable&#39;s likelihood function ll = function(t, x1, x2, x3, par){ # Get total failures r1 = sum(x1) r2 = sum(x2) r3 = sum(x3) # Record total sample size in each n1 = 200 n2 = 175 n3 = 300 tmax = max(t) # Record last time step # Get the product of the log-densities at each time step, for all failures then prob_d1 = ((d(t, c = par[1], m = par[4]) %&gt;% log()) * x1) %&gt;% sum() prob_d2 = ((d(t, c = par[2], m = par[4]) %&gt;% log()) * x2) %&gt;% sum() prob_d3 = ((d(t, c = par[3], m = par[4]) %&gt;% log()) * x3) %&gt;% sum() # For the last time step, get the probability of each remaining unit surviving prob_r1 = r(t = tmax, c = par[1], m = par[4])^(n1 - r1) %&gt;% log() prob_r2 = r(t = tmax, c = par[2], m = par[4])^(n2 - r2) %&gt;% log() prob_r3 = r(t = tmax, c = par[3], m = par[4])^(n3 - r3) %&gt;% log() # Get joint log-likelihood, across ALL vectors prob_d1 + prob_r1 + prob_d2 + prob_r2 + prob_d3 + prob_r3 } # And let&#39;s run MLE! mle &lt;- optim(par = c(1000, 1000, 1000, 1), t = wheels$t, x1 = wheels$temp_100, x2 = wheels$temp_150, x3 = wheels$temp_200, fn = ll, control = list(fnscale = -1)) # Check out our 3 characteristic life parameters, # for temp_100, temp_150, and temp_200, and our shared shape parameter! mle$par ## [1] 794.06372 1491.73881 1747.74451 1.02723 We can apply MLE to estimate as many parameters as our computers and patience for coding functions will allow! 23.6.2 Estimating \\(\\Delta H\\) with MLE Next, letâ€™s use our MLE values to estimate \\(\\Delta H\\), the impact of temperature on lifespan parameters. # Remember our function to calculate temperature factors tf = function(temp){ 1 / ((1 / 11605) * (temp + 273.15)) } # Let&#39;s collect our parameter estimates param &lt;- tibble( # For each temperature temp = c(100, 150, 200), # report the MLE c estimates c = mle$par[1:3], # and the shared MLE m estimate m = mle$par[4], # and Calculate TF (for each temperature...) # This will be our independent variable tf = tf(temp)) # Check it! param ## # A tibble: 3 Ã— 4 ## temp c m tf ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 794. 1.03 31.1 ## 2 150 1492. 1.03 27.4 ## 3 200 1748. 1.03 24.5 Now, weâ€™ve got three different \\(c\\) estimates. Weâ€™d really like to project, for any temperature temp, what would the characteristic life c be? Fortunately, we know we can estimate that with a line of best fit. m4 &lt;- param %&gt;% lm(formula = log(c) ~ tf) # Pretty good fit (93%) m4 %&gt;% glance() ## # A tibble: 1 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.932 0.863 0.154 13.6 0.168 1 3.00 0.00617 -2.70 ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; Now that weâ€™ve built a fairly good model, we can use it to predict the characteristic life \\(c\\) for any temperature temp. And if we have c_hat(), then all of a sudden, we can calculate the probability of failure at any time t! Suppose our wheel was used at 30 degrees Celsius. Our model projects a probability of failure of 32% by the 100th hour! (Donâ€™t buy that wheel!) tibble( t = 100, temp = 10, tf = tf(temp), # Predict c-hat for our specified temperature c_hat = predict(m4, newdata = tibble(tf)) %&gt;% exp(), # Grab our MLE estimate of m m = mle$par[4], # And calculate the probability of failure, given a use temperature of 30 prob_f = f(t, c = c_hat, m = m) ) 23.6.3 Visualizing Probabilities We can even then make some rad graphs! For example, we can vary time but hold constant temperature to calculate the probability of failure over time at a specific temperature. tibble( t = seq(0, 2000, by = 10), tf = tf(temp = 30), # Get parameters c_hat = predict(m4, newdata = tibble(tf)) %&gt;% exp(), m = mle$par[4], # Calculate Probability prob_f = f(t, c = c_hat, m = m) ) %&gt;% ggplot(mapping = aes(x = t, y = prob_f)) + geom_area() + labs(x = &quot;Time&quot;, y = &quot;Failure Function F(t)&quot;, subtitle = &quot;Probability of Failure at 30 Degrees Celsius&quot;) Or, we can hold constant time but vary temperature, to show the changing probability of failure given different stress levels by temperature. tibble( t = 1000, temp = seq(0, 200, by = 10), tf = tf(temp), # Get parameters c_hat = predict(m4, newdata = tibble(tf)) %&gt;% exp(), m = mle$par[4], # Calculate Probability prob_f = f(t, c = c_hat, m = m) ) %&gt;% ggplot(mapping = aes(x = temp, y = prob_f)) + geom_area() + labs(x = &quot;Temperature (Celsius)&quot;, y = &quot;Failure Function F(t)&quot;, subtitle = &quot;Probability of Failure after 1000 hours&quot;) 23.7 Conclusion All done! You have covered several major models for estimating change in lifespan parameters! Hooray! "],["bivariate-regression-modeling-diamond-pricing.html", "24 Bivariate Regression: Modeling Diamond Pricing Getting Started 24.1 Review 24.2 Regression and the Line of Best Fit Learning Check 1 24.3 Statistical Significance Learning Check 2 24.4 Visualization Learning Check 3 24.5 Finding the Line of Best Fit Learning Check 4 24.6 F-statistic 24.7 summary() Learning Check 5", " 24 Bivariate Regression: Modeling Diamond Pricing Social systems are full of numeric variables, like voter turnout, percentage of votes for party X, income, unemployment rates, and rates of policy implementation or people affected. So how do we analyze the association between two numeric variables? Today, weâ€™re going to investigate a popular dataset on commerce. The ggplot2 packageâ€™s diamonds dataset contains 53,940 diamond sales gathered from the Loose Diamonds Search Engine in 2017. Weâ€™re going to examine a random sample of 1000 of these diamonds, saved as mydiamonds.csv. This dataset lets us investigate a popular question for consumers: Are diamondsâ€™ size, measured by carat, actually related to their cost, measured by price? Letâ€™s investigate using the techniques below. Getting Started Load Data In this dataset, each row is a diamond! library(tidyverse) # for data wrangling library(viridis) # for colors library(broom) # for regression library(gtools) # for statistical significance # Save the diamonds dataset as an object in my environment mydiamonds &lt;- read_csv(&quot;workshops/mydiamonds.csv&quot;) View Data # View first 3 rows of dataset mydiamonds %&gt;% head(3) price carat cut 4596 1.20 Ideal 1934 0.62 Ideal 4840 1.14 Very Good Codebook In this dataset, our variables mean: price: price of diamond in US dollars (from $326 to $18,823!) carat: weight of the diamond (0.2 to 5.01 carats) cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal) 24.1 Review We have several tools in our toolkit for measuring the association between two variables: (1) Scatterplots, (2) Correlation, and (3) Regression / Line of Best Fit (New!). Letâ€™s investigate! 24.1.1 Scatterplots First, we can visualize the relationship between 2 numeric variables using a scatterplot, putting one on the x-axis and one on the y-axis. In a scatterplot, each dot represents a row in our dataset. So, we can visualize just five randomly selected dots, like this: mydiamonds %&gt;% # pipe from dataframe sample_n(5) %&gt;% # take a random sample ggplot(mapping = aes(x = carat, y = price)) + # Pro-tip: if you say, shape = 21, # this lets us change both the fill and the outline color of the dot geom_point(size = 5, shape = 21, fill = &quot;steelblue&quot;, color = &quot;white&quot;) + theme_classic(base_size = 30) Or we can visualize all the dots, like this: mydiamonds %&gt;% # just pipe directly from data.frame ggplot(mapping = aes(x = carat, y = price)) + # Pro-tip: if you say, shape = 21, # this lets us change both the fill and the outline color of the dot geom_point(size = 3, shape = 21, fill = &quot;white&quot;, color = &quot;steelblue&quot;) + theme_classic() We can see that thereâ€™s a strong, positive relationship. As carat increases, price increases to! 24.1.2 Correlation We can measure the relationship between two numeric variables using Pearsonâ€™s r, the correlation coefficient! This statistic ranges from -1 to 0 to +1. -1 indicates the strongest possible negative relationship, 0 indicates no relationship, and 1 indicates the strongest possible positive relationship. That could help us learn (1) how strongly associated are they, and (2) how positive or negative is that association. The animation below shows the full range of possible correlations we might get. 24.1.3 Example 24.1.4 cor.test() and tidy() We can use cor.test() to test correlation in R. Letâ€™s, for example, get the correlation between price and carat for each different cut of diamond. There are 5 cuts of diamonds, so we should get 5 correlations, using group_by(cut). cor.test() lets us test 3 things: direction (positive or negative?) strength of association (closer to +/-1 = stronger, closer to 0 = weaker) statistical sigificance (p-value), an indicator of how extreme our statistics are - how likely is it we got this statistic due to chance? (Likely due to chance = nearer to 1; not likely due to chance = near 0, eg. p &lt; 0.05.) To extract cor.test()â€™s output, we can use the broom packageâ€™s tidy() function. This takes the output of the cor.test() function and puts it in a nice tidy data.frame, which we can then give to summarize(), allowing us to still use group_by(). Then, the correlation is reported in the estimate column, a standardized t-statistic is calculated in the statistic column, significance is given in the p.value column, and the upper (97.5%) and lower (2.5%) 95% confidence intervals are reported in conf.low and conf.high. library(broom) mydiamonds %&gt;% group_by(cut) %&gt;% summarize( cor.test(x = price, y = carat) %&gt;% tidy() )# convert to dataframe cut estimate statistic p.value parameter conf.low conf.high method alternative Fair 0.854 10.10 0 38 0.739 0.739 Pearsonâ€™s two.sided Good 0.912 22.23 0 100 0.872 0.872 Pearsonâ€™s two.sided Ideal 0.919 45.89 0 385 0.903 0.903 Pearsonâ€™s two.sided Premium 0.927 37.21 0 228 0.906 0.906 Pearsonâ€™s two.sided Very Good 0.924 37.28 0 239 0.903 0.903 Pearsonâ€™s two.sided For correlation, most people just report the correlation coefficient, and donâ€™t go into significance, but itâ€™s always an option. 24.2 Regression and the Line of Best Fit Wouldnâ€™t it be nice if we could say, how much does the price of a diamond tend to increase when the size of that diamond increases? Economists, consumers, and aspiring fiancees could use that information to determine what size of diamond they can afford. We learned to do this in physical acceleration models, but can we apply it to large datasets? 24.2.1 Model Equation We do this intuitively all the time, making projections based on data weâ€™ve seen in the past. What we, and computers, are doing is building a â€˜model.â€™ Weâ€™re taking lots of data weâ€™ve observed and building a simplified version of it, a general trend line. Itâ€™s not intended to a complete replica - itâ€™s just a model! This is the meaning of a regression model. Regression models create a straight line that best approximates the shape of our data. The line of best fit is a model of the data. The line can be represented as: \\(Y_{observed} = Alpha + X_{observed} \\times Beta + Error\\) [Click Here to Review Definitions!] Letâ€™s break this down. \\(Y_{observed}\\): the raw, observed outcome for each observation (price for each diamond). \\(Y_{predicted}\\): the predicted outcome for each observation, based on the supplied data (`carat for each diamond). \\(Alpha\\): the predicted value of the outcome if all predictors equal zero. Also called the \\(Intercept\\), the point at which the line crosses the y-axis. \\(X_{Observed}\\) a vector of observed values of our predictor/explanatory variable. We feed each value into our model to generate a predicted outcome. \\(Beta\\): how much our outcome y (price) increases by when our predictor/explanatory variable x (carat) increases by 1. Also known as the slope of the line. \\(Error\\) or \\(Residuals\\): predicted outcome might deviate a little from the observed outcome. This deviation (\\(Y_{observed} - Y_{predicted}\\)) is call the \\(Residual\\) for each observation. Colloquially, we call it \\(Error\\). In other words, plus or minus a few \\(residuals\\), the \\(Alpha\\) (\\(Intercept\\)) plus the value of \\(X\\) times the \\(Beta\\) coefficient always gives you the value of \\(Y\\). 24.2.2 Coding Regression Models 24.2.3 lm() We can use the lm() function in R to estimate the model equation for our line of best fit. m &lt;- mydiamonds %&gt;% # save output as object &#39;m&#39; lm(formula = price ~ carat) # And display its contents here! m ## ## Call: ## lm(formula = price ~ carat, data = .) ## ## Coefficients: ## (Intercept) carat ## -2161 7559 This meansâ€¦ \\[ Price_{predicted} = -2161 + Carats_{observed} \\times 7559 \\] Learning Check 1 Question Write out the meaning of the equation above in a sentence, replacing X, Y, ALPHA, and BETA, and UNIT OF Y below with their appropriate values and meanings. Use the format below: If the value of X is zero, the model projects that Y equals ALPHA. As X increases by 1, Y is projected to increase by BETA, plus or minus a few UNIT OF Y. [View Answer!] If a diamond weighed 0 carats, the model projects that the price of that diamond would be -2161 USD. But, as the weight of that diamond increases by 1 carat, that diamondâ€™s price is projected to increase by 7559 USD, plus or minus a few dollars. 24.3 Statistical Significance We can even assess statistical significance for our alpha and beta coefficients. We can use tidy() from the broom package. mydiamonds %&gt;% lm(formula = price ~ carat) %&gt;% tidy() And if youâ€™re not satisfied with that layout, we can write our own function tidier() to get even tidier formatting! Copy and run the tidier() function, and compare your output to tidy() above. # Let&#39;s write a little tidier function.. tidier = function(model, ci = 0.95, digits = 3){ model %&gt;% # for a model object # get data.frame of coefficients # ask for a confidence interval matching the &#39;ci&#39; above! broom::tidy(conf.int = TRUE, conf.level = ci) %&gt;% # And round and relabel them summarize( term = term, # Round numbers to a certain number of &#39;digits&#39; estimate = estimate %&gt;% round(digits), se = statistic %&gt;% round(digits), statistic = statistic %&gt;% round(digits), p_value = p.value %&gt;% round(digits), # Get stars to show statistical significance stars = p.value %&gt;% gtools::stars.pval(), # Get better names upper = conf.high %&gt;% round(digits), lower = conf.low %&gt;% round(digits)) } # Let&#39;s try it out! mydiamonds %&gt;% lm(formula = price ~ carat) %&gt;% tidier() ## # A tibble: 2 Ã— 8 ## term estimate se statistic p_value stars upper lower ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2161. -22.4 -22.4 0 *** -1972. -2351. ## 2 carat 7559. 71.5 71.5 0 *** 7766. 7351. A little cleaner, right? I hope this function can help you out. term estimate se statistic p_value stars upper lower (Intercept) -2161.429 -22.401 -22.401 0 *** -1972.090 -2350.768 carat 7558.676 71.527 71.527 0 *** 7766.047 7351.305 The table above outputs several columns of importance to us! Click Here to See Definitions term: the name of the intercept as well as the predictor whose be estimate: the value of the alpha coefficient (-2161) and beta coefficient (7559). statistic: a standardized â€˜t-statisticâ€™ measuring how extreme each estimate is, based on sample size and variance in our data. se: standard error for each beta coefficient, describing the standard deviation of that statisticâ€™s sampling distribution. p_value: the probability that our alpha or beta coefficients were that large just due to chance (eg. random sampling error). Our measure of statistical significance. When 4 or more decimal places, sometimes gets abbreviated to just â€˜0â€™ in R. stars: shorthand for significance. p &lt; 0.001 = ***; p &lt; 0.01 = **; p &lt; 0.05 = *; p &lt; 0.10 = .. lower: the lower bound for the range weâ€™re 95% sure the true estimate lies in. upper: the upper bound for the range weâ€™re 95% sure the true estimate lies in. Learning Check 2 Question Write out the meaning of the equation above in a sentence, replacing X, Y, ALPHA, and BETA, UNIT OF Y, and UNIT OF OBSERVATION below with their appropriate values and meanings. Use the format below: There is a less than [0.001, 0.01, 0.05, or 0.10] probability that our alpha coefficient of ALPHA occurred due to chance. We are 95% certain that the true Y for a UNIT OF OBSERVATION that weighs 0 UNIT OF X lies between LOWER CONFIDENCE INTERVAL and UPPER CONFIDENCE INTERVAL UNIT OF Y. There is a less than [0.001, 0.01, 0.05, or 0.10] probability that our beta coefficient of BETA occurred due to chance. We are 95% certain that as X increases by 1, the true Y for a UNIT OF OBSERVATION increases by between LOWER CONFIDENCE INTERVAL and UPPER CONFIDENCE INTERVAL UNIT OF Y. [View Answer!] There is a less than probability that our alpha coefficient of USD per carat occurred due to chance. We are 95% certain that the true price for a diamond that weighs 0 carats lies between and dollars. There is a less than 0.001 probability that our beta coefficient of 7559 USD per carat occurred due to chance. We are 95% certain that as the weight of our diamond increases by 1 carat, the true price for a diamond increases by between 7351 and 7766 dollars. Notice that the value of the intercept might be nonsensical sometimes, like a negative price. 24.4 Visualization Finally, visualizing the line of best fit is quite easy! We make a scatterplot in using the ggplot2 packageâ€™s ggplot() function. Then, we add geom_smooth(method = \"lm\"). This uses the lm() function internally to make a line of best fit between our x and y variables in the aes() section of our plot. mydiamonds %&gt;% ggplot(mapping = aes(x = carat, y = price)) + geom_point(size = 3, shape = 21, fill = &quot;white&quot;, color = &quot;steelblue&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + # se = FALSE removes extra stuff theme_classic() Learning Check 3 Question Add color = cut to the aes() in the plot above. What happens? What does this tell us about the relationship between price and carats for each cut of diamond? [View Answer!] ggplot generates 5 different lines of best fit, one for each level of cut. The slope of the line differs for each cut. As carats increase, price increases at a faster rate for \"Ideal\" cut diamonds than for \"Fair\" cut diamonds. mydiamonds %&gt;% ggplot(mapping = aes(x = carat, y = price, color = cut)) + geom_point(size = 3, shape = 21, fill = &quot;white&quot;, color = &quot;steelblue&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + # se = FALSE removes extra stuff theme_classic() 24.5 Finding the Line of Best Fit We know that lm() finds the line of best fit, but how exactly does it do it? It all has to do with predicted values, residuals, and R-squared. 24.5.1 Predicted Values All models have a lot of information stored inside them, which you can access using the $ sign. (Note: select() canâ€™t extract them, because model objects are not data.frames) # This code gives you the original values put into your model (y ~ x...) # We&#39;re using head() here to show just the first few rows. m$model %&gt;% head(3) price carat 4596 1.20 1934 0.62 4840 1.14 This code gives you the the predicted values for your outcome, dubbed price_hat (\\(Y_{Predicted}\\)), and your residuals (\\(Y_{Observed} âˆ’ Y_{Predicted}\\)), given each row of data. mdat = tibble( # We can also grab the data that went into our model m$model, # We can extract the predicted values for our outcome price_hat = m %&gt;% predict(), # And our residuals residual = m %&gt;% residuals(), ) # Check it out! mdat %&gt;% head(3) ## # A tibble: 3 Ã— 4 ## price carat price_hat residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4596 1.2 6909. -2313. ## 2 1934 0.62 2525. -591. ## 3 4840 1.14 6455. -1615. 24.5.2 Residuals Residuals represent the difference between the observed outcome and the outcome predicted by the model. A regression model finds the line which minimizes these residuals, thus getting the best â€œmodelâ€ for the overall trend in the data. The animation below below visualizes residuals as lines stemming from the best fit line. The alpha and beta coefficients are varied to show changing residuals for each of these different lines, compared to the blue line, which is the actual line of best fit. Cases well predicted by the model are tiny, and close to the best fit line. Cases poorly predicted by the model are BIG, and far from the best fit line. Extra: code this as a static visual! (optional) # Visualize residuals, pulling from our new data.frame mdat mdat %&gt;% ggplot(mapping = aes( # Make x axis diamond carats, x = carat, # Make y axis price of diamonds (outcome) y = price, # Make the size of points how far away they are from predicted value size = abs(residuals), # Draw a line whose starting point is the predicted value (line) ymin = price_hat, # And whose end point is the observed value (dot) ymax = price)) + # Now draw the points geom_point(color = &quot;darkgrey&quot;, alpha = 0.5) + # Draw the line of best fit geom_smooth(method = &quot;lm&quot;, se = FALSE) + # Draw the &#39;residuals&#39;, the distance between predicted and observed geom_linerange(color = &quot;steelblue&quot;, size = 0.5, alpha = 0.5) + # Add a classic theme theme_classic() + # Remove the size legend guides(size = &quot;none&quot;) 24.5.3 R-squared R-squared (\\(R^{2}\\)) is a simple statistic that ranges from 0 - 1. R2 measures the percentage of variation in the outcome that is explained by the model. Benchmarks for \\(R^{2}\\) \\(R^{2}\\) = 1.00 -&gt; perfect fit. All variation perfectly explained. \\(R^{2}\\) = 0.85 -&gt; pretty great fit; 85% of variation in outcome explained! \\(R^{2}\\) = 0.5 -&gt; pretty good; 50% explained! \\(R^{2}\\) = 0.2 -&gt; not good, but 20% explained is better than nothing! \\(R^{2}\\) = 0 -&gt; nothing. Just nothing. Calculating \\(R^{2}\\) We calculate \\(R^{2}\\) using this formula: \\(R^{2}\\) = 1 - (Residual Sum of Squares / Total Sum of Squares) Total Sum of Squares (TSS): the sum of the squared differences between observed outcomes and their overall mean outcome. A single number describing how much the outcome varies. sum( (y - mean(y))^2 ) Residual Sum of Squares (RSS): describes on average the difference between observed outcomes and predicted outcomes. A single number describing how much the model errs from the real data. sum( (y - ypredicted)^2 ) We can combine these to understand our model: RSS / TSS: percentage of variation that remains unexplained by the model. 1 - (RSS / TSS): percentage of variation that was explained by the model. \\(R^{2}\\): percentage of variation in the outcome that was explained by the model. We can manually code this in R2! # Using our data.frame of model inputs and outputs, # let&#39;s create two summary statistics mdat %&gt;% summarize( # get sum of each squared difference between observed and mean price tss = sum((price - mean(price))^2), # get sum of each squared difference between observed and predicted price rss = sum((price - price_hat)^2)) %&gt;% # calculate R2 based on these stats mutate(R2 = 1 - rss / tss) tss rss R2 14222520095 2321500034 0.8367729 For example, the following animation shows how each of the possible lines plotted above produces a different residual sum of squares, leading to a different \\(R^{2}\\). The sweet spot, where \\(R^{2}\\) is highest (and therefore the residuals are minimized) is when the slope is closest to our actual observed beta value, $7559 per carat. Otherwise, both higher and lower slopes lead to a lower \\(R^{2}\\). Learning Check 4 Question How much of the variation in the outcome did the model explain? What statistic tells us this information? Does this mean our model fits well or poorly? [View Answer!] This model explained 84% of the variation in the outcome. The \\(R^{2}\\) statistic tells us this information. A higher \\(R^{2}\\) statistic means better model fit, and 0.84 is quite close to 1.00, the max possible model fit. 24.6 F-statistic Finally, we also want to measure how useful this model is, compared to the intercept. 24.6.1 Interpreting an F-statistic Models are imperfect approximations of trends in data. The simplest possible model of data is the intercept line, the amount of the outcome you would have if X were zero. (Imagine a flat line through your data at the intercept.) If we have a good model, it had better explain more variation than the amount explained by the intercept line, right? To do this, we can calculate an F-statistic for our model. F statistics (just like Chi-squared) range from 0 to infinity. If your F statistic is small, the model ainâ€™t much better than the intercept. If your F statistic is large, the model explains much, much more variation than the intercept. We can compare your F statistic to a null distribution of scores weâ€™d get due to chance, and find the probability we got this statistic due to chance, our p-value. The broom packageâ€™s glance() function lets us do this below, giving us a statistic and p.value. m %&gt;% glance() r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.8367729 0.8366094 1525.173 5116.183 0 1 -8747.801 17501.6 17516.32 2321500034 998 1000 ### How to calculate F statisic manually! (optional)** The F statistic requires five main ingredients: The residual sum of squares (variation NOT explained by the model) The total sum of squares (all variation IN the data) The explained sum of squares (variation EXPLAINED by the model), which is the difference between the total and residual sum of squares. The sample size (number of diamonds analyzed) Number of variables in the model (outcome + predictor = 2) ingredients &lt;- mdat %&gt;% summarize( # Calculate residual sum of squares residual_sum_of_squares = sum( (price - price_hat) ^2), # Calculate total sum of squares total_sum_of_squares = sum( (price - mean(price))^2), # Get sample size n = n(), # Calculate number of variables in your model # Here, I grabbed the model, asked it to give us the column names, # and calculated the length of that vector (2 names, so p = 2) p = length(c(&quot;price&quot;, &quot;carat&quot;) )) %&gt;% # Calculate explained sum of squares mutate(explained_sum_of_squares = total_sum_of_squares - residual_sum_of_squares) # View our result! ingredients %&gt;% glimpse() ## Rows: 1 ## Columns: 5 ## $ residual_sum_of_squares &lt;dbl&gt; 2321500034 ## $ total_sum_of_squares &lt;dbl&gt; 14222520095 ## $ n &lt;int&gt; 1000 ## $ p &lt;int&gt; 2 ## $ explained_sum_of_squares &lt;dbl&gt; 11901020061 What do we do with our ingredients to make the F-statistic then? We need to calculate on average, how much variation was explained, relative to the number of predictors, compared to on average, how much error was not explained, relative to the number of cases analyzed and variables used. ingredients %&gt;% mutate( # Mean Squares due to Regression, given the no. of predictors mean_squares_due_to_regression = explained_sum_of_squares / (p - 1), # Mean Squared Error # How much variation was NOT explained, given the sample size and no. of variables mean_squared_error = residual_sum_of_squares / (n - p)) %&gt;% # Compute the F-statistic, which is a ratio of explained vs. unexplained variation mutate(f_statistic = mean_squares_due_to_regression / mean_squared_error) %&gt;% # Finally, throw it into this pf function, # which plots a theoretical null distribution # based on the number of variables and sample size, # and identifies how extreme our F-statistic is compared to one we&#39;d get by chance # Computes p-value, from an F-statistic distribution, with 2 df statistics! mutate(p_value = pf(f_statistic, df1 = p - 1, df2 = n - p, lower.tail = FALSE)) %&gt;% # View output glimpse() ## Rows: 1 ## Columns: 9 ## $ residual_sum_of_squares &lt;dbl&gt; 2321500034 ## $ total_sum_of_squares &lt;dbl&gt; 14222520095 ## $ n &lt;int&gt; 1000 ## $ p &lt;int&gt; 2 ## $ explained_sum_of_squares &lt;dbl&gt; 11901020061 ## $ mean_squares_due_to_regression &lt;dbl&gt; 11901020061 ## $ mean_squared_error &lt;dbl&gt; 2326152 ## $ f_statistic &lt;dbl&gt; 5116.183 ## $ p_value &lt;dbl&gt; 0 And there you have it! Thatâ€™s how you calculate an F-statistic and its p-value manually. As you can see, itâ€™s a lot faster to just use the broom packageâ€™s glance() function. 24.7 summary() We learned in this workshop that every regression model generates several statistics: (1) intercept (alpha coefficient), (2) beta coefficient(s), (3) \\(R^{2}\\), and (4) F-statistic. 24.7.1 Get all stats, all at once with summary() Wouldnâ€™t it be handy if there were a convenient function that let us see all of this in one place? Try the summary() function. It can be overwhelming - it outputs lots of information. However, we only need to look in 4 places for key information. # Take our model object and get the summary! m %&gt;% summary() ## ## Call: ## lm(formula = price ~ carat, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8024.3 -760.3 -11.9 527.1 10368.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2161.43 96.49 -22.40 &lt;2e-16 *** ## carat 7558.68 105.68 71.53 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1525 on 998 degrees of freedom ## Multiple R-squared: 0.8368, Adjusted R-squared: 0.8366 ## F-statistic: 5116 on 1 and 998 DF, p-value: &lt; 2.2e-16 24.7.2 4 Key Statistics So what can you find in the summary() output? coefficients: Under Coefficients, we can see the intercept and predictor(s) in our model. Each row shows the name of a term in our model (eg. carat) and an Estimate. This is the Beta Coefficient, or in the case of the Intercept, the Alpha Coefficient. p-value: Then, at the end of each row is something called â€œPr(&gt;|t|)â€. This is the weirdest way Iâ€™ve ever seen it written, but in simple terms: itâ€™s the p-value for the Alpha or Beta coefficient. Nothing else in the table matters much. R2: Below the coefficient table are the model information. Youâ€™ll notice R-squared: ~0.84. This is where you can find R2 calculated for you. F-statistic: Beneath that is the F-statistic, 5116. At the end of the row is the p-value of the F-statistic, the more readable statistic. Learning Check 5 Question Using the filter() and lm() functions, test the effect of carat on diamond price, first looking at just \"Ideal\" diamonds. Use the summary() function, and report alpha, beta, R2, the F-statistic, and its p-value for this model. [View Answer!] m_ideal &lt;- mydiamonds %&gt;% filter(cut == &quot;Ideal&quot;) %&gt;% lm(formula = price ~ carat) m_ideal %&gt;% summary() ## ## Call: ## lm(formula = price ~ carat, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6325.2 -728.3 -12.4 480.1 9892.0 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2184.9 141.3 -15.47 &lt;2e-16 *** ## carat 7971.8 173.7 45.89 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1458 on 385 degrees of freedom ## Multiple R-squared: 0.8454, Adjusted R-squared: 0.845 ## F-statistic: 2106 on 1 and 385 DF, p-value: &lt; 2.2e-16 For ideal diamonds, the model projected that a diamond weighing 0 carats would cost -2185 USD, increasing by 7972 for every additional carat. Both coefficients were statistically significant at the p &lt; 0.001 level. The model explained 85% of the variation in ideal-cut diamond prices, and fit much better than the intercept, shown by our statistically significant f-statistic (F = 2106, p &lt; 0.001). "],["multivariate-regression-modeling-effects-of-disaster-on-social-capital.html", "25 Multivariate Regression: Modeling Effects of Disaster on Social Capital Getting Started 25.1 Multiple Regression Learning Check 1 25.2 Effect Sizes Learning Check 2 25.3 Multiple Models Learning Check 3 25.4 Great Tables Learning Check 4", " 25 Multivariate Regression: Modeling Effects of Disaster on Social Capital Why do some communities see stronger social capital than others? Social capital refers to the social ties that bind results, enabling trust and collective action among residents to tackle public issues. Recent studies suggest that after disasters, residentsâ€™ social capital actually increases, because people recognize the value of friends and family as they work to recover and rebuild. We can use regression analysis to test this hypothesis on new data! This workshop examines 151 Japanese municipalities over 7 years, from 2011 to 2017 (jp_matching_experiment.csv), totaling 1057 city-year observations. This includes 85 coastal municipalities hit by the 2011 tsunami and 66 municipalities as similar as possible, just next door, that were not hit. Letâ€™s load in our data and get started. Getting Started Load Data &amp; Packages In this dataset, each row is a city-year! # Load Packages library(tidyverse) # for data manipulation library(broom) # for each model summaries library(texreg) # for nice model tables # Load Data cities &lt;- read_csv(&quot;workshops/jp_matching_experiment.csv&quot;) %&gt;% # Tell R to treat year and pref as ordered categories mutate(year = factor(year), pref = factor(pref)) View Data # View first 3 rows of dataset cities %&gt;% head(3) muni_code muni pref year by_tsunami social_capital damage_rate pop_density exp_dis_relief_per_capita income_per_capita unemployment pop_women pop_over_age_65 02201 Aomori Aomori 2011 Hit 0.393 0.000 1119.2 0.01 1.11 5.9 53.6 27.9 02202 Hirosaki Aomori 2011 Not Hit 0.413 0.000 602.1 0.06 0.99 4.5 54.1 29.2 02203 Hachinohe Aomori 2011 Hit 0.408 0.001 1137.5 2.44 1.13 5.5 52.2 27.5 Codebook In this dataset, our variables mean: muni_code unique 5 digit idenfier for each municipality. muni: municipality where election took place pref: prefecture that municipality is in year: year of observation. by_tsunami: was that city struck by the tsunami (â€œHitâ€), not hit but just next door (â€œNot Hitâ€), or some other municipality (â€œOtherâ€)? Outcome Variable social_capital: index measuring overall social capital, the social ties between residents that build trust, using several dimensions. Measured on a scale from 0 (low) to 1 (high). Explanatory Variable damage_rate: rate of buildings damaged or destroyed by earthquake and tsunami, per million residents. Control Variables exp_dis_relief_per_capita: spending on disaster relief in 1000s of yen per capita. income_per_capita: income per capita in millions of yen per capita. unemployment: unemployment rate per 1,000 residents. pop_women: % residents who are women pop_over_age_65: % residents over age 65 pop_density: population in 1000s of residents per square kilometer 25.1 Multiple Regression 25.1.1 Beta coefficients We can use a regression model to test the association between our outcome variable social_capital and our explanatory variable by_tsunami. Using the lm() function, we can get a beta coefficient estimating how much higher a social capital index score they received for every additional building damaged per million residents. cities %&gt;% lm(formula = social_capital ~ damage_rate) ## ## Call: ## lm(formula = social_capital ~ damage_rate, data = .) ## ## Coefficients: ## (Intercept) damage_rate ## 0.39419 0.07485 25.1.2 Controls Butâ€¦ many other things might affect social capital in a community, not just getting hit by the tsunami: For example, (1) population density, (2) wealth, (3) unemployment, (4) age, (5) government capacity, (6) disaster relief, (7) time, and even (8) regional differences. We need to add control variables to our model to control for these alternative explanations for variation in social capital. This will refine our beta coefficient for the effect of the tsunami, getting us closer to the truth. We can add extra control variables using + in the lm() function. For example, we test the effect of damage_rate below, controlling for income_per_capita. cities %&gt;% lm(formula = social_capital ~ damage_rate + income_per_capita) ## ## Call: ## lm(formula = social_capital ~ damage_rate + income_per_capita, ## data = .) ## ## Coefficients: ## (Intercept) damage_rate income_per_capita ## 0.46141 0.06303 -0.06148 Our model tells us that for every building damaged per million residents, the social capital index increased by 0.06. For every additional million yen per capita in income, the average cityâ€™s social capital index increased by -0.06. 25.1.3 Planes Instead of a line of best fit, for 2 variables, this regression model now essentially predicts a plane of best fit for 3 variables. See below. And, given 4 or more variables, a regression model will predict a hyperplane of best fit. Not easy to visualize, but just think: 4 variables means 4 dimensions. 5 variables means 5 dimensions. Learning Check 1 Question Make a regression model testing the effect of a cityâ€™s damage_rate on social_capital, controlling for pop_density. Whatâ€™s the effect of damage_rate on social_capital? [View Answer!] cities %&gt;% lm(formula = social_capital ~ damage_rate + pop_density) ## ## Call: ## lm(formula = social_capital ~ damage_rate + pop_density, data = .) ## ## Coefficients: ## (Intercept) damage_rate pop_density ## 4.029e-01 6.812e-02 -9.173e-06 Controlling for population density, as damage rates increase by 1 building per million residents, the social capital index increases by 0.068 points. 25.2 Effect Sizes One big challenge with multiple regression is that itâ€™s not really clear how to compare the size of our beta coefficients. Is 1 damaged building per million residents greater than or less than 1 square kilometer per 1000 residents, or 1 million yen per capita? To compare the size of our beta coefficients, our variables must have the same units. We can do this by turning our numeric variables into Z-scores. Remember that a Z-score is a measure of how many standard deviations from the mean a specific value is for a given variable. We can mutate() variables into Z-scores using the scale() function. rescaled &lt;- cities %&gt;% # For each numeric variable, rescale its values mutate(social_capital = scale(social_capital), damage_rate = scale(damage_rate), pop_density = scale(pop_density), exp_dis_relief_per_capita = scale(exp_dis_relief_per_capita), income_per_capita = scale(income_per_capita), unemployment = scale(unemployment), pop_women = scale(pop_women), pop_over_age_65 = scale(pop_over_age_65)) Check out our new rescaled variables. rescaled %&gt;% head(3) muni_code muni pref year by_tsunami social_capital damage_rate pop_density exp_dis_relief_per_capita income_per_capita unemployment pop_women pop_over_age_65 02201 Aomori Aomori 2011 Hit -0.1296000 -0.6351305 0.1290887 -0.3617464 0.09763001 1.0562282 1.2937535 -0.6429615 02202 Hirosaki Aomori 2011 Not Hit 0.3907693 -0.6351305 -0.1932630 -0.3611821 -0.34526666 -0.1842532 1.5495231 -0.4271015 02203 Hachinohe Aomori 2011 Hit 0.2606770 -0.6225776 0.1404966 -0.3343254 0.17144612 0.7018050 0.5775988 -0.7093800 Okay, letâ€™s repeat our model, this time using our new data.frame rescaled, and save the model as m0. m0 &lt;- rescaled %&gt;% lm(formula = social_capital ~ damage_rate + pop_density) # View model m0 ## ## Call: ## lm(formula = social_capital ~ damage_rate + pop_density, data = .) ## ## Coefficients: ## (Intercept) damage_rate pop_density ## 2.387e-15 1.412e-01 -3.829e-01 We can now interpret our results as: As the damage rate increases by 1 standard deviation (new unit of predictor), the social capital index increases by 0.14 standard deviations (new unit of outcome), controlling for population density. Learning Check 2 Question Make a second regression model called m1, that also controls for the effect of year. Because we made it a factor(), we control for each year. The beta coefficient tells us now how many more standard deviations of social capital we got in year X compared to the first year (2011), our baseline for comparison. The alpha coefficient tells us how many standard deviations we got during the our baseline year. Which year had the largest effect on social capital, and how much was that effect? [View Answer!] m1 &lt;- cities %&gt;% lm(formula = social_capital ~ damage_rate + pop_density + year) # View model m1 ## ## Call: ## lm(formula = social_capital ~ damage_rate + pop_density + year, ## data = .) ## ## Coefficients: ## (Intercept) damage_rate pop_density year2012 year2013 year2014 ## 3.989e-01 6.812e-02 -9.173e-06 4.987e-03 2.238e-03 3.172e-03 ## year2015 year2016 year2017 ## 5.391e-03 5.868e-03 6.351e-03 2017 had the largest effect on social capital, compared to 2011. In 2011, the average city saw 0.3989 standard deviations above the mean of social capital. In 2017, the average city saw 0.0064 standard deviation more social capital than in 2011 (totaling 0.4053 standard deviation above the mean). Notice that we didnâ€™t rescale categorical variables. In regression, categorical variables canâ€™t be rescaled or compared to numeric variables. 25.3 Multiple Models To find the best model, it helps to make several, in a logical, systematic way. Choose your explanatory variable whose effect you really want to test. For us, thatâ€™s disaster damage (damage_rate). Add choose your absolutely most essential control variables, without which the model isnâ€™t very valid. For us, thatâ€™s pop_density and year. (Already done and saved as m1!) # For your reference m1 &lt;- rescaled %&gt;% lm(formula = social_capital ~ damage_rate + pop_density + year) Add more controls, to wean out effects of other phenomena and get a more accurate beta coefficient for damage_rate. Letâ€™s add exp_dis_relief_per_capita, to control for city government spending on disaster relief. Save that as m2. m2 &lt;- rescaled %&gt;% lm(formula = social_capital ~ damage_rate + pop_density + year + exp_dis_relief_per_capita) Examine our two tables, using the texreg packageâ€™s htmlreg() function. Weâ€™re going to list() our models m1 and m2, and ask R to save a nice table in our files as \"table_1.html\". Try it out, then go to your files in the right-hand corner and click 'View in Web Browser'! htmlreg(list(m1,m2), bold = 0.05, include.fstat = TRUE, file = &quot;workshops/workshop_11_table_1.html&quot;) [Click to view table!] Statistical models &nbsp; Model 1 Model 2 (Intercept) -0.10 -0.09 &nbsp; (0.07) (0.07) damage_rate 0.14*** 0.02 &nbsp; (0.03) (0.03) pop_density -0.38*** -0.37*** &nbsp; (0.03) (0.03) year2012 0.13 0.08 &nbsp; (0.11) (0.10) year2013 0.06 0.02 &nbsp; (0.11) (0.10) year2014 0.08 0.09 &nbsp; (0.11) (0.10) year2015 0.14 0.15 &nbsp; (0.11) (0.10) year2016 0.15 0.15 &nbsp; (0.11) (0.10) year2017 0.17 0.16 &nbsp; (0.11) (0.10) exp_dis_relief_per_capita &nbsp; 0.22*** &nbsp; &nbsp; (0.03) R2 0.17 0.21 Adj. R2 0.17 0.20 Num. obs. 1057 1057 F statistic 27.50 30.63 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Pretty nice, right? The bold = 0.05 says, if your p-value is below p &lt; 0.05, make the estimate bold in the chart, so itâ€™s easy to see. include.fstat = TRUE means, please include the F-statistic at the bottom of the chart. Learning Check 3 Question Make another model called m3, adding as controls income_per_capita, unemployment, pop_women, and pop_over_age_65. Then make a model called m4, which adds pref, the prefecture each city is in (like their state). Finally, put them together in a htmlreg() table that visualizes m1, m2, m3, and m4 side by side, called \"table_2.html\". Look at the R-squared statistic at the bottom. Which model fits best? [View Answer!] Adding controls income_per_capita, unemployment, pop_women, and pop_over_age_65â€¦ m3 &lt;- rescaled %&gt;% lm(formula = social_capital ~ damage_rate + pop_density + year + exp_dis_relief_per_capita + income_per_capita + unemployment + pop_women + pop_over_age_65) Adding prefectural controlsâ€¦ m4 &lt;- rescaled %&gt;% lm(formula = social_capital ~ damage_rate + year + pop_density + exp_dis_relief_per_capita + income_per_capita + unemployment + pop_women + pop_over_age_65 + pref) Making a nice table! htmlreg(list(m1,m2,m3,m4), bold = 0.05, include.fstat = TRUE, file = &quot;workshops/workshop_11_table_2.html&quot;) Model 4 fits best, with an R2 of 0.89. It explains 89% of the variation in social capital! Thatâ€™s wild! [Click to view Table from Answer!] Statistical models &nbsp; Model 1 Model 2 Model 3 Model 4 (Intercept) -0.10 -0.09 -0.18&#42;&#42; 1.41&#42;&#42;&#42; &nbsp; (0.07) (0.07) (0.06) (0.10) damage_rate 0.14&#42;&#42;&#42; 0.02 -0.03 -0.10&#42;&#42;&#42; &nbsp; (0.03) (0.03) (0.03) (0.02) pop_density -0.38&#42;&#42;&#42; -0.37&#42;&#42;&#42; -0.12&#42;&#42;&#42; -0.20&#42;&#42;&#42; &nbsp; (0.03) (0.03) (0.03) (0.02) year2012 0.13 0.08 0.01 0.13&#42;&#42;&#42; &nbsp; (0.11) (0.10) (0.09) (0.04) year2013 0.06 0.02 0.04 0.05 &nbsp; (0.11) (0.10) (0.09) (0.04) year2014 0.08 0.09 0.14 0.08&#42; &nbsp; (0.11) (0.10) (0.09) (0.04) year2015 0.14 0.15 0.24&#42;&#42; 0.13&#42;&#42;&#42; &nbsp; (0.11) (0.10) (0.09) (0.04) year2016 0.15 0.15 0.35&#42;&#42;&#42; 0.13&#42;&#42; &nbsp; (0.11) (0.10) (0.09) (0.04) year2017 0.17 0.16 0.46&#42;&#42;&#42; 0.13&#42;&#42; &nbsp; (0.11) (0.10) (0.09) (0.04) exp_dis_relief_per_capita &nbsp; 0.22&#42;&#42;&#42; 0.11&#42;&#42;&#42; 0.04&#42; &nbsp; &nbsp; (0.03) (0.03) (0.02) income_per_capita &nbsp; &nbsp; -0.77&#42;&#42;&#42; 0.10&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; (0.05) (0.03) unemployment &nbsp; &nbsp; -0.29&#42;&#42;&#42; -0.13&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; (0.03) (0.01) pop_women &nbsp; &nbsp; -0.02 0.03&#42; &nbsp; &nbsp; &nbsp; (0.03) (0.01) pop_over_age_65 &nbsp; &nbsp; -0.49&#42;&#42;&#42; -0.14&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; (0.03) (0.02) prefAomori &nbsp; &nbsp; &nbsp; -1.24&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.10) prefChiba &nbsp; &nbsp; &nbsp; -2.82&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.10) prefFukushima &nbsp; &nbsp; &nbsp; -0.24&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.10) prefIbaraki &nbsp; &nbsp; &nbsp; -1.52&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.11) prefIwate &nbsp; &nbsp; &nbsp; -0.25&#42;&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.10) prefMiyagi &nbsp; &nbsp; &nbsp; -1.30&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.10) R2 0.17 0.21 0.43 0.89 Adj. R2 0.17 0.20 0.43 0.89 Num. obs. 1057 1057 1057 1057 F statistic 27.50 30.63 61.45 446.20 &#42;&#42;&#42;p &lt; 0.001; &#42;&#42;p &lt; 0.01; &#42;p &lt; 0.05 25.4 Great Tables Finally, letâ€™s add a few bells and whistles to our model table, to make it look really nice. [Click here to learn about texreg() arguments!] custom.model.names lets you add names for each column. custom.coef.map lets you rename variables. It also lets you rearrange them in whatever order makes sense to you. Only variables you rename will stay in the table, so it also will let us exclude the year effects, which are a few too numerous to report. caption adds a nice title. caption.above = TRUE puts it on top of the table. custom.note adds a footnote. Always indicate levels of statistical significance. single.row = TRUE puts everything on one row, which is helpful. htmlreg( list(m1,m2,m3,m4), bold = 0.05, include.fstat = TRUE, file = &quot;workshops/workshop_11_table_3.html&quot;, # Add column labels custom.model.names = c( &quot;Basic Model&quot;, &quot;with Controls&quot;, # You can split lines in two with &lt;br&gt; &quot;With Extended&lt;br&gt;Controls&quot;, &quot;With Geographic&lt;br&gt;Controls&quot;), # Add labels custom.coef.map = list( &quot;damage_rate&quot; = &quot;Damage Rate&quot;, &quot;exp_dis_relief_per_capita&quot; = &quot;Disaster Spending Rate&quot;, &quot;income_per_capita&quot; = &quot;Income per capita&quot;, &quot;unemployment&quot; = &quot;Unemployment Rate&quot;, &quot;pop_women&quot; = &quot;% Women&quot;, &quot;pop_over_age_65&quot; = &quot;% Over Age 65&quot;, &quot;prefAomori&quot; = &quot;Aomori&quot;, &quot;prefChiba&quot; = &quot;Chiba&quot;, &quot;prefFukushima&quot; = &quot;Fukushima&quot;, &quot;prefIbaraki&quot; = &quot;Ibaraki&quot;, &quot;prefIwate&quot; = &quot;Iwate&quot;, &quot;prefMiyagi&quot; = &quot;Miyagi&quot;, &quot;(Intercept)&quot; = &quot;Intercept&quot;), # Add a table caption caption = &quot;OLS Model of Social Capital in Japanese Cities over 7 years&quot;, # You can still add a custom note too! custom.note = &quot;Statistical Significance: *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Akita is the baseline prefecture. All models also control for each year (2011-2017) as a categorical variable.&quot;) [Click to view table!] OLS Model of Social Capital in Japanese Cities over 7 years &nbsp; Basic Model with Controls With ExtendedControls With GeographicControls Damage Rate 0.14*** 0.02 -0.03 -0.10*** &nbsp; (0.03) (0.03) (0.03) (0.02) Disaster Spending Rate &nbsp; 0.22*** 0.11*** 0.04* &nbsp; &nbsp; (0.03) (0.03) (0.02) Income per capita &nbsp; &nbsp; -0.77*** 0.10*** &nbsp; &nbsp; &nbsp; (0.05) (0.03) Unemployment Rate &nbsp; &nbsp; -0.29*** -0.13*** &nbsp; &nbsp; &nbsp; (0.03) (0.01) % Women &nbsp; &nbsp; -0.02 0.03* &nbsp; &nbsp; &nbsp; (0.03) (0.01) % Over Age 65 &nbsp; &nbsp; -0.49*** -0.14*** &nbsp; &nbsp; &nbsp; (0.03) (0.02) Aomori &nbsp; &nbsp; &nbsp; -1.24*** &nbsp; &nbsp; &nbsp; &nbsp; (0.10) Chiba &nbsp; &nbsp; &nbsp; -2.82*** &nbsp; &nbsp; &nbsp; &nbsp; (0.10) Fukushima &nbsp; &nbsp; &nbsp; -0.24* &nbsp; &nbsp; &nbsp; &nbsp; (0.10) Ibaraki &nbsp; &nbsp; &nbsp; -1.52*** &nbsp; &nbsp; &nbsp; &nbsp; (0.11) Iwate &nbsp; &nbsp; &nbsp; -0.25** &nbsp; &nbsp; &nbsp; &nbsp; (0.10) Miyagi &nbsp; &nbsp; &nbsp; -1.30*** &nbsp; &nbsp; &nbsp; &nbsp; (0.10) Intercept -0.10 -0.09 -0.18** 1.41*** &nbsp; (0.07) (0.07) (0.06) (0.10) R2 0.17 0.21 0.43 0.89 Adj. R2 0.17 0.20 0.43 0.89 Num. obs. 1057 1057 1057 1057 F statistic 27.50 30.63 61.45 446.20 Statistical Significance: *** p Learning Check 4 Question Make a new texreg table called \"table_4.html\", but this time, remove the pref categorical effects from the table, and make a note in the custom note of in which model we controlled for prefecture. Finally, whatâ€™s the effect of disaster damage in our final model? How significant is that effect? [View Answer!] htmlreg( list(m1,m2,m3,m4), bold = 0.05, include.fstat = TRUE, file = &quot;workshops/workshop_11_table_4.html&quot;, custom.model.names = c( &quot;Basic Model&quot;, &quot;with Controls&quot;, &quot;With Extended&lt;br&gt;Controls&quot;, &quot;With Geographic&lt;br&gt;Controls&quot;), custom.coef.map = list( &quot;damage_rate&quot; = &quot;Damage Rate&quot;, &quot;exp_dis_relief_per_capita&quot; = &quot;Disaster Spending Rate&quot;, &quot;income_per_capita&quot; = &quot;Income per capita&quot;, &quot;unemployment&quot; = &quot;Unemployment Rate&quot;, &quot;pop_women&quot; = &quot;% Women&quot;, &quot;pop_over_age_65&quot; = &quot;% Over Age 65&quot;, # Notice I removed the prefectures here &quot;(Intercept)&quot; = &quot;Intercept&quot;), caption = &quot;OLS Model of Social Capital in Japanese Cities over 7 years&quot;, # Notice I added more to the note. custom.note = &quot;Statistical Significance: *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. All models also control for each year (2011-2017) as a categorical variable. Final model also controls for prefectures. Akita is the baseline prefecture.&quot;) [Click to view table from Answer!] OLS Model of Social Capital in Japanese Cities over 7 years &nbsp; Basic Model with Controls With ExtendedControls With GeographicControls Damage Rate 0.14*** 0.02 -0.03 -0.10*** &nbsp; (0.03) (0.03) (0.03) (0.02) Disaster Spending Rate &nbsp; 0.22*** 0.11*** 0.04* &nbsp; &nbsp; (0.03) (0.03) (0.02) Income per capita &nbsp; &nbsp; -0.77*** 0.10*** &nbsp; &nbsp; &nbsp; (0.05) (0.03) Unemployment Rate &nbsp; &nbsp; -0.29*** -0.13*** &nbsp; &nbsp; &nbsp; (0.03) (0.01) % Women &nbsp; &nbsp; -0.02 0.03* &nbsp; &nbsp; &nbsp; (0.03) (0.01) % Over Age 65 &nbsp; &nbsp; -0.49*** -0.14*** &nbsp; &nbsp; &nbsp; (0.03) (0.02) Intercept -0.10 -0.09 -0.18** 1.41*** &nbsp; (0.07) (0.07) (0.06) (0.10) R2 0.17 0.21 0.43 0.89 Adj. R2 0.17 0.20 0.43 0.89 Num. obs. 1057 1057 1057 1057 F statistic 27.50 30.63 61.45 446.20 Statistical Significance: *** p "],["design-of-experiments-in-r.html", "26 Design of Experiments in R Getting Started 26.1 Difference of Means (t-tests) 26.2 Definitions: Null vs.Â Sampling Distributions 26.3 Null Distributions 26.4 Sampling Distribution 26.5 Speedy t-tests 26.6 ANOVA Conclusion", " 26 Design of Experiments in R Figure 2.1: Donuts! This workshop introduces basic statistical techniques for conducting experiments, including t-tests, anova, and F-tests, among others. Getting Started Suppose Kimâ€™s Coffee is a growing coffee chain in upstate New York. For the past 10 years, they have been using old Krispy-Kreme donut-making machines to make their locally renowned donuts. Since the machines are quickly approaching the end of their lifespan, the owners of Kimâ€™s Coffee want to try out an alternative donut-machine - the Monster-Donut Maker 5000 - to see if it can produce donuts of comparable quality! They decide to design an experiment! The Experiment At their main branch, they install a Monster-Donut Maker 5000, which weâ€™ll call type = \"b\", right next to their Krispy-Kreme Donut Machine, which weâ€™ll call type = \"a\". They plan to measure the weight (grams), lifespan at room temperature (hours), and tastiness (0-10) of each donut produced from both machines. Using the same ingredients, they bake the donuts for the same amount of time under the same level of heat. Each scoop they take from their dough, they randomly assign it to get baked in machine a or machine b. But they need help! Theyâ€™ve hired us to measure the impacts of their potential new donut machine (type = \"b\"), compared to their existing machines (type = \"a\"). Letâ€™s get started! Import Data First, letâ€™s import our data, where each row is a donut, with produced in one type of machine, with a specific weight, lifespan, and tastiness, each made by a baker named \"Kim\", \"Melanie\", or \"Craig\". # Load packages, to get our dplyr, ggplot, tibble, and readr functions library(tidyverse) library(broom) # get our tidy() function # Read data! donuts = read_csv(&quot;workshops/donuts.csv&quot;) # Having trouble reading in your data? # You can also use this code: donuts = read_csv(&quot;https://raw.githubusercontent.com/timothyfraser/sysen/main/workshops/donuts.csv&quot;) # Check it out! donuts %&gt;% glimpse() ## Rows: 50 ## Columns: 5 ## $ type &lt;chr&gt; &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, â€¦ ## $ weight &lt;dbl&gt; 28.2, 26.1, 27.2, 25.7, 29.4, 28.0, 27.4, 30.5, 29.8, 27.9, â€¦ ## $ lifespan &lt;dbl&gt; 52, 54, 38, 52, 49, 50, 45, 48, 53, 52, 52, 47, 54, 41, 49, â€¦ ## $ tastiness &lt;dbl&gt; 1, 4, 1, 2, 4, 2, 3, 3, 3, 4, 5, 2, 2, 4, 2, 3, 1, 1, 3, 3, â€¦ ## $ baker &lt;chr&gt; &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Melâ€¦ 26.1 Difference of Means (t-tests) 26.1.1 Calculating dbar Whenever we set up an experiment, we need to select a control group and a treatment group. In this case, our control group is the donuts produced by the original machine (type = \"a\"), while our treatment group is the donuts produced by the new machine (type = b). We can then calculate a single number to summarize the relationship between these 2 groups - a statistic called the difference of means \\(\\bar{d}\\)! Itâ€™s literally: \\(\\bar{d} = \\bar{x}_{treatment} - \\bar{x}_{control}\\), where \\(\\bar{x}_{treatment}\\) is the mean outcome for the treatment group and \\(\\bar{x}_{control}\\) is the mean outcome for the control group. We can use group_by() and summarize() from the dplyr package to calculate the difference diff. diff = donuts %&gt;% group_by(type) %&gt;% summarize(xbar = mean(weight)) Then, we can pivot the shape of this data.frame with summarize() again, so that our data.frame has just all itâ€™s info in just 1 row: Weâ€™ll grab the xbar value from the row where type == \"a\" and putting it into the column xbar_a. Weâ€™ll repeat this with xbar_b, putting the xbar value from the row where type == \"b\" into the xbar_b column. Finally, we can calculate dbar, the difference of means, by subtracting xbar_b from xbar_a! stat = diff %&gt;% summarize(xbar_a = xbar[type == &quot;a&quot;], xbar_b = xbar[type == &quot;b&quot;], dbar = xbar_b - xbar_a) In summary, the mean weight of a donut from the Krispy Kreme Machine (type = a) was 28.364 grams, while the mean weight of a donut from the Monster Donut Maker 5000 (type = b) was 31.716 grams. This means that our treatment group of donuts (our type = b donuts) tended to be 3.352 grams heavier than our control group! 26.1.2 Visualizing the difference of means We can visualize the difference of means pretty quickly, using geom_jitter(). donuts %&gt;% ggplot(mapping = aes(x = type, y = weight)) + geom_jitter(height = 0, width = 0.1, size = 3, alpha = 0.5, # Let&#39;s make a a bunch of donuts, using shape = 21 with white fill shape = 21, fill = &quot;white&quot;, color = &quot;goldenrod&quot;, # and increase the width of the outline with &#39;stroke&#39; stroke = 5) + theme_classic(base_size = 14) We can see from above that donuts from machine b do sure seem to weight a little bit more. Letâ€™s estimate whether this difference is significant, below! 26.2 Definitions: Null vs.Â Sampling Distributions But, how much should we trust our statistic \\(\\bar{d}\\) (dbar)? We can use (1) null distributions and (2) sampling distributions to characterize our level of trust in our statistic using different statistics. Specifically: p-values are statistics that describe Null Distributions confidence intervals are statistics that describe Sampling Distributions. Letâ€™s build a working definition of these two terms. 26.3 Null Distributions Maybe our statistic dbar is not much different from the many possible dbars we might get if our choice of donut maker were not related to donut weight. To account for this, we can approximate the null distribution of all statistics we would get if our grouping variable were not related to our outcome, and generate a p-value for dbar. 26.3.1 Permutation Test (Computational Approach) How can we see the null distribution? We can use resampling without replacement on our donuts data, permuting or shuffling which treatment group our donut ingredients got assigned to. This breaks any association between the outcome weight and our grouping variable type, such that any statistic we get is purely due to chance. Weâ€™ll mutate() our type vector using the sample(replace = FALSE) function, for 1000 repetitions of our data, creating 1000 random datasets, then calculate 1000 differences of means. perm = tibble(rep = 1:1000) %&gt;% group_by(rep) %&gt;% summarize(donuts) %&gt;% group_by(rep) %&gt;% mutate(type = sample(type, size = n(), replace = FALSE)) %&gt;% group_by(rep, type) %&gt;% summarize(xbar = mean(weight)) %&gt;% group_by(rep) %&gt;% summarize(xbar_a = xbar[type == &quot;a&quot;], xbar_b = xbar[type == &quot;b&quot;], dbar = xbar_b - xbar_a) # Many different dbar statistics from many permuted datasets, where the statistic is ENTIRELY due to chance. perm %&gt;% head(3) ## # A tibble: 3 Ã— 4 ## rep xbar_a xbar_b dbar ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 30.0 30.1 0.128 ## 2 2 29.7 30.4 0.648 ## 3 3 30.4 29.7 -0.760 Now, letâ€™s calculate - what percentage of random statistics were more extreme than than our observed statistic? perm %&gt;% summarize( # Get our observed statistic estimate = stat$dbar, # For a 2 tailed test... # Turn all dbars positive (this takes care of the 2-tails) # Now, what percentage of random stats were greater than the observed? # That&#39;s our p-value! p_value = ( sum(abs(dbar) &gt;= abs(estimate) ) / n() )) ## # A tibble: 1 Ã— 2 ## estimate p_value ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.35 0 Wow! That is a super statistically significant difference of means! We can visualize it like this. Pretty extreme, huh? perm %&gt;% ggplot(mapping = aes(x = dbar)) + geom_density(fill = &quot;steelblue&quot;, color = &quot;white&quot;) + theme_classic(base_size = 14) + geom_vline(xintercept = stat$dbar, linetype = &quot;dashed&quot;) + labs(x = &quot;Difference of Means&quot;, y = &quot;Probability (%)&quot;) 26.3.2 t-statistics (Theoretical Approach) Alternatively, if we know the standard error of our statistic, we can calculate a t-statistic, a standardized measure telling us how extreme is our statistic in terms of a \\(t\\) distribution. We can calculate it as \\(t = \\bar{d} / \\sigma\\), where \\(\\sigma\\) is the standard error. Much like the \\(\\chi^{2}\\) distribution, a \\(t\\) distribution is a null distribution of random statistics we could get due to chance, except the \\(t\\) distribution is two-tailed. \\(t\\) ranges from -Infinity to +Infinity, with a mean of 0. Like the \\(\\chi^{2}\\) distribution, the \\(t\\) distribution gets stretched out wider for low sample sizes, but narrower for large sample sizes, to reflect our increasing precision as sample sizes increase. For any df (degrees of freedom), we can draw a t-distribution, where df = \\(n_{pairs} - 1\\). \\(t\\) statistics have their own dt(), pt(), and qt() functions in R, just like all other distributions. we can use pt() to calculate p-values, our chance of error. Suppose for a moment that we already found the standard error and we went and calculated our t-statistic, and we also have the degrees of freedom. We can approximate the null distribution as a t-distribution, for any number of degrees of freedom. See below for an example! tibble(df = c(2, 5, 100)) %&gt;% group_by(df) %&gt;% summarize(t = seq(from = -5, to = 5, by = 0.05), prob = dt(t, df)) %&gt;% ggplot(mapping = aes(x = t, y = prob, color = factor(df), group = df)) + geom_line() + theme_classic() + labs(color = &quot;Degrees of\\nFreedom (df)&quot;, x = &quot;t-statistic&quot;, y = &#39;Probability&#39;, subtitle = &quot;Student&#39;s T distribution by df&quot;) Weâ€™ll learn below how to calculate a t-statistic. The main value added behind a t-statistic is that it is computationally very easy to obtain, whereas a permutation test requires us to generate 1000s of random permutations - a more computationally expensive method. Why Do we Assume Normal Distributions?: The reason why we rely on these assumptions is because for decades, it was not possible to approximate and visualize the shape of the of these distributions, because it was so computationally expensive! Thatâ€™s why we wrote up giant tables of \\(k\\), \\(\\chi^{2}\\), and \\(t\\) statistics instead, so that researchers could just calculate those statistics once and share the tables, rather than computing them over and over! But, today, itâ€™s actually quite easy, thanks to faster computers and great statistical calculators like R, to compute \\(t\\), \\(k\\), and \\(\\chi^{2}\\) values in milliseconds, and we can even gather thousands of random samples to visualize the exact shape of our null and sampling distributions. 26.4 Sampling Distribution Maybe our statistic dbar might have been just a little higher or lower had we produced just a slightly different sample of donuts by chance. To account for this, we can approximate the sampling distribution for our statistic dbar and generate a confidence interval around dbar. 26.4.1 Bootstrapped CIs for dbar If we have lots of computational power available (we do now that itâ€™s 2020), we could bootstrap our data, taking 1000 versions of our dataset and resampling with replacement to simulate sampling error, and calculate dbar 1000 times to get the sampling distribution. # The the donuts 1000 times stat_boot = tibble(rep = 1:1000) %&gt;% group_by(rep) %&gt;% summarize(donuts) %&gt;% # For each rep, randomly resample donuts group_by(rep) %&gt;% sample_n(size = n(), replace = TRUE) %&gt;% # For each rep and treatment group, get the mean group_by(rep, type) %&gt;% summarize(xbar = mean(weight)) %&gt;% # For each rep, get dbar group_by(rep) %&gt;% summarize(xbar_a = xbar[type == &quot;a&quot;], xbar_b = xbar[type == &quot;b&quot;], dbar = xbar_b - xbar_a) # Voila! Look at those beautiful `dbar` statistics! # We&#39;ve got a full 1000 slightly varying dbars! stat_boot %&gt;% head() ## # A tibble: 6 Ã— 4 ## rep xbar_a xbar_b dbar ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 28.3 31.9 3.60 ## 2 2 28.2 32.7 4.50 ## 3 3 28.3 30.7 2.39 ## 4 4 28.3 31.7 3.42 ## 5 5 28.2 31.2 2.99 ## 6 6 28.4 30.9 2.56 We can then use summarize() to compute quantities of interest from our bootstrapped sampling distribution of dbar in stat_boot, like the standard deviation (which would be the literal standard error, in this case), and confidence intervals. stat_boot %&gt;% summarize( # Let&#39;s grab our observed dbar statistic from &#39;stat&#39; estimate = stat$dbar, # standard error of sampling distribution.... se = sd(dbar), # lower 2.5th percentile lower = quantile(dbar, prob = 0.025), # upper 97.5th percentile upper = quantile(dbar, prob = 0.975)) ## # A tibble: 1 Ã— 4 ## estimate se lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.35 0.764 1.92 4.89 26.4.2 Assume Normal CIs for dbar Alternatively, if we donâ€™t have lots of computational power, we could assume a normally distributed sampling distribution. Though bootstrapping is usually more accurate, it is still more common to assume your sampling distribution is normally distributed and calculate confidence intervals using a standard error instead. Hereâ€™s how you do it. 26.4.2.1 Paired-Sample/Dependent Sample T-Test (\\(n_1 = n_2\\)) Suppose our treatment and control group had the same sample size (they do in this case). Then, we can calculate the standard error of the difference of means \\(\\bar{d}\\) as: \\[ \\sigma = \\frac{ s_{\\bar{d}} }{ \\sqrt{n} } = \\sqrt{ \\frac{ \\sum{ (d - \\bar{d} )^{2} } }{ n - 1} } \\times \\frac{1}{\\sqrt{n}} \\] Here, \\(d\\) refers to the differences between group a and b. (This is pretty unrealistic.) You can only calculate it if the sample size of your treatment and control group is identical. For example: # In a paired sample t-test, our data might look like this.s paired = donuts %&gt;% summarize(weight_a = weight[type == &quot;a&quot;], weight_b = weight[type == &quot;b&quot;]) %&gt;% # and we could calculate their differences &#39;d&#39; like so: mutate(d = weight_b - weight_a) # Check it paired %&gt;% glimpse() ## Rows: 25 ## Columns: 3 ## $ weight_a &lt;dbl&gt; 28.2, 26.1, 27.2, 25.7, 29.4, 28.0, 27.4, 30.5, 29.8, 27.9, 2â€¦ ## $ weight_b &lt;dbl&gt; 28.1, 29.3, 28.4, 28.8, 31.7, 30.4, 27.5, 28.8, 30.0, 34.7, 3â€¦ ## $ d &lt;dbl&gt; -0.1, 3.2, 1.2, 3.1, 2.3, 2.4, 0.1, -1.7, 0.2, 6.8, 8.6, 1.5,â€¦ And we could then calculate the standard error like soâ€¦ paired %&gt;% summarize( # Get the total pairs of donuts analyzed n = n(), # Get mean difference / difference of means &#39;d&#39; dbar = mean(d), # Calculate the variance of d, called &#39;variance&#39; variance = sum( (d - dbar)^2 ) / (n - 1), # Calculate the standard deviation of d, by taking the square root of the variance s_d = sqrt(variance), # Calculate the standard error sigma of d, # by dividing the standard deviation by the total sample size se = s_d / sqrt(n)) ## # A tibble: 1 Ã— 5 ## n dbar variance s_d se ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 3.35 15.6 3.95 0.789 So, knowing what we do about the t-distribution, we can now run our paired sample t-test pretty quickly. Once we have a t-statistic, itâ€™s pretty easy to then calculate a p-value, the share of statistics more extreme than ours. Letâ€™s try calculating it all in one step! paired %&gt;% summarize(n = n(), dbar = mean(d), se = sqrt( sum( (d - dbar)^2 ) / (n - 1) ) / sqrt(n), df = n - 1, # Let&#39;s calculate our t-statistic, called &#39;statistic&#39; statistic = dbar / se, # Get the **two-tailed** p-value # by looking at the right side of the null distribution # getting the percentage of stats greater than abs(statistic) # subtracting that from 1 to get the one-tailed p-value # then multiplying that percentage by 2, # to represent the combined error on BOTH tails p_value = 2*(1 - pt(abs(statistic), df)), # Let&#39;s also calculate our multiplier t on the t-distribution # which will tell us how many standard deviations from the mean is the 97.5th percentile t = qt(0.975, df), # Next, let&#39;s calculate the upper and lower confidence intervals! upper = dbar + t * se, lower = dbar - t * se) ## # A tibble: 1 Ã— 9 ## n dbar se df statistic p_value t upper lower ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 3.35 0.789 24 4.25 0.000282 2.06 4.98 1.72 Wheeeeee! Statistics!!!!!! 26.4.2.2 Unpaired-Sample/Independent Sample T-Test On the other hand, suppose that our treatment and control group did not have the same sample size. (They usually donâ€™t.) We can alternatively calculate the standard error using the standard deviations (\\(s_1\\) and \\(s_2\\)) and sample sizes (\\(n_1\\) and \\(n_2\\)) of the treatment and control groups. \\[ \\sigma = \\sqrt{ \\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}} } \\] Letâ€™s try it! # For our unpaired sampled t-test... unpaired = donuts %&gt;% # For each type (donut maker) group_by(type) %&gt;% # Calculate our basic ingredients... summarize(xbar = mean(weight), # mean of each group s = sd(weight), # standard deviation of each group n = n()) %&gt;% # sample size of each group # then let&#39;s pivot the data into one row... summarize( # Get the means.... xbar_a = xbar[type == &quot;a&quot;], xbar_b = xbar[type == &quot;b&quot;], # Get the standard deviations... s_a = s[type == &quot;a&quot;], s_b = s[type == &quot;b&quot;], # Get the sample sizes... n_a = n[type == &quot;a&quot;], n_b = n[type == &quot;b&quot;]) unpaired ## # A tibble: 1 Ã— 6 ## xbar_a xbar_b s_a s_b n_a n_b ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 28.4 31.7 1.63 3.60 25 25 We should also probably assume that our variances are not equal, meaning that we need to use a new formula to calculate our degrees of freedom. That formula is absolutely ridiculously large! \\[ df = \\frac{ ( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2 }{ \\frac{ (s_1^2 / n_1)^2 }{ n_1 - 1 } + \\frac{(s_2^2 / n_2)^2 }{ n_2 - 1 }} \\] Now, thatâ€™s pretty silly, so Iâ€™ve written a short function for us here. In a moment, weâ€™re about to learn a really-really short method for t-tests, but itâ€™s important that you know how it works first. df = function(s1, n1, s2, n2){ # Calculate our ingredients... top = (s1^2 / n1 + s2^2 / n2)^2 bottom_left = (s1^2/n1)^2 / (n1 - 1) bottom_right = (s2^2/n2)^2 / (n2 - 1) # Return the degrees of freedom top / (bottom_left + bottom_right) } Now, letâ€™s calculate our quantities of interest! unpaired %&gt;% # Let&#39;s calculate... summarize( # difference of means dbar = xbar_b - xbar_a, # standard error se = sqrt(s_a^2 / n_a + s_b^2 / n_b), # t-statistic statistic = dbar / se, # degrees of freedom df = df(s1 = s_a, s2 = s_b, n1 = n_a, n2 = n_b), # p-value p_value = 2*(1 - pt(abs(statistic), df = df)), # multiplier t t = qt(0.975, df), # lower 2.5% CI lower = dbar - t * se, # upper 97.5% CI upper = dbar + t * se) ## # A tibble: 1 Ã— 8 ## dbar se statistic df p_value t lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.35 0.790 4.24 33.5 0.000164 2.03 1.75 4.96 26.5 Speedy t-tests Wow, that took a bit! Wouldnâ€™t it be nice if some statisticians, social scientists, and natural scientists teamed up to write a single R function that did all that really quickly? They did! Itâ€™s called t.test(). You can query it using the code below, then extract its results into a data.frame using tidy() from the broom package. It allows you to simply dictate whether you want a paired test or not (paired) and whether you assume the variances in each group to be equal or noth (var.equal). Thereâ€™s just one catch. You must (!!!) turn your grouping variable type into a factor, where the treatment group comes first, followed by the control group. Otherwise, R will default to alphabetical order when calculating the difference of means. (Very important!) donuts2 = donuts %&gt;% # Convert type to factor, where treatment group b is first mutate(type = factor(type, levels = c(&quot;b&quot;, &quot;a&quot;))) paired t-test (equal variance) t.test(outcome_group_a ~ outcome_group_b, paired = TRUE, var.equal = TRUE) donuts2 %&gt;% # extract individual vectors of weight for each type reframe(a = weight[type == &quot;a&quot;], b = weight[type == &quot;b&quot;]) %&gt;% # Run our t-test using the data from donuts, then tidy() it into a data.frame summarize(t.test(b, a, paired = TRUE, var.equal = TRUE) %&gt;% tidy()) ## # A tibble: 1 Ã— 8 ## estimate statistic p.value parameter conf.low conf.high method alternative ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3.35 4.25 0.000282 24 1.72 4.98 Paired tâ€¦ two.sided unpaired t-test (equal variance): t.test(outcome_var ~ group_var, var.equal = TRUE) # Or, more simply for an unpaired t-test... donuts2 %&gt;% # Run our t-test using a formula format (eg. y ~ x) on the data from donuts, then tidy() it into a data.frame summarize( t.test(weight ~ type, var.equal = TRUE) %&gt;% tidy() ) ## # A tibble: 1 Ã— 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.35 31.7 28.4 4.24 0.000100 48 1.76 4.94 ## # â„¹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; unpaired t-test (unequal variance): t.test(outcome_var ~ group_var, var.equal = FALSE) donuts2 %&gt;% # Run our t-test using the data from donuts, then tidy() it into a data.frame summarize( t.test(weight ~ type, var.equal = FALSE) %&gt;% tidy() ) ## # A tibble: 1 Ã— 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.35 31.7 28.4 4.24 0.000164 33.5 1.75 4.96 ## # â„¹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; 26.6 ANOVA Alternatively, what if weâ€™re comparing across multiple groups? Suppose, for a moment, that 3 people were involved in the donut experiment, where each person handled a different number of donuts. What if any differences were not due to the donut machine, but actually due to the person who made the donut? We can test differences in a numeric outcome like weight between 3 or more groups (eg. baker) using Analysis of Variance (ANOVA). 26.6.1 Visualizing ANOVA Practically speaking, ANOVA tries to ascertain how far apart three distributions are from each other, and the F statistic (ranging from 0 to Inf) shows how extreme is the difference between these distributions. If minor, F is near 0. If large gaps, F grows. donuts %&gt;% ggplot(mapping = aes(x = baker, y = weight, color = baker)) + geom_jitter(height = 0, width = 0.1, size = 3, alpha = 0.5, # Let&#39;s make a a bunch of donuts, using shape = 21 with white fill shape = 21, fill = &quot;white&quot;, color = &quot;goldenrod&quot;, # and increase the width of the outline with &#39;stroke&#39; stroke = 5) + theme_classic(base_size = 14) I dunno what Craigâ€™s doing, but his donuts are definitely not like Melanie or Kimâ€™s donuts. Craigâ€™s donuts are like lead bricks. So, can we put a number to just how bad Craig is at making donuts? 26.6.2 ANOVA Explained To do analysis of variance, weâ€™re going to go through a few quick steps. First, letâ€™s get our values. Weâ€™ll need to know the grand mean \\(\\bar{\\bar{x}}\\), the mean \\(\\bar{x_t}\\) of each treatment group \\(t\\), as well as the values for the treatment group \\(t\\) and the outcome of interest \\(x\\) (in this case, weight). ## # A tibble: 6 Ã— 4 ## baker weight xbbar xbar ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Kim 28.2 30.0 28.5 ## 2 Kim 26.1 30.0 28.5 ## 3 Kim 27.2 30.0 28.5 ## 4 Kim 25.7 30.0 28.5 ## 5 Kim 29.4 30.0 28.5 ## 6 Kim 28 30.0 28.5 Next, weâ€™ll take our ingredients to calculate our qi, our quantities of interest. These include the TSS (total sum of squares), RSS (residual sum of squares), and ESS (explained sum of squares). Total Sum of Squares measures the total variation in your data, using the grand mean \\(\\bar{\\bar{x}}\\) as a benchmark. Residual Sum of Squares measures how much deviation remains between your treatment group means and other values within each treatment group. This is the unexplained error in your model. Explained Sum of Squares measures how much deviation was explained by your treatment group means (your model). qi = values %&gt;% summarize( # Calculate the *total* sum of squares # compared to the grand mean tss = sum( (weight - xbbar)^2), # Calculate the *residual* sum of squares # compared to the your &#39;model&#39; prediction, xbar # in this case, your model is the idea that the # mean of each treatment group explains the difference in weight. rss = sum( (weight - xbar)^2 ), # calculate the ess - the explained sum of squares # a.k.a the variation explained by the &#39;model&#39; ess = tss - rss, # Get the total number of treatment groups, k k = baker %&gt;% unique() %&gt;% length(), # Get the total sample size n = n()) # View it! qi ## # A tibble: 1 Ã— 5 ## tss rss ess k n ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 515. 407. 108. 3 50 Then, letâ€™s use our qi to calculate an F statistic! Though our ESS and RSS give us the sum of our squared deviations, we know that some data points are bound to vary more than others. So, we should really evaluate the relative distance between our distributions using an average of the ESS (called the mean_squares_explained) and an average of the RSS (called the mean_squares_explained - its squart root is the root-mean-squared-error, a.k.a. sigma!!!). qi %&gt;% mutate( # Get the average variation explained by dividing by k - 1 groups mean_squares_explained = ess / (k - 1), # Get the average variation unexplained by dividing by n - k mean_squared_error = rss / (n - k) ) %&gt;% # Calculate the f-statistic, # the ratio of the variation explained versus unexplained by your model! mutate(f = mean_squares_explained / mean_squared_error) ## # A tibble: 1 Ã— 8 ## tss rss ess k n mean_squares_explained mean_squared_error f ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 515. 407. 108. 3 50 54.0 8.66 6.24 26.6.3 Functions for ANOVA There are a bunch of different ways to do ANVOA, depending on how much specificity you need. We could use the oneway.test(), aov() or lm() functions. Letâ€™s compare to show their results. 26.6.4 lm() - the easy way Here, sigma gives you the residual standard error (which is the square root of the root-mean-squared-error that anova gives you). statistic gives you the F-statistic, while p.value gives you the p-value for the F-statistic. m1 = donuts %&gt;% summarize(lm(weight ~ baker) %&gt;% glance()) %&gt;% select(sigma, statistic, p.value, df) m1 ## # A tibble: 1 Ã— 4 ## sigma statistic p.value df ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.94 6.24 0.00395 2 Practically speaking, I tend to use the lm() function for my ANOVA needs, because I can use the predict() function to flexibly make predictions. 26.6.5 aov() - the normal way Here, we get the F statistic and its p.value, as well as several quantities used to calculate the f-statistic, including the root-mean-squared-error (row 2 Residuals, column meansq). donuts %&gt;% summarize(aov(weight ~ baker) %&gt;% tidy()) ## # A tibble: 2 Ã— 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 baker 2 108. 54.0 6.24 0.00395 ## 2 Residuals 47 407. 8.66 NA NA 26.6.6 oneway.test() - if you know your group variances are unequal Alternatively, oneway.test() can let you use var.equal = TRUE or FALSE to lightly adjust your statistics to accommodate uneven variances among groups. Randomization should usually deal with unequal variances, but if youâ€™re working with non-random samples, be sure to check your variances and then use var.equal = FALSE. # If even... donuts %&gt;% summarize(oneway.test(data = ., formula = weight ~ baker, var.equal = TRUE) %&gt;% tidy()) # If uneven... donuts %&gt;% summarize(oneway.test(formula = weight ~ baker, var.equal = FALSE) %&gt;% tidy()) 26.6.7 Multiple Variables But what if we need to test the impact of multiple variables simultaneously? For example, what if we want to assess how much differences in average weight were due to the type of machine versus the baker? # We can add both type and baker to the model m2 = donuts %&gt;% summarize(lm(formula = weight ~ type + baker) %&gt;% glance()) %&gt;% # and compare our statistic and sigma values to before. select(sigma, statistic, p.value, df) Letâ€™s stack our earlier ANOVA model m1 on top of our new model m2. We can see from the sigma statistic that the residual standard error (average prediction error) has decreased in our new model, indicating better model fit. We can also see that adding baker had led to a greater F statistic with a lower, more significant p.value. bind_rows( m1 %&gt;% mutate(vars = &quot;baker&quot;), m2 %&gt;% mutate(vars = &quot;type + baker&quot;) ) ## # A tibble: 2 Ã— 5 ## sigma statistic p.value df vars ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2.94 6.24 0.00395 2 baker ## 2 2.41 14.3 0.00000102 3 type + baker Alternatively, what if Craig is pretty okay at making donuts with the old machine, but is just particularly bad at making donuts on the new machine? For this, we can use interaction effects. m3 = donuts %&gt;% lm(formula = weight ~ type * baker) %&gt;% glance() %&gt;% select(sigma, statistic, p.value) m3 ## # A tibble: 1 Ã— 3 ## sigma statistic p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.24 11.7 0.000000310 We see here that our statistic is greater than in the original model m1, and with a significance p.value, but m2 had a higher F statistic. However, if we had empirical reasons that supported this model - eg. we randomly assigned donuts by type and baker - then we would need to use an interaction model to estimate effects accurately. Conclusion Great work making it through this workshop. Weâ€™ll follow up with more DoE in our subsequent classes! For practice, you can go through any of the sections of this workshop again, this time, selecting your variable of interest - weight, lifespan, or tastiness - and compare your results. "],["factorial-design-and-interaction-effects-in-r.html", "27 Factorial Design and Interaction Effects in R Getting Started 27.1 Estimating Direct Effects 27.2 Standard Errors for \\(\\bar{\\bar{d}}\\) 27.3 Pivoting Data with pivot_longer() &amp; pivot_wider() 27.4 Visualizing &amp; Reporting Treatment Effects 27.5 Interaction Effects 27.6 Estimating Interactions in lm() Conclusion", " 27 Factorial Design and Interaction Effects in R Figure 2.1: Lattes! Photo Credit to Fahmi Fakhrudin This workshop introduces methods for computing interaction effects in experiments in R, through a very caffeinated case of commercial product testing - lattes! Getting Started Factorial Design Suppose a local coffee chain in upstate New York, Kimâ€™s Coffee, faces repeated complaints from customers: while their coffeeshops are beloved, their lattes are just plain bad. With the holidays right around the corner, they need to produce better lattes ASAP, to attract cold holiday shoppers! Their local six-sigma expert recommends that one of their shops designs and implements a factorial-design experiment. When you have a limited amount of time and/or resources, factorial-design experiments can be very cost-efficient. In this case, Kimâ€™s Coffee staff are not sure why their lattes are bad - is it due to the espresso machine, the milk, the coffee syrups, or the latte art? They create 240 lattes, and randomly assign each latte order to the following treatments: machine: Each latte order is randomly assigned to 1 of 2 espresso machines (machine \"a\" or \"b\"). milk: For each machine, they then randomly assign the lattes to 1 of 3 types of milk (\"whole\", \"skim\", and \"oat\" milk). syrup: For each type of milk, they randomly assign lattes with that milk to 1 of 2 brands of simple syrup (\"torani\" vs.Â \"monin\"). art: Finally, each is assigned to 1 of 2 types of latte art presentation (\"heart\" vs.Â \"foamy\"). Finally, they measure the tastiness on a scale from 0 to 100. Data &amp; Packages &lt;img src=â€œimages/12_shop.jpgâ€ alt=â€œCoffee Shop Experiments? Photo by Photo by &lt;a href=â€https://unsplash.com/@nputra?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\"&gt;Nafinia Putra&lt;/a&gt; on &lt;a href=â€œhttps://unsplash.com/s/photos/coffee-shop?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\"&gt;Unsplash&lt;/a&gt;;â€ width=â€œ960â€ /&gt; Figure 12.1: Coffee Shop Experiments? Photo by Photo by Nafinia Putra on Unsplash They produce the following data, saved in lattes.csv, and available on Github at this link. Use the code below to load in our data! # Load packages library(tidyverse) library(broom) # Load in data! # If you upload it to your workshops folder, read it in like this... lattes = read_csv(&quot;workshops/lattes.csv&quot;) # or # If you want to download it straight from github, read it in like this... lattes = read_csv(&quot;https://raw.githubusercontent.com/timothyfraser/sysen/main/workshops/lattes.csv&quot;) # Check it out! lattes %&gt;% glimpse() ## Rows: 240 ## Columns: 6 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1â€¦ ## $ tastiness &lt;dbl&gt; 42.79289, 34.51512, 46.33578, 44.57469, 44.40918, 56.27327, â€¦ ## $ machine &lt;chr&gt; &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, â€¦ ## $ milk &lt;chr&gt; &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oatâ€¦ ## $ syrup &lt;chr&gt; &quot;monin&quot;, &quot;monin&quot;, &quot;monin&quot;, &quot;monin&quot;, &quot;monin&quot;, &quot;monin&quot;, &quot;moninâ€¦ ## $ art &lt;chr&gt; &quot;foamy&quot;, &quot;foamy&quot;, &quot;foamy&quot;, &quot;foamy&quot;, &quot;foamy&quot;, &quot;foamy&quot;, &quot;foamyâ€¦ 27.1 Estimating Direct Effects First, we can estimate the direct effects of one treatment versus another within the same variable (eg. lattes made by machine b versus machine a). 27.1.1 Difference of Grand Means \\(\\bar{\\bar{d}}\\) To start, we have our raw data, where each row is a latte (1 observation), and for each group, we produced 10 lattes/observations (sometimes called 10 â€œreplicatesâ€). We need to compute for each unique set of treatment conditions, what was the within-group mean (xbar), standard deviation sd, and sample size n? To get all unique combinations of machine, milk, syrup, and art that were tested, we can group_by() these variables, and then summarize() the mean(), sd(), and n() to get back a single row of statistics for each â€˜setâ€™ of unique treatments. Weâ€™ll call this data.frame of summary statistics per set groups. # Let&#39;s make a data.frame &#39;groups&#39; groups = lattes %&gt;% # For each unique pairing of treatments tested, group_by(machine, milk, syrup, art) %&gt;% # Return one row containing these summary statistics summarize(xbar = mean(tastiness), s = sd(tastiness), n = n()) %&gt;% ungroup() # Check it out! groups %&gt;% head() ## # A tibble: 6 Ã— 7 ## machine milk syrup art xbar s n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 a oat monin foamy 43.3 6.60 10 ## 2 a oat monin heart 57.4 5.65 10 ## 3 a oat torani foamy 65.3 5.13 10 ## 4 a oat torani heart 83.2 4.85 10 ## 5 a skim monin foamy 45.0 4.65 10 ## 6 a skim monin heart 56.9 4.29 10 Next, we can compute the direct effect of a single variableâ€™s treatment effect (eg. effect of machine b in the machine variable, compared to machine b), by taking (1) the mean tastiness of lattes produced by machine a (xbar[machine == \"a\"]), as well as (2) those produced by machine b (written as xbar[machine == \"b\"] below). We can do this several ways, some slower or faster, so Iâ€™ll write out the slower-but-clearer method, followed by the faster-but-messier method. 27.1.2 Slower-but-Clearer Method of Getting the Difference of Grand Means \\(\\bar{\\bar{d}}\\) # Suppose you&#39;ve got a series of means `xbar` across multiple treatment groups. We can.... step1 = groups %&gt;% summarize( # put all means for treatment groups where machine == &quot;b&quot; into one column xbar_machine_b = xbar[machine == &quot;b&quot;], # put all means for treatment groups where machine == &quot;a&quot; into another column xbar_machine_a = xbar[machine == &quot;a&quot;]) # View it step1 %&gt;% head(3) ## # A tibble: 3 Ã— 2 ## xbar_machine_b xbar_machine_a ## &lt;dbl&gt; &lt;dbl&gt; ## 1 22.4 43.3 ## 2 30.1 57.4 ## 3 31.8 65.3 Thenâ€¦ step2 = step1 %&gt;% summarize( # Take the grand mean of each vector of means xbbar_machine_b = mean(xbar_machine_b), xbbar_machine_a = mean(xbar_machine_a), # Subtract the grand mean of a (control) from the grand mean of b (treatment) # It helps to record for yourself the variable and order of subtraction (eg. b_a = b - a) dbbar_machine_b_a = xbbar_machine_b - xbbar_machine_a) # Check it! step2 ## # A tibble: 1 Ã— 3 ## xbbar_machine_b xbbar_machine_a dbbar_machine_b_a ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 32.2 62.6 -30.4 27.1.3 Faster-but-Messier Method of Getting the Difference of Grand Means \\(\\bar{\\bar{d}}\\) The method discussed above is slower but clean and clear. On the other hand, if we need to make a lot of dbbar statistics for different groups, it could be helpful to speed this process up. Hereâ€™s a way to do that. Weâ€™ll use this below a bunch, so be sure to try both methods out yourself so you fully understand how they work. # Taking our group means xbar, groups %&gt;% # Take the grand mean of means from machine b and subtract the grand mean of means from machine a. summarize(dbar_machine_b_a = mean(xbar[machine == &quot;b&quot;]) - mean(xbar[machine == &quot;a&quot;])) ## # A tibble: 1 Ã— 1 ## dbar_machine_b_a ## &lt;dbl&gt; ## 1 -30.4 # Boom! All in one line - Shazam! 27.1.4 Estimating Many Difference of Grand Means \\(\\bar{\\bar{d}}\\) Using the technique we learned above, letâ€™s estimate many difference of grand means \\(\\bar{\\bar{d}}\\) all at once! We can estimate direct effects by type of (1) machine, (2) milk, (3) syrup, and (4) art on the quality of our lattes (tastiness) by calculating overall difference of means statistics for each possible pairing of machine (a vs.Â b), milk (skim vs.Â oat, skim vs.Â whole, and whole vs.Â skim), syrup (torani vs.Â monin), and art (heart vs.Â foamy). Notice also that when the number of treatments in a variable is 3 or greater (eg. milk contains oat, skim, and whole), it is no longer sufficient to just get the difference of grand means between 2 categories; we now can compute not just 1 \\(\\bar{\\bar{d}}\\) statistic, but 3! Letâ€™s do it! dbbar = groups %&gt;% # Calculate a single line of summary statistics dbbar (d-double-bar, the difference of grand means) summarize( # How much tastier is a latte from machine &quot;b&quot; than machine &quot;a&quot;, on average? machine_b_a = mean(xbar[machine == &quot;b&quot;]) - mean(xbar[machine == &quot;a&quot;]), # How much tastier is an oat milk latte than a skim milk latte, on average? milk_oat_skim = mean(xbar[milk == &quot;oat&quot;]) - mean(xbar[milk == &quot;skim&quot;]), # How much tastier is an oat milk latte than a whole milk latte, on average? milk_oat_whole = mean(xbar[milk == &quot;oat&quot;]) - mean(xbar[milk == &quot;whole&quot;]), # How much tastier is a skim milk latte than a whole milk latte, on average? milk_skim_whole = mean(xbar[milk == &quot;skim&quot;]) - mean(xbar[milk == &quot;whole&quot;]), # How much tastier is a latte with Torani brand syrup than with Monin brand syrup, on average? syrup_torani_monin = mean(xbar[syrup == &quot;torani&quot;]) - mean(xbar[syrup == &quot;monin&quot;]), # How much tastier is a latte with a &#39;heart&#39; foam art, than just a normal foamy latte, on average? art_heart_foamy = mean(xbar[art == &quot;heart&quot;]) - mean(xbar[art == &quot;foamy&quot;]) ) # Glimpse the results! dbbar %&gt;% glimpse() ## Rows: 1 ## Columns: 6 ## $ machine_b_a &lt;dbl&gt; -30.37756 ## $ milk_oat_skim &lt;dbl&gt; -1.447917 ## $ milk_oat_whole &lt;dbl&gt; 0.2062967 ## $ milk_skim_whole &lt;dbl&gt; 1.654213 ## $ syrup_torani_monin &lt;dbl&gt; 18.81986 ## $ art_heart_foamy &lt;dbl&gt; 11.73731 Figure 7.4: Does it matter if it has a heart? Photo Credit to Olivia Anne Snyder 27.2 Standard Errors for \\(\\bar{\\bar{d}}\\) But how much would those effects \\(\\bar{\\bar{d}}\\) vary simply due to random sampling error, had they been drawn from a slightly different sample? We need to estimate a standard error around the sampling distribution of these statistics. For that, weâ€™ll need some overall estimate of variance and sample size shared across all these \\(\\bar{\\bar{d}}\\) statistics. 27.2.1 Estimating Standard Error when \\(s^2_i\\) is known If we suppose that the variances of each group are unequal (they probably are), then we can calculate a single standard error by taking the sum of the average variance within each group, then taking the square root of that total average variance. To do so, we must know the variance \\(s^2_i\\) for every treatment group \\(i\\) in our dataset. That can be written as: \\[ standard \\ error \\ \\sigma = \\sqrt{ \\sum_{i=1}^{n}{ \\frac{ s^{2}_i }{n_{i} } } } = \\sqrt{ \\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2} + ... \\frac{s^2_n}{n_n} } \\\\ where \\ i = group, \\ n = group \\ sample \\ size, \\\\ s^2_i = variance \\ of \\ group \\ i \\] We can code this extremely quickly as: error = groups %&gt;% # calculate a single row of statistics overall summarize( # If we know the standard deviation and sample size of each group # we can estimate the standard error directly se = sqrt( sum(s^2 / n) ) ) # Check it! error ## # A tibble: 1 Ã— 1 ## se ## &lt;dbl&gt; ## 1 8.39 27.2.2 Using Pooled Variance to Estimate Standard Errors On the other hand, if we have reason to believe that the variances of each group are more or less equal, we can simplify this equation, calculating just a single pooled variance \\(s^2_i\\) that gets applied throughout, like so: \\[ pooled \\ variance = s^{2}_{p} = \\frac{ \\sum_{i=1}^{n}{ s^2_{i} \\times v_i } }{ \\sum_{i=1}^{n}{v_i} }, \\\\ where \\ i = group, \\\\ v_i = n_i - 1 = degrees \\ of \\ freedom \\ of \\ group \\ i, \\\\ s^2_i = variance \\ of \\ group \\ i, \\\\ assuming \\ equal \\ variance \\] Then, we could calculate the standard error with need for even fewer statistics, as: $ = $. With R, the calculations are so quick for unequal variances that it is no longer especially helpful to assume equal variances. But, if you ever find yourself without the specific subgroup variances and need to estimate a standard error, you can use the equal variance formula for pooled variances to find a standard error. We could code it like this: error = groups %&gt;% # Get degrees of freedom for each group... mutate(df = n - 1) %&gt;% # then calculate a single row of statistics overall summarize( # If we assume the variance is constant across groups (it probably isn&#39;t) # we can take the weighted average of variances, weighted by group degrees of freedom var_pooled = sum(s^2 * df) / sum(df), s_pooled = var_pooled %&gt;% sqrt(), # Finally, get the standard error! se = sqrt(s_pooled^2 * sum(1 / n)) ) # Take a peek! error ## # A tibble: 1 Ã— 3 ## var_pooled s_pooled se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 29.3 5.41 8.39 Our standard errors assuming and not assuming equal variances end up pretty similar, so it looks like our variances must have been pretty equal across groups. 27.3 Pivoting Data with pivot_longer() &amp; pivot_wider() Letâ€™s be honest - that was a lot of typing, and though it was fast, it got a little old! Good news: The tidyr and dplyr packages, which get loaded automatically with the tidyverse package, includes several functions for pivoting your data - meaning rearranging it quickly. 27.3.1 contains() select(contains(\"string\")) in dplyr lets you grab all variables in a data.frame that contain the following \"string\". For example: dbbar %&gt;% select(contains(&quot;milk&quot;)) ## # A tibble: 1 Ã— 3 ## milk_oat_skim milk_oat_whole milk_skim_whole ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.45 0.206 1.65 27.3.2 pivot_longer() pivot_longer() takes a set of column vectors cols = c(column1, column2, etc.), then stacks them on top of each other, sending the names of these columns to a names vector and the values of these columns to a values vector. It pairs well with contains(\"string\"). dbbar %&gt;% select(contains(&quot;milk&quot;)) %&gt;% # We can pivot these columns... pivot_longer(cols = c( milk_oat_skim, milk_oat_whole, milk_skim_whole) ) ## # A tibble: 3 Ã— 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 milk_oat_skim -1.45 ## 2 milk_oat_whole 0.206 ## 3 milk_skim_whole 1.65 dbbar %&gt;% select(contains(&quot;milk&quot;)) %&gt;% # We can also just use contains to write a short hand pivot_longer(cols = c( contains(&quot;milk&quot;)) ) ## # A tibble: 3 Ã— 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 milk_oat_skim -1.45 ## 2 milk_oat_whole 0.206 ## 3 milk_skim_whole 1.65 Generally, I like to specify a set of column vectors NOT to pivot, which sends all other vectors to the names and values columns. This is much faster! We can write cols = -c(whatever), which says send all variables except whatever. You could even write cols = -c(), which just sends all the columns. # For example... dbbar %&gt;% select(contains(&quot;milk&quot;)) %&gt;% # pivot all columns EXCEPT milk_oat_skim pivot_longer(cols = -c(milk_oat_skim)) ## # A tibble: 2 Ã— 3 ## milk_oat_skim name value ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 -1.45 milk_oat_whole 0.206 ## 2 -1.45 milk_skim_whole 1.65 # My favorite: dbbar %&gt;% select(contains(&quot;milk&quot;)) %&gt;% pivot_longer(cols = -c()) ## # A tibble: 3 Ã— 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 milk_oat_skim -1.45 ## 2 milk_oat_whole 0.206 ## 3 milk_skim_whole 1.65 Finally, if you donâ€™t select() anything, you can pivot all the variables at once! (as long as they are all numeric or all character vectors). # If you don&#39;t select, you can get **all** the variables too! dbbar %&gt;% pivot_longer(cols = -c()) ## # A tibble: 6 Ã— 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 machine_b_a -30.4 ## 2 milk_oat_skim -1.45 ## 3 milk_oat_whole 0.206 ## 4 milk_skim_whole 1.65 ## 5 syrup_torani_monin 18.8 ## 6 art_heart_foamy 11.7 27.3.3 pivot_wider() Finally, sometimes we need to spread out values from a long, â€œtidyâ€ format into a wider, â€œmatrixâ€ format! For example, we did this earlier when getting the xbar values for machine a and machine b in their own columns, by typing xbar[machine == \"a\"], etc. But this takes a while, whereas pivot_wider() can do it quickly, all in one line. Itâ€™s up to you which to use, but pivot_wider() can make your life a lot easier! When you use pivot_wider(), we need to provide a set of id_cols, which is a vector of unique id columns we will use to make the rows in our data. For example, if we want to pivot the machine and xbar columns into an xbar_a and xbar_b column, we need to tell R that each row should refer to an xbar value for lattes of a specific milk, syrup, and art, by saying id_cols = c(milk, syrup, art). We then can tell pivot_wider(), grab the vector names_from the machine vector and the values_from the xbar vector. # Check it out! groups %&gt;% pivot_wider(id_cols = c(milk, syrup, art), names_from = machine, values_from = xbar) %&gt;% head() ## # A tibble: 6 Ã— 5 ## milk syrup art a b ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 oat monin foamy 43.3 22.4 ## 2 oat monin heart 57.4 30.1 ## 3 oat torani foamy 65.3 31.8 ## 4 oat torani heart 83.2 42.4 ## 5 skim monin foamy 45.0 24.8 ## 6 skim monin heart 56.9 28.7 27.4 Visualizing &amp; Reporting Treatment Effects Using the dbbar data.frame from section 1 and error data.frame from section 2 above, we can report and visualize the overall treatment effects. 27.4.1 Building a Table of Direct Effects # Compile our direct effects of treatments direct = dbbar %&gt;% # let&#39;s pivot our data, pivot_longer( cols = -c(), # specifying names for our &#39;name&#39; and &#39;value&#39; columns names_to = &quot;term&quot;, values_to = &quot;dbbar&quot;) %&gt;% # Let&#39;s add in our singular standard error shared across experiments mutate(se = error$se) %&gt;% # And then estimate a 95% confidence interval! mutate(lower = dbbar - qnorm(0.975) * se, upper = dbbar + qnorm(0.975) * se) %&gt;% # And then write up a nice label mutate(label = paste(round(dbbar, 2), &quot;\\n(&quot;, round(se, 2), &quot;)&quot;, sep = &quot;&quot;)) # Let&#39;s view them! direct ## # A tibble: 6 Ã— 6 ## term dbbar se lower upper label ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 machine_b_a -30.4 8.39 -46.8 -13.9 &quot;-30.38\\n(8.39)&quot; ## 2 milk_oat_skim -1.45 8.39 -17.9 15.0 &quot;-1.45\\n(8.39)&quot; ## 3 milk_oat_whole 0.206 8.39 -16.2 16.6 &quot;0.21\\n(8.39)&quot; ## 4 milk_skim_whole 1.65 8.39 -14.8 18.1 &quot;1.65\\n(8.39)&quot; ## 5 syrup_torani_monin 18.8 8.39 2.38 35.3 &quot;18.82\\n(8.39)&quot; ## 6 art_heart_foamy 11.7 8.39 -4.70 28.2 &quot;11.74\\n(8.39)&quot; 27.4.2 Visualization Strategies We can visualize these using geom_linerange(), geom_errorbar(), or geom_crossbar(). Letâ€™s take a peek. # You could use geom_crossbar, which uses ymin, ymax, and y direct %&gt;% ggplot(mapping = aes(x = term, y = dbbar, ymin = lower, ymax = upper)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_crossbar(fill = &quot;steelblue&quot;) # Or with geom_linerange(), which also uses ymin and ymax direct %&gt;% ggplot(mapping = aes(x = term, y = dbbar, ymin = lower, ymax = upper)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_linerange(size = 1) + geom_point(size = 3) Orâ€¦. # You could use geom_errorbar, which uses ymin, ymax, plus an optional width direct %&gt;% ggplot(mapping = aes(x = term, y = dbbar, ymin = lower, ymax = upper)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_errorbar(width = 0.25) + geom_point() 27.4.3 An Excellent Visual But it we wanted to make a prettier chart, we could add color and labels, like so! direct %&gt;% ggplot(mapping = aes(x = term, y = dbbar, ymin = lower, ymax = upper, # specify a label and fill variable label = label, fill = dbbar)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_crossbar() + # Add labels geom_label(fill = &quot;white&quot;) + # Add a color pallet scale_fill_gradient2(low = &quot;red&quot;, mid = &quot;white&quot;, high = &quot;blue&quot;) + # Add x and y axis labels labs(y = &quot;Difference of Means across Groups&quot;, x = &quot;Treatment Groups&quot;, fill = &quot;Effect&quot;) + # Relabel our axis ticks scale_x_discrete( # Each label should take the original value and map it to a new, more readable version labels = c(&quot;syrup_torani_monin&quot; = &quot;Torani vs. Monin&quot;, &quot;milk_skim_whole&quot; = &quot;Skim Milk - Whole Milk&quot;, &quot;milk_oat_whole&quot; = &quot;Oatmilk - Whole Milk&quot;, &quot;milk_oat_skim&quot; = &quot;Oatmilk - Skim Milk&quot;, &quot;machine_b_a&quot; = &quot;Machine B - A&quot;, &quot;art_heart_foamy&quot; = &quot;Heart - Foam Art&quot;)) + # Flip the scale coord_flip() + # Add a theme theme_classic(base_size = 14) 27.5 Interaction Effects 27.5.1 Estimating a Pooled Standard Error In a factorial experiment, each combination of factors can vary a little due to random noise. This makes it hard to compare effects, so we want to use a pooled standard error that represents the average amount of noise across all groups before estimating treatment effects. Letâ€™s create a function to calculate this Pooled SE for the tastiness. # Rename tastiness by y for easier handling lattes = lattes %&gt;% rename(y = tastiness) se_factorial = function(formula = y ~ machine + syrup + art, data){ # formula = y ~ machine + syrup + art # data = lattes # Get frame of data frame = model.frame(formula, data) %&gt;% rename(y = 1) # Get names of x variables xvars = names(frame)[-1] # Calculate a standard error the tastiness metric in this factorial experiment output = frame %&gt;% group_by(across(any_of(xvars))) %&gt;% summarize(s = sd(y), n = n()) %&gt;% ungroup() %&gt;% summarize(se = sqrt( sum(s^2 / n))) %&gt;% # A trick - with() is like the dollar sign - it will let you access se with(se) return(output) } se_factorial(formula = y ~ machine + syrup + art, lattes) ## [1] 2.796643 27.5.2 Estimating 1-way interations In factorial design, one-way effect shows how changing one factor affects the outcome. For example, we want to know whether Machine B makes better-tasting lattes than Machine A, regardless of the other factors like syrup or art. The function below measures how much the tastiness changes when moving from the low level to the high level of one factor. dbar_oneway = function(formula, data){ # formula = y ~ machine # data = lattes frame = model.frame(formula, data) %&gt;% select(y = 1, a = 2) %&gt;% mutate(a = factor(a)) %&gt;% mutate(a = as.integer(a) - 1) # Compute the one-way effect frame %&gt;% reframe( # The mean difference between the &quot;High&quot; group (a == 1) and the &quot;Low&quot; group (a == 0) dbar = mean(y[a == 1] - y[a == 0]), ) %&gt;% with(dbar) } dbar_oneway(formula = y ~ machine, lattes) ## [1] -30.37756 This result shows that Machine B makes much worse-tasting lattes than Machine A on average! 27.5.3 Estimating 2-way interactions Alternatively, we might want to estimate treatment effects for combinations of treatments. For example, do these lattes tend to taste better specifically when both conditions are true? Eg. letâ€™s test the effect of having a latte from Machine B with Torani syrup, by comparing it against lattes made from Machine A with Monin syrup. This demonstrates the value added of both conditions. The dbar_twoway() function tests whether two factors (e.g., Machine and Syrup) together affect tastiness differently than they do individually. It compares â€œsame-directionâ€ pairs (HH + LL) against â€œopposite-directionâ€ pairs (HL + LH) to find the combined effect. dbar_twoway = function(formula, data){ # formula = y ~ machine * syrup # data = lattes # Extract model frame frame = model.frame(formula, data) %&gt;% # Rename columns as y, a, b select(y = 1, a = 2, b = 3) %&gt;% mutate(a = factor(a), b = factor(b)) levels_a = levels(frame$a) levels_b = levels(frame$b) # Convert factors to integer codes frame = frame %&gt;% mutate(a = as.integer(a), b = as.integer(b)) # Now pick the combinations for comparison output = frame %&gt;% reframe( # Same factors: HH or LL x1 = y[(a == 2 &amp; b == 2) | (a == 1 &amp; b == 1)], # Opposite factors: HL or LH x0 = y[(a == 1 &amp; b == 2) | (a == 2 &amp; b == 1)] ) %&gt;% reframe( # Calculate dbar dbar = mean(x1) - mean(x0) ) %&gt;% with(dbar) return(output) } value = dbar_twoway(y ~ machine * syrup, lattes) value ## [1] -6.433374 We can see that lattes made with Machine B and Torani syrup taste -6.43 worse, on average, than lattes made with machine A and Monin syrup. 27.5.4 Estimating 3-way interactions We could even calculate a three-way interaction between machine type, syrup brand, and latte art. Letâ€™s create a function to check if the effect of Machine Ã— Art depends on which Syrup is used. A positive result means the interaction strengthens with the syrup change, and a negative result means it weakens. dbar_threeway = function(formula, data){ # formula = y ~ machine * syrup * art # data = lattes frame = model.frame(formula, data) %&gt;% select(y = 1, a = 2, b = 3, c = 4) %&gt;% mutate(a = factor(a), b = factor(b), c= factor(c)) %&gt;% mutate(a = as.integer(a) - 1, b = as.integer(b) - 1, c = as.integer(c) - 1) differences = frame %&gt;% reframe( # Get the AC interaction when B = 0 d1a = y[a == 1 &amp; b == 0 &amp; c == 1] - y[a == 0 &amp; b == 0 &amp; c == 1], d0a = y[a == 1 &amp; b == 0 &amp; c == 0] - y[a == 0 &amp; b == 0 &amp; c == 0], # Get the AC interaction when B = 1 d1b = y[a == 1 &amp; b == 1 &amp; c == 1] - y[a == 0 &amp; b == 1 &amp; c == 1], d0b = y[a == 1 &amp; b == 1 &amp; c == 0] - y[a == 0 &amp; b == 1 &amp; c == 0] ) output = differences %&gt;% reframe( # Get AC interaction effect when B = 0 dbar_a = (mean(d1a) - mean(d0a)) / 2, # Get AC interaction effect when B = 1 dbar_b = (mean(d1b) - mean(d0b) ) / 2, # Get the average of the two effects dbar = (dbar_b - dbar_a) / 2 ) %&gt;% select(dbar) %&gt;% with(dbar) return(output) } value = dbar_threeway(formula = y ~ machine * syrup * art, lattes) value ## [1] -0.3024043 This negative result indicates the combination of Machine type and Latte art dosenâ€™t improve tastiness when the syrup brand changes. 27.6 Estimating Interactions in lm() A quick strategy for estimating interaction effects in R can be using the lm() function. 27.6.1 Modeling Interactions While it uses a regression model, rather than the difference of means, to estimate these effects, it tends to be pretty brief and doesnâ€™t require making our groups, dbbar, or error data.frames. # Using our raw data of observed lattes m = lattes %&gt;% # Make a linear model, showing interactions with the &#39;*&#39; sign lm(formula = y ~ machine * milk * syrup * art) # View our model! m %&gt;% tidy() ## # A tibble: 24 Ã— 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 43.3 1.71 25.3 1.64e-66 ## 2 machineb -20.9 2.42 -8.63 1.37e-15 ## 3 milkskim 1.67 2.42 0.689 4.92e- 1 ## 4 milkwhole -1.90 2.42 -0.786 4.33e- 1 ## 5 syruptorani 22.0 2.42 9.09 6.48e-17 ## 6 artheart 14.1 2.42 5.84 1.88e- 8 ## 7 machineb:milkskim 0.734 3.42 0.214 8.30e- 1 ## 8 machineb:milkwhole 1.82 3.42 0.532 5.95e- 1 ## 9 machineb:syruptorani -12.6 3.42 -3.69 2.80e- 4 ## 10 milkskim:syruptorani 1.20 3.42 0.350 7.27e- 1 ## # â„¹ 14 more rows We see potentially significant interaction effects for the joint impact of machine b and torani syrup, as well as several strong direct effects. Three way effects though, are minimal at best. While lm() returns a regression-based approximation, the dbar functions are more helpful when you want to directly calculate the interaction effects using one single standard error. 27.6.2 Using predict() to visualize interaction effects The big power of lm() in factorial experiments is getting to visualize these effects. # We can generate a set of all observed combinations of our treatments as &#39;newdata newdata = lattes %&gt;% group_by(machine, milk, syrup, art) %&gt;% summarize() newdata %&gt;% head() ## # A tibble: 6 Ã— 4 ## # Groups: machine, milk, syrup [3] ## machine milk syrup art ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a oat monin foamy ## 2 a oat monin heart ## 3 a oat torani foamy ## 4 a oat torani heart ## 5 a skim monin foamy ## 6 a skim monin heart Then, we can generate either \"confidence\" or \"prediction\" intervals - confidence intervals are a little narrower, used in estimating effects youâ€™ve already observed, while prediction intervals are a little wider, used for estimating predicted quantities that you naturally want a little more caution around. I like to save this as an object pred, then extract the results in a second step. # For confidence intervals pred = predict(m, newdata = newdata, se.fit = TRUE, interval = &quot;confidence&quot;, level = 0.95) %&gt;% # Extract just the fitted results and intervals with(fit) %&gt;% # And convert it to a tibble as_tibble() # or for prediction intervals... #predict(m, newdata = newdata, # se.fit = TRUE, interval = &quot;prediction&quot;, level = 0.95) # See? Not very tidy pred %&gt;% head() ## # A tibble: 6 Ã— 3 ## fit lwr upr ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 43.3 39.9 46.7 ## 2 57.4 54.1 60.8 ## 3 65.3 61.9 68.7 ## 4 83.2 79.9 86.6 ## 5 45.0 41.6 48.3 ## 6 56.9 53.5 60.2 Now, we can bind the two sets of values together, either using tibble() or bind_col(), because they share the same rows. cis = tibble( newdata, yhat = pred$fit, lower = pred$lwr, upper = pred$upr ) To visual an interaction effect, we can now narrow into 4 predictions, showing all possible combinations of machine and syrup, while zooming into just one type of milk and art, to hold these other concepts constant. viz = cis %&gt;% filter(machine %in% c(&quot;a&quot;, &quot;b&quot;), syrup %in% c(&quot;torani&quot;, &quot;monin&quot;), milk == &quot;whole&quot;, art == &quot;heart&quot;) %&gt;% # add a label! mutate(label = round(yhat, 2) ) # view it! viz ## # A tibble: 4 Ã— 8 ## machine milk syrup art yhat lower upper label ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a whole monin heart 55.9 52.5 59.2 55.9 ## 2 a whole torani heart 86.8 83.4 90.1 86.8 ## 3 b whole monin heart 27.9 24.5 31.2 27.9 ## 4 b whole torani heart 43.2 39.8 46.6 43.2 And last, we can visualize the effect with geom_ribbon() or any other technique that accepts ymin and ymax. We just have to remember to put one variable on the x axis (eg. machine) and one variable as the group and fill, to distinguish our two lines. viz %&gt;% ggplot(mapping = aes(x = machine, group = syrup, fill = syrup, y = yhat, ymin = lower, ymax = upper, label = label)) + geom_ribbon() + geom_line() + # Plot the labels, bumping the text upwards! geom_text(nudge_y = 5) + labs(x = &quot;Espresso Machine Type&quot;, y = &quot;Predicted Tastiness (0-100) with 95% CIs&quot;, fill = &quot;Syrup Brand&quot;) + theme_classic(base_size = 14) How should we interpret this visual? Well, letâ€™s just describe what we see! More descriptive language is always better than less, when it comes to explaining results for new viewers. Hereâ€™s an example descriptive! The figure above visualizes our model mâ€™s predicted values for latte tastiness with 95% confidence intervals, assuming those lattes were made with whole milk and had heart foam art, but their only differences were the machine that made them and the brand of syrup they used. We see that lattes made from espresso machine B tend to be much less tasty than lattes made from espress machine A, given either type of syrup brand. Torani syrups see a sharper decrease than Monin syrups, but lattes made from Torani syrups remain much tastier. The lines do not cross in the given treatment categories, indicating that, at least as far as these dichotomous categories are concerned, the interaction effect between machine type and syrup brand is statistically significant at a 95% level of confidence - that is, we are at least 95% certain that lattes from torani syrups and machine B are tastier than lattes from monin syrups and machine A. 27.6.3 Finding the best model with anova() We can even use the anova() function (not to be confused with aov()) to find the best fitting model from a set of model objects. Here, we compare the second model against the first (as opposed to the model against an intercept, as in traditional anova). Then, we calculate the residual sum of squares in both, calculate the extra explained sum of squares in the second model, and estimate an F statistic. If the F statistic is statistically significant and the RSS has decreased, we would say that our second model fits better. # Let&#39;s make a second model for comparison against the first. m2 = lattes %&gt;% # Make a linear model, showing interactions with the &#39;*&#39; sign lm(formula = y ~ machine * milk * syrup) # Here, for instance, there is a significant difference, but the RSS increased, # so we know that model 1 is better than model 2 anova(m, m2) ## Analysis of Variance Table ## ## Model 1: y ~ machine * milk * syrup * art ## Model 2: y ~ machine * milk * syrup ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 216 6330 ## 2 228 16182 -12 -9852 28.015 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Conclusion In summary, Kimâ€™s Coffee can rather quickly estimate the treatment effects of specific changes in their latte brewing process, as long as they randomly assign their lattes to different treatments and estimate their treatment effects with appropriate standard errors. Happy experimenting! "],["response-surface-methodology-in-r.html", "28 Response Surface Methodology in R Getting Started 28.1 Models for RSM Learning Check 1 28.2 Contour Plots Learning Check 2 28.3 Iterate! Learning Check 3 28.4 Quantities of Interest in RSM 28.5 Extra Concepts of Interest: Canonical Form", " 28 Response Surface Methodology in R Figure 12.1: RSM Contour Plots! Getting Started Imagine: A team of enterprising Systems Engineering students have decided to start their own baking company, selling gingerbread cookies in the month of December in the greater Ithaca area! None of them are particularly good at baking, but theyâ€™re rad good at design of experiments, so they set out to discover the ultimate gingerbread cookie through factorial design and our new tool, response surface methodology! Follow along below to see a surprisingly accurate example of how you can apply RSM to find the optimal design of your product (in this case, gingerbread!) Packages # Load packages! library(tidyverse) # dplyr and ggplot! library(broom) # glance() and tidy()! library(viridis) # color palettes! install.packages(c(&quot;rsm&quot;, &quot;metR&quot;)) # you&#39;ll need to install these! library(rsm) # for RSM library(metR) # for contour plot labels in ggplot (#fig:img_intro)Gingerbread Cookies! Courtesy of Casey Chae @ Unsplash Our Data They know from past research that the amount of molasses and ginger in gingerbread cookies are likely significantly related to the overall tastiness (called the yum factor in our dataset). But, theyâ€™re not sure how much molasses and how much ginger are needed. Molasses can be somewhat expensive too, compared to other ingredients; so they want to optimize the amount of molasses necessary to produce the best cookies. So, holding all other conditions in the recipe constant, they ran a factorial experiment, making 16 batches of cookies (about 20 cookies per batch). In their experiment, they tested 4 different amounts of molasses, including \\(\\frac{1}{2}, \\ \\frac{3}{4}, \\ 1,\\ \\&amp; \\ 1 \\frac{1}{4}\\) cups of molasses. They also tested 4 different amounts of ginger, including \\(\\frac{1}{2}, \\ 1, \\ 1 \\frac{1}{2}, \\ \\&amp; \\ 2\\) tablespoons of ginger. Each batch was randomly assigned one of the 16 unique pairings of amounts of ginger and molasses; there are \\(4 \\times 4 = 16\\) unique ways to assign these ingredients, and they included them all to fully account for all the possibilities. Then, they randomly handed out cookies to folks on campus in exchange for them briefly ranking the yum factor of that cookie on a scale from 0 (disgusting!) to 100 (delicious!). Import Data They compiled their data in the following dataset. Read it in to help them analyze their data! This dataset includes the following variables: id: unique ID for each cookie (320 cookies!) batch: unique group ID for each batch of about 20 cookies. Outcome yum: numeric scale measuring deliciousness of cookie, from 0 (disgusting) to 100 (delicious) Predictors molasses: cups of molasses in batch: 0.75, 1, 1.25, or 1.5 cups. ginger: tablespoons of ginger in batch: 0.5, 1, 1.5, or 2 tablespoons. Fixed Conditions cinnamon: 1 tablespoon butter: 1 cup flour: 3 cups # Import our data cookies = read_csv(&quot;workshops/gingerbread_test1.csv&quot;) # Check it out! cookies %&gt;% glimpse() ## Rows: 320 ## Columns: 8 ## $ id &lt;dbl&gt; 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 9â€¦ ## $ batch &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4â€¦ ## $ yum &lt;dbl&gt; 5, 0, 13, 8, 9, 4, 1, 15, 10, 2, 3, 0, 5, 5, 8, 2, 0, 0, 8, 0â€¦ ## $ molasses &lt;dbl&gt; 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0â€¦ ## $ ginger &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2â€¦ ## $ cinnamon &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦ ## $ butter &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦ ## $ flour &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3â€¦ 28.1 Models for RSM In our previous workshop, we learned to calculate the difference of means for any set of groups (in this case, batch of cookies). When we get many different levels in our predictors (eg. not just a treatment and control), we might prefer to use lm() to estimate a linear model of our outcome (yum), rather than computing the difference of means many times. 28.1.1 Specifying an Interaction Model with Polynomials However, we will see quickly that different specifications of our model may work better than others. Weâ€™re going to try three different models that predict yum using cups of molasses and tablespoons of ginger, and weâ€™ll evaluate the \\(R^{2}\\) of each (% of variation in outcome yum explained by model). These include: Basic First-Order Polynomial Model, where: \\[ Yum = \\alpha + \\beta_{m} X_{m} + \\beta_{g} X_{g} \\] \\(B_{m}\\) is the effect of a 1 cup increase of Molasses. \\(B_{g}\\) is the effect of a 1 tablespoon increase of Ginger. m1 = cookies %&gt;% lm(formula = yum ~ molasses + ginger) Interaction Model, where: \\[ Yum = \\alpha + \\beta_{m} X_{m} + \\beta_{g} X_{g} + \\beta_{mg} X_{m} X_{g} \\] \\(B_{mg}\\) is the interaction effect as molasses increases by 1 cup AND ginger increases by 1 tablespoon. m2 = cookies %&gt;% lm(formula = yum ~ molasses * ginger) # Also written manually as: # yum ~ molasses + ginger + I(molasses * ginger) Second-Order Polynomial Model with Interaction, where: \\[ Yum = \\alpha + \\beta_{m} X_{m} + \\beta_{g} X_{g} + \\beta_{mg} X_{m} X_{g} + \\beta_{m^{2}} X_{m}^{2} + \\beta_{g^{2}} X_{g}^{2} \\] - \\(\\beta{m^2} X_{m}^{2}\\) is the effect as the square of molasses increases by 1. Together, \\(\\beta_{m} X_{m}\\) and \\(\\beta_{m^2} X_{m}^{2}\\) act as a polynomial term predicting yum. # Add a polynomial by to your existing interactions by using I(variable^2) m3 = cookies %&gt;% lm(formula = yum ~ molasses * ginger + I(molasses^2) + I(ginger^2)) To review, this model equation can be viewed by just checking m3$coefficients. We can also write it out below; Iâ€™ve rounded to 2 decimal places for simplicity below. \\[ \\hat{yum} = \\hat{Y} = 35.18 + -50.05 X_{m} + 34.15 X_{m}^{2} + -5.06 X_{g} + 4.66 X_{g}^{2} + -7.13 X_{m} X_{g} \\] Letâ€™s evaluate the r.squared of our three models below using the glance() function from the broom package, and bind together those data.frames into one using bind_rows() from dplyr. We see that the polynomial terms dramatically improve the predictive power of our model, jumping from \\(R^{2}\\) = 0.03 to \\(R^{2}\\) = 0.14. bind_rows( glance(m1), glance(m2), glance(m3) ) ## # A tibble: 3 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00167 -0.00463 7.30 0.265 0.767 2 -1089. 2185. 2200. ## 2 0.0252 0.0159 7.22 2.72 0.0447 3 -1085. 2180. 2198. ## 3 0.137 0.123 6.82 9.97 0.00000000719 5 -1065. 2145. 2171. ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; Not really amazing quality model fit here - and that does happen! We can tidy() our model m3 to confirm. The low p.value for many of our predictors tells us that our predictors do tend to have statistically significant relationships with the yum factor of our cookies. (Admittedly, gingerâ€™s direct effect is not very significant - just ~75% confidence). But, it looks like other factors not currently in our model might also impact yum factor. m3 %&gt;% tidy() ## # A tibble: 6 Ã— 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 35.2 5.53 6.36 7.24e-10 ## 2 molasses -50.1 11.2 -4.48 1.07e- 5 ## 3 ginger -5.06 4.42 -1.15 2.53e- 1 ## 4 I(molasses^2) 34.1 6.10 5.60 4.69e- 8 ## 5 I(ginger^2) 4.66 1.52 3.06 2.42e- 3 ## 6 molasses:ginger -7.13 2.44 -2.92 3.71e- 3 28.1.2 Modeling with rsm() Finally, we can also write this model using the rsm() function in the rsm package. It works very similarly to lm(), but has some shortcut tricks built it. Weâ€™ll use it more later in the workshop. For now, letâ€™s make some rsm model objects to match our lm model terms exactly. Letâ€™s make a simple â€˜First-Orderâ€™ polynomial model with FO(). That means just one term per predictor (eg. no \\(x^2\\), just \\(x\\)). Our r1 model will match our m1 model. r1 = cookies %&gt;% rsm(formula = yum ~ FO(molasses, ginger)) # Check it! r1$coefficients ## (Intercept) FO(molasses, ginger)molasses ## 13.67312 0.79500 ## FO(molasses, ginger)ginger ## 0.35250 # This is the same as m1 r1$coefficients == m1$coefficients ## (Intercept) FO(molasses, ginger)molasses ## TRUE TRUE ## FO(molasses, ginger)ginger ## TRUE Letâ€™s make a more complex â€˜Second-Orderâ€™ polynomial model with SO(). That means just two terms per predictor (eg. \\(x\\) and \\(x^2\\)), as well as an interaction effect (called TWI() for two-way interaction). # Let&#39;s make a &#39;Second-Order&#39; model with SO() r3 = cookies %&gt;% rsm(formula = yum ~ SO(molasses, ginger)) # Check it! r3$coefficients ## (Intercept) FO(molasses, ginger)molasses ## 35.17875 -50.05250 ## FO(molasses, ginger)ginger TWI(molasses, ginger) ## -5.06325 -7.13200 ## PQ(molasses, ginger)molasses^2 PQ(molasses, ginger)ginger^2 ## 34.15000 4.66250 # These coefficients match our m3$coefficients too. # FO(...)molasses = molasses # FO(...)ginger = ginger # PQ(...)molasses^2 = I(molasses^2) # PQ(...)ginger^2 = I(ginger^2) # TWI = molasses:ginger m3$coefficients ## (Intercept) molasses ginger I(molasses^2) I(ginger^2) ## 35.17875 -50.05250 -5.06325 34.15000 4.66250 ## molasses:ginger ## -7.13200 28.1.3 Transforming Variables By default, linear models estimate linear relationships between predictors and outcomes, but many relationship are indeed not linear! Here are 8 ways we might model associations! Figure 28.1: 8 Common x~y Modeling Strategies A logit function can sometimes help - that is designed for when a variable ranges between 0 and 1; we could write a classic logit as logit = function(p){ log(p / (1 - p) ) }. # Write a custom logit function for data from 0 to 100 logit = function(p){ log( p / (1 - p) ) } # Notice how it ONLY accepts our positive values greater than 0 and less than 1? c(-1, 0, 0.1, 0.2, 0.5, 1, 2) %&gt;% logit() ## [1] NaN -Inf -2.197225 -1.386294 0.000000 Inf NaN Letâ€™s try a few of these strategies for our x and y variables, and see if any of them improve our predictive power (\\(R^{2}\\)). Spoiler alert: In our data, they donâ€™t but in other datasets, they very well might! Always a good thing to check. For example, we can try transforming the outcome variable, using a standard linear trend (business as usual), a log transformation, or a square root transformation. # Linear (normal) cookies %&gt;% lm(formula = yum ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.137 0.123 6.82 9.97 0.00000000719 5 -1065. 2145. 2171. ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.137 # Logged (add 1 since yum contains 0s) cookies %&gt;% lm(formula = log(yum + 1) ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.110 0.0963 0.707 7.80 0.000000616 5 -340. 694. 721. ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.11 (Worse) # Square Root cookies %&gt;% lm(formula = sqrt(yum) ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.125 0.111 1.13 9.00 0.0000000522 5 -489. 992. 1018. ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.125 (Worse) Alternatively, we could try transforming the predictor variables, using a log-transformation. cookies %&gt;% lm(formula = yum ~ log(molasses) * log(ginger) + I(log(molasses)^2) + I(log(ginger)^2)) %&gt;% glance() ## # A tibble: 1 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.173 0.160 6.67 13.2 1.17e-11 5 -1058. 2131. 2157. ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; It turns out that few of these transformations really dramatically change the predictive power of the model, so Iâ€™ll stick with our original models m3/r3 for the time being. Figure 28.2: A poorly predicted Gingerbread Cookie Photo by Noelle Otto Learning Check 1 Question What happens when you (1) square \\(y\\), (2) cube \\(y\\), or (3) take the logit of \\((y + 1) / 100\\)? Find the r.squared for each of these models. [View Answer!] Looks like a linear, business-as-usual modeling strategy for our outcome variable \\(y\\) (yum) is best for this data. # Logit cookies %&gt;% lm(formula = log( (yum + 1)/100 / (1 - (yum + 1)/100) ) ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.115 0.101 0.779 8.19 0.000000274 5 -371. 756. 782. ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.115 (Worse) # Squared cookies %&gt;% lm(formula = I(yum^2) ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.119 0.105 208. 8.46 0.000000158 5 -2160. 4333. 4360. ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.118 (Worse) # Cubed cookies %&gt;% lm(formula = I(yum^3) ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0913 0.0769 6147. 6.31 0.0000134 5 -3243. 6499. 6526. ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.091 (Worse) 28.2 Contour Plots So now that we have this model, what do we do with it? Response Surface Methodology refers to using statistical models to predict an outcome (a.k.a. response variable) given a series of varying conditions. This lets us predict and visualize the full range/surface for that outcome. 28.2.1 Simple contour() plots The easiest way to think of this is in 3-dimensions, meaning 3 variables (1 outcome and 2 predictors). A regression model traditionally finds us the plane of best fit when looking at 3 dimensions, or the hyperplane of best fit when looking at +4 dimensions. However, when we use polynomial terms in our model equation, we can map that plane almost perfectly to our observed data, creating more of a contour or topographical surface than a simple plane. We can use our model object m3 or r3 from above to generate a contour plot, predicting the yum factor (shown by color and lines) while we varying ~molasses + ginger levels. We can add a heatmap by saying image = TRUE. Our model predicts that middling levels of ginger and molasses produce a kind of sad coldspot where the yum factor is about 11 (middle), but our model projects the yum factor will increase when you increase ginger and/or molasses from that center amount. contour(m3, ~molasses + ginger, image = TRUE) Thatâ€™s beautiful - but a little unclear how it was produced! How could we make that plot ourselves in ggplot? 28.2.2 geom_tile() plots However, weâ€™ve learned this term that ggplot can give us greater flexibility when designing and communicating information, so how would we make this in ggplot? Itâ€™s really quick! We need to (1) make a grid of predictor values with expand_grid() to feed to predict(), (2) extract the predicted yum values (usually called yhat), and (3) then pipe the result to ggplot! # Let&#39;s check the range of our predictors... cookies$molasses %&gt;% range() ## [1] 0.50 1.25 cookies$ginger %&gt;% range() ## [1] 0.5 2.0 # Great! So we could vary our ingredient amounts from 0 to ~5 while still being realistic. Step 1: Weâ€™ll use expand_grid() to build a grid of molasses and ginger values, called myx, where molasses spans its observed range and ginger spans its own observed range. # Make the grid of conditions! myx = expand_grid( molasses = seq(from = min(cookies$molasses), to = max(cookies$molasses), length.out = 50), ginger = seq(from = min(cookies$ginger), to = max(cookies$ginger), length.out = 50) ) # Optionally, you could pick some arbitrary ranges, like 0 to 5 #myx = expand_grid( # molasses = seq(from = 0, to = 5, length.out = 50), # ginger = seq(from = 0, to = 3, length.out = 50) #) # Check it out! myx %&gt;% glimpse() ## Rows: 2,500 ## Columns: 2 ## $ molasses &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0â€¦ ## $ ginger &lt;dbl&gt; 0.5000000, 0.5306122, 0.5612245, 0.5918367, 0.6224490, 0.6530â€¦ Note: You have to pick these values!! (eg. 0 to 5, 0 to 3, etc.) contour() uses the min and max of molasses and ginger each, but often, we want to make predictions slightly beyond our observed data. Just remember, a grid of 20 by 20 items produces 400 cells; 100 by 100 produces 10,000 cells; etc. Once you get above a few 1000, ggplot starts to slow down quickly. Step 2: Next, weâ€™ll mutate() our myx data.frame to add a column yhat. In that column, we predict() the yum factor for those conditions based on our model m3 (or r3 - either work). As shown in previous workshops, we must give predict() a data.frame containing hypothetical values of each predictor in our model, called newdata. Weâ€™ll save the result in a data.frame called mypred. # Make predictions! mypred = myx %&gt;% mutate(yhat = predict(m3, newdata = tibble(molasses, ginger))) # Check it out! mypred %&gt;% glimpse() ## Rows: 2,500 ## Columns: 3 ## $ molasses &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0â€¦ ## $ ginger &lt;dbl&gt; 0.5000000, 0.5306122, 0.5612245, 0.5918367, 0.6224490, 0.6530â€¦ ## $ yhat &lt;dbl&gt; 15.54100, 15.42394, 15.31561, 15.21603, 15.12518, 15.04308, 1â€¦ Step 3: Finally, weâ€™ll visualize it using geom_tile(), which maps a fill (yhat) to every x (molasses) and y (ginger) coordinate. # Voila! g1 = ggplot() + geom_tile(data = mypred, mapping = aes(x = molasses, y = ginger, fill = yhat)) + scale_fill_viridis(option = &quot;plasma&quot;) # View it! g1 This matches the same pattern from our contour() plot, and was pretty painless! 28.2.3 Pretty geom_contour() plots! But can we make this prettier and clearer for our reader? ggplot includes a geom_contour() function that will plot the contour outlines on top. A few requirements: In the aes(), geom_contour() requires you to make x, y, and z aesthetics, where z is the predicted value yhat. You can set the number of bins (eg. bins = 10 intervals) OR the binwidth (eg. binwidth = 10, where each interval is 10 units of yhat wide). [Just like with cut_interval()!] # Using bins.... # Add contour lines, where each line is 1 unit apart on the `yum` factor scale! g1 + geom_contour(data = mypred, mapping = aes(x = molasses, y = ginger, z = yhat), color = &quot;white&quot;, bins = 10) # Using binwidth... # Add contour lines, where each line is 1 unit apart on the `yum` factor scale! g1 + geom_contour(data = mypred, mapping = aes(x = molasses, y = ginger, z = yhat), color = &quot;white&quot;, binwidth = 1) Alternatively, we could do this all in one fell swoop, using geom_contour_fill(), which combines geom_tile() and geom_contour() together. (Note: geom_contour_filled() is a different function. You want _fill(), not _filled().) g2 = ggplot() + # Make a filled contour plot, with a binwidth of 1 geom_contour_fill(data = mypred, mapping = aes(x = molasses, y = ginger, z = yhat), binwidth = 1, color = &quot;white&quot;) + scale_fill_viridis(option = &quot;plasma&quot;) # View it! g2 Finally, some coding wizards out there developed some ggplot add-on functions in the metR package that will let us add nice labels to our contour plots, using geom_label_contour(). We supply it the same information as geom_contour_filled(), including x, y, and z vectors Optional: Tell it to skip = 0 lines, labeling every contour line. Optional: Tell it to add a white border around our text with stroke.color = \"white\", where that border is stroke = 0.2 points wide. Optional: If you add label.placer = label_placer_n(1), it will label each contour line n = 1 time. All other rules of ggplot apply, eg. size, text color, and alpha transparency. Note: you must have loaded the metR package for this to work. g2 + geom_text_contour(data = mypred, mapping = aes(x = molasses, y= ginger, z = yhat), skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, label.placer = label_placer_n(1)) Beautiful! 28.2.4 One-step RSM in ggplot Finally, letâ€™s practice doing this all in one code chunk in ggplot. # Make our predictors... - this time let&#39;s expand the range mypred2 = expand_grid( molasses = seq(from = 0, to = 4, length.out = 50), ginger = seq(0, to = 4, length.out = 50) ) %&gt;% mutate(yhat = predict(m3, newdata = tibble(molasses, ginger))) mypred2 %&gt;% # Set aesthetics ggplot(mapping = aes(x = molasses, y = ginger, z = yhat)) + # Make a filled contour with 15 bins geom_contour_fill(bins = 15, color = &quot;white&quot;) + # Add labels geom_text_contour(skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, label.placer = label_placer_n(1)) + # Add a beautiful plasma viridis palette scale_fill_viridis(option = &quot;plasma&quot;) + # Add theming and labels theme_classic(base_size = 14) + theme(axis.line = element_blank()) + # get rid of axis line labs(x = &quot;Molasses (cups)&quot;, y = &quot;Ginger (tablespoons)&quot;, fill = &quot;Predicted\\nYum\\nFactor&quot;, subtitle = &quot;Contour of Predicted Gingerbread Cookie Tastiness&quot;) Excellent! Our plot can serve as a visual diagnostic. Tentatively, our model results suggest that increasing molasses may lead to considerable gains in our outcome, with ginger contributing some impact early on. Notably, we see that though our actual outcomeâ€™s measurement ranged from 0 to 100, our predictions might exceed those limits. 28.2.5 More Realistic Plots Even though transformations donâ€™t improve our predictive accuracy, they might make our predictions more realistic. Letâ€™s try a few transformations. A logit() transformation could help with bounding yum to 0 and 1, if we scale down yum from 0-100 to 0-1. Weâ€™ll have to add +1 to the yum scale though, because some cookies got a score of zero, which can be logit-transformed. A log() transformation to molasses and ginger could help with bounding these conditions to only positive values, since we know we need at least a little of each, and we canâ€™t have â€˜negative ginger.â€™ # Write a quick adjusted logit function adj_logit = function(p){ p = (p + 1) / 100 # adjust p from 0 - 100 to 0 - 1 log(p / (1 - p)) # logit transformation } # Transform outcome and predictors m4 = cookies %&gt;% lm(formula = adj_logit(yum) ~ log(molasses) * log(ginger) + I(log(molasses)^2) + I(log(ginger)^2)) # Get conditions and predictions mypred3 = expand_grid( molasses = seq(from = 0.01, to = 4, length.out = 50), ginger = seq(0.01, to = 4, length.out = 50) ) %&gt;% mutate(yhat = predict(m4, newdata = tibble(molasses, ginger)), # Undo the logit transformation! yhat = exp(yhat) / (1 + exp(yhat)), # Undo the (y + 1) / 100 transformation yhat = 100*yhat - 1) # Visualize it! g3 = mypred3 %&gt;% ggplot(mapping = aes(x = molasses, y = ginger, z = yhat)) + geom_contour_fill(bins = 15, color = &quot;white&quot;) + geom_text_contour(skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, # Label each contour twice, but check_overlap deletes labels that overlap! label.placer = label_placer_n(2), check_overlap = TRUE) + scale_fill_viridis(option = &quot;plasma&quot;) + theme_classic(base_size = 14) + theme(axis.line = element_blank()) + # get rid of axis line labs(x = &quot;Molasses (cups)&quot;, y = &quot;Ginger (tablespoons)&quot;, fill = &quot;Predicted\\nYum\\nFactor&quot;, subtitle = &quot;Contour of Predicted Gingerbread Cookie Tastiness&quot;) g3 Ta-da! Now we have much more reasonable predictions, even though we lost 2% predictive power. Itâ€™s always a trade-off between predictive power and our ability to generate reasonable, useful quantities of interest. Ideally, letâ€™s get a much better \\(R^{2}\\)! Learning Check 2 Suppose we expanded our factorial experiment based on this contour plot, adding more permutations of molasses and ginger, such that we now have 1280 cookies under test! Weâ€™ve saved this data in workshops/gingerbread_test2.csv. Question Generate a second-order polynomial model like m3 and visualize the contour plot in ggplot. How do our predictions change? cookies2 = read_csv(&quot;workshops/gingerbread_test2.csv&quot;) [View Answer!] # Check the range cookies$ginger %&gt;% range() ## [1] 0.5 2.0 cookies$molasses %&gt;% range() ## [1] 0.50 1.25 # Write a quick adjusted logit function adj_logit = function(p){ p = (p + 1) / 100 # adjust p from 0 - 100 to 0 - 1 log(p / (1 - p)) # logit transformation } # Transform outcome and predictors m_lc = cookies2 %&gt;% lm(formula = adj_logit(yum) ~ log(molasses) * log(ginger) + I(log(molasses)^2) + I(log(ginger)^2)) # Check the R2 (still terrible! whoops!) m_lc %&gt;% glance() ## # A tibble: 1 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.140 0.136 0.619 41.3 1.75e-39 5 -1199. 2411. 2447. ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # Get conditions and predictions mypred_lc = expand_grid( molasses = seq(from = 0.01, to = 4, length.out = 50), ginger = seq(0.01, to = 4, length.out = 50) ) %&gt;% mutate(yhat = predict(m_lc, newdata = tibble(molasses, ginger)), # Undo the logit transformation! yhat = exp(yhat) / (1 + exp(yhat)), # Undo the (y + 1) / 100 transformation yhat = 100*yhat - 1) # Visualize it! g_lc = mypred_lc %&gt;% ggplot(mapping = aes(x = molasses, y = ginger, z = yhat)) + geom_contour_fill(bins = 15, color = &quot;white&quot;) + geom_text_contour(skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, # Label each contour twice, but check_overlap deletes labels that overlap! label.placer = label_placer_n(2), check_overlap = TRUE) # Add good colors and theming! g_lc + scale_fill_viridis(option = &quot;plasma&quot;) + theme_classic(base_size = 14) + theme(axis.line = element_blank()) + # get rid of axis line labs(x = &quot;Molasses (cups)&quot;, y = &quot;Ginger (tablespoons)&quot;, fill = &quot;Predicted\\nYum\\nFactor&quot;, subtitle = &quot;Contour of Predicted Gingerbread Cookie Tastiness&quot;) Our predictive power is still not quite that good. Ironically, our model (based on fake data) suggests that the best gingerbread cookies you can make should either have very little molasses OR lots of molasses and ginger, but the payoff for using very little molasses will be higher! This plot demonstrates how even though your original model m4 predicted really high payoff for adding more molasses, when we compare those predictions to updated model predictions based on new experiments, we might find that the new empirical data tempers our earlier predictions. This is good news. This probably means that our earlier predictions were not very accurate, and our extra experiments paid off by helping clarify. New results can be suprising, but are never a bad thing - because they get you closer to truth. 28.3 Iterate! Suppose now that we expanded our factorial experiment to vary the amount of flour, butter, and cinnamon too! Weâ€™ve saved this data in workshops/gingerbread_test3.csv. How would we model this data? cookies3 = read_csv(&quot;workshops/gingerbread_test3.csv&quot;) cookies3 %&gt;% glimpse() ## Rows: 46,080 ## Columns: 8 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18â€¦ ## $ batch &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2â€¦ ## $ yum &lt;dbl&gt; 0, 4, 9, 10, 4, 8, 7, 3, 1, 12, 0, 4, 6, 10, 0, 0, 10, 5, 1, â€¦ ## $ molasses &lt;dbl&gt; 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0â€¦ ## $ ginger &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦ ## $ cinnamon &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1â€¦ ## $ butter &lt;dbl&gt; 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0â€¦ ## $ flour &lt;dbl&gt; 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2â€¦ 28.3.1 Modeling many Interactions We can make a second-order polynomial for these 5 variables with lm() or rsm(), like so: # model using rsm() r5 = cookies3 %&gt;% rsm(formula = yum ~ SO(molasses, ginger, cinnamon, butter, flour)) # model using lm() m5 = cookies3 %&gt;% lm(formula = yum ~ molasses * ginger * cinnamon * butter * flour + I(molasses^2) + I(ginger^2) + I(cinnamon^2) + I(butter^2) + I(flour^2)) # Check out our coefficients! Wow that&#39;s a long list! m5$coefficients %&gt;% round(3) ## (Intercept) molasses ## 6.628 11.816 ## ginger cinnamon ## 10.739 -60.324 ## butter flour ## -1.272 0.078 ## I(molasses^2) I(ginger^2) ## 2.585 0.302 ## I(cinnamon^2) I(butter^2) ## -0.034 -4.601 ## I(flour^2) molasses:ginger ## -0.868 -11.493 ## molasses:cinnamon ginger:cinnamon ## 32.053 5.694 ## molasses:butter ginger:butter ## -9.758 -5.691 ## cinnamon:butter molasses:flour ## 65.636 -1.146 ## ginger:flour cinnamon:flour ## -2.347 21.764 ## butter:flour molasses:ginger:cinnamon ## 7.047 -2.030 ## molasses:ginger:butter molasses:cinnamon:butter ## 6.798 -36.209 ## ginger:cinnamon:butter molasses:ginger:flour ## -11.122 2.366 ## molasses:cinnamon:flour ginger:cinnamon:flour ## -12.649 -2.576 ## molasses:butter:flour ginger:butter:flour ## 0.025 0.667 ## cinnamon:butter:flour molasses:ginger:cinnamon:butter ## -23.226 5.925 ## molasses:ginger:cinnamon:flour molasses:ginger:butter:flour ## 1.566 -0.987 ## molasses:cinnamon:butter:flour ginger:cinnamon:butter:flour ## 13.912 4.207 ## molasses:ginger:cinnamon:butter:flour ## -2.742 # Check predictive power (still pretty bad!) m5 %&gt;% glance() ## # A tibble: 1 Ã— 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.168 0.167 7.06 258. 0 36 -155424. 310924. 311256. ## # â„¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # Check which variables are significant # (if some were not, we might cut them if we wanted to make as parsimonious a model as possible) m5 %&gt;% tidy() %&gt;% filter(p.value &lt; 0.05) ## # A tibble: 10 Ã— 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cinnamon -60.3 24.8 -2.43 1.51e- 2 ## 2 I(molasses^2) 2.58 0.115 22.5 1.35e-111 ## 3 I(ginger^2) 0.302 0.0287 10.5 6.74e- 26 ## 4 I(butter^2) -4.60 1.12 -4.12 3.77e- 5 ## 5 cinnamon:butter 65.6 24.3 2.70 6.99e- 3 ## 6 cinnamon:flour 21.8 8.26 2.64 8.41e- 3 ## 7 molasses:cinnamon:butter -36.2 16.3 -2.22 2.66e- 2 ## 8 molasses:cinnamon:flour -12.6 5.54 -2.28 2.25e- 2 ## 9 cinnamon:butter:flour -23.2 8.09 -2.87 4.10e- 3 ## 10 molasses:cinnamon:butter:flour 13.9 5.43 2.56 1.04e- 2 28.3.2 Contours with Multiple Variables Now, whenever we analyze contours, since we have more than 2 predictors, we need multiple plots. For example, letâ€™s examine variation in yum as 3 predictions change simultaneously. These include molasses, ginger, and cinnamon. We can just write in contour() the formula ~molasses + ginger + cinnamon. It will place molasses and ginger on the x and y axes, because they came first, and then report the values of cinnamon, butter, and flour for each panel. However, the mechanics of contour() can be tricky, and its tough to compare plots like these, since they are switching the x and y axis in every plot! But what if we could make our own in ggplot? # We can split it into 1 row and 3 columns using par(mfrow = c(1, 3)) par(mfrow = c(1,3)) # And plot the contous like so contour(m5, ~molasses + ginger + cinnamon, image = TRUE) 28.3.3 ggplot contour plots! This is the power of ggplot - since you have to work with the data yourself, you actually know what your plots mean and can design the plots most useful to your team. For example, I would love to see 2 panels showing the contours of molasses x ginger when cinnamon = 0, cinnamon = 1, and cinnamon = 2 tablespoons. All other conditions would be held constant, allowing us to see how the contour changes shape. If we hold constant the other values though, we should hold them at meaningful values, like the average or perhaps a value you know to be sufficient. cookies3$cinnamon %&gt;% range() ## [1] 0.5 2.0 cookies3$butter %&gt;% mean() ## [1] 1 # Get a grid... mygrid = expand_grid( molasses = seq(from = 0, to = 4, length.out = 30), ginger = seq(from = 0, to = 4, length.out = 30), # Now repeat that grid for each of these values of cinnamon! cinnamon = c(0, 1, 2), # Hold other constant at meaningful values flour = cookies3$flour %&gt;% mean(), butter = cookies3$butter %&gt;% mean()) %&gt;% # Then predict your outcome! mutate(yhat = predict(m5, newdata = tibble(molasses, ginger, cinnamon, flour, butter))) Next, letâ€™s use our grid to visualize the contours in ggplot! # Let&#39;s check it out! g4 = mygrid %&gt;% # Map aesthetics ggplot(mapping = aes(x = molasses, y = ginger, z = yhat)) + # SPLIT INTO PANELS by amount of cinnamon! facet_wrap(~cinnamon) + geom_contour_fill(binwidth = 5, color = &quot;white&quot;) + geom_text_contour(skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, label.placer = label_placer_n(1), check_overlap = TRUE) # View it! g4 Finally, letâ€™s improve the labels, colors, and theming for this plot. g5 = g4 + theme_classic(base_size = 14) + theme(axis.line = element_blank(), # clean up the lines axis.ticks = element_blank(), # clean up the ticks strip.background = element_blank()) + # clean up the facet labels scale_fill_viridis(option = &quot;plasma&quot;) + labs(x = &quot;Molasses (cups)&quot;, y = &quot;Ginger (tablespoons)&quot;, fill = &quot;Predicted\\nYum\\nFactor&quot;, subtitle = &quot;Predicted Gingerbread Cookie Tastiness\\nby Tablespoons of Cinnamon&quot;) # view it! g5 What can we learn from this plot? When we add more cinnamon (2 tbsp; right), the zone in which cookies are truly bad (&lt;15 points of yum) shrinks greatly (compared to left and center panels). Otherwise, cinnamon has fairly minimal interaction effects with ginger and molasses on yum scores. Learning Check 3 Suppose we want to examine other interactions! Design your own ggplot to test how butter shapes the yum factor as molasses and flour vary. Question Put molasses on the x-axis from 0 to 4 cups, flour on the y-axis from 0 to 4 cups, and vary the level of butter across panels from 0.75 to 1 to 1.25 cups. Hold other conditions at their mean values. [View Answer!] # Make the grid!s mygrid_lc3 = expand_grid( # Vary molasses and flour... molasses = seq(from = 0, to = 3, length.out = 30), flour = seq(from = 0, to = 3, length.out = 30), # Now repeat that grid for each of these values of butter! butter = c(0.75, 1, 1.25), # Hold other constant at meaningful values ginger = cookies3$ginger %&gt;% mean(), cinnamon = cookies3$cinnamon %&gt;% mean()) %&gt;% # Then predict your outcome! mutate(yhat = predict(m5, newdata = tibble(molasses, ginger, cinnamon, flour, butter))) # Visualize! mygrid_lc3 %&gt;% ggplot(mapping = aes(x = molasses, y = flour, z = yhat)) + # SPLIT INTO PANELS by amount of butter! facet_wrap(~butter) + geom_contour_fill(binwidth = 5, color = &quot;white&quot;) + geom_text_contour(skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, label.placer = label_placer_n(1), check_overlap = TRUE) + # Add theming! scale_fill_viridis(option = &quot;plasma&quot;) + theme_classic(base_size = 14) + theme(axis.line = element_blank(), # clean up the lines axis.ticks = element_blank(), # clean up the ticks strip.background = element_blank()) + labs(x = &quot;Molasses (cups)&quot;, y = &#39;Flour (cups)&#39;, fill = &quot;Predicted\\nYum\\nFactor&quot;, subtitle = &quot;Predicted Gingerbread Cookie Tastiness\\nby Cups of Butter&quot;) This plot tells us that adding more butter to the cookies tends to reduce the amount of the contour with low yum scores, and increases the relative share of the the response surface with scores of 15 or 20. 28.4 Quantities of Interest in RSM Finally, we might be interested in calculating (and annotating our charts) with some key quantities of interest! Letâ€™s use our model m5 from earlier and its rsm counterpart r5. 28.4.1 Percent Change in Bins First, when comparing change across panels, weâ€™re essentially comparing change in area. So we can use our grid of conditions and predictions mygrid to calculate those percentages! area = mygrid %&gt;% # Cut the outcome into bins, 5 units wide on the yum scale mutate(bin = cut_interval(yhat, length = 5)) %&gt;% # For each panel and bin, count up the predictions in that interval group_by(cinnamon, bin) %&gt;% summarize(count = n()) %&gt;% ungroup() %&gt;% # Now, for each panel, calculate the percentage of predictions in that panel located in each bin group_by(cinnamon) %&gt;% mutate(percent = count / sum(count), percent = round(percent*100, 2)) # Zoom into the lowest bin. # What percentage of the area was in that bin given each level of cinnamon? qi1 = area %&gt;% filter(bin == &quot;[10,15]&quot;) qi1 ## # A tibble: 3 Ã— 4 ## # Groups: cinnamon [3] ## cinnamon bin count percent ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 [10,15] 92 10.2 ## 2 1 [10,15] 66 7.33 ## 3 2 [10,15] 36 4 We computed that the area predicted to score lowest on the yum scale (10-15) decreased from 10.22 given 0 tablespoons of cinnamon to 7.33 given 1 tablespoon and then to 4 given 2 tablespoons of cinnamon. 28.4.2 Study Range We might want our reader to know what is the area that we actually had data on, versus what was the area we were generating predictions from. For this, we can just draw a box from our raw data, using summarize() and geom_rect(). geom_rect() requires an xmin, xmax, ymin, and ymax. For example, since molasses is our x variable and ginger has been our y variable in our ggplot analyses, we can do the following: box = cookies3 %&gt;% summarize(xmin = min(molasses), xmax = max(molasses), ymin = min(ginger), ymax = max(molasses)) # For example, we can start a new ggplot ggplot() + # Mapping the contour fill geom_contour_fill(data = mygrid, mapping = aes(x = molasses, y = ginger, z = yhat), color = &#39;white&#39;) + facet_wrap(~cinnamon) + # And then plotting a box overtop, with no fill geom_rect(data = box, mapping = aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), color = &quot;white&quot;, linetype = &#39;dashed&#39;, fill = NA) 28.5 Extra Concepts of Interest: Canonical Form The canonical form is an abbreviation of your long model equation into a much more understandable, short form. rsm will calculate the canonical form of your model equation for you, using canonical(). For example, if you have 2 predictors \\(X_{1}\\) and \\(X_{2}\\), the canonical form would look like: \\[ \\hat{Y} = Y_{s} + X_{1}^{2} + X_{2}^{2}\\] where \\(Y_{s}\\) is the value of \\(\\hat{Y}\\) at the â€˜stationary pointâ€™, when \\(X_{1}\\) and \\(X_{2}\\) equal 0. The tricky thing is that the canonical form is not actually in units of our original predictors, say, cups of molasses and tablespoons of ginger. Instead, the canonical form is like a standardized format that maps every value of cups of molasses (written \\(x_{1}\\)) to a new value \\(X_{1}\\), which is a certain distance away from the stationary point. Why would we have a system like this? Itâ€™s because certain shapes of contour plots can be recognized from their canonical form alone, with no other detail, so the canonical form can be useful for us. Itâ€™s a little more than we can go into at this moment, but suffice it to say that the canonical form of a model is like a shortcut for interpreting the shape of a contour plot. 28.5.1 Finding the Canonical Form We can run the calculations for obtaining the canonical form using canonical() in rsm. canon = canonical(r5) Inside this object are several great quantities of interest. 28.5.2 Canonical Form Coefficients # These coefficients can form the canonical form model equation canon$eigen$values ## [1] 2.622365 0.000000 0.000000 -0.725773 -4.803057 28.5.3 Stationary Points # These are your stationary points. canon$xs ## molasses ginger cinnamon butter flour ## 0.5267221 -0.1922253 -0.5966090 1.3474297 3.2895704 28.5.4 Shift Points # These values can help you convert from x_1 (normal units) to X_1 (canonical form) canon$eigen$vectors ## [,1] [,2] [,3] [,4] [,5] ## molasses 0.99458167 -0.01095125 0.09352416 -0.004894275 -0.04377947 ## ginger -0.01596009 -0.99629842 0.02526971 -0.061114741 -0.05254671 ## cinnamon 0.08987515 -0.01363529 -0.97819754 -0.185204008 -0.02378214 ## butter -0.03957109 0.03864526 -0.02037369 0.210855287 -0.97573851 ## flour 0.03015222 -0.07476727 -0.18256277 0.957852054 0.20661795 In general, I find that the contour plots themselves tend to be the most effective tools for decision-making in most cases, but advanced applications can make great use of the canonical forms to identify key points after which increasing the amount of an ingredient will make no more difference. "],["conclusion-21.html", "Conclusion", " Conclusion Congratulations! You made it! Thank you for your hard work this term. You have built up considerable prowess making probabilistic predictions, making statistical models, and visualizing and communicating your findings in R. We hope that this course has given you a firm grounding in key tools for building safer, more reliable technologies for our communities! We wish you all the best in your future work! Figure 28.3: Courtesy of the Internet "],["appendix-ggplot-tips.html", "29 Appendix: ggplot tips Getting Started 29.1 Flipping Axes 29.2 Legend Positioning 29.3 Breaking Up a Visual", " 29 Appendix: ggplot tips Getting Started ggplot2 and the Grammar of Graphics behind it is an incredible resource for data visualization. Looking for extra information on making the perfect visualization? Consider these helpful resources, and see below for extra Learning Checks! 29.0.1 Resources RGraphGallery: hundreds of example charts in R. ggplot2 Documentation: online guide describing ggplot functions. ggplot2 Cheat Sheets: helpful cheat sheets of ggplot functions. 29.0.2 Our Packages Be sure to load these packages before proceeding! library(dplyr) # for data wrangling library(ggplot2) # for data visualization 29.0.3 Our Data These exercises use the diamonds dataset from the ggplot2 package. # Import the diamonds dataset from the ggplot2 package! diamonds = ggplot2::diamonds # Let&#39;s glimpse it! diamonds %&gt;% glimpse() ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.â€¦ ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Verâ€¦ ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,â€¦ ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, â€¦ ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64â€¦ ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58â€¦ ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34â€¦ ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.â€¦ ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.â€¦ ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.â€¦ 29.1 Flipping Axes Learning Check 1 Question Sometimes, the names of categories wonâ€™t fit well. We can try the following. Compare these two plots. What did we do? library(ggplot2) diamonds = ggplot2::diamonds # Plot 1 gg1 = gg1 = ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() # Plot2 ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() [View Answer!] We used coord_flip() to flip the coordinates of the x and y axis, which gives our cut labels more room! 29.2 Legend Positioning Learning Check 8 Question Sometimes, the legend doesnâ€™t fit well. We can try this: What happens when you change legend.position from \"right\" to \"bottom\" to \"left\" to \"top\"? library(ggplot2) diamonds = ggplot2::diamonds ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;bottom&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;right&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;left&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;top&quot;) [View Answer!] These four values for legend.position will relocate the fill legend (and any other legends) to be at the top, bottom, left, or right of the visual! 29.3 Breaking Up a Visual Finally, we might want to break up our visual into multiple parts. We can use facet_wrap to do this, but how exactly does it work? Try the learning check below. Learning Check 10 Question What changed in the code below, and what did it result in? library(ggplot2) diamonds = ggplot2::diamonds ggplot(data = diamonds, mapping = aes(x = price, fill = cut)) + geom_histogram(color = &quot;white&quot;) + facet_wrap(~cut) + # must be categorical variable labs(x = &quot;Price (USD)&quot;, y = &quot;Frequency of Price (Count)&quot;, title = &quot;US Diamond Sales&quot;) [View Answer!] This visual split up our histograms into separate panels (making it much more readable), and easier to compare distributions. We write facet_wrap(~ before the variable name (eg. cut) to specify that we want to split up the data by the values of cut. This sorts our rows of data into 5 different piles (since there are 5 different categories in cut) and makes a panel out of each. "],["appendix-mermaid-block-diagrams-flowcharts-in-r.html", "30 Appendix: mermaid Block Diagrams (Flowcharts) in R Getting Started 30.1 Mermaid 30.2 Diagram of Ice Cream Shipment 30.3 Diagram Failures", " 30 Appendix: mermaid Block Diagrams (Flowcharts) in R Getting Started To describe and communicate the structure of social systems, we need good visuals! Failure Modes and Effects Analyses particularly rely on block diagrams (a fancy word for flowchart), so in this tutorial, weâ€™re going to learn how to make flowcharts in R using mermaid. Block Diagrams A block diagram shows how a series of components are connected in a social, physical, or technological system. Together, they visualize a network, where the blocks/components are nodes with lines, sometimes called edges, connecting them. Load Packages Letâ€™s load our packages, and then go through a few examples! # Load packages library(tidyverse) # for data management library(DiagrammeR) # for speedy mermaid plotting in R 30.1 Mermaid We can do this using the mermaid function from the DiagrammeR package. mermaid is a popular language for coding small diagrams, and DiagrammeR lets us draw them in R! mermaid will help you map out exactly what is happening in your six sigma case studies. How does it work? We take a bunch of small character values, like \"graph TB\", \"a\", \"b\", and \"a--&gt;b\", and we paste() them together into a single big string that the mermaid function reads. When we paste() them, we have to be sure that every value goes on a separate line, so we separate them by using collapse = \"\\n \". \"\\n\" is a handy shortcut that means, linebreak! Letâ€™s do a quick example, then weâ€™ll apply it to our hypothetical Ben and Jerryâ€™s example. 30.1.1 Example Block Diagram with mermaid # Let&#39;s make a big vector of values, using c() example &lt;- c( # say it&#39;s a graph, going Left-to-Right (LR) &quot;graph LR&quot;, # list out our main nodes &quot;a[Tim]&quot;, &quot;z[Coffee]&quot;, # make a subgraph # (a box to contain some nodes) &quot;subgraph Process&quot;, # Put at least one node in it &quot;b[Froth Milk]&quot;, &quot;c[Brew Coffee]&quot;, &quot;d[Pour in Milk]&quot;, # Close the subgraph &quot;end&quot;, # Draw relationships &quot;a--&gt;b&quot;, &quot;b--&gt;c&quot;, &quot;c--&gt;d&quot;, &quot;d--&gt;z&quot;) # Check it out! example ## [1] &quot;graph LR&quot; &quot;a[Tim]&quot; &quot;z[Coffee]&quot; &quot;subgraph Process&quot; ## [5] &quot;b[Froth Milk]&quot; &quot;c[Brew Coffee]&quot; &quot;d[Pour in Milk]&quot; &quot;end&quot; ## [9] &quot;a--&gt;b&quot; &quot;b--&gt;c&quot; &quot;c--&gt;d&quot; &quot;d--&gt;z&quot; If we paste() that example with collapse = \"\\n \", we can see itâ€™s all just one big value now. example %&gt;% paste(collapse = &quot;\\n &quot;) ## [1] &quot;graph LR\\n a[Tim]\\n z[Coffee]\\n subgraph Process\\n b[Froth Milk]\\n c[Brew Coffee]\\n d[Pour in Milk]\\n end\\n a--&gt;b\\n b--&gt;c\\n c--&gt;d\\n d--&gt;z&quot; And if we append mermaid() to that, we can get a really nice chart! example %&gt;% paste(collapse = &quot;\\n&quot;) %&gt;% mermaid() Whohoo! You made your first mermaid plot! Learning Check 1 Question Try making your own mermaid plot now, which tells the following tragedy, which I hope will not happen to you: You go to the store, buy ice cream, try to check your phone, drop your ice cream, go back to store, buy ice cream, check phone again, and drop ice cream again. (True story! A sad day.) Make sure to include at least 1 subgraph and 3 relationships. [View Answer!] # Here&#39;s one of several ways you could draw that! mystory &lt;- c(&quot;graph LR&quot;, &quot;a[Tim]&quot;, # Start a subgraph about what happens in the store &quot;subgraph Shopping&quot;, &quot;b[Store]&quot;, &quot;c[Ice Cream]&quot;, # end subgraph &quot;end&quot;, # Start subgraph about what happens when eating &quot;subgraph Eating&quot;, &quot;d[Checks Phone]&quot;, &quot;e[Drops Ice Cream]&quot;, # end subgraph &quot;end&quot;, # Add interactions # Tim goes to store &quot;a--&gt;b&quot;, # Buys ice cream at store &quot;b--&gt;c&quot;, # Checks phone &quot;c--&gt;d&quot;, # Drops Ice cream &quot;d--&gt;e&quot;, # Goes back to store &quot;e--&gt;b&quot;, # Buys ice cream again &quot;b--&gt;c&quot;, # Looks at phone &quot;c--&gt;d&quot;, # Drops ice cream again &quot;d--&gt;e&quot; ) And visualize it! mystory %&gt;% paste(collapse = &quot;\\n &quot;) %&gt;% mermaid() 30.2 Diagram of Ice Cream Shipment Letâ€™s try and visualize what the process of shipping out ice cream looks like once it has been made, using mermaid. To make this easier, we can do it piecemeal, a few lines at a time, saved into logically named vectors. First, letâ€™s get the starting settings of our graph! Weâ€™ll name it intro. # Get the basic graph, going left-to-right intro &lt;- &quot;graph LR&quot; Second, letâ€™s get a subgraph of people! Weâ€™ll call it subgraph_people. (Note: whenever we make multi-line chunks, gotta bind that stuff together using paste()!) subgraph_people &lt;- c( # Make a subgraph &quot;subgraph People&quot;, # Draw people in circles with () &quot;w1(Worker 1)&quot;, &quot;w2(Worker 2)&quot;, &quot;w3(Worker 3)&quot;, # Conclude the subgraph &quot;end&quot;) %&gt;% # remember to bind it together paste(collapse = &quot;\\n&quot;) Third, letâ€™s get a subgraph of events these people are involved in. Weâ€™ll call it subgraph_events. Note: mermaid understands some HTML, so to get breaks in the labels, weâ€™re going to use &lt;br&gt; below. subgraph_events &lt;- paste( # Make another subgraph &quot;subgraph Events&quot;, # Draw events as boxes with [] &quot;freezer[Freeze&lt;br&gt;Ice Cream]&quot;, &quot;loading[Load&lt;br&gt;onto Truck]&quot;, &quot;transport[Transport&lt;br&gt;to Store]&quot;, # Conclude the subgraph &quot;end&quot;, sep = &quot;\\n &quot;) Fourth, letâ€™s list out the ways our nodes are related, and weâ€™ll call it ties. # Draw main ties ties &lt;- paste( # Worker 1 puts ice cream in Freezer &quot;w1 --&gt; freezer&quot;, # Worker 2 loads ice cream into Truck &quot;w2 --&gt; loading&quot;, # Worker 3 transports ice cream to Store &quot;w3 --&gt; transport&quot;, # Also, Worker 2 takes the ice cream from the Freezer for loading &quot;freezer --&gt; w2&quot;, # And Worker 3 drives the ice cream from loading dock to Store &quot;loading --&gt; w3&quot;, sep = &quot;\\n &quot;) Alright! Letâ€™s visualize this chart! c(intro, # Add subgraphs of nodes subgraph_people, subgraph_events, # Add ties ties) %&gt;% # Paste them together paste(collapse = &quot;\\n &quot;) %&gt;% # And visualize it! mermaid() 30.3 Diagram Failures Now, we could imagine that there are several potential failure modes here. Letâ€™s add them to our drawing! First, weâ€™ll make a subgraph of failures aptly named subgraph_fail. subgraph_fail &lt;- c( # Make another subgraph &quot;subgraph Failures&quot;, &quot;fail_break[freezer breaks]&quot;, &quot;fail_time[left out too long]&quot;, &quot;fail_eat[worker eats it]&quot;, # Conclude subgraph &quot;end&quot;) %&gt;% # Bind it! paste(collapse = &quot;\\n &quot;) Second, weâ€™ll compile a list of relationships, or ways our nodes could lead to these failures. failures &lt;- c( # Worker 1 could leave ice cream out before freezing #&quot;w1 --&gt; fail_time&quot;, # Worker 1 could eat the ice cream before freezing #&quot;w1 --&gt; fail_eat&quot;, # Frezzer could break &quot;freezer --&gt; fail_break&quot;, # Worker 2 could leave ice cream out while loading &quot;loading --&gt; fail_time&quot;, # Worker 2 could eat the ice cream while loading &quot;loading --&gt; fail_eat&quot;, # Worker 3 could leave the ice cream out in transit &quot;transport --&gt; fail_time&quot;, # Worker 3 could eat the ice cream in transit &quot;transport --&gt; fail_eat&quot;) %&gt;% # bind it! paste(collapse = &quot;\\n &quot;) So what does our new diagram look like? c(&quot;graph LR&quot;, # Add subgraphs of nodes subgraph_people, subgraph_events, # Add ties ties, # Add subgraph of failure nodes subgraph_fail, # Add ways possible failures failures ) %&gt;% # Paste them together paste(collapse = &quot;\\n &quot;) %&gt;% # And visualize it! mermaid() Thereâ€™s so much you can do with mermaid, but for now, this will do. Hooray! Youâ€™re ready to use mermaid! "],["appendix-using-fitdistr-to-fitting-distribution-parameters.html", "31 Appendix: Using fitdistr to Fitting Distribution Parameters Getting Started 31.1 Example: Exponential Distribution 31.2 Method of Moments 31.3 MLE with fitdistr() 31.4 MLE with optim() 31.5 Applications Learning Check 1 31.6 Conclusion", " 31 Appendix: Using fitdistr to Fitting Distribution Parameters This tutorial will introduce you to **Fitting Distribution Parameters in R, teaching you how to use the fitdistr() function from the MASS package in R. This training continues on our previous work on Descriptive Statistics. Often, we might want to approximate statistics describing the shape of distributions, but there may not be a clear analytical method (eg. method of moments) to do so. We can use the power of optimization to help us instead, using a brute-force method to find the value most likely to be the statistic that actually fits our distribution. We can ask R to compute the values of those statistics using the MASS packageâ€™s fitdistr(). Getting Started Please open up your project on Posit.Cloud, for our Github class repository. Start a new R script (File &gt;&gt; New &gt;&gt; R Script). Save the R script as appendix_fitdistr.R. And letâ€™s get started! Load Packages # Load dplyr, which contains most data wrangling functions library(dplyr) # Load the fitdistr() function directly from the MASS package. # We&#39;ll do this, rather than loading the whole MASS package, because MASS will cancel out some dplyr functions that share the same names otherwise. See how fitdistr has now shown up in your environment as a function? fitdistr = MASS::fitdistr 31.0.1 Our Data As our raw data, letâ€™s use our vector of seawall heights sw. Its raw distribution can be visualized with hist(sw). # Let&#39;s remake again our vector of seawall heights sw &lt;- c(4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4) hist(sw) 31.1 Example: Exponential Distribution We have our vector of seawall heights sw. We have 2 main ways of calculating statistics that describe the distribution of sw. These include an analytical approach (method of moments) and a brute-force approach (maximum likelihood estimation). In the analytical approach (method of moments), which we learned in the main textbook, we use formula that have been derived by mathematicians to describe the parameters of a particular distribution. In the brute-force approach (maximum likelihood estimation), we use a secondary parameter called likelihood to find the parameter values most likely to fit your data. We say, if the parameter had value A (for example), whatâ€™s the joint probability (likelihood) of finding your observed values (sw) in a distribution with that trait A? We use maximum likelihood estimation to iteratively test various different values for parameter A, and choose the value that provides the highest likelihood - a.k.a. the maximum likelihood. Note: Remember: a parameter is a single number that describes a full population. A statistic is a single number that describes a sample. This key difference aside, the terms are largely interchangeable. Letâ€™s use the exponential distribution as a helpful example. It has one parameter - rate (also known as \\(\\lambda\\)), describing \\(\\frac{1}{ mean }\\). Letâ€™s calculate the rate parameter a few ways, using (1) the method of moments, (2) maximum likelihood estimation (MLE) using fitdistr(), and (3) maximum likelihood estimation (MLE) using optim(). 31.2 Method of Moments # Let&#39;s use the method of moments to find the inverse mean 1 / (sum(sw) / length(sw)) ## [1] 0.1869159 # Equivalent to... 1 / mean(sw) ## [1] 0.1869159 31.3 MLE with fitdistr() Letâ€™s ask fitdistr to run maximum likelihood estimation. Maximum likelihood estimation requires a benchmark distribution to compare against, so we need to specify the distribution type as densfun = [type name]. In this case, letâ€™s do exponential. (See a list of supported distributions using ?MASS::fitdistr) sw %&gt;% MASS::fitdistr(densfun = &quot;exponential&quot;) ## rate ## 0.18691589 ## (0.05910799) Pretty darn similar to the value we got from the method of moments, right? 31.4 MLE with optim() Alternatively, we could run maximum likelihood estimation manually, using optim(). optim() is Râ€™s built in optimization function. Weâ€™ll learn maximum likelihood estimation a little more later in the book. The key idea is this: dexp(x = 2, rate = 0.1) gives the probability of the value x = 2 showing up in an exponential distribution characterized by a parameter rate = 0.1. dexp(x = 2, rate = 0.1) ## [1] 0.08187308 dexp(x = sw, rate = 0.1) gives the probabilities for each value of x if they showed up in an exponential distribution characterized by a parameter rate = 0.1. # You can also pipe the values of x into dexp() like this sw %&gt;% dexp(rate = 0.1) ## [1] 0.06376282 0.06065307 0.05769498 0.06065307 0.05769498 0.05220458 ## [7] 0.05220458 0.05488116 0.06065307 0.06703200 The joint probability of these values of x occurring together is called the likelihood. We can take the product using prod(). # Let&#39;s get the likelihood of these values... sw %&gt;% dexp(rate = 0.1) %&gt;% prod() ## [1] 4.748151e-13 Likelihood tend to be very small numbers, so a helpful trick is to calculate the log-likelihood instead, meaning the sum of logged probabilities. # See how these two processes produce the same output? # Get the log of probabilities multiplied together... sw %&gt;% dexp(rate = 0.1) %&gt;% prod() %&gt;% log() ## [1] -28.37585 # Get the sum of logged probabilities... sw %&gt;% dexp(rate = 0.1) %&gt;% log() %&gt;% sum() ## [1] -28.37585 # They&#39;re equivalent Then, we write up a short function called loglikelihood(), including two inputs (1) par and (2) our data x. I added an example value 0.1 to par just as a reminder for what it means. loglikelihood = function(par = 0.1, x){ dexp(x = x, rate = par) %&gt;% log() %&gt;% sum() } # Try it! loglikelihood(par = 0.1, x = sw) ## [1] -28.37585 Finally, we run an optimizer using optim(), supplying a starting value for search par = 0.1, our raw data x, and our function loglikelihood. We want to maximize the loglikelihood, but optim() minimizes by default, so weâ€™ll say, control = list(fnscale = -1) to flip the scale. optim(par = c(0.1), x = sw, fn = loglikelihood, control = list(fnscale = -1)) ## $par ## [1] 0.1869531 ## ## $value ## [1] -26.77097 ## ## $counts ## function gradient ## 24 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL Compare the final parameter value against fitdistrâ€™s results! Theyâ€™re about the same. fitdistr(x = sw, densfun = &quot;exponential&quot;) ## rate ## 0.18691589 ## (0.05910799) Voila! You made your own maximum likelihood estimator manually. Certainly, optim() was a little more time consuming, but now you know how fitdistr truly works inside! 31.5 Applications Letâ€™s try applying the same general approach with fitdistr to other distributions. 31.5.1 Normal Distribution What parameter values would best describe our distributionâ€™s shape, if this data were from a normal distribution? Remember, normal distributions have a mean and a sd. # Method of Moments sw %&gt;% mean() ## [1] 5.35 sw %&gt;% sd() ## [1] 0.8181958 # Maximum Likelihood Estimation sw %&gt;% fitdistr(densfun = &quot;normal&quot;) ## mean sd ## 5.3500000 0.7762087 ## (0.2454588) (0.1735655) 31.5.2 Poisson Distribution What parameter values would best describe our distributionâ€™s shape, if this data were from a Poisson distribution? Remember, poisson distributions have a lambda parameter describing the mean. # Method of moments sw %&gt;% mean() ## [1] 5.35 # Maximum Likelihood Estimation sw %&gt;% fitdistr(densfun = &quot;poisson&quot;) ## lambda ## 5.3500000 ## (0.7314369) 31.5.3 Gamma Distribution What parameter values would best describe our distributionâ€™s shape, if this data were from a Gamma distribution? Remember, gamma distributions have a shape parameter ` \\(\\approx \\frac{mean^{2}}{ variance}\\) and a scale parameter \\(\\approx \\frac{variance}{ mean }\\). # Method of Moments # For shape, we want the rate of how much greater the mean-squared is than the variance. mean(sw)^2 / var(sw) ## [1] 42.7556 # For rate, we like to get the inverse of the variance divided by the mean. 1 / (var(sw) / mean(sw) ) ## [1] 7.991701 # Maximum Likelihood Estimation sw %&gt;% fitdistr(densfun = &quot;gamma&quot;) ## shape rate ## 46.711924 8.731202 ## (20.815522) ( 3.911666) 31.5.4 Weibull Distribution What parameter values would best describe our distributionâ€™s shape, if this data were from a Weibull distribution? Remember, gamma distributions have a shape parameter and a scale parameter. (But we canâ€™t easily use the method of moments here right now.) # Estimate the shape and scale parameters for a weibull distribution sw %&gt;% fitdistr(densfun = &quot;weibull&quot;) ## shape scale ## 7.7312532 5.6881102 ## (1.9038045) (0.2461222) Learning Check 1 Question Youâ€™ve been recruited to evaluate the frequency of Corgi sightings in the Ithaca Downtown. A sample of 10 students each reported the number of corgis they saw last Tuesday in town. Calculate the statistics summarizing each distribution, if it were a normal, poisson, exponential, gamma, or weibull distribution. Please use fitdistr for all your calculations. Beth saw 5, Javier saw 1, June saw 10(!), Tim saw 3, Melanie saw 4, Mohammad saw 3, Jenny say 6, Yosuke saw 4, Jimena saw 5, and David saw 2. [View Answer!] First, letâ€™s make the data. # Make distribution of Corgis corgi &lt;- c(5, 1, 10, 3, 4, 3, 6, 4, 5, 2) Next, letâ€™s compute the estimated statistics using maximum likelihood estimation. # Compute statistics for each distributions corgi %&gt;% fitdistr(densfun = &quot;normal&quot;) ## mean sd ## 4.3000000 2.3685439 ## (0.7489993) (0.5296225) corgi %&gt;% fitdistr(densfun = &quot;poisson&quot;) ## lambda ## 4.3000000 ## (0.6557439) corgi %&gt;% fitdistr(densfun = &quot;exponential&quot;) ## rate ## 0.23255814 ## (0.07354134) corgi %&gt;% fitdistr(densfun = &quot;gamma&quot;) ## shape rate ## 3.2628624 0.7588054 ## (1.3910572) (0.3497135) corgi %&gt;% fitdistr(densfun = &quot;weibull&quot;) ## shape scale ## 1.9207427 4.8617234 ## (0.4566410) (0.8459497) 31.6 Conclusion Congratulations! You now know how to use fitdistr() to approximate the parameters for a dataset, assuming various different types of distributions. You also learned maximum likelihood estimation, the core technique underneath fitdistr(), and how to perform it manually using optim(). Great work! "],["appendix-statistical-tables-and-common-equations.html", "32 Appendix: Statistical Tables and Common Equations 32.1 Table 1: Chi-Squared 32.2 Table 2: k-factor rules 32.3 Table 3: k-factor (upper) 32.4 Table 4: k-factor (lower) 32.5 Table 5: Control Constants (d) 32.6 Table 6: Control Constants (b) 32.7 Formula", " 32 Appendix: Statistical Tables and Common Equations 32.1 Table 1: Chi-Squared Table 1: Chi2 Statistics Chi2 = qchisq(p = ci, df) confidence interval ci = 1 - alpha level df 0.5% 2.5% 80% 90% 95% 97.5% 98% 99% 99.5% 99.8% 99.9% 1 0.0000392 0.0009820 1.64 2.70 3.84 5.02 5.41 6.63 7.87 9.54 10.82 2 0.010 0.050 3.21 4.60 5.99 7.37 7.82 9.21 10.59 12.42 13.81 3 0.071 0.215 4.64 6.25 7.81 9.34 9.83 11.34 12.83 14.79 16.26 4 0.20 0.48 5.98 7.77 9.48 11.14 11.66 13.27 14.86 16.92 18.46 5 0.41 0.83 7.28 9.23 11.07 12.83 13.38 15.08 16.74 18.90 20.51 6 0.67 1.23 8.55 10.64 12.59 14.44 15.03 16.81 18.54 20.79 22.45 7 0.98 1.68 9.80 12.01 14.06 16.01 16.62 18.47 20.27 22.60 24.32 8 1.34 2.17 11.03 13.36 15.50 17.53 18.16 20.09 21.95 24.35 26.12 9 1.73 2.70 12.24 14.68 16.91 19.02 19.67 21.66 23.58 26.05 27.87 10 2.15 3.24 13.44 15.98 18.30 20.48 21.16 23.20 25.18 27.72 29.58 11 2.60 3.81 14.63 17.27 19.67 21.92 22.61 24.72 26.75 29.35 31.26 12 3.07 4.40 15.81 18.54 21.02 23.33 24.05 26.21 28.29 30.95 32.90 13 3.56 5.00 16.98 19.81 22.36 24.73 25.47 27.68 29.81 32.53 34.52 14 4.07 5.62 18.15 21.06 23.68 26.11 26.87 29.14 31.31 34.09 36.12 15 4.60 6.26 19.31 22.30 24.99 27.48 28.25 30.57 32.80 35.62 37.69 16 5.14 6.90 20.46 23.54 26.29 28.84 29.63 31.99 34.26 37.14 39.25 17 5.69 7.56 21.61 24.76 27.58 30.19 30.99 33.40 35.71 38.64 40.79 18 6.26 8.23 22.75 25.98 28.86 31.52 32.34 34.80 37.15 40.13 42.31 19 6.84 8.90 23.90 27.20 30.14 32.85 33.68 36.19 38.58 41.61 43.82 20 7.43 9.59 25.03 28.41 31.41 34.16 35.01 37.56 39.99 43.07 45.31 21 8.03 10.28 26.17 29.61 32.67 35.47 36.34 38.93 41.40 44.52 46.79 32.2 Table 2: k-factor rules Table 32.1: Table 2: Rules for Finding k-factors Data Censor Bounds Rule Code Steps Complete None both \\(k_{ci,r} = \\chi^{2}_{ci, 2r} \\times \\frac{1}{2r}\\) qchisq(ci, df = 2r) / (2r) Lookup r in Table 3 or 4. Type I Time lower \\(k_{ci,r} = \\chi^{2}_{ci, 2r} \\times \\frac{1}{2r}\\) qchisq(ci, df = 2r) / (2r) Lookup r in Table 4. Type I Time upper \\(k_{ci,r} = \\chi^{2}_{ci, 2(r + 1)} \\times \\frac{1}{2r}\\) qchisq(ci, df = 2(r + 1)) / (2r) Lookup r + 1 in Table 3. Type II Failures lower \\(k_{ci,r} = \\chi^{2}_{ci, 2r} \\times \\frac{1}{2r}\\) qchisq(ci, df = 2r) / (2r) Lookup r in Table 4. Type II Failures upper \\(k_{ci,r} = \\chi^{2}_{ci, 2((r-1) + 1)} \\times \\frac{1}{2(r - 1)} \\times \\frac{(r - 1)}{r}\\) qchisq(ci, df = 2 * ( (r - 1) + 1)) / (2 * (r - 1) ) * (r - 1) / r Lookup r - 1 in Table 3. Multiply k by \\(\\frac{(r-1)}{r}\\). 32.3 Table 3: k-factor (upper) Table 3: k-factors for One-Sided Exponential Upper Bound Time/Type I Censoring; If Type II Censoring, be sure to adjust. k-upper = qchisq(ci, df = 2(r+1)) / (2r) r fails confidence interval (ci = 1 - alpha level) r 60% 80% 90% 95% 97.5% 99% 99.9% 1 2.02 2.99 3.89 4.74 5.57 6.64 9.23 2 1.55 2.14 2.66 3.15 3.61 4.20 5.61 3 1.39 1.84 2.23 2.58 2.92 3.35 4.35 4 1.31 1.68 2.00 2.29 2.56 2.90 3.70 5 1.26 1.58 1.85 2.10 2.33 2.62 3.29 6 1.22 1.51 1.76 1.97 2.18 2.43 3.01 7 1.20 1.46 1.68 1.88 2.06 2.29 2.80 8 1.18 1.42 1.62 1.80 1.97 2.18 2.64 9 1.16 1.39 1.58 1.75 1.90 2.09 2.52 10 1.15 1.37 1.54 1.70 1.84 2.01 2.41 11 1.14 1.34 1.51 1.66 1.79 1.95 2.33 12 1.13 1.32 1.48 1.62 1.75 1.90 2.25 13 1.12 1.31 1.46 1.59 1.71 1.86 2.19 14 1.12 1.29 1.44 1.56 1.68 1.82 2.13 15 1.11 1.28 1.42 1.54 1.65 1.78 2.08 16 1.11 1.27 1.40 1.52 1.62 1.75 2.04 17 1.10 1.26 1.39 1.50 1.60 1.72 2.00 18 1.10 1.25 1.38 1.48 1.58 1.70 1.96 19 1.10 1.24 1.36 1.47 1.56 1.68 1.93 20 1.09 1.24 1.35 1.45 1.54 1.66 1.90 21 1.09 1.23 1.34 1.44 1.53 1.64 1.87 22 1.09 1.22 1.33 1.43 1.51 1.62 1.85 23 1.08 1.22 1.32 1.42 1.50 1.60 1.83 24 1.08 1.21 1.32 1.41 1.49 1.59 1.81 25 1.08 1.21 1.31 1.40 1.48 1.57 1.79 26 1.08 1.20 1.30 1.39 1.47 1.56 1.77 27 1.07 1.20 1.29 1.38 1.45 1.55 1.75 28 1.07 1.19 1.29 1.37 1.45 1.53 1.73 29 1.07 1.19 1.28 1.36 1.44 1.52 1.72 30 1.07 1.19 1.28 1.36 1.43 1.51 1.70 31 1.07 1.18 1.27 1.35 1.42 1.50 1.69 32 1.07 1.18 1.27 1.34 1.41 1.49 1.68 33 1.07 1.18 1.26 1.34 1.40 1.49 1.66 34 1.06 1.17 1.26 1.33 1.40 1.48 1.65 35 1.06 1.17 1.25 1.33 1.39 1.47 1.64 36 1.06 1.17 1.25 1.32 1.38 1.46 1.63 37 1.06 1.16 1.25 1.32 1.38 1.45 1.62 38 1.06 1.16 1.24 1.31 1.37 1.45 1.61 39 1.06 1.16 1.24 1.31 1.37 1.44 1.60 40 1.06 1.16 1.23 1.30 1.36 1.43 1.59 41 1.06 1.15 1.23 1.30 1.36 1.43 1.58 32.4 Table 4: k-factor (lower) fix = function(x, n){x %&gt;% format(scientific = FALSE) %&gt;% str_extract(paste(&quot;.[0-9]+[.][0-9]{&quot;, n, &quot;}&quot;, sep = &quot;&quot;))} table_k_lower = tidyr::expand_grid( r = c(1:41), alpha = c(0.60, .80, 0.90, .95, .975, .99, .999)) %&gt;% mutate( # Calculate chi-squared for each percentile and df k = qchisq(1 - alpha, df = (2*r)) / (2*r)) %&gt;% mutate(k = k %&gt;% round(2)) %&gt;% mutate(ci = paste((1 - alpha) * 100, &quot;%&quot;, sep = &quot;&quot;)) %&gt;% tidyr::pivot_wider(id_cols = c(r), names_from = ci, values_from = k) table_k_lower %&gt;% write_csv(&quot;workshops/k_lower.csv&quot;) k3 = table_k_lower %&gt;% kable(booktabs = TRUE, format = &quot;html&quot;, align = &quot;c&quot;, escape = TRUE) %&gt;% add_header_above( bold = TRUE, escape = FALSE, header = c(&quot;r fails&quot; = 1, &quot;confidence interval (ci = 1 - alpha level)&quot; = 7)) %&gt;% add_header_above(italic = TRUE, header = c(&quot; &quot; = 1, &quot;k-upper = qchisq(ci, df = 2*r) / (2*r)&quot; = 7)) %&gt;% add_header_above(bold = TRUE, header = c(&quot; &quot; = 1, &quot;Type I &amp; II Censoring, No Censoring&quot; = 7)) %&gt;% add_header_above(bold = TRUE, header = c(&quot;Table 4: k-factors for One-Sided Exponential Lower Bound&quot; = 8)) %&gt;% column_spec(1,border_right = TRUE) %&gt;% kable_styling(font_size = 12, bootstrap_options = c(&quot;striped&quot;, &quot;responsive&quot;),full_width = TRUE) # View table k3 Table 4: k-factors for One-Sided Exponential Lower Bound Type I &amp; II Censoring, No Censoring k-upper = qchisq(ci, df = 2r) / (2r) r fails confidence interval (ci = 1 - alpha level) r 40% 20% 10% 5% 2.5% 1% 0.1% 1 0.51 0.22 0.11 0.05 0.03 0.01 0.00 2 0.69 0.41 0.27 0.18 0.12 0.07 0.02 3 0.76 0.51 0.37 0.27 0.21 0.15 0.06 4 0.80 0.57 0.44 0.34 0.27 0.21 0.11 5 0.83 0.62 0.49 0.39 0.32 0.26 0.15 6 0.85 0.65 0.53 0.44 0.37 0.30 0.18 7 0.86 0.68 0.56 0.47 0.40 0.33 0.22 8 0.87 0.70 0.58 0.50 0.43 0.36 0.25 9 0.88 0.71 0.60 0.52 0.46 0.39 0.27 10 0.89 0.73 0.62 0.54 0.48 0.41 0.30 11 0.90 0.74 0.64 0.56 0.50 0.43 0.32 12 0.90 0.75 0.65 0.58 0.52 0.45 0.34 13 0.91 0.76 0.67 0.59 0.53 0.47 0.35 14 0.91 0.77 0.68 0.60 0.55 0.48 0.37 15 0.91 0.78 0.69 0.62 0.56 0.50 0.39 16 0.92 0.79 0.70 0.63 0.57 0.51 0.40 17 0.92 0.79 0.70 0.64 0.58 0.52 0.41 18 0.92 0.80 0.71 0.65 0.59 0.53 0.43 19 0.93 0.80 0.72 0.65 0.60 0.54 0.44 20 0.93 0.81 0.73 0.66 0.61 0.55 0.45 21 0.93 0.81 0.73 0.67 0.62 0.56 0.46 22 0.93 0.82 0.74 0.68 0.63 0.57 0.47 23 0.93 0.82 0.74 0.68 0.63 0.58 0.48 24 0.94 0.83 0.75 0.69 0.64 0.59 0.49 25 0.94 0.83 0.75 0.70 0.65 0.59 0.49 26 0.94 0.83 0.76 0.70 0.65 0.60 0.50 27 0.94 0.84 0.76 0.71 0.66 0.61 0.51 28 0.94 0.84 0.77 0.71 0.66 0.61 0.52 29 0.94 0.84 0.77 0.72 0.67 0.62 0.52 30 0.94 0.84 0.77 0.72 0.67 0.62 0.53 31 0.94 0.85 0.78 0.72 0.68 0.63 0.54 32 0.95 0.85 0.78 0.73 0.68 0.64 0.54 33 0.95 0.85 0.78 0.73 0.69 0.64 0.55 34 0.95 0.85 0.79 0.74 0.69 0.64 0.55 35 0.95 0.86 0.79 0.74 0.70 0.65 0.56 36 0.95 0.86 0.79 0.74 0.70 0.65 0.56 37 0.95 0.86 0.80 0.75 0.70 0.66 0.57 38 0.95 0.86 0.80 0.75 0.71 0.66 0.57 39 0.95 0.86 0.80 0.75 0.71 0.67 0.58 40 0.95 0.87 0.80 0.75 0.71 0.67 0.58 41 0.95 0.87 0.81 0.76 0.72 0.67 0.59 32.5 Table 5: Control Constants (d) d-factor control constants Subgroup Size Control Constants n d2 d3 D3 D4 2 1.116 0.850 0.000 3.285 3 1.691 0.890 0.000 2.579 4 2.053 0.869 0.000 2.271 5 2.328 0.869 0.000 2.120 6 2.531 0.851 0.000 2.009 7 2.712 0.837 0.074 1.926 8 2.837 0.815 0.138 1.862 9 2.979 0.819 0.175 1.825 10 3.068 0.795 0.223 1.777 11 3.190 0.792 0.256 1.744 12 3.258 0.768 0.293 1.707 13 3.340 0.771 0.308 1.692 14 3.409 0.765 0.326 1.674 15 3.475 0.757 0.346 1.654 16 3.536 0.750 0.364 1.636 17 3.584 0.739 0.381 1.619 18 3.637 0.733 0.395 1.605 19 3.667 0.726 0.406 1.594 20 3.728 0.732 0.411 1.589 21 3.777 0.716 0.431 1.569 22 3.807 0.704 0.445 1.555 23 3.853 0.714 0.444 1.556 24 3.889 0.716 0.448 1.552 25 3.933 0.714 0.455 1.545 26 3.966 0.700 0.470 1.530 27 3.987 0.697 0.475 1.525 28 4.027 0.703 0.476 1.524 29 4.051 0.690 0.489 1.511 30 4.079 0.694 0.489 1.511 31 4.109 0.690 0.496 1.504 32 4.138 0.694 0.497 1.503 33 4.155 0.678 0.511 1.489 34 4.188 0.671 0.519 1.481 35 4.211 0.675 0.519 1.481 36 4.242 0.673 0.524 1.476 37 4.260 0.669 0.529 1.471 38 4.278 0.677 0.526 1.474 39 4.301 0.678 0.527 1.473 40 4.309 0.670 0.534 1.466 41 4.346 0.674 0.535 1.465 42 4.355 0.672 0.537 1.463 43 4.374 0.660 0.547 1.453 44 4.395 0.663 0.548 1.452 45 4.418 0.657 0.554 1.446 46 4.420 0.655 0.555 1.445 47 4.453 0.663 0.554 1.446 48 4.473 0.669 0.551 1.449 49 4.491 0.650 0.566 1.434 50 4.513 0.651 0.567 1.433 32.6 Table 6: Control Constants (b) b-factor control constants Subgroup Size Control Constants n b2 b3 C4 A3 B3 B4 2 0.796 0.601 0.796 2.665 0.000 3.267 3 0.882 0.459 0.882 1.963 0.000 2.561 4 0.918 0.388 0.918 1.633 0.000 2.269 5 0.935 0.335 0.935 1.436 0.000 2.074 6 0.949 0.304 0.949 1.291 0.039 1.961 7 0.954 0.283 0.954 1.189 0.109 1.891 8 0.968 0.264 0.968 1.095 0.183 1.817 9 0.968 0.247 0.968 1.033 0.235 1.765 10 0.975 0.233 0.975 0.973 0.284 1.716 11 0.978 0.222 0.978 0.925 0.318 1.682 12 0.980 0.212 0.980 0.884 0.350 1.650 13 0.979 0.200 0.979 0.850 0.386 1.614 14 0.982 0.193 0.982 0.817 0.409 1.591 15 0.984 0.183 0.984 0.787 0.441 1.559 16 0.985 0.180 0.985 0.761 0.452 1.548 17 0.980 0.175 0.980 0.742 0.466 1.534 18 0.986 0.173 0.986 0.717 0.474 1.526 19 0.986 0.166 0.986 0.698 0.496 1.504 20 0.985 0.161 0.985 0.681 0.511 1.489 21 0.987 0.158 0.987 0.663 0.521 1.479 22 0.988 0.152 0.988 0.647 0.538 1.462 23 0.986 0.149 0.986 0.634 0.547 1.453 24 0.991 0.147 0.991 0.618 0.556 1.444 25 0.987 0.145 0.987 0.608 0.560 1.440 26 0.990 0.140 0.990 0.595 0.576 1.424 27 0.992 0.136 0.992 0.582 0.589 1.411 28 0.990 0.135 0.990 0.572 0.591 1.409 29 0.989 0.131 0.989 0.563 0.602 1.398 30 0.992 0.132 0.992 0.552 0.601 1.399 31 0.992 0.129 0.992 0.543 0.611 1.389 32 0.992 0.128 0.992 0.535 0.613 1.387 33 0.993 0.126 0.993 0.526 0.620 1.380 34 0.992 0.124 0.992 0.518 0.625 1.375 35 0.993 0.120 0.993 0.511 0.637 1.363 36 0.992 0.118 0.992 0.504 0.643 1.357 37 0.996 0.117 0.996 0.495 0.647 1.353 38 0.991 0.116 0.991 0.491 0.648 1.352 39 0.993 0.114 0.993 0.484 0.654 1.346 40 0.994 0.113 0.994 0.477 0.659 1.341 41 0.994 0.112 0.994 0.471 0.661 1.339 42 0.995 0.109 0.995 0.465 0.671 1.329 43 0.994 0.110 0.994 0.460 0.669 1.331 44 0.997 0.108 0.997 0.454 0.676 1.324 45 0.996 0.107 0.996 0.449 0.679 1.321 46 0.995 0.105 0.995 0.445 0.684 1.316 47 0.996 0.103 0.996 0.440 0.691 1.309 48 0.993 0.102 0.993 0.436 0.692 1.308 49 0.996 0.101 0.996 0.430 0.696 1.304 50 0.992 0.100 0.992 0.427 0.697 1.303 32.7 Formula c1 = box(title = &quot;Exponential Distribution&quot;, span(&quot;$$ f(t) = \\\\lambda e^{-\\\\lambda t } $$&quot;), span(&quot;$$ F(t) = 1 - e^{-\\\\lambda t } $$&quot;), span(&quot;$$ MTTF = \\\\frac{1}{\\\\lambda} $$&quot;)) c2 = box(title = &quot;Weibull Distribution&quot;, span(&quot;$$ f(t) = \\\\frac{m}{c} (\\\\frac{t}{c})^{m-1} \\\\times e^{-(t / c)^m } $$&quot;), span(&quot;$$ F(t) = 1 - e^{-(t/c)^m} $$&quot;) ) c3 = box( title = &quot;Estimating Lambda&quot;, span(&quot;$$ \\\\hat{\\\\lambda} = \\\\frac{r}{\\\\sum_{i=1}^{r}{ t_i } + (n - r) \\\\times T_{max} } $$&quot;), span(&quot;$$ Type \\\\ I: \\\\ T_{max} = \\\\ end \\\\ of \\\\ study \\\\ period \\\\\\\\ Type \\\\ II: \\\\ T_{max} = \\\\ time \\\\ of \\\\ last \\\\ failure $$&quot;) ) c4 = box(title = &quot;Maximum Failure Rate&quot;, span(&quot;$$ \\\\ when \\\\ r \\\\geq 1 \\\\ failures \\\\\\\\ \\\\lambda_{(1 - \\\\alpha)} = \\\\hat{\\\\lambda} \\\\times k_{ \\\\ r, (1-\\\\alpha)} $$&quot;), span(&quot;$$ when \\\\ r = 0 \\\\ failures \\\\\\\\ \\\\lambda_{(1 - \\\\alpha)} = \\\\frac{-ln(\\\\alpha ) }{n \\\\times T} $$&quot;)) c5 = box(title = &quot;Chi-squared Statistic&quot;, span(&quot;$$ \\\\chi^{2} = \\\\sum{ \\\\frac{(observed - expected)^{2} }{ expected }} $$ &quot;), span(&quot;$$ Degrees \\\\ of \\\\ Freedom \\\\ (df) \\\\ for \\\\ \\\\chi^{2} \\\\\\\\ df = n_{intervals} - 1 - n_{parameters} $$&quot;) ) c6 = box(title = &quot;Process Indices&quot;, span(&quot;$$ C_{p} = \\\\frac{ E_{upper} - E_{lower} }{6 \\\\sigma_{short} } \\\\\\\\ C_{pk} = \\\\frac{ | E_{limit} - \\\\mu | }{3 \\\\sigma_{short} } $$&quot;), span(&quot;$$ P_{p} = \\\\frac{ E_{upper} - E_{lower} }{6 \\\\sigma_{total} } \\\\\\\\ P_{pk} = \\\\frac{ | E_{limit} - \\\\mu | }{3 \\\\sigma_{total} } $$&quot;), ) bslib::layout_column_wrap( c1, c2, c3, c4, c5, c6, width = 0.5, fill = TRUE ) Exponential Distribution $$ f(t) = \\lambda e^{-\\lambda t } $$ $$ F(t) = 1 - e^{-\\lambda t } $$ $$ MTTF = \\frac{1}{\\lambda} $$ Weibull Distribution $$ f(t) = \\frac{m}{c} (\\frac{t}{c})^{m-1} \\times e^{-(t / c)^m } $$ $$ F(t) = 1 - e^{-(t/c)^m} $$ Estimating Lambda $$ \\hat{\\lambda} = \\frac{r}{\\sum_{i=1}^{r}{ t_i } + (n - r) \\times T_{max} } $$ $$ Type \\ I: \\ T_{max} = \\ end \\ of \\ study \\ period \\\\ Type \\ II: \\ T_{max} = \\ time \\ of \\ last \\ failure $$ Maximum Failure Rate $$ \\ when \\ r \\geq 1 \\ failures \\\\ \\lambda_{(1 - \\alpha)} = \\hat{\\lambda} \\times k_{ \\ r, (1-\\alpha)} $$ $$ when \\ r = 0 \\ failures \\\\ \\lambda_{(1 - \\alpha)} = \\frac{-ln(\\alpha ) }{n \\times T} $$ Chi-squared Statistic $$ \\chi^{2} = \\sum{ \\frac{(observed - expected)^{2} }{ expected }} $$ $$ Degrees \\ of \\ Freedom \\ (df) \\ for \\ \\chi^{2} \\\\ df = n_{intervals} - 1 - n_{parameters} $$ Process Indices $$ C_{p} = \\frac{ E_{upper} - E_{lower} }{6 \\sigma_{short} } \\\\ C_{pk} = \\frac{ | E_{limit} - \\mu | }{3 \\sigma_{short} } $$ $$ P_{p} = \\frac{ E_{upper} - E_{lower} }{6 \\sigma_{total} } \\\\ P_{pk} = \\frac{ | E_{limit} - \\mu | }{3 \\sigma_{total} } $$ "]]
