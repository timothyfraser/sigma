[["index.html", "System Reliability and Six Sigma in R Introduction", " System Reliability and Six Sigma in R Timothy Fraser, PhD 2025-09-15 Introduction Your online textbook for learning reliability and six sigma techniques in R! These coding workshops were made for Cornell University Course SYSEN 5300. Follow along with Posit.Cloud to learn to apply six sigma techniques in R! Figure 0.1: Photo by Naser Tamimi on Unsplash "],["indices-and-confidence-intervals-for-spc-in-python.html", "1 Indices and Confidence Intervals for SPC in Python Getting Started 1.1 Process Capability vs. Stability 1.2 Confidence Intervals (Normal Approximation) 1.3 Bootstrapping Cp (Example) Conclusion", " 1 Indices and Confidence Intervals for SPC in Python Figure 1.1: Bootstrapping Sampling Distributions for Statistics!! This workshop extends our toolkit developed earlier, discussing Process Capability and Stability Indices, and introducing means to calculate confidence intervals for these indices using Python. We’ll calculate Cp, Cpk, Pp, and Ppk, then construct normal-approximation confidence intervals and a simple bootstrap, paralleling the R chapter. Getting Started Packages We’ll be using pandas for data manipulation, plotnine for visualization, scipy for statistical functions, and custom functions from the functions_distributions module. # Remember to install these packages using a terminal, if you haven&#39;t already! !pip install pandas plotnine scipy # Load our packages import pandas as pd from plotnine import * import numpy as np from scipy import stats Custom Functions This workshop uses custom functions from the functions_distributions.py module. To use these functions, you need to acquire them from the repository at github.com/timothyfraser/sigma/tree/main/functions. Add the functions directory to your Python path import sys import os # Add the functions directory to Python path sys.path.append(&#39;functions&#39;) # or path to wherever you placed the functions folder Once you have the functions available, you can import them: from functions_distributions import * Our Data We’ll be continuing to analyze our quality control data from a local hot springs inn (onsen) in sunny Kagoshima Prefecture, Japan. Every month, for 15 months, you systematically took 20 random samples of hot spring water and recorded its temperature, pH, and sulfur levels. How might you determine if this onsen is at risk of slipping out of one sector of the market (eg. Extra Hot!) and into another (just normal Hot Springs?). Let’s read in our data from workshops/onsen.csv! # Let&#39;s import our samples of bathwater over time! water = pd.read_csv(&#39;workshops/onsen.csv&#39;) # Take a peek! print(water.head(3)) ## id time temp ph sulfur ## 0 1 1 43.2 5.1 0.0 ## 1 2 1 45.3 4.8 0.4 ## 2 3 1 45.5 6.2 0.9 print(f&quot;Dataset shape: {water.shape}&quot;) ## Dataset shape: (160, 5) print(f&quot;Columns: {list(water.columns)}&quot;) ## Columns: [&#39;id&#39;, &#39;time&#39;, &#39;temp&#39;, &#39;ph&#39;, &#39;sulfur&#39;] Our dataset contains: id: unique identifier for each sample of onsen water. time: categorical variable describing date of sample (month 1, month 3, … month 15). temp: temperature of the hot spring water in Celsius. ph: pH level of the hot spring water. sulfur: sulfur content in mg per kg of water. 1.0.1 Process Overview Let’s get a visual overview of our temperature data across time to understand the process: # Create a process overview plot g = (ggplot(water, aes(x=&#39;time&#39;, y=&#39;temp&#39;)) + geom_point(alpha=0.6, size=2) + geom_hline(yintercept=[42, 50], color=[&#39;red&#39;, &#39;red&#39;], linetype=&#39;dashed&#39;) + theme_classic(base_size=14) + labs(x=&#39;Time (Month)&#39;, y=&#39;Temperature (°C)&#39;, title=&#39;Hot Spring Temperature Over Time&#39;, subtitle=&#39;Red dashed lines show specification limits (42-50°C)&#39;) + theme(plot_title=element_text(hjust=0.5), plot_subtitle=element_text(hjust=0.5))) # Save the plot g.save(&#39;images/06_process_overview.png&#39;, width=10, height=6, dpi=100) This plot shows our temperature measurements over time, with the specification limits for “Extra Hot Springs” (42-50°C) marked as red dashed lines. We can see the process variation and how it relates to our target specifications. 1.1 Process Capability vs. Stability 1.1.1 Definitions Production processes can be categorized in terms of Capability (does it meet required specifications?) and Stability (is production consistent and predictable?) Both are vital. A capable process delivers goods that can actually perform their function, like a 20-foot ladder that is actually 20 feet! A stable process delivers products with consistent and predictable traits (regardless of whether those traits are good). We need to maximize both process capability and stability to make an effective process, be it in manufacturing, health care, local businesses, or social life! Depending on the shape and stability of our data, we should choose one of the 4 statistics to evaluate our data. 1.1.2 Table of Indices These statistics rely on some combination of (1) the mean \\(\\mu\\), (2) the standard deviation \\(\\sigma\\), and (3) the upper and lower “specification limits”; the specification limits are our expected values \\(E_{upper}\\) and \\(E_{lower}\\), as compared to our actual observed values, summarized by \\(\\mu\\) and \\(\\sigma\\). Index Shape Sided Stability Formula Meaning \\(C_{p}\\) Centered 2-sided Stable \\(\\frac{E_{upper} - E_{lower}}{6 \\sigma_{short}}\\) How many times wider is the expected range than the observed range, assuming it is stable? \\(C_{pk}\\) Uncentered 1-sided Stable \\(\\frac{\\mid E_{limit} - \\mu \\mid}{3 \\sigma_{short}}\\) How many times wider is the expected vs. observed range for the left/right side, assuming it is stable? \\(P_{p}\\) Centered 2-sided Unstable \\(\\frac{E_{upper} - E_{lower}}{6 \\sigma_{total}}\\) How many times wider is the expected vs. observed range, stable or not? \\(P_{pk}\\) Uncentered 1-sided Unstable \\(\\frac{\\mid E_{limit} - \\mu \\mid}{3 \\sigma_{total}}\\) How many times wider is the expected vs. observed range for the left/right side, stable or not? Capability Indices (How could it perform, if stable?): \\(C_p\\), \\(C_{pk}\\) Process Performance Indices (How is it performing, stable or not?): \\(P_p\\), \\(P_{pk}\\) 1.1.3 Index Functions Let’s do ourselves a favor and write up some simple functions for these. def cp(sigma_s, upper, lower): &quot;&quot;&quot;Capability index for centered, stable processes&quot;&quot;&quot; return abs(upper - lower) / (6*sigma_s) def pp(sigma_t, upper, lower): &quot;&quot;&quot;Performance index for centered, unstable processes&quot;&quot;&quot; return abs(upper - lower) / (6*sigma_t) def cpk(mu, sigma_s, lower=None, upper=None): &quot;&quot;&quot;Capability index for uncentered, stable processes&quot;&quot;&quot; a = None; b = None if lower is not None: a = abs(mu - lower) / (3*sigma_s) if upper is not None: b = abs(upper - mu) / (3*sigma_s) if (lower is not None) and (upper is not None): return min(a,b) return a if upper is None else b def ppk(mu, sigma_t, lower=None, upper=None): &quot;&quot;&quot;Performance index for uncentered, unstable processes&quot;&quot;&quot; a = None; b = None if lower is not None: a = abs(mu - lower) / (3*sigma_t) if upper is not None: b = abs(upper - mu) / (3*sigma_t) if (lower is not None) and (upper is not None): return min(a,b) return a if upper is None else b 1.1.4 Ingredients Now we need to calculate the key statistics that feed into our capability indices. We’ll calculate both within-subgroup statistics (for capability indices) and total statistics (for performance indices). stat_s = (water.groupby(&#39;time&#39;).apply(lambda d: pd.Series({ &#39;xbar&#39;: d[&#39;temp&#39;].mean(), &#39;s&#39;: d[&#39;temp&#39;].std(), &#39;n_w&#39;: len(d) })).reset_index()) stat = pd.DataFrame({ &#39;xbbar&#39;: [stat_s[&#39;xbar&#39;].mean()], &#39;sigma_s&#39;: [(stat_s[&#39;s&#39;]**2).mean()**0.5], &#39;sigma_t&#39;: [water[&#39;temp&#39;].std()], &#39;n&#39;: [stat_s[&#39;n_w&#39;].sum()], &#39;n_w&#39;: [stat_s[&#39;n_w&#39;].iloc[0]], &#39;k&#39;: [len(stat_s)] }) stat ## xbbar sigma_s sigma_t n n_w k ## 0 44.85 1.986174 1.989501 160.0 20.0 8 These statistics give us: xbbar: The grand mean across all subgroups sigma_s: The pooled within-subgroup standard deviation (for capability indices) sigma_t: The total process standard deviation (for performance indices) n: Total number of observations n_w: Number of observations per subgroup k: Number of subgroups 1.1.5 Cp and Pp Now let’s calculate the capability and performance indices. We’ll use specification limits for “Extra Hot Springs” (42-50°C) as our target range. limit_lower = 42; limit_upper = 50 estimate_cp = cp(stat[&#39;sigma_s&#39;][0], upper=limit_upper, lower=limit_lower) estimate_pp = pp(stat[&#39;sigma_t&#39;][0], upper=limit_upper, lower=limit_lower) print(f&quot;Cp (capability): {estimate_cp:.3f}&quot;) ## Cp (capability): 0.671 print(f&quot;Pp (performance): {estimate_pp:.3f}&quot;) ## Pp (performance): 0.670 1.1.6 Cpk and Ppk Now let’s calculate the uncentered indices that account for process centering: estimate_cpk = cpk(mu=stat[&#39;xbbar&#39;][0], sigma_s=stat[&#39;sigma_s&#39;][0], lower=limit_lower, upper=limit_upper) estimate_ppk = ppk(mu=stat[&#39;xbbar&#39;][0], sigma_t=stat[&#39;sigma_t&#39;][0], lower=limit_lower, upper=limit_upper) print(f&quot;Cpk (capability, uncentered): {estimate_cpk:.3f}&quot;) ## Cpk (capability, uncentered): 0.478 print(f&quot;Ppk (performance, uncentered): {estimate_ppk:.3f}&quot;) ## Ppk (performance, uncentered): 0.478 1.1.7 Equality There’s an interesting mathematical relationship between these indices: equality_check = (estimate_pp * estimate_cpk) == (estimate_ppk * estimate_cp) print(f&quot;Pp × Cpk = Ppk × Cp: {equality_check}&quot;) ## Pp × Cpk = Ppk × Cp: True print(f&quot;Pp × Cpk = {estimate_pp * estimate_cpk:.6f}&quot;) ## Pp × Cpk = 0.320554 print(f&quot;Ppk × Cp = {estimate_ppk * estimate_cp:.6f}&quot;) ## Ppk × Cp = 0.320554 1.2 Confidence Intervals (Normal Approximation) Now let’s construct confidence intervals for our capability indices using the normal approximation method. This gives us a range of plausible values for the true population parameters. 1.2.1 Cp Confidence Interval import math v_short = stat[&#39;k&#39;][0]*(stat[&#39;n_w&#39;][0] - 1) se_cp = estimate_cp * math.sqrt(1 / (2*v_short)) z = 1.959963984540054 # 95% confidence level ci_cp = (estimate_cp - z*se_cp, estimate_cp + z*se_cp) print(f&quot;Cp estimate: {estimate_cp:.3f}&quot;) ## Cp estimate: 0.671 print(f&quot;Standard error: {se_cp:.3f}&quot;) ## Standard error: 0.039 print(f&quot;95% Confidence interval: ({ci_cp[0]:.3f}, {ci_cp[1]:.3f})&quot;) ## 95% Confidence interval: (0.596, 0.747) 1.2.2 Cpk Confidence Interval se_cpk = estimate_cpk * math.sqrt(1 / (2*v_short) + 1 / (9*stat[&#39;n&#39;][0]*(estimate_cpk**2))) ci_cpk = (estimate_cpk - z*se_cpk, estimate_cpk + z*se_cpk) print(f&quot;Cpk estimate: {estimate_cpk:.3f}&quot;) ## Cpk estimate: 0.478 print(f&quot;Standard error: {se_cpk:.3f}&quot;) ## Standard error: 0.038 print(f&quot;95% Confidence interval: ({ci_cpk[0]:.3f}, {ci_cpk[1]:.3f})&quot;) ## 95% Confidence interval: (0.404, 0.553) 1.2.3 Pp and Ppk Confidence Intervals v_total = stat[&#39;n_w&#39;][0]*stat[&#39;k&#39;][0] - 1 se_pp = estimate_pp * math.sqrt(1 / (2*v_total)) ci_pp = (estimate_pp - z*se_pp, estimate_pp + z*se_pp) print(f&quot;Pp estimate: {estimate_pp:.3f}&quot;) ## Pp estimate: 0.670 print(f&quot;Standard error: {se_pp:.3f}&quot;) ## Standard error: 0.038 print(f&quot;95% Confidence interval: ({ci_pp[0]:.3f}, {ci_pp[1]:.3f})&quot;) ## 95% Confidence interval: (0.597, 0.744) se_ppk = estimate_ppk * math.sqrt(1 / (2*v_total) + 1 / (9*stat[&#39;n&#39;][0]*(estimate_ppk**2))) ci_ppk = (estimate_ppk - z*se_ppk, estimate_ppk + z*se_ppk) print(f&quot;\\nPpk estimate: {estimate_ppk:.3f}&quot;) ## ## Ppk estimate: 0.478 print(f&quot;Standard error: {se_ppk:.3f}&quot;) ## Standard error: 0.038 print(f&quot;95% Confidence interval: ({ci_ppk[0]:.3f}, {ci_ppk[1]:.3f})&quot;) ## 95% Confidence interval: (0.404, 0.551) 1.2.4 Visualizing Confidence Intervals Let’s create a visualization to show our Cp estimate with its confidence interval and important benchmark lines: # Create a DataFrame for plotting bands_data = pd.DataFrame({ &#39;index&#39;: [&#39;Cp Index&#39;], &#39;estimate&#39;: [estimate_cp], &#39;lower&#39;: [ci_cp[0]], &#39;upper&#39;: [ci_cp[1]] }) # Create the plot g = (ggplot(bands_data, aes(x=&#39;index&#39;, y=&#39;estimate&#39;, ymin=&#39;lower&#39;, ymax=&#39;upper&#39;)) + geom_hline(yintercept=[0, 1, 2], color=[&#39;grey&#39;, &#39;black&#39;, &#39;grey&#39;]) + geom_point(size=3) + geom_linerange(size=1) + theme_classic(base_size=14) + coord_flip() + labs(y=&#39;Index Value&#39;, x=None) + theme(axis_text_y=element_blank(), axis_ticks_y=element_blank())) # Save the plot g.save(&#39;images/06_confidence_interval_plot.png&#39;, width=8, height=4, dpi=100) It’s not the most exciting plot, but it does show very clearly that the value of \\(C_{p}\\) and its 95% confidence interval are nowhere even close to 1.0, the key threshold. This means we can say with 95% confidence that the true value of \\(C_{p}\\) is less than 1. 1.3 Bootstrapping Cp (Example) The normal approximation method assumes certain distributional properties. As an alternative, we can use bootstrapping to estimate confidence intervals by resampling our data. This method is more robust to distributional assumptions. import numpy as np reps = 500 def boot_cp(seed=1): np.random.seed(seed) vals = [] for r in range(reps): sample = water.sample(n=len(water), replace=True) stat_s_b = (sample.groupby(&#39;time&#39;).apply(lambda d: pd.Series({&#39;xbar&#39;: d[&#39;temp&#39;].mean(), &#39;s&#39;: d[&#39;temp&#39;].std(), &#39;n_w&#39;: len(d)})).reset_index()) sigma_s_b = (stat_s_b[&#39;s&#39;]**2).mean()**0.5 vals.append(cp(sigma_s_b, upper=limit_upper, lower=limit_lower)) s = pd.Series(vals) return pd.DataFrame({&#39;cp&#39;: [estimate_cp], &#39;lower&#39;: [s.quantile(0.025)], &#39;upper&#39;: [s.quantile(0.975)], &#39;se&#39;: [s.std()]}) bootstrap_results = boot_cp() # Print out the results boot_cp() ## cp lower upper se ## 0 0.671307 0.620466 0.785899 0.04336 1.3.1 Bootstrap Sampling Distributions Visualization So cool! We’ve now generated the sampling distributions for our bootstrap statistics! Let’s visualize the raw distributions to see those wicked cool bootstrapped sampling distributions: # Create a more comprehensive bootstrap function that returns all statistics def boot_comprehensive(seed=1): np.random.seed(seed) results = [] for r in range(reps): sample = water.sample(n=len(water), replace=True) stat_s_b = (sample.groupby(&#39;time&#39;).apply(lambda d: pd.Series({ &#39;xbar&#39;: d[&#39;temp&#39;].mean(), &#39;s&#39;: d[&#39;temp&#39;].std(), &#39;n_w&#39;: len(d) })).reset_index()) xbbar_b = stat_s_b[&#39;xbar&#39;].mean() sigma_s_b = (stat_s_b[&#39;s&#39;]**2).mean()**0.5 cp_b = cp(sigma_s_b, upper=limit_upper, lower=limit_lower) results.append({ &#39;rep&#39;: r, &#39;xbbar&#39;: xbbar_b, &#39;sigma_s&#39;: sigma_s_b, &#39;cp&#39;: cp_b }) return pd.DataFrame(results) # Generate bootstrap data bootstrap_data = boot_comprehensive() # Reshape for plotting plot_data = [] for _, row in bootstrap_data.iterrows(): plot_data.extend([ {&#39;type&#39;: &#39;xbbar&#39;, &#39;value&#39;: row[&#39;xbbar&#39;]}, {&#39;type&#39;: &#39;sigma_s&#39;, &#39;value&#39;: row[&#39;sigma_s&#39;]}, {&#39;type&#39;: &#39;cp&#39;, &#39;value&#39;: row[&#39;cp&#39;]} ]) plot_df = pd.DataFrame(plot_data) # Create the plot g = (ggplot(plot_df, aes(x=&#39;value&#39;, fill=&#39;type&#39;)) + geom_histogram(bins=30, alpha=0.7) + facet_wrap(&#39;~type&#39;, scales=&#39;free&#39;) + theme_classic(base_size=12) + labs(x=&#39;Value&#39;, y=&#39;Frequency&#39;) + theme(legend_position=&#39;none&#39;)) # Save the plot g.save(&#39;images/06_bootstrap_distributions.png&#39;, width=10, height=6, dpi=100) 1.3.2 Bootstrap Confidence Intervals Now let’s take our bootstrapped \\(C_{p}\\) statistics and estimate a confidence interval and standard error for this sampling distribution. Because we have the entire distribution, we can extract values at specific percentiles in the distribution using quantile(), rather than theoretical distributions. print(&quot;Bootstrap Results for Cp:&quot;) ## Bootstrap Results for Cp: print(f&quot;Original estimate: {bootstrap_results[&#39;cp&#39;][0]:.3f}&quot;) ## Original estimate: 0.671 print(f&quot;Bootstrap 95% CI: ({bootstrap_results[&#39;lower&#39;][0]:.3f}, {bootstrap_results[&#39;upper&#39;][0]:.3f})&quot;) ## Bootstrap 95% CI: (0.620, 0.786) print(f&quot;Bootstrap standard error: {bootstrap_results[&#39;se&#39;][0]:.3f}&quot;) ## Bootstrap standard error: 0.043 This suggests a wider confidence interval than our normal distribution assumes by default - interesting! Conclusion You’ve successfully computed capability and performance indices (Cp, Cpk, Pp, Ppk) and their confidence intervals using both normal approximation and bootstrap methods in Python. These tools help us assess process capability and performance, providing insights into whether our processes meet specifications and how consistently they perform. "]]
