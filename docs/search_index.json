[["index.html", "System Reliability and Six Sigma in R Introduction", " System Reliability and Six Sigma in R Timothy Fraser, PhD 2023-11-07 Introduction Your online textbook for learning reliability and six sigma techniques in R! These coding workshops were made for Cornell University Course SYSEN 5300. Follow along with RStudio.Cloud to learn to apply six sigma techniques in R! Figure 0.1: Photo by Naser Tamimi on Unsplash "],["workshop-coding-in-r.html", "1 Workshop: Coding in R Getting Started 1.1 Introduction to R 1.2 Basic Calculations in R Learning Check 1 1.3 Types of Values in R 1.4 Types of Data in R Learning Check 2 1.5 Common Functions in R 1.6 Missing Data Learning Check 3 1.7 Packages 1.8 Visualizing Data with Histograms Learning Check 4 Conclusion", " 1 Workshop: Coding in R Welcome to RStudio Cloud! You made it! This document will introduce you to how to start coding in R, using RStudio Cloud. We will use the R statistical coding language frequently in class to conduct analyses and visualization. Hello world! We are coding in R! Getting Started Making an RStudio.Cloud account Well be using RStudio.Cloud, a virtual version of R you can access from any computer with an internet browser (PC, Mac, Chromebook, anything). To get set up, please follow the steps in this short Video playlist! Using R for the First Time For a quick visual orientation, take a peek at the image below. Read and follow along with the instructions on the webpage! Read the tutorial code (below), and then type it in and run it in your R session! (#fig:graphic_1)Visual Intro to Using RStudio.Cloud 1.1 Introduction to R The document in your RStudio Cloud project document is an R script. (its name ends in .R). It contains two kinds of text: code - instructions to our statistical calculator comments - any text that immediately follows a # sign. # For example, # Comments are ignored by the calculator, so we can write ourselves notes. Notice: 4 windows in R. Window 1 (upper left): Scripts! Window 2 (bottom left): Console (this shows the output for our calculator) Window 3 (upper right): Environment (this shows any data the computer is holding onto for us) Window 4 (bottom right): Files (this shows our working project folder, our scripts, and any data files.) A few tips: To change the background theme (and save your eyes), go to Tools &gt;&gt; Global Options &gt;&gt; Appearance &gt;&gt; Editor Theme &gt;&gt; Dracula To increase the font size, go to Tools &gt;&gt; Global Options &gt;&gt; Appearance &gt;&gt; Editor Font Size To make a script, go to File &gt;&gt; New File &gt;&gt; R Script, then save it and name it. Figure 1.1: Open New Script Figure 1.2: Save New Script! Lets learn to use R! 1.2 Basic Calculations in R Try highlighting the following with your cursor, and then press CTRL and ENTER simultaneously, or the Run button above. Addition: 1 + 5 ## [1] 6 Subtraction: 5 - 2 ## [1] 3 Multiplication: 2 * 3 ## [1] 6 Division: 15 / 5 ## [1] 3 Exponents: 2^2 ## [1] 4 Square-Roots: sqrt(4) ## [1] 2 Order of Operations: Still applies! Like in math normally, R calculations are evaluated from left to right, prioritizing parentheses, then multiplication and division, then addition and subtraction. 2 * 2 - 5 ## [1] -1 Use Parentheses! 2 * (2 - 5) ## [1] -6 Learning Check 1 Learning Checks (LC) are short questions that appear throughout this book, providing short coding challenges to try and work through. Below is the question tab. Read the question, and try to answer it on your own! Then, click the answer button to see the answer. (Note: There are often many different ways to code the same thing!) Feeling stumped? You can check the answer, but be sure to code it yourself afterwards! Question Try calculating something wild in R! Solve for x below using the commands you just learned in R! \\(x = \\sqrt{ (\\frac{2 - 5 }{5})^{4} }\\) \\(x = (1 - 7)^{2} \\times 5 - \\sqrt{49}\\) \\(x = 2^{2} + 2^{2} \\times 2^{2} - 2^{2} \\div 2^{2}\\) [View Answer!] Heres how we coded it! How does yours compare? If your result is different, compare code. Whats different? Be sure to go back and adjust your code so you understand the answer! \\(x = \\sqrt{ (\\frac{2 - 5 }{5})^{4} }\\) sqrt( ((2 - 5) / 5)^4 ) ## [1] 0.36 \\(x = (1 - 7)^{2} \\times 5 - \\sqrt{49}\\) (1 - 7)^2 * 5 - sqrt(49) ## [1] 173 \\(x = 2^{2} + 2^{2} \\times 2^{2} - 2^{2} \\div 2^{2}\\) 2^2 + 2^2 * 2^2 - 2^2 / 2^2 ## [1] 19 1.3 Types of Values in R R accepts 2 type of data: # Numeric Values 15000 ## [1] 15000 0.0005 ## [1] 5e-04 -8222 # notice no commas allowed ## [1] -8222 and # Character Strings &quot;Coding!&quot; # Uses quotation marks ## [1] &quot;Coding!&quot; &quot;Corgis!&quot; # Can contain anything - numbers, characters, etc. ## [1] &quot;Corgis!&quot; &quot;Coffee!&quot; ## [1] &quot;Coffee!&quot; (Note: R also uses something called factors, which are characters, but have a specific order. Well learn them later.) 1.4 Types of Data in R 1.4.1 Values First, R uses values - which are single numbers or characters. 2 # this is a value ## [1] 2 &quot;x&quot; # this is also a value ## [1] &quot;x&quot; You can save a value as a named object in the R Environment. That means, we tell R to remember that whenever you use a certain name, it means that value. To name something as an object, use an arrow! myvalue &lt;- 2 Now lets highlight and press CTRL ENTER on myvalue (or the Mac Equivalent). myvalue ## [1] 2 Notice how its listed in the R Environment (upper right), and how it outputs as 2 in the console? We can do operations too! secondvalue &lt;- myvalue + 2 # add 2 to myvalue secondvalue # check new value - oooh, it&#39;s 4! ## [1] 4 We can also overwrite old objects with new objects. myvalue &lt;- &quot;I overwrote it!&quot; myvalue ## [1] &quot;I overwrote it!&quot; And we can also remove objects from the Environment, with remove(). remove(myvalue, secondvalue) 1.4.2 Vectors Second, R contains values in vectors, which are sets of values. # This is a numeric vector c(1, 4, 8) # is the same as 1, 4, 8 ## [1] 1 4 8 and # This is a character vector c(&quot;Boston&quot;, &quot;New York&quot;, &quot;Los Angeles&quot;) ## [1] &quot;Boston&quot; &quot;New York&quot; &quot;Los Angeles&quot; But if you combine numeric and character values in one vector # This doesn&#39;t work - R immediately makes it into a character vector c(1, &quot;Boston&quot;, 2) ## [1] &quot;1&quot; &quot;Boston&quot; &quot;2&quot; Why do we use vectors? Because you can do mathematical operations on entire vectors of values, all at once! c(1,2,3,4) * 2 # this multiplies each value by 2! ## [1] 2 4 6 8 c(1,2,3,4) + 2 # this adds 2 to each value! ## [1] 3 4 5 6 We can save vectors as objects too! # Here&#39;s a vector of (hypothetical) seawall heights in 10 towns. myheights &lt;- c(4, 4.5, 5, 5, 5, 5.5, 5.5, 6, 6.5, 6.5) # And here&#39;s a list of hypothetical names for those towns mytowns &lt;- c(&quot;Gloucester&quot;, &quot;Newburyport&quot;, &quot;Provincetown&quot;, &quot;Plymouth&quot;, &quot;Marblehead&quot;, &quot;Chatham&quot;, &quot;Salem&quot;, &quot;Ipswich&quot;, &quot;Falmouth&quot;, &quot;Boston&quot;) # And here&#39;s a list of years when those seawalls were each built. myyears &lt;- c(1990, 1980, 1970, 1930, 1975, 1975, 1980, 1920, 1995, 2000) Plus, we can still do operations on entire vectors! myyears + 1 ## [1] 1991 1981 1971 1931 1976 1976 1981 1921 1996 2001 1.4.3 Dataframes Third, R bundles vectors into data.frames. # Using the data.frame command, we make a data.frame, data.frame( height = myheights, # length 10 town = mytowns, # length 10 year = myyears) # length 10 ## height town year ## 1 4.0 Gloucester 1990 ## 2 4.5 Newburyport 1980 ## 3 5.0 Provincetown 1970 ## 4 5.0 Plymouth 1930 ## 5 5.0 Marblehead 1975 ## 6 5.5 Chatham 1975 ## 7 5.5 Salem 1980 ## 8 6.0 Ipswich 1920 ## 9 6.5 Falmouth 1995 ## 10 6.5 Boston 2000 And inside, we put a bunch of vectors of EQUAL LENGTHS, giving each vector a name. And when it outputs in the console, it looks like a spreadsheet! BECAUSE ALL SPREADSHEETS ARE DATAFRAMES! AND ALL COLUMNS ARE VECTORS! AND ALL CELLS ARE VALUES! Actually, we can make data.frames into objects too! # Let&#39;s name our data.frame about seawalls &#39;sw&#39; sw &lt;- data.frame( height = myheights, town = mytowns, year = myyears) # Notice this last parenthesis; very important # Check the contents of sw! sw ## height town year ## 1 4.0 Gloucester 1990 ## 2 4.5 Newburyport 1980 ## 3 5.0 Provincetown 1970 ## 4 5.0 Plymouth 1930 ## 5 5.0 Marblehead 1975 ## 6 5.5 Chatham 1975 ## 7 5.5 Salem 1980 ## 8 6.0 Ipswich 1920 ## 9 6.5 Falmouth 1995 ## 10 6.5 Boston 2000 Although, we could do this too, and it would be equivalent: sw &lt;- data.frame( # It&#39;s okay to split code across multiple lines. # It keeps things readable. height = c(4, 4.5, 5, 5, 5, 5.5, 5.5, 6, 6.5, 6.5), town = c(&quot;Gloucester&quot;, &quot;Newburyport&quot;, &quot;Provincetown&quot;, &quot;Plymouth&quot;, &quot;Marblehead&quot;, &quot;Chatham&quot;, &quot;Salem&quot;, &quot;Ipswich&quot;, &quot;Falmouth&quot;, &quot;Boston&quot;), year = c(1990, 1980, 1970, 1930, 1975, 1975, 1980, 1920, 1995, 2000)) # Let&#39;s check out our dataframe! sw ## height town year ## 1 4.0 Gloucester 1990 ## 2 4.5 Newburyport 1980 ## 3 5.0 Provincetown 1970 ## 4 5.0 Plymouth 1930 ## 5 5.0 Marblehead 1975 ## 6 5.5 Chatham 1975 ## 7 5.5 Salem 1980 ## 8 6.0 Ipswich 1920 ## 9 6.5 Falmouth 1995 ## 10 6.5 Boston 2000 But what if we want to work with the vectors again? We can use the $ sign to say, grab the following vector from inside this data.frame. sw$height ## [1] 4.0 4.5 5.0 5.0 5.0 5.5 5.5 6.0 6.5 6.5 We can also do operations on that vector from within the dataframe. sw$height + 1 ## [1] 5.0 5.5 6.0 6.0 6.0 6.5 6.5 7.0 7.5 7.5 We can also update values, like the following: # sw$height &lt;- sw$height + 1 # I&#39;ve put this in comments, since I don&#39;t actually want to do it (it&#39;ll change our data) # but good to know, right? Learning Check 2 Question How would you make your own data.frame? Please make up a data.frame of with 3 vectors and 4 values each. Make 1 vector numeric and 2 vectors character data. How many rows are in that data.frame? [View Answer!] Heres my example! # Make a data.frame called &#39;mayhem&#39; mayhem &lt;- data.frame( # make a character vector of 4 dog by their names dogs = c(&quot;Mocha&quot;, &quot;Domino&quot;, &quot;Latte&quot;, &quot;Dot&quot;), # Classify the type of dog as a character vector types = c(&quot;corgi&quot;, &quot;dalmatian&quot;, &quot;corgi&quot;, &quot;dalmatian&quot;), # Record the number of treats eaten per year per dog treats_per_year = c(5000, 3000, 2000, 10000)) # View the resulting &#39;mayhem&#39;! mayhem ## dogs types treats_per_year ## 1 Mocha corgi 5000 ## 2 Domino dalmatian 3000 ## 3 Latte corgi 2000 ## 4 Dot dalmatian 10000 1.5 Common Functions in R We can also run functions that come pre-installed to analyze vectors. These include: mean(), median(), sum(), min(), max(), range(), quantile(), sd(), var(), and length(). Figure 1.3: Descriptive Stats function Cheatsheet! 1.5.1 Measures of Central Tendency mean(sw$height) # the mean seawall height among these towns ## [1] 5.35 median(sw$height) # the median seawall height ## [1] 5.25 sum(sw$height) # total meters of seawall height! (weird number, but okay) ## [1] 53.5 1.5.2 Measures of Dispersion min(sw$height) # smallest seawall height ## [1] 4 max(sw$height) # tallest seawall height ## [1] 6.5 range(sw$height) # range of seawalls (min &amp; max) ## [1] 4.0 6.5 quantile(sw$height, probs = 0.25) # 25th percentile ## 25% ## 5 quantile(sw$height, probs = 0.75) # 75th percentile ## 75% ## 5.875 sd(sw$height) # the standard deviation of seawall heights ## [1] 0.8181958 var(sw$height) # the variance of seawall heights (= standard deviation squared) ## [1] 0.6694444 1.5.3 Other Good Functions length(sw$height) # the number of values in this vector ## [1] 10 length(sw) # the number of vectors in this data.frame ## [1] 3 Thats really fast! Well learn more about these descriptive statistics in later lessons! 1.6 Missing Data Sometimes, data.frames include missing data for a case/observation. For example, lets say there is an 11th town, where the seawall height is unknown. # We would write: mysw &lt;- c(4, 4.5, 5, 5, 5, 5.5, 5.5, 6, 6.5, 6.5, NA) # see the &#39;NA&#39; for non-applicable If you run mean(mysw) now, R doesnt know how to add 6.5 + NA. The output will become NA instead of 5.35. mean(mysw) ## [1] NA To fix this, we can add an argument to the function, telling it to omit NAs from the calculation. mean(mysw, na.rm = TRUE) # short for, &#39;remove NAs&#39; ## [1] 5.35 Pretty cool, no? Each function is unique, often made by different people, so only these functions have na.rm as an argument. Learning Check 3 Question Jun Kanda (2015) measured max seawall heights (seawall_m) in 13 Japanese towns (town) after the 2011 tsunami in Tohoku, Japan, compared against the height of the tsunami wave (wave_m). Using this table, please code and answer the questions below. town seawall_m wave_m Kuji South 12.0 14.5 Fudai 15.5 18.4 Taro 13.7 16.3 Miyako 8.5 11.8 Yamada 6.6 10.9 Ohtsuchi 6.4 15.1 Tohni 11.8 21.0 Yoshihama 14.3 17.2 Hirota 6.5 18.3 Karakuwa East 6.1 14.4 Onagawa 5.8 18.0 Souma 6.2 14.5 Nakoso 6.2 7.7 Reproduce this table as a data.frame in R, and save it as an object named jp. How much greater was the mean height of the tsunami than the mean height of seawalls? Evaluate how much these heights varied on average among towns. Did seawall height vary more than tsunami height? How much more/less? [View Answer!] Reproduce this table as a data.frame in R, and save it as an object named jp. # Make a dataframe named jp, jp &lt;- data.frame( # containing a character vector of 13 town names, town = c(&quot;Kuji South&quot;, &quot;Fudai&quot;, &quot;Taro&quot;, &quot;Miyako&quot;, &quot;Yamada&quot;, &quot;Ohtsuchi&quot;, &quot;Tohni&quot;, &quot;Yoshihama&quot;, &quot;Hirota&quot;, &quot;Karakuwa East&quot;, &quot;Onagawa&quot;, &quot;Souma&quot;, &quot;Nakoso&quot;), # and a numeric vector of 13 max seawall heights in meters seawall_m = c(12.0, 15.5, 13.7, 8.5, 6.6, 6.4, 11.8, 14.3, 6.5, 6.1, 5.8, 6.2, 6.2), # and a numeric vector of 13 max tsunami heights in meters wave_m = c(14.5, 18.4, 16.3, 11.8, 10.9, 15.1, 21.0, 17.2, 18.3, 14.4, 18.0, 14.5, 7.7) ) # View contents! jp ## town seawall_m wave_m ## 1 Kuji South 12.0 14.5 ## 2 Fudai 15.5 18.4 ## 3 Taro 13.7 16.3 ## 4 Miyako 8.5 11.8 ## 5 Yamada 6.6 10.9 ## 6 Ohtsuchi 6.4 15.1 ## 7 Tohni 11.8 21.0 ## 8 Yoshihama 14.3 17.2 ## 9 Hirota 6.5 18.3 ## 10 Karakuwa East 6.1 14.4 ## 11 Onagawa 5.8 18.0 ## 12 Souma 6.2 14.5 ## 13 Nakoso 6.2 7.7 How much greater was the mean height of the tsunami than the mean height of seawalls? # Get mean of wave height mean(jp$wave_m) ## [1] 15.23846 The average wave was 15.24 meters tall. # Get mean of seawall height mean(jp$seawall_m) ## [1] 9.2 The average seawall was 9.2 meters tall. # Get difference in mean seawall height mean(jp$wave_m) - mean(jp$seawall_m) ## [1] 6.038462 The average wave was 6.04 meters taller than the average seawall. Evaluate how much these heights varied on average among towns. Did seawall height vary more than tsunami height? How much more/less? # Get standard deviation of wave height sd(jp$wave_m) ## [1] 3.587603 On average, wave height varied by 3.59 meters. # Get standard deviation of seawall height sd(jp$seawall_m) ## [1] 3.675368 On average, seawall height varied by 3.68 meters. # Get difference sd(jp$wave_m) - sd(jp$seawall_m) ## [1] -0.08776516 That means wave height varied by -0.09 meters less than seawall height. 1.7 Packages 1.7.1 Using Packages Some functions come pre-built into R, but lots of people have come together to build packages of functions that help R users all over the world do more, cool things, so we dont each have to reinvent the wheel. ggplot2, which we use below, is one of these! 1.7.2 Installing Packages We can use the library() function to load a package (like fipping an on switch for the package). After loading it, R will recognize that packages functions when you run them! But if you try to load a package that has never been installed on your computer, you might get this error: library(ggplot2) Error in library(ggplot2) : there is no package called ggplot2 In this case, we need to install those packages (only necessary once), using install.packages(). (If a message pops up, just accept Yes.) install.packages(c(&quot;ggplot2&quot;, &quot;dplyr&quot;)) After a successful install, youll get a message like this: ================================================== downloaded 1.9 MB * installing *binary* package ggplot2 ... * DONE (ggplot2) * installing *binary* package dplyr ... * DONE (dplyr) The downloaded source packages are in /tmp/RtmpefCnYe/downloaded_packages 1.7.3 Loading Packages Finally, we can load our packages with library(). library(ggplot2) library(dplyr) Tada! You have turned on your packages! 1.8 Visualizing Data with Histograms The power of R is that you can process data, calculate statistics, and visualize it all together, very quickly. We can do this using hist() and geom_histogram(), among other functions. 1.8.1 hist() For example, lets imagine that we had seawall height data from cities in several states. We might want to compare those states. # Create 30 cities, ten per state (MA, RI, ME) allsw &lt;- data.frame( height = c(4, 4.5, 5, 5, 5.5, 5.5, 5.5, 6, 6, 6.5, 4, 4,4, 4, 4.5, 4.5, 4.5, 5, 5, 6, 5.5, 6, 6.5, 6.5, 7, 7, 7, 7.5, 7.5, 8), states = c(&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;,&quot;MA&quot;, &quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;,&quot;RI&quot;, &quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;,&quot;ME&quot;)) # Take a peek! allsw ## height states ## 1 4.0 MA ## 2 4.5 MA ## 3 5.0 MA ## 4 5.0 MA ## 5 5.5 MA ## 6 5.5 MA ## 7 5.5 MA ## 8 6.0 MA ## 9 6.0 MA ## 10 6.5 MA ## 11 4.0 RI ## 12 4.0 RI ## 13 4.0 RI ## 14 4.0 RI ## 15 4.5 RI ## 16 4.5 RI ## 17 4.5 RI ## 18 5.0 RI ## 19 5.0 RI ## 20 6.0 RI ## 21 5.5 ME ## 22 6.0 ME ## 23 6.5 ME ## 24 6.5 ME ## 25 7.0 ME ## 26 7.0 ME ## 27 7.0 ME ## 28 7.5 ME ## 29 7.5 ME ## 30 8.0 ME Every vector is a distribution - a range of low to high values. We can use the hist() function to quickly visualize a vectors distribution. hist(allsw$height) 1.8.2 geom_histogram() in ggplot2 hist() is great for a quick check, but for anything more complex, were going to use ggplot2, the most popular visualization package in R. # Load ggplot2 package library(ggplot2) # Tell the ggplot function to... ggplot( # draw data from the &#39;allsw&#39; data.frame data = allsw, # and &#39;map&#39; the vector &#39;height&#39; to be an &#39;aes&#39;thetic on the &#39;x&#39;-axis. mapping = aes(x = height)) + # make histograms of distribution, geom_histogram( # With white outlines color = &quot;white&quot;, # With blue inside fill fill = &quot;steelblue&quot;, # where every half meter gets a bin (binwidth = 0.5) binwidth = 0.5) + # add labels labs(x = &quot;Seawall Height&quot;, y = &quot;Frequency (# of cities)&quot;) Looks much nicer, right? Lots more code, but lots more options for customizing. Well learn ggplot2 more over this term, and it will become second nature in time! (Just takes practice!) The value of ggplot2 really comes alive when we make complex visuals. For example, our data allsw$height essentially contains 3 vectors, one per state; one for MA, one for RI, one for ME. Can we visualize each of these vectors distributions using separate histograms? # Repeat code from before... ggplot(data = allsw, mapping = aes(x = height)) + geom_histogram(color = &quot;white&quot;, fill = &quot;steelblue&quot;, binwidth = 0.5) + labs(x = &quot;Seawall Height&quot;, y = &quot;Frequency (# of cities)&quot;) + # don&#39;t forget the &#39;+&#39;! # But also ## Split into panels by state! facet_wrap(~states) We can now see, according to our hypothetical example, that states host different distributions of seawall heights. Massachusetts (MA) has lower seawalls, evenly distributed around 5.5 m. Maine (ME) has higher seawalls, skewed towards 7 m. Rhode Island (RI) has lower seawalls, skewed towards 4 m. Learning Check 4 Question Challenge: Please make a histogram of Jun Kandas sample of seawall heights (seawall_m) in the jp object from LC 3. First, make a histogram using the hist() function. Then, try and use the geom_histogram() function from ggplot2! [View Answer!] First, make a histogram using the hist() function. # Tell R to make a histogram from the &#39;seawall_m&#39; vector inside &#39;jp&#39;! hist(jp$seawall_m) Then, try and use the geom_histogram() function from ggplot2! # Tell ggplot to grab the &#39;seawall_m&#39; vector from the &#39;jp&#39; data.frame, # and make a histogram! ggplot(data = jp, mapping = aes(x = seawall_m)) + geom_histogram() Looks pretty weird, huh? hist() automatically chooses the binwidth, but ggplot() gives us more control over the whole plot. Well learn more about this soon! Conclusion Next Steps Throughout the rest of the course, were going to advance each of these skills: working with types of data in R calculating meaningful statistics in R visualizing meaningful trends in R Advice Be sure to clear your environment often. That means, using remove() or the broom tool in the upper right hand corner. remove(allsw, mysw, sw, myheights, mytowns, myyears) You can clean your console too, using broom in consoles upper right corner. Save often. (Control + Save usually works on PC.) You can download files using more / export, or upload them. Youll be a rockstar at using R in no time! Stay tuned for our next Workshop! Troubleshooting If your session freezes, go to Session &gt;&gt; Restart R. If that doesnt work, go to Session &gt;&gt; Terminate. If that doesnt work, click on the elipsis () in the white banner at the top, and select Relaunch Project. If that doesnt work, let me know! Having problems? There are three causes of most all problems in R. theres a missing parenthesis or missing quotation mark in ones code. Youre using a function from a package that needs to be loaded (well talk about this in later workshops). Too much data in your environment is causing R to crash. Clear the environment. "],["skill-visualization-with-ggplot-in-r.html", "Skill: Visualization with ggplot in R Getting Started Your first scatterplot Learning Check 1 Learning Check 2 Learning Check 3 Improving our Visualizations Learning Check 4 Learning Check 5 Visualizing diamonds data Learning Check 6 Learning Check 7 Learning Check 8 Visualizing Distributions Learning Check 9 Breaking Up a Visual Learning Check 10 Conclusion", " Skill: Visualization with ggplot in R Visualization is a key part of statistical analyses, especially in systems engineering! Visuals themselves are often the analysis themselves! In this tutorial, were going to learn how to visualize data in the ggplot2 package. Please follow along using the code below! Getting Started Loading Packages Lets load our packages with library(). # Data viz and data manipulation packages library(ggplot2) library(dplyr) # Data sources library(gapminder) Notes: SAVE YOUR SCRIPT. Always comment your code (what Im doing now), use lots of spaces, and keep it clean. Gapminder data Economist Hans Rosling made a dataset that examines change in life expectancy over time for most countries in the world. It is contained in the gapminder package! # Let&#39;s view it. (see console below) gapminder ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## #  1,694 more rows Each row is a country-year, marking the life expectancy, population, and gross domestic product (GDP) per capita. On your end, you can only can see some of it, right? Lets check out what vectors are in this dataframe, using the glimpse function from the dplyr package. # (Remember, a vector is a column in a spreadsheet; # a data.frame is a spreadsheet.) glimpse(gapminder) ## Rows: 1,704 ## Columns: 6 ## $ country &lt;fct&gt; &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;, &quot;Afghanistan&quot;,  ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,  ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,  ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8 ## $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12 ## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134,  # Nice, we can see things more concisely. Our data has six variables. Great! Your first scatterplot Using the gapminder data, lets map a series of vectors to become aesthetic features in the visualization (point, colors, fills, etc.). ggplot(data = gapminder, mapping = aes( # Let&#39;s make the x-axis gross-domestic product per capita (wealth per person) x = gdpPercap, # Let&#39;s make the y-axis country life expectancy y = lifeExp)) Huh! We made an empty graph. Cool. Thats because ggplot needs helper functions to add aesthetic features to the graph. For example, adding + geom_point() will overlay a scatterplot. # Make a scatterplot ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + # same as above, except &quot;+&quot; geom_point() Learning Check 1 Question What kind of relationship does this graph show? Why might it matter to policymakers? [View Answer!] The graph above shows that as average wealth (GDP per capita) in a country increases, those countries life expectancy increases swiftly, but then tapers off. This highlights that there is a strong relationship between wealth and health globally. Learning Check 2 Question What happens when you add the alpha, changing its values in the 3 visuals below? # Run the following code: ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.2) ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.5) ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 1) [View Answer!] alpha ranges from 0 to 1 and describes feature transparency. Increasing alpha to 1 makes points fully opaque! Decreasing alpha to 0 makes points fully transparent! Learning Check 3 Question We can make it more visually appealing. What happens when we do each of the following? If you want to make it a single color, where do you need to write color = ...? If you want to make it multiple colors according to a vector, where do you need to write color =? # Run the following code: # Version 1 ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point(alpha = 0.5, color = &quot;steelblue&quot;) # Version 2 ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(alpha = 0.5) [View Answer!] To assign a single color, you need to put color outside the aes() phrase, and write the name of the color. To assign multiple colors, you need to put the color inside the aes(...) phrase, and write the name of the vector in the data that it corresponds to (eg. continent). Improving our Visualizations We can (and should!) make our visualizations much more readable by adding appropriate labels. ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(alpha = 0.5) + # Add labels! labs(x = &quot;GDP per capita (USD)&quot;, # label for x-values y = &quot;Life Expectancy (years)&quot;, # label for y-values color = &quot;Continent&quot;, # label for colors title = &quot;Does Wealth affect Health?&quot;, # overall title subtitle = &quot;Global Health Trends by Continent&quot;, # subtitle! caption = &quot;Points display individual country-year observations.&quot;) # caption We can actually save visualizations as objects too, which can make things faster. Lets save our visual as myviz myviz &lt;- ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(alpha = 0.5) + labs(x = &quot;GDP per capita (USD)&quot;, y = &quot;Life Expectancy (years)&quot;, color = &quot;Continent&quot;, title = &quot;Does Wealth affect Health?&quot;, # overall title subtitle = &quot;Global Health Trends by Continent&quot;, # subtitle! caption = &quot;Points display individual country-year observations.&quot;) # caption Next, lets try a few more learning check that will ask you to try our ways to improve the quality and readability of your visuals! Learning Check 4 Question Now run myviz - what happens? myviz [View Answer!] When you save a ggplot to an object, eg. naming it myviz, you can call up the visual again as many times as you want by just running the myviz object, just like any other object. Learning Check 5 Question We can do better, adding things onto our myviz object! Try changing themes. What happens below? # Version theme_bw myviz + # How about this theme? theme_bw() # Version theme_dark myviz + # How about this theme? theme_dark() # Version theme_classic myviz + # How about this theme? theme_classic() [View Answer!] theme_bw() makes a nice black-and-white graph; theme_dark() makes a funky graph with a dark grey background; theme_classic() makes a very simple graph, with fewer distractions. I personally really like the default theme or theme_bw(). Sometimes theme_classic() can be really helpful if you have a particularly busy visual. Visualizing diamonds data Next, lets use the diamonds dataset, which comes with the ggplot2 package This is a dataset of over 50,000 diamond sales. # Check out first 3 rows... diamonds %&gt;% head(3) ## # A tibble: 3 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 We can use this visualization to check whether the cut of diamonds really has any relationship with price. glimpse(diamonds) ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0. ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I, ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1,  ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64 ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58 ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34 ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4. ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4. ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2. Looks like cut is an ordinal variable (fair, good, ideal, etc.), while price is numeric (eg. dollars). A boxplot might be helpful! ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut)) + # notice how we added group = cut, to tell it to use 5 different boxes, one per cut? geom_boxplot() Huh. How odd. Looks like the cut of diamonds has very little impact on what price they are sold at! We can see lots of outliers at the top - really expensive diamonds for that cut. Learning Check 6 Question Lets make this visualization more visually appealing. What changed in the code to make these two different visual effects? Why? (Hint: fill.) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut)) + geom_boxplot(fill = &quot;steelblue&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() [View Answer!] In the first visual, we assigned all the boxplots to have the same fill (fill = \"steelblue\"), but in the second visual, we assigned the boxplot fill to be shaded based on the cut of diamond. This adds a cool color range! Learning Check 7 Question Sometimes, the names of categories wont fit well. We can try the following. What did we do? ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() [View Answer!] We used coord_flip() to flip the coordinates of the x and y axis, which gives our cut labels more room! Learning Check 8 Question Sometimes, the legend doesnt fit well. We can try this: What happens when you change legend.position from \"right\" to \"bottom\" to \"left\" to \"top\"? ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;bottom&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;right&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;left&quot;) ggplot(data = diamonds, mapping = aes(x = cut, y = price, group = cut, fill = cut)) + geom_boxplot() + coord_flip() + theme(legend.position = &quot;top&quot;) [View Answer!] These four values for legend.position will relocate the fill legend (and any other legends) to be at the top, bottom, left, or right of the visual! Visualizing Distributions Different geom_ functions use colors in different ways, but this is a good example. For example, below is a histogram. It visualizes the approximate distribution of a set of values. We can see how frequently diamonds are sold for certain prices versus others. ggplot(data = diamonds, mapping = aes(x = price, group = cut, fill = cut)) + geom_histogram(color = &quot;white&quot;) + # notice new function here labs(x = &quot;Price (USD)&quot;, y = &quot;Frequency of Price (Count)&quot;, title = &quot;US Diamond Sales&quot;) Learning Check 9 Question Are most diamonds cheap or expensive? What type of distribution would you call this? Normal? Uniform? Left Skewed? Right Skewed? [View Answer!] This is strongly right-skewed distribution, because the majority of the distribution leans to the left (the clump of the data), while it has a long tail that skews to the right. The median is less than the mean in a right skewed distribution. Breaking Up a Visual Finally, we might want to break up our visual into multiple parts. We can use facet_wrap to do this, but how exactly does it work? Lets test it out in the Learning Check below. Learning Check 10 Question What changed in the code below, and what did it result in? ggplot(data = diamonds, mapping = aes(x = price, fill = cut)) + geom_histogram(color = &quot;white&quot;) + facet_wrap(~cut) + # must be categorical variable labs(x = &quot;Price (USD)&quot;, y = &quot;Frequency of Price (Count)&quot;, title = &quot;US Diamond Sales&quot;) [View Answer!] This visual split up our histograms into separate panels (making it much more readable), and easier to compare distributions. We write facet_wrap(~ before the variable name (eg. cut) to specify that we want to split up the data by the values of cut. This sorts our rows of data into 5 different piles (since there are 5 different categories in cut) and makes a panel out of each. Conclusion You made it! You have now tried out a series of visuals in ggplot. We will use ggplot a lot in this course, so please be sure to reach out when you have questions, talk with others in your group, and work together to build great visualization skills! (Plus, its super applicable professionally!) "],["skill-failure-modes-and-effects-analysis-in-r.html", "Skill: Failure Modes and Effects Analysis in R Getting Started Example: Ben and Jerrys Ice Cream Calculating Criticality Learning Check 1 Conclusion", " Skill: Failure Modes and Effects Analysis in R This tutorial will introduce you to Failure Modes and Effects Analysis (FMEA) in R! Getting Started Please open up your project on RStudio.Cloud, for our Github class repository. Start a new R script (File &gt;&gt; New &gt;&gt; R Script). Save the R script as lesson_3.R. And lets get started! Load Packages # Load tidyverse, which contains dplyr and most data wrangling functions library(tidyverse) # Load DiagrammeR, which we&#39;ll use to make diagrams today! library(DiagrammeR) Example: Ben and Jerrys Ice Cream Ben and Jerrys main headquarters is in Waterbury, VT, just outside of Burlington, where it makes a lot of ice cream. (Its also fun to visit.) Their staff likely has to take considerable care to make sure that all that ice cream stays refrigerated! Suppose Ben and Jerrys has decided to build a new ice cream production plant in Ithaca, NY. For the sake of Ben and Jerrys (nay, the world!) lets use Failure Modes and Effects Analysis (FMEA) to identify any hypothetical vulnerability that might occur at this new ice cream business! Scope &amp; Resolution As our scope, were going to just focus on melting. What are all the possible ways that ice cream could melt during this process? Melting would have several negative impacts, such as getting exposed to heat, bacteria, and, worst of all, melting the ice cream! This example is primarily people-centric, because its important to remember that people are part of our technological systems! Measuring Criticality FMEA includes uses 3 measures to calculate a criticality index, meaning the overall risk of each combination of severity and underlying conditions. $ severity occurence detection = criticality $ Each gets classified on a scale from 1-10: severity: 1 = none, 10 = hazardous/catastrophic occurrence: 1 = almost impossible, 10 = almost certain detection: 1 = almost certain, 10 = almost impossible These will produce a criticality index from 1 to 1000. Suppose we want to be 99% sure that our technology wont fail and negatively impact society. We would need a criticality index (also known as RPN) of 990 points or less! (Because 1000 - 10 = 990).) So, lets analyze them! Block Diagram Below, weve visualized what the process of shipping out ice cream looks like once it has been made. This involves the following several steps: Worker 1 puts ice cream in Freezer Worker 2 loads ice cream into Truck Worker 3 transports ice cream to Store Also, Worker 2 takes the ice cream from the Freezer for loading And Worker 3 drives the ice cream from loading dock to Store Plus, several possible failure modes are involved, as discussed below. Figure 1.4: Ben &amp; Jerrys Ice Cream Block Diagram Failure Modes Well make a tidy data.frame() of each of the ways our block diagram above could fail, which were contained above in failures. Well call this data.frame f. f &lt;- tibble( # Make a vector of routes to failure failure_mode = c( &quot;freezer --&gt; fail_break&quot;, &quot;loading --&gt; fail_time&quot;, &quot;loading --&gt; fail_eat&quot;, &quot;transport --&gt; fail_time&quot;, &quot;transport --&gt; fail_eat&quot;) ) # Worker 2 could leave ice cream out while loading # Worker 2 could eat the ice cream while loading # Worker 3 could leave the ice cream out in transit # Worker 3 could eat the ice cream in transit Calculating Criticality Next, were going to make a few judgement calls, to calculate the overall risk for this FMEA. Estimate Severity Whats the severity of the effects of these failures, on a scale from 1 (low) to 10 (high)? Well mutate() the data.frame to include a new column severity, and save it as a new data.frame f1. fail_break: Its pretty bad it the freezer breaks; that could ruin days worth of product. Lets call that an 8. Not catastrophic, but not good for the company! fail_time: Its not great it a single shipment gets left out and melts while waiting for pickup. But its just one shipment. Lets call that a 5. fail_eat: How much ice cream could one worker really eat? Thats probably a 1. f1 &lt;- f %&gt;% mutate(severity = c(8, 5, 1, 5, 1)) # Check out the contents! f1 ## # A tibble: 5 × 2 ## failure_mode severity ## &lt;chr&gt; &lt;dbl&gt; ## 1 freezer --&gt; fail_break 8 ## 2 loading --&gt; fail_time 5 ## 3 loading --&gt; fail_eat 1 ## 4 transport --&gt; fail_time 5 ## 5 transport --&gt; fail_eat 1 Estimate Occurrence How often does this occur, from 1 (almost never) to 10 (almost always)? Lets rank occurrence as follows: fail_break: Its pretty rare that the freezer would break (eg. 2). fail_time: Its probably somewhat rare that shipments melt (eg. 5). fail_eat: If I were a worker, I would eat that all the time (eg. 8). f2 &lt;- f1 %&gt;% mutate(occurrence = c(2, 5, 8, 5, 8)) Estimate Detection Finally, how likely is it that we would detect the occurrence? If very likely, thats a 1. If very unlikely, thats a 10. fail_break: Workers would very quickly detect if the freezer were broken. (eg. 1). fail_time: You might not know it had melted until the product gets to the store. (eg. 8) fail_eat: Might get caught. Low chance. (eg. 3). f3 &lt;- f2 %&gt;% mutate(detection = c(1, 8, 3, 8, 3)) Estimate Criticality (RPN) Using our data in f3, lets estimate criticality (aka RPN, the risk priority number). f4 &lt;- f3 %&gt;% mutate(criticality = severity * occurrence * detection) We can add up the criticality/RPN to estimate the total risk priority, out of 1000, which is the max_criticality possible. We can divide these two to get the probability of system failure. Is that risk greater than 0.010, aka 0.1%? If so, bad news! f4 %&gt;% summarize( total_criticality = sum(criticality), max_criticality = 10*10*10, probability = total_criticality / max_criticality) ## # A tibble: 1 × 3 ## total_criticality max_criticality probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 464 1000 0.464 Well, thats not good! Looks like the new factory will need to figure out a way to keep their product from melting! (In reality, Im sure Ben and Jerrys has strict quality control!) Learning Check 1 Question What other ways could failure occur here? Add three more kinds of failure to your tibble f, then estimate their severity, occurence, detection, and criticality, and recalculate the total probability of failure at this ice cream plant. [View Answer!] fprime &lt;- tibble( # Make a vector of routes to failure failure_mode = c( &quot;freezer --&gt; fail_break&quot;, &quot;loading --&gt; fail_time&quot;, &quot;loading --&gt; fail_eat&quot;, &quot;transport --&gt; fail_time&quot;, &quot;transport --&gt; fail_eat&quot;, # Technically, worker 1 could eat it before taking it to freezer &quot;w1 --&gt; fail_eat&quot;, # A fourth worker at the store could eat it before delivering it &quot;w4 --&gt; fail_eat&quot;, # The fourth worker could also leave it out! &quot;w4 --&gt; fail_time&quot;) ) %&gt;% # Estimate quantities of interest mutate(severity = c(8, 5, 1, 5, 1, 1, 1, 5), occurrence = c(2, 5, 8, 5, 8, 8, 8, 5), detection = c(1, 8, 3, 8, 3, 3, 3, 8)) %&gt;% # Calculate criticality mutate(criticality = severity * occurrence * detection) # Let&#39;s check it out! fprime ## # A tibble: 8 × 5 ## failure_mode severity occurrence detection criticality ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 freezer --&gt; fail_break 8 2 1 16 ## 2 loading --&gt; fail_time 5 5 8 200 ## 3 loading --&gt; fail_eat 1 8 3 24 ## 4 transport --&gt; fail_time 5 5 8 200 ## 5 transport --&gt; fail_eat 1 8 3 24 ## 6 w1 --&gt; fail_eat 1 8 3 24 ## 7 w4 --&gt; fail_eat 1 8 3 24 ## 8 w4 --&gt; fail_time 5 5 8 200 # Let&#39;s calculate the total risk! fprime %&gt;% summarize( total_criticality = sum(criticality), max_criticality = 10*10*10, probability = total_criticality / max_criticality) ## # A tibble: 1 × 3 ## total_criticality max_criticality probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 712 1000 0.712 Ooph! Not good! Conclusion Great work! All done! See you in class! "],["workshop-distributions-and-descriptive-statistics.html", "2 Workshop: Distributions and Descriptive Statistics Getting Started 2.1 Distributions Learning Check 1 2.2 Descriptive Statistics Learning Check 2 2.3 Describing Shape Learning Check 3 2.4 Simulating Distributions 2.5 Comparing Distributions Learning Check 4 Conclusion", " 2 Workshop: Distributions and Descriptive Statistics Figure 2.1: Yay Distributions! This tutorial will introduce you to how to code and analyses distributions in R, using descriptive statistics and visualization! Getting Started Please open up last weeks RStudio.Cloud project. Start a new R script (File &gt;&gt; New &gt;&gt; R Script). Save the R script as workshop_2.R. And lets get started! Load Packages Were going to use extra functions from 3 packages today, including ggplot2, dplyr (pronounced DIP-LER), and MASS. [Note: Please be sure to load them first, otherwise your functions will not work.] library(ggplot2) # for visualization library(dplyr) # for pipelines! library(MASS) # for fitting distributions The Pipeline Well also be using a new coding symbol today, %&gt;%, called a pipeline. Pipelines let us connect data to functions, with fewer parentheses! Figure 1.2: Old-School Pipeline For example: # let&#39;s make a vector ```x``` and do some operations on it. x &lt;- c(1,2,3) # These are the same! mean(x) ## [1] 2 x %&gt;% mean() ## [1] 2 Using pipelines keeps our code neat and tidy. It lets us run long sequences of code without saving it bit by bit as objects. For example, we can take them mean()`` ofx*and* then get thelength()``` of the resulting vector, all in one sequence. Without a pipeline, you end up in parenthesis hell very quickly. # without pipe length(mean(x)) ## [1] 1 # with pipe x %&gt;% mean() %&gt;% length() ## [1] 1 Handy, right? To simplify things, theres a special hotkey shortcut for making pipelines too. In Windows and Linux, use Ctrl Shift M. In Mac, use Cmd Shift M. 2.1 Distributions Any vector can be expressed as a distribution (especially numeric vectors). A distribution stacks the values in a vector in order from lowest to highest to show the frequency of values. There are several ways to visualize distributions, including histograms, density plots, violin plots, jitter plots, ribbon plots, and more; the most common are histograms and density plots, which we will learn today. For example, Figure 2 shows our seawall vector from Workshop 1 in part A (left). In part B (right), that vector is shown as a distribution: its blocks are stacked to make a histogram (bars), while the distribution itself (line) is approximated by a curve, known as a density function. Figure 2.2: Figure 2: Seawall Vector as a Distribution Any distribution can be described with 4 traits, shown above in part C. These include: Size (how many values are in it), Location (eg. where is it clumped), Spread (how much do values vary?), and Shape (eg. bell curve). Learning Check 1 Question Using the hist() function we learned before, draw the histogram of this vector of seawalls, naming the vector sw! [View Answer!] Using the hist() function we learned before, draw the histogram of this vector of seawalls, naming the vector sw! # Many options! # You could code it as a vector, save it as an object, then use your functions! sw &lt;- c(4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4) sw %&gt;% hist() # or you could do it like this! # hist(sw) 2.2 Descriptive Statistics Whats a statistic? A statistic is a single number that summarizes something about a sample. Thats it! No magic! Statistics is the process of making statistics (eg. many single numbers) so we can understand samples of data! They help people make decisions when faced with uncertainty. Well learn several functions to make statistics that describe our distributions. Trait Meaning Type Functions Size How many values? statistics length() Location Where is it clumped? statistics mean(), median() Spread How much do values vary? statistics sd(), var(), range(), quantile() Shape What shape does it resemble? distributions rnorm(), rbinom(), rpois(),[skewness &amp; kurtosis - no functions] Click on the tabs below to review our functions for Size, Location, and Spread from Workshop 1. (Well get to shape in a minute.) 2.2.1 Size How big is our sample? Use length() on a vector to find the number of values in the vector sw we made in LC1. length(sw) ## [1] 10 2.2.2 Location Where is our sample clumped? Figure 2.3: Figure 3: Statistics for Location Use mean() and median() to find the most central values. sw %&gt;% mean() ## [1] 5.35 sw %&gt;% median() ## [1] 5.25 Fun fact: mode() doesnt work in R; its huge pain. You have to use this code instead. sw %&gt;% table() %&gt;% sort(decreasing = TRUE) ## . ## 5 5.5 6.5 4 4.5 6 ## 3 2 2 1 1 1 2.2.3 Spread (1) How much does our sample vary? Figure 2.4: Figure 4: Statistics for Spread Use quantile() to check for any percentile in a vector, from 0 (min) to 0.5 (median) to 1 (max). If you have quantile(), you dont need to remember min(), max(), range(), or even median(). sw %&gt;% quantile(probs = 0) # min ## 0% ## 4 sw %&gt;% quantile(probs = 1) # max ## 100% ## 6.5 2.2.4 Spread (2) 2.2.4.1 Standard Deviation But we can also evaluate how much our values vary from the mean on average - the standard deviation, often abbreviated as \\(\\sigma\\) (sigma). This is written as: $ = $ Figure 2.5: Figure 5: Standard Deviation, the ultimate Statistic for Spread We can calculate this by hand, or use the sd() function. # Calculating in R still faster than on your own! sqrt( sum((sw - mean(sw))^2) / (length(sw) - 1) ) ## [1] 0.8181958 # Get the standard deviation by code! sw %&gt;% sd() ## [1] 0.8181958 2.2.4.2 Variance Sometimes, we might want the variance, which is the standard deviation squared. This accentuates large deviations in a sample. # Get the variance! sw %&gt;% var() ## [1] 0.6694444 sd(sw)^2 ## [1] 0.6694444 # See? var = sd^2! 2.2.4.3 Coefficient of Variation (CV) We could also calculate the coefficient of variation (CV), meaning how great a share of the mean does that average variation constitute? (Also put, how many times does the mean fit into the standard deviation.) sd(sw) / mean(sw) ## [1] 0.1529338 The standard deviation constitutes 15% of the size of the mean seawall height. 2.2.4.4 Standard Error (SE) But these numbers dont have much meaning to us, unless we know seawalls really well. Wouldnt it be nice if we had a kind of uniform measure, that told us how big is the variation in the data, given how big the data is itself? Good news! We do! We can calculate the sample size-adjusted variance like so: var(sw) / length(sw) ## [1] 0.06694444 # or sd(sw)^2 / length(sw) ## [1] 0.06694444 This means we could take this set of seawalls and compare it against samples of coastal infrastructure in Louisiana, in Japan, in Australia, and make meaningful comparisons, having adjusted for sample size. However, sample-size adjusted variance is a little bit of a funky concept, and so its much more common for us to use the sample-size adjusted standard deviation, more commonly known as the standard error, or se. $ SE = = = $ # Calculated as: se &lt;- sd(sw) / sqrt(length(sw)) # Or as: se &lt;- sqrt( sd(sw)^2 / length(sw) ) # Or as: se &lt;- sqrt( var(sw) / length(sw)) # See standard error se ## [1] 0.2587362 Learning Check 2 Question Suppose we collected data on 10 randomly selected chunks of cheese from a production line! We measured their moisture in grams (g) in each product We want to make sure were making some quality cheesy goodness, so lets find out how much those moisture (cheesiness) levels vary! The moisture in our cheese weighed 5.52 g, 5.71 g, 5.06 g, 5.10 g, 4.98 g, 5.50 g, 4.81 g, 5.55 g, 4.74 g, &amp; 5.39 g. Please convert the following values into a vector named cheese! How much did moisture levels vary, on average? We need to compare these levels with cheese produced in Vermont, France, and elsewhere. Whats the coefficient of variance and standard error for these moisture levels? [View Answer!] Please convert the following values into a vector named cheese! cheese &lt;- c(5.52, 5.71, 5.06, 5.10, 4.98, 5.50, 4.81, 5.55, 4.74, 5.39) How much did moisture levels vary, on average? We need to compare these levels with cheese produced in Vermont, France, and elsewhere. Whats the coefficient of variance and standard error for these moisture levels? # Coefficient of variation cv &lt;- sd(cheese) / mean(cheese) # Check it! cv ## [1] 0.06491759 # Standard Error se &lt;- sd(cheese) / sqrt(length(cheese)) # Check it se ## [1] 0.1074885 # When you&#39;re finished, remove extra data. remove(cheese, se, cv) 2.3 Describing Shape How then do we describe the shape of a distribution? We can use skewness and kurtosis for this. Theres no direct function for skewness or kurtosis in R, but as youll see below, we can quickly calculate it using the functions we already know. 2.3.1 Skewness Skewness describes whether the bulk of the distribution sits to the left or right of the center, and its formula are written out below. It is commonly estimated using the formula on the left, while the formula on the right closely approximates it. (Were going to use the right-hand formula below, since its a little cleaner.) \\[ Skewness = \\frac{ \\sum^{N}_{i=1}{(x - \\bar{x})^{3} / n } }{ [\\sum^{N}_{i=1}{ (x - \\bar{x})^{2} / n }]^{3/2} } \\approx \\frac{ \\sum^{N}_{i=1}{ (x - \\bar{x})^{3} } }{ (n - 1) \\times \\sigma^{3} } \\] When people say that a certain persons perspective is skewed, they mean, its very far from the mean. In this case, we want to know, how skewed are the heights of seawalls overall compared to the mean? To figure this out, well need 4 ingredients: \\(x_{i \\to N}\\): our vector of values (seawall heights! sw) \\(N\\): the length of our vector (how many seawalls? length(sw)) \\(\\bar{x}\\): our mean value: (the mean seawall height? mean(sw)) \\(\\sigma\\): the standard deviation of our vector (how much do the seawall heights vary on average? sd(sw)) Yeah! You just used them a bunch! So lets calculate skewness! First, we measure diff, how far is each value from the mean? diff &lt;- sw - mean(sw) # Check it out! diff ## [1] -0.85 -0.35 0.15 -0.35 0.15 1.15 1.15 0.65 -0.35 -1.35 diff measures how far / how skewed each of these values (\\(x\\)) are from the mean \\(\\bar{x}\\)). See the visual below! Next, were going to cube diff, to emphasize extreme differences from the mean Squaring would turn everything positive, but we care whether those differences are positive or negative, so we cube it instead. diff^3 ## [1] -0.614125 -0.042875 0.003375 -0.042875 0.003375 1.520875 1.520875 ## [8] 0.274625 -0.042875 -2.460375 Then, were going to get a few helper values, like: # Get the sample-size # To be conservative, we&#39;ll subtract 1; this happens often in stats n &lt;- length(sw) - 1 # Get the standard deviation sigma &lt;- sw %&gt;% sd() Now, we can calculate, on average, how big are these cubed differences? sum(diff^3) / n ## [1] 0.01333333 Well, thats nifty, how do we compare this funky number to other samples? Were going to need to put it in terms of a common unit, a standard unit - like the standard deviation! Plus, well have to cube the standard deviation, so that its in the same terms as our numerator \\(diff^{3}\\). skew &lt;- sum(diff^3) / ( n * sigma^3) # Check it! skew ## [1] 0.0243426 Voila! A standardized measure you can use to compare the skew of our sample of seawalls to any other sample! For comparison, here are a few other values of skew we might possibly get. 2.3.2 Kurtosis Kurtosis describes how tightly bound the distribution is around the mean. Is it extremely pointy, with a narrow distribution (high kurtosis), or does it span wide (low kurtosis)? We can estimate it using the formula on the left, and the formula on the right is approximately the same. \\[ Kurtosis = \\frac{ \\sum^{N}_{i=1}{(x - \\bar{x})^{4} / n } }{ [\\sum^{N}_{i=1}{ (x - \\bar{x})^{2} / n }]^2 } \\approx \\frac{ \\sum^{N}_{i=1}{ (x - \\bar{x})^{4} } }{ (n - 1) \\times \\sigma^{4} } \\] Like skew, we calculate how far each value is from the mean, but we take those differences to the 4th power (\\((x - \\bar{x})^{4}\\)), which hyper-accentuates any extreme deviations and returns only positive values. Then, we calculate the sample-size adjusted average of those differences. Finally, to measure it in a consistent unit comparable across distributions, we divide by the standard deviation taken to the 4th power; the powers in the numerator and denominator then more-or-less cancel each other out. moments::skewness(sw) ## [1] 0.02565935 # 0.2565 x &lt;- sw sum( (x - mean(x))^3 ) / ((length(x) - 1) *sd(x)^3) ## [1] 0.0243426 a &lt;- sum( (x - mean(x))^3 ) / length(x) b &lt;- (sum( (x - mean(x))^2 ) / length(x))^(3/2) a/b ## [1] 0.02565935 # 0.256 # Get the differences again diff &lt;- sw - mean(sw) # And take them to the fourth power diff^4 ## [1] 0.52200625 0.01500625 0.00050625 0.01500625 0.00050625 1.74900625 ## [7] 1.74900625 0.17850625 0.01500625 3.32150625 Theyre all positive! Next, same as above, well get the conservative estimate of the sample size (n - 1) and the standard deviation. # Get the sample-size # To be conservative, we&#39;ll subtract 1; this happens often in stats n &lt;- length(sw) - 1 # Get the standard deviation sigma &lt;- sw %&gt;% sd() So when we put it all together kurt &lt;- sum(diff^4) / ( n * sigma^4) # Check it! kurt ## [1] 1.875851 We can measure kurtosis! A pretty normal bell curve has a kurtosis of about 3, so our data doesnt demonstrate much kurtosis. Kurtosis ranges from 0 to infinity (it is always positive), and the higher it goes, the pointier the distribution! Finally, just a heads up: As mentioned above, there are a few different formulas floating around there for skewness and kurtosis, so dont be too surprised if your numbers vary when calculating it in one package versus another versus by hand. (But, if the numbers are extremely different, thats probably a sign something is up.) Learning Check 3 A contractor is concerned that the majority of seawalls in her region might skew lower than their regions vulnerability to storms requires. Assume (hypothetically) that our samples seawalls are the appropriate height for our level of vulnerability, and that both regions share the same level of vulnerability. The mean seawall in her region is about the same height as in our sample (~5.35), but how do the skewness and kurtosis of her regions seawalls compare to our sample? Her region has 12 seawalls! Their height (in meters) are 4.15, 4.35, 4.47, 4.74, 4.92, 5.19, 5.23, 5.35, 5.55, 5.70, 5.78, &amp; 7.16. Question Calculate these statistics and interpret your results in a sentence or two. [View Answer!] # Make a vector of these 12 seawalls x &lt;- c(4.15, 4.35, 4.47, 4.74, 4.92, 5.19, 5.23, 5.35, 5.55, 5.70, 5.78, 7.16) # Calculate skewness skewness &lt;- sum( (x - mean(x))^3) / ((length(x) - 1) * sd(x)^3) # Calculate Kurtosis kurtosis &lt;- sum( (x - mean(x))^4) / ((length(x) - 1) * sd(x)^4) # View them! c(skewness, kurtosis) ## [1] 0.9016585 3.5201492 Her regions seawalls are somewhat positively, right skewed, with a skewness of about +0.90. This is much more skewed than our hypothetical areas seawalls, which are skewed at just +0.02. But, her regions seawalls traits are much more closely clustered around the mean than ours, with a kurtosis of 3.52 compared to our 1.88. Since both hypothetical regions have comparable levels of vulnerability to storm surges, her regions seawalls do appear to skew low. 2.4 Simulating Distributions Finally, to describe shape, we need some shapes to compare our distributions to. Fortunately, the rnorm(), rbinom(), rpois(), and rgamma() functions allow us to draw the shapes of several common distributions. Table 2 shows the shape of these distributions, and their ranges. (#tab:w2_table2)Table 2: Example Distributions Distributions Span Function Parameters Example Resources Normal -Inf to +Inf rnorm() mean, sd .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Poisson 0, 1, 2, 3 rpois() lambda (mean) .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Gamma 0.1, 2.5, 5.5, +Inf rgamma() shape, rate .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Exponential same rexp() rate .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Weibull same rweibull() shape, scale .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Binomial 0 vs. 1 rbinom() probability .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Uniform min to max runif() min, max .svglite line, .svglite polyline, .svglite polygon, .svglite path, .svglite rect, .svglite circle { fill: none; stroke: #000000; stroke-linecap: round; stroke-linejoin: round; stroke-miterlimit: 10.00; } .svglite text { white-space: pre; } Wiki Note: Wikipedia is actually a pretty fantastic source on distributions. To determine what kind of distribution our vector has, we can visually compare it using simulation. We can compare our real observed distribution against random distributions to determine whether our data matches the shape of a normal vs. poisson distribution, for example. To do so, lets get some statistics from our data to help us visualize what a distribution with those traits would look like. As our raw data, lets use our vector of seawall heights sw. # Let&#39;s remake again our vector of seawall heights sw &lt;- c(4.5, 5, 5.5, 5, 5.5, 6.5, 6.5, 6, 5, 4) To simulate, you feed your simulator function (1) n values to draw and (2) any required statistics necessary for computing draws from that distribution. (For example, rnorm() requires the mean and standard deviation.) Fortunately, statisticians have figured out for us 2 ways to figure out what statistics to provide. First, there are a few equations called method of moments estimators that do a great job of estimating those statistics. Alternatively, we can ask R to compute the values of those statistics using the MASS packages fitdistr(). Well show both below. 2.4.1 Normal Distribution rnorm() randomly generates for us any number of values randomly sampled from a normal distribution. We just need to supply: (1) n values to draw, (2) the mean of that distribution, and (3) the sd of that distribution. # For example mymean &lt;- sw %&gt;% mean() mysd &lt;- sw %&gt;% sd() # simulate! mynorm &lt;- rnorm(n = 1000, mean = mymean, sd = mysd) # Visualize! mynorm %&gt;% hist() How do mymean and mysd compare to the fitdistr() estimates? mymean ## [1] 5.35 mysd ## [1] 0.8181958 Lets compare with fitdistr()! sw %&gt;% fitdistr(densfun = &quot;normal&quot;) ## mean sd ## 5.3500000 0.7762087 ## (0.2454588) (0.1735655) Pretty darn close! Great! 2.4.2 Poisson Distribution rpois() randomly samples integers (eg. 0, 1, 2, 3) from a poisson distribution, based on lambda, the average rate of occurrence. We can approximate that by taking the mean of sw. mypois &lt;- rpois(1000, lambda = mymean) mypois %&gt;% hist() Results in a somewhat skewed distribution, bounded at zero. So how well did mymean match up with the best estimate of lambda from fitdistr()? mymean ## [1] 5.35 sw %&gt;% fitdistr(densfun = &quot;Poisson&quot;) ## lambda ## 5.3500000 ## (0.7314369) Excellent! 2.4.3 Gamma Distribution rgamma() randomly samples positive real numbers greater than zero from a gamma distribution. Its like the continuous version of rpois(). It requires 2 paramters, shape and rate. You can estimate these a couple of different ways, but a simple one is to use the method of moments, which says that: shape \\(\\approx \\frac{mean^{2}}{ variance}\\). scale \\(\\approx \\frac{variance}{ mean }\\). # For shape, we want the rate of how much greater the mean-squared is than the variance. myshape &lt;- mean(sw)^2 / var(sw) # For rate, we like to get the inverse of the variance divided by the mean. myrate &lt;- 1 / (var(sw) / mean(sw) ) # Simulate it! mygamma &lt;- rgamma(1000, shape = myshape, rate = myrate) ## View it! mygamma %&gt;% hist() So how well did myshape and myrate match up with the best-fit estimates from fitdistr()? myshape ## [1] 42.7556 myrate ## [1] 7.991701 sw %&gt;% fitdistr(densfun = &quot;gamma&quot;) ## shape rate ## 46.711924 8.731202 ## (20.815512) ( 3.911664) Pretty good! In the same ballpark. 2.4.4 Exponential Distribution rexp() randomly simulates positive real numbers over zero from an exponential distribution. Here, the method of moments says: rate \\(\\approx \\frac{ 1 }{ mean })\\) # We&#39;ll name this myrate2! myrate_e &lt;- 1 / mean(sw) # Simulate it! myexp &lt;- rexp(n = 1000, rate = myrate_e) # Visualize it! myexp %&gt;% hist() How then did myrate_e match up with the estimates from fitdistr()? myrate_e ## [1] 0.1869159 sw %&gt;% fitdistr(densfun = &quot;exponential&quot;) ## rate ## 0.18691589 ## (0.05910799) Pretty solid! 2.4.5 Weibull Distribution rweibull() randomly samples positive real numbers over zero too, but from a Weibull distribution. It requires a shape and scale parameter, but its method of moments equation is pretty complex. Once we get into Weibull distribution territory, its better to just use fitdistr(). mystats &lt;- sw %&gt;% fitdistr(densfun = &quot;weibull&quot;) # Here, we&#39;re going to extract the estimate for shape myshape_w &lt;- mystats$estimate[1] # and the estimate for scale myscale_w &lt;- mystats$estimate[2] # simulate! myweibull &lt;- rweibull(n = 1000, shape = myshape_w, scale = myscale_w) # View it! myweibull %&gt;% hist() 2.4.6 Binomial Distribution Next, the binomial distribution is a bit of a special case, in that its mostly only helpful for binary variables (with values 0 and 1). But lets try an example anyways. rbinom() randomly draws n simulated values from a set of provided values at a given probability (prob). Its usually used for drawing binary variables (0 and 1); a coin flip would have prob = 0.5, or a 50-50 chance. (This time we cant use fitdistr().) rbinom(n = 10, size = 1, prob = 0.5) ## [1] 1 0 0 0 0 0 1 0 1 0 To get a meaningful simulation, maybe we calculate the proportion of values that are greater than the mean. # In how many cases was the observed value greater than the mean? myprob &lt;- sum(sw &gt; mymean) / length(sw) # Sample from binomial distribution with that probability mybinom &lt;- rbinom(1000, size = 1, prob = myprob) # View histogram! mybinom %&gt;% hist() 2.4.7 Uniform Distribution Finally, the uniform distribution is also a special case. The frequency of values in a uniform distribution is more-or-less uniform. It also only spans the length of a specified interval \\(a \\to b\\). A common range is a = 0 to b = 1. So, the frequency of 1.5 in that interval would be zero. # Simulate a uniform distribution ranging from 0 to 1 myunif &lt;- runif(n = 1000, min = 0, max = 1) # View histogram! myunif %&gt;% hist(xlim = c(-0.5,1.5)) 2.5 Comparing Distributions Finally, were going to want to outfit those vectors in nice data.frames (skipping rbinom() and runif()), and stack them into 1 data.frame to visualize. We can do this using the bind_rows() function from the dplyr package. # Using bind_rows(), mysim &lt;- bind_rows( # Make a bunch of data.frames, all with the same variable names, data.frame(x = sw, type = &quot;Observed&quot;), # and stack them! data.frame(x = mynorm, type = &quot;Normal&quot;), # and stack it! data.frame(x = mypois, type = &quot;Poisson&quot;), # Stack, stack, stack stack it! data.frame(x = mygamma, type = &quot;Gamma&quot;), # so many stacks! data.frame(x = myexp, type = &quot;Exponential&quot;), # so much data!!!!!! data.frame(x = myweibull, type = &quot;Weibull&quot;) ) Next, we can visualize those distributions using geom_density() in ggplot (or geom_histogram(), really, if that floats your boat). # Let&#39;s write the initial graph and save it as an object g1 &lt;- ggplot(data = mysim, mapping = aes(x = x, fill = type)) + geom_density(alpha = 0.5) + labs(x = &quot;Seawall Height (m)&quot;, y = &quot;Density (Frequency)&quot;, subtitle = &quot;Which distribution fits best?&quot;, fill = &quot;Type&quot;) # Then view it! g1 Personally, I cant read much out of that, so it would be helpful to narrow in the x-axis a bit. We can do that with xlim(), narrowing to just between values 0 and 10. g1 + xlim(0,10) Beautiful! Wow! It looks like the Normal, Gamma, and Weibull distributions all do a pretty excellent job of matching the observed distribution. &lt;br. Learning Check 4 Question Youve been recruited to evaluate the frequency of Corgi sightings in the Ithaca Downtown. A sample of 10 students each reported the number of corgis they saw last Tuesday in town. Using the method of moments (or fitdistr() for Weibull) and ggplot(), find out which type of distribution best matches the observed corgi distribution! Beth saw 5, Javier saw 1, June saw 10(!), Tim saw 3, Melanie saw 4, Mohammad saw 3, Jenny say 6, Yosuke saw 4, Jimena saw 5, and David saw 2. [View Answer!] First, lets get the stats. # Make distribution of Corgis corgi &lt;- c(5, 1, 10, 3, 4, 3, 6, 4, 5, 2) # Compute statistics for distributions corgi_mean &lt;- mean(corgi) corgi_sd &lt;- sd(corgi) corgi_shape &lt;- mean(corgi)^2 / var(corgi) corgi_rate &lt;- 1 / (var(corgi) / mean(corgi) ) corgi_rate_e &lt;- 1 / mean(corgi) # For Weibull, use fitdistr() from MASS package corgi_stats &lt;- corgi %&gt;% fitdistr(densfun = &quot;weibull&quot;) corgi_shape_w &lt;- corgi_stats$estimate[1] corgi_scale_w &lt;- corgi_stats$estimate[2] Next, lets bind them together. corgisim &lt;- bind_rows( # Get observed vector data.frame(x = corgi, type = &quot;Observed&quot;), # Get normal dist data.frame(x = rnorm(1000, mean = corgi_mean, sd = corgi_sd), type = &quot;Normal&quot;), # Get poisson data.frame(x = rpois(1000, lambda = corgi_mean), type = &quot;Poisson&quot;), # Get gamma data.frame(x = rgamma(1000, shape = corgi_shape, rate = corgi_rate), type = &quot;Gamma&quot;), # Get exponential data.frame(x = rexp(1000, rate = corgi_rate_e), type = &quot;Exponential&quot;), # Get weibull data.frame(x = rweibull(1000, shape = corgi_shape_w, scale = corgi_scale_w), type = &quot;Weibull&quot;) ) Finally, lets visualize it! # Visualize! ggplot(data = corgisim, mapping = aes(x = x, fill = type)) + geom_density(alpha = 0.5) + # Narrow it to 0 to 15 to suit plot xlim(0,15) + labs(x = &quot;Corgi Sightings!&quot;, y = &quot;Density (Frequency)&quot;) Neat - looks like the Poisson, Gamma, and Weibull function match well, although the Poisson looks pretty odd! Conclusion So now, you know how to use descriptive statistics in R, how to visualize and evaluate a distribution, and how to simulate several different types of distributions! Youre well on your way to some serious stats for systems engineering! "],["skill-functions-in-r.html", "Skill: Functions in R Getting Started Coding your own function! Functions with Default Inputs", " Skill: Functions in R Weve learned how to use built-in R functions like dnorm() and pnorm() to analyze distributions, but sometimes its going to be more helpful to be able to (A) do the math by hand or (B) code your function to do it. So, lets learn how in the world you do that! Getting Started Packages Well be using the tidyverse package, a super-package that auto-loads dplyr, ggplot2, and other common functions. library(tidyverse) Coding your own function! Functions are machines that do a specific calculation using an input to produce a specific output. Below, well write an example function, called add(a, b). This function takes two numeric values, a and b, as inputs, and adds them together. Using function(), well tell R that our function contains two inputs, a and b. Then, using { ... }, well put the action we want R to do in there. The function can involve multiple operations inside it. But at the end, you need to print one final output, or put return() around your output. # Make function add &lt;- function(a, b){ # Compute and directly output a + b } add(1, 2) ## [1] 3 # This also works add &lt;- function(a, b){ # Assign output to a temporary object output &lt;- a + b # Return the temporary object &#39;output&#39; return(output) } # add(1, 2) ## [1] 3 Functions with Default Inputs You can also assign default input values to your function. Below, we write that by default, b = 2. If we supply a different b, the default will get overwritten, but otherwise, we wont need to supply b. add = function(a, b = 2){ a + b } Lets try it! # See? I only need to write &#39;a&#39; now add(1) ## [1] 3 # But if I write &#39;b&#39; too.... add(1, 2) ## [1] 3 # And if I change &#39;b&#39;... add(1, 3) ## [1] 4 # It will adjust accordingly # clear data remove(add) Great! Lets go make some functions! "],["workshop-probability.html", "3 Workshop: Probability Getting Started 3.1 Probability 3.2 Probability Functions Learning Check 1 Hypothetical Probability Functions Learning Check 2 Conclusion", " 3 Workshop: Probability This tutorial will introduce you to probability and how to code and visualize probabilistic analyses in R! Getting Started Please open up your RStudio.Cloud project. Start a new R script (File &gt;&gt; New &gt;&gt; R Script). Save the R script as workshop_3.R. And lets get started! Load Packages In this tutorial, were going to use more of the dplyr and ggplot2 packages, plus the broom package and mosaicCalc package. library(dplyr) library(ggplot2) library(broom) library(mosaicCalc) Key Functions Were going to use 3 functions a lot below. This includes bind_rows(), mutate(), and summarize(). So what are they? bind_rows(): stacks 2 or more data.frames on top of each other, matching columns by name. mutate(): creates or edits variables in a data.frame. summarize(): consolidates many rows of data into a single summary statistic (or a set of them.) Examples How might we use bind_rows()? mycoffee &lt;- bind_rows( # Make first data.frame data.frame( # Containing these vectors style and price style = c(&quot;latte&quot;, &quot;cappuccino&quot;, &quot;americano&quot;), price = c(5, 4, 3)), # Make second data.frame data.frame( # Containing these vectors style, price, and shop style = c(&quot;coffee&quot;, &quot;hot cocoa&quot;), price = c(3, 2), shop = c(&quot;Gimme Coffee&quot;, &quot;Starbucks&quot;)) ) # Notice how they stack, # but in first data.frame values, # shop gets filled with NA, # since it wasn&#39;t in first dataframe mycoffee ## style price shop ## 1 latte 5 &lt;NA&gt; ## 2 cappuccino 4 &lt;NA&gt; ## 3 americano 3 &lt;NA&gt; ## 4 coffee 3 Gimme Coffee ## 5 hot cocoa 2 Starbucks How might we use mutate()? mycoffee &lt;- mycoffee %&gt;% # Add a new vector (must be of same length as data.frame) # vector is number of those drinks purchased mutate(purchased = c(5, 4, 10, 2, 1)) How might we use summarize()? mycoffee %&gt;% # Summarize data.frame into one row summarize( # Calculate mean price of drinks mean_price = mean(price), # Calculate total drinks purchased total_purchased = sum(purchased)) ## mean_price total_purchased ## 1 3.4 22 Great! Now lets apply these to probability! 3.1 Probability Probability refers to how often a specify event is expected to occur, given a sufficient number of times. Were going to learn (and compute!) several common probability formula in R. 3.1.1 Conditional Probability Conditional Probability: probability of two events happening together reflects the probability of the first happening, times the probability of the second happening given that the first has already occurred. \\[ P(AB) = P(A) \\times P(B|A)\\] In other words, if two events are interdependent, you multiply. 3.1.2 Example: Twizzlers (#fig:img_twizzlers)Twizzlers vs. Red Vines Youve been hired by the Hersheys Chocolate Company to investigate quality control on their Twizzlers sweets packaging line. At the start of an assembly line, you mixed in 8,000 Red Vines with a sample of 10,000 Twizzlers. Whats the probability of a packer picking up a Red Vine on the assembly line? sweets &lt;- data.frame( # We know there are 10,000 twizzlers twizzlers = 10000, # and 8,000 redvines redvines = 8000) %&gt;% # So together, there are 18,000 sweets available # So there&#39;s a 8000-in-18,000 chance of picking a redvine mutate(prob_1 = redvines / (twizzlers + redvines)) # Check it! sweets ## twizzlers redvines prob_1 ## 1 10000 8000 0.4444444 But then, whats the probability of a packer picking up 2 Red Vines in a row on the assembly line? sweets %&gt;% # After picking 1 Red Vine, # there&#39;s now 1 less Red Vine in circulation mutate( # Subtract 1 redvine redvines = redvines - 1, # Recalculate total total = twizzlers + redvines) %&gt;% # calculate probability of picking a second red vine now that 1 is gone mutate(prob_2 = redvines / total) %&gt;% # Finally, multiply the first and second probability together # When it&#39;s this AND that, you multiply mutate(prob = prob_1 * prob_2) ## twizzlers redvines prob_1 total prob_2 prob ## 1 10000 7999 0.4444444 17999 0.4444136 0.1975171 Alternatively, if two events are independent (mutually exclusive), meaning they do not affect each other, you add those probabilities together. You dump in a 1000 pieces of Black Licorice. If a packer picks up 2 sweets, whats the probability its a piece of Black Licorice or Red Vines? sweets %&gt;% # Add a column for black licorice mutate(black_licorice = 1000) %&gt;% # Get total mutate(total = twizzlers + redvines + black_licorice) %&gt;% # Recompute probabilities mutate(prob_1 = redvines / total, prob_2 = black_licorice / total) %&gt;% # When it&#39;s this OR that, you add probabilities mutate(prob_3 = prob_1 + prob_2) ## twizzlers redvines prob_1 black_licorice total prob_2 prob_3 ## 1 10000 8000 0.4210526 1000 19000 0.05263158 0.4736842 remove(mycoffee, sweets) Total Probabilities We can also examine total probabilities. Any event \\(A\\) that is mutually exclusive from event \\(E\\) (cant happen at the same time) has the following probability \\[ P(A) = \\sum_{i=1}^{n}{P(A|E_{i}) \\times P(E_{i}) } \\] Example: Marbles! (#fig:img_marbles)So many marbles! Youve got 3 bags (\\(E_{1 \\to 3}\\)), each containing 3 marbles, each with a different split of red vs. blue marbles. If we choose a bag at random and sample a marble at random (2 mutually exclusive events), whats the probability that marble will be red (\\(P(A)\\))? I like to map these out, so I understand visually what all the possible pathways are. Heres a chart I made (using mermaid), where Ive diagrammed each possible set of actions, like choosing Bag 1 then Marble a (1 pathway), choosing Bag 1 then Marble b (a second pathway), etc. If we look at the ties to the marbles, youll see I labeled each tie to a red marble as 1 and each tie to a blue marble as 0. If we add these pathways up, well get the total probability: 0.67 (aka 2/3). (#fig:img_mermaid)Drawing Probability Diagrams But what if we cant diagram it out? Perhaps were choosing from 100s of marbles, or were limited on time! How would we solve this problem mathematically? The key here is knowing that: the blue marbles dont really matter we need the probability of choosing a bag we need the probability of choosing a red marble in each bag. Heres what we know: Theres an equal chance of choosing any bag of the 3 bags (because random). (If 1 bag were on a really high shelf, then maybe the probabilities would be different, i.e. not random, but lets assume theyre random this time.) # there are 3 bags n_bags &lt;- 3 # So.... # In this case, P(Bag1) = P(Bag2) = P(Bag3) # and P(Bag1) + P(Bag2) + P(Bag3) = 100% = 1.0 # 1/3 chance of picking Bag 1 # written P(Bag1) pbag1 &lt;- 1 / n_bags # 1/3 chance of picking Bag 2 # written P(Bag2) pbag2 &lt;- 1 / n_bags # 1/3 chance of picking Bag 3 # written P(Bag3) pbag3 &lt;- 1 / n_bags # Check it! c(pbag1, pbag2, pbag3) ## [1] 0.3333333 0.3333333 0.3333333 There are 3 marbles in each bag. # Total marbles in Bag 1 m_bag1 &lt;- 3 # Total marbles in Bag 2 m_bag2 &lt;- 3 # Total marbles in Bag 3 m_bag3 &lt;- 3 There are 3 red marbles in bag 1, 1 red marbles in bag 2, and 2 red marbles in bag 3. # So, we can calculate the percentages in each bag. # percentage of red marbles in Bag 1 # written P(Red|Bag1) pm_bag1 &lt;- 3 / m_bag1 # percentage of red marbles in Bag 2 # written P(Red|Bag2) pm_bag2 &lt;- 1 / m_bag2 # percentage of red marbles in Bag 3 # written P(Red|Bag3) pm_bag3 &lt;- 2 / m_bag3 # Check it! c(pm_bag1, pm_bag2, pm_bag3) ## [1] 1.0000000 0.3333333 0.6666667 Selecting Bag 1 and then selecting a Red Marble are interdependent events, so we multiply them. # For example # P(Bag1 &amp; Red) = P(Red|Bag1) * P(Bag1) pm_bag1 * pbag1 ## [1] 0.3333333 But each pathway (eg. Bag 1 x Marble A) is distinct and independent of the other pathways, so we can add them together. # P(Bag1 &amp; Red) = P(Red|Bag1) * P(Bag1) pm_bag1 * pbag1 + # P(Bag2 &amp; Red) = P(Red|Bag2) * P(Bag2) pm_bag2 * pbag2 + # P(Bag3 &amp; Red) = P(Red|Bag3) * P(Bag3) pm_bag3 * pbag3 ## [1] 0.6666667 And that gives us the same answer: 0.67 or 2/3. However, that required making a lot of objects in R. Can we do this more succinctly using vectors and data.frames? We could compute a bag-wise data.frame, where each row represents a choice (bag) from event1. bags &lt;- data.frame( bag_id = 1:3, # For each bag, how many do you get to choose? bags = c(1, 1, 1), # For each bag, how many marbles do you get to choose? marbles = c(3, 3, 3), # For each bag, how many marbles are red? red = c(3, 1, 2)) %&gt;% # Then, we can calculate the probability of... mutate( # choosing that bag out of all bags prob_bag = bags / sum(bags), # choosing red out of all marbles in that bag prob_red = red / marbles, # choosing BOTH that bag AND a red marble in that bag prob_bagred = prob_red * prob_bag) Finally, we could just sum the joint probabilities all together. bags %&gt;% summarize(prob_bagred = sum(prob_bagred)) ## prob_bagred ## 1 0.6666667 Much faster! # Let&#39;s remove this now unnecessary data remove(bags, n_bags, m_bag1, m_bag2, m_bag3, pbag1, pbag2, pbag3, pm_bag1, pm_bag2, pm_bag3) Bayes Rule A cool trick, called Bayes Rule, reveals that we can figure out a probability of interest that depends on other probabilities. Lets say, we want to know, whats the probability of OUTCOME given CONDITION. Bayes Rule states that the probability of the OUTCOME occurring given CONDITION is equal to the joint probability of the Outcome and Condition both occurring, divided by the probability of the condition occurring. \\[ P(Outcome = 1| Condition = 1) = \\frac{ P(Outcome = 1\\ \\&amp; \\ Condition = 1) }{ P(Condition = 1)} \\] Thanks to Conditional Probability and Total Probability tricks, we can break that down into quantities we can calculate. \\[ P(Outcome=1 | Condition=0) = \\frac{ P(Condition=1|Outcome=1) \\times P(Outcome=1) }{ \\sum{ P(Condition | Outcome) } \\times P(Outcome)} \\] \\[ = \\frac{ P(Condition=1|Outcome=1) \\times P(Outcome=1) }{ P(Condition=1|Outcome=1) \\times P(Outcome=1) + P(Condition=1|Outcome=0) \\times P(Outcome=0)} \\] Lets define some terms: posterior: Posterior probability is the probability that the outcome occurs given that the condition occurs. prior: the probability that the outcome occurs, independent of anything else. likelihood: the probability that the condition occurs, given that the outcome occurs. evidence: the total probability that the condition does or does not occur. Example: Coffee Shop (Incomplete Information) (#fig:img_coffee)Yay Coffee! A local coffee chain needs your help to analyze their supply chain issues. They know that their scones help them sell coffee, but does their coffee help them sell scones? Over the last week, when 7 customers bought scones, 3 went on to buy coffee. When 3 customers didnt buy scones, just 2 bought coffee. In general, 7 out of 10 of customers ever bought scones. Whats the probability that a customer will buy a scone, given that they just bought coffee? # We want to know this p_scone_coffee &lt;- NULL # But we know this! p_coffee_scone &lt;- 3/7 p_coffee_no_scone &lt;- 2/3 p_scone &lt;- 7/10 # AND # If 7 out of 10 customers ever bought scones, # then 3 out of 10 NEVER bought scones p_no_scone &lt;- 3 / 10 Using these 3~4 probabilities, we can deduce the total probability of coffee (the denominator), meaning whether you got coffee OR whether you didnt get coffee. # Total Prob of Coffee = Getting Cofee + Not getting coffee p_coffee &lt;- p_coffee_scone * p_scone + p_coffee_no_scone * p_no_scone # Check it! p_coffee ## [1] 0.5 So lets use p_coffee to get the probability of getting a scone given that you got coffee! p_scone_coffee &lt;- p_coffee_scone * p_scone / p_coffee Its magic! Bayes Rule is helpful when we dont have complete information, and just have some raw percentages or probabilities. Example: Coffee Shop (Complete Information) But, if we do have complete information, then we can actually prove Bayes Rule quite quickly. For example, say those percentages the shop owner gave us were actually meticulously tabulated by a barista. We talk to the barista, and she explains that she can tell us right away the proportion of folks who got a scone given that they got coffee. She shows us her spreadsheet of orders, listing for each customer, whether they got coffee and whether they got a scone. orders &lt;- tibble( coffee = c(&quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;), scone = c(&quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;)) We can tabulate these quickly using table(), tallying up how many folks did this. orders %&gt;% table() ## scone ## coffee no yes ## no 1 4 ## yes 2 3 # Let&#39;s skip to the end and just calculate the proportion directly! # Out of all people who got coffee, how many got scones? orders %&gt;% summarize(p_scone_coffee = sum(scone == &quot;yes&quot; &amp; coffee == &quot;yes&quot;) / sum(coffee == &quot;yes&quot;) ) ## # A tibble: 1 × 1 ## p_scone_coffee ## &lt;dbl&gt; ## 1 0.6 # The end! # Now that we know this, let&#39;s prove that Bayes works. orders %&gt;% summarize( # The goal (posterior) p_scone_coffee = sum(scone == &quot;yes&quot; &amp; coffee == &quot;yes&quot;) / sum(coffee == &quot;yes&quot;), # The data p_coffee_scone = sum(coffee == &quot;yes&quot; &amp; scone == &quot;yes&quot;) / sum(scone == &quot;yes&quot;), p_coffee_no_scone = sum(coffee == &quot;yes&quot; &amp; scone == &quot;no&quot;) / sum(scone == &quot;no&quot;), p_scone = sum(scone == &quot;yes&quot;) / sum(coffee == &quot;yes&quot; | coffee == &quot;no&quot;), p_no_scone = sum(scone == &quot;no&quot;) / sum(coffee == &quot;yes&quot; | coffee == &quot;no&quot;), # Now recalculate the goal, using the data we have collected. # Does &#39;bayes&#39; equal &#39;p_scone_coffee&#39;? bayes = p_coffee_scone * p_scone / (p_coffee_scone * p_scone + p_coffee_no_scone * p_no_scone)) ## # A tibble: 1 × 6 ## p_scone_coffee p_coffee_scone p_coffee_no_scone p_scone p_no_scone bayes ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.6 0.429 0.667 0.7 0.3 0.6 # It should! And it does! Tada! 3.2 Probability Functions How then do we use probability in our statistical analyses of risk, performance, and other systems engineering concepts? Probability allows us to measure for any statistic (or parameter) mu, how extreme is that statistic? This is called type II error, measured by a p-value, the probability that a more extreme value occurred than our statistic. Its an extremely helpful benchmark. In order to evaluate how extreme it is, we need values to compare it to. We can do this using (1) an observed distribution, (2) making a probability function curve of the observed distribution, or (3) assuming the probability function of a hypothetical distribution. 3.2.1 Example: Observed Distributions (#fig:img_er)Figure 1. Your Local ER For example, a local hospital wants to make their health care services more affordable, given the surge in inflation. They measured n = 15 patients who stayed 1 night over the last 7 days, how much were they charged (before insurance)? Lets call this vector obs (for observed data). A 16th patient received a bill of $3000 (above the national mean of ~$2500). Well record this as stat below. # Let&#39;s record our vector of 15 patients obs &lt;- c(1126, 1493, 1528, 1713, 1912, 2060, 2541, 2612, 2888, 2915, 3166, 3552, 3692, 3695, 4248) # And let&#39;s get our new patient data point to compare against stat &lt;- 3000 Here, we know the full observed distribution of values (cost), so we can directly compute the p_value from them, using the logical operator &gt;=. # Which values of in vector obs were greater than or equal to stat? obs &gt;= stat ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE ## [13] TRUE TRUE TRUE R interprets TRUE == 1 &amp; FALSE == 0, so we can take the mean() to get the percentage of values in obs greater than or equal to stat. # Get p-value, the probability of getting a value greater than or equal to stat mean(obs &gt;= stat) ## [1] 0.3333333 # This means Total Probability, where probability of each cost is 1/n sum( (obs &gt;= stat) / length(obs) ) ## [1] 0.3333333 Unfortunately, this only takes into account the exact values we observed (eg. $1493), but it cant tell us anything about values we didnt observe (eg. $1500). But logically, we know that the probability of getting a bill of $1500 should be pretty similar to a bill of $1493. So how do we fill in the gaps? 3.2.2 Observed Probability Density Functions Above, we calculated the probability of getting a more extreme hospital bill based on a limited sample of points, but for more precise probabilities, we need to fill in the gaps between our observed data points. For a vector x, the probability density function is a curve providing the probability (y) of each value across the range of x. It shows the relative frequency (probability) of each possible value in the range. We can ask R to estimate the probability density function for any observed vector using density(). This returns the density (y) of a bunch of hypothetical values (x) matching our distributions curve. We can access those results using the broom package, by tidy()-ing it into a data.frame. obs %&gt;% density() %&gt;% tidy() %&gt;% tail(3) ## # A tibble: 3 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5715. 0.000000725 ## 2 5727. 0.000000675 ## 3 5739. 0.000000628 But thats data, not a function, right? Functions are equations, machines you can pump an input into to get a specific output. Given a data.frame of 2 vectors, R can actually approximate the function (equation) connecting vector 1 to vector 2 using approxfun(), creating your own function! So cool! # Let&#39;s make dobs(), the probability density function for our observed data. dobs &lt;- obs %&gt;% density() %&gt;% tidy() %&gt;% approxfun() # Now let&#39;s get a sequence (seq()) of costs from 1000 to 3000, in units of 1000.... seq(1000, 3000, by = 1000) ## [1] 1000 2000 3000 # and let&#39;s feed it a range of data to get the frequencies at those costs! seq(1000, 3000, by = 1000) %&gt;% dobs() ## [1] 0.0001505136 0.0003081445 0.0003186926 For now, lets get the densities for costs ranging from the min to the max observed cost. mypd &lt;- tibble( # Get sequence from min to max, in units of $10 cost = seq(min(obs), max(obs), by = 10), # Get probability densities prob_cost_i = dobs(cost)) %&gt;% # Classify each row as TRUE (1) if cost greater than or equal to stat, or FALSE (0) if not. # This is the probability that each row is extreme (1 or 0) mutate(prob_extreme_i = cost &gt;= stat) # Check it out! mypd %&gt;% head(3) ## # A tibble: 3 × 3 ## cost prob_cost_i prob_extreme_i ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 1126 0.000186 FALSE ## 2 1136 0.000189 FALSE ## 3 1146 0.000191 FALSE Well save it to mypd, naming the x-axis cost and the y-axis prob_cost_i, to show the probability of each cost in row i (eg. $1126, $1136, $1146,  n). Well also calculate prob_extreme_i, the probability that each ith cost is extreme (greater than or equal to our 16th patients bill). Either it is extreme (TRUE == 100% = 1) or it isnt (FALSE == 0% == 0). Our density function dobs() estimated prob_cost_i (y), the probability/relative frequency of cost (x) occurring, where x represents every possible value of cost. We can visualize mypd using geom_area() or geom_line() in ggplot2! We can add geom_vline() to draw a vertical line at the location of stat on the xintercept. mypd %&gt;% ggplot(mapping = aes(x = cost, y = prob_cost_i, fill = prob_extreme_i)) + # Fill in the area from 0 to y along x geom_area() + # Or just draw curve with line geom_line() + # add vertical line geom_vline(xintercept = stat, color = &quot;red&quot;, size = 3) + # Add theme and labels theme_classic() + labs(x = &quot;Range of Patient Costs (n = 15)&quot;, y = &quot;Probability&quot;, subtitle = &quot;Probability Density Function of Hospital Stay Costs&quot;) + # And let&#39;s add a quick label annotate(&quot;text&quot;, x = 3500, y = 1.5e-4, label = &quot;(%) Area\\nunder\\ncurve??&quot;, size = 5) (#fig:plot_pdf)Figure 2. Visualizing a Probability Density Function! 3.2.3 Applying Probability Density Functions Great! We can view the probability density function now above. But how do we translate that into a single probability that measures how extreme Patient 16s bill is? We have the probability prob_cost_i at points cost estimated by the probability density function saved in mypd. We can calculate the total probability or p_value that a value of cost will be greater than our statistic stat, using our total probability formula. We can even restate it, so that it looks a little more like the weighted average it truly is. \\[ P_{\\ Extreme} = \\sum_{i = 1}^{n}{ P (Cost | Extreme_{\\ i}) \\times P (Cost_{\\ i}) } = \\frac{ \\sum_{i = 1}^{n}{ P (Cost_{i}) \\times P(Extreme)_{\\ i} } }{ \\sum_{i = 1}^{n}{ P(Cost_{i}) } } \\] p &lt;- mypd %&gt;% # Calculate the conditional probability of each cost occurring given that condition mutate(prob_cost_extreme_i = prob_cost_i * prob_extreme_i) %&gt;% # Next, let&#39;s summarize these probabilities summarize( # Add up all probabilities of each cost given its condition in row i prob_cost_extreme = sum(prob_cost_extreme_i), # Add up all probabilities of each cost in any row i prob_cost = sum(prob_cost_i), # Calculate the weighted average, or total probability of getting an extreme cost # by dividing these two sums! prob_extreme = prob_cost_extreme / prob_cost) # Check it out! p ## # A tibble: 1 × 3 ## prob_cost_extreme prob_cost prob_extreme ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0317 0.0868 0.365 Very cool! Visually, whats happening here? ggplot() + geom_area(data = mypd, mapping = aes(x = cost, y = prob_cost_i, fill = prob_extreme_i)) + geom_vline(xintercept = stat, color = &quot;red&quot;, size = 3) + theme_classic() + labs(x = &quot;Range of Patient Costs (n = 15)&quot;, y = &quot;Probability&quot;, subtitle = &quot;Probability Density Function of Hospital Stay Costs&quot;) + annotate(&quot;text&quot;, x = 3500, y = 1.5e-4, label = paste(&quot;P(Extreme)&quot;, &quot;\\n&quot;, &quot; = &quot;, p$prob_extreme %&gt;% round(2), sep = &quot;&quot;), size = 5) (#fig:plot_pdf_area)Figure 3. PDF with Area Under Curve! 3.2.4 Observed Cumulative Distribution Functions Alternatively, we can calculate that p-value for prob_extreme a different way, by looking at the cumulative probability. To add a values/probabilities in a vector together sequentially, we can use cumsum() (short for cumulative sum). For example: # Normally c(1:5) ## [1] 1 2 3 4 5 # Cumulatively summed c(1:5) %&gt;% cumsum() ## [1] 1 3 6 10 15 # Same as c(1, 2+1, 3+2+1, 4+3+2+1, 5+4+3+2+1) ## [1] 1 3 6 10 15 Every probability density function (PDF) can also be represented as a cumulative distribution function (CDF). Here, we calculate the cumulative total probability of receiving each cost, applying cumsum() to the probability (prob_cost) of each value (cost). In this case, were basically saying, were interested in all the costs, so dont discount any. \\[ P_{\\ Extreme} = \\sum_{i = 1}^{n}{ P (Cost | Extreme_{\\ i} = 1) \\times P (Cost_{\\ i}) } = \\frac{ \\sum_{i = 1}^{n}{ P (Cost_{i}) \\times 1 } }{ \\sum_{i = 1}^{n}{ P(Cost_{i}) } } \\] mypd %&gt;% # For instance, we can do the first step here, # taking the cumulative probability of costs i through j.... mutate(prob_cost_cumulative = cumsum(prob_cost_i)) %&gt;% head(3) ## # A tibble: 3 × 4 ## cost prob_cost_i prob_extreme_i prob_cost_cumulative ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 1126 0.000186 FALSE 0.000186 ## 2 1136 0.000189 FALSE 0.000375 ## 3 1146 0.000191 FALSE 0.000566 Our prob_cost_cumulative in row 3 above shows the total probability of n = 3 patients receiving a cost of 1126 OR 1136 OR 1146. But, we want an average estimate for 1 patient. So, like in a weighted average, we can divide by the total probability of all (n) hypothetical patients in the probability density function receiving any of these costs. This gives us our revised prob_cost_cumulative, which ranges from 0 to 1! mycd &lt;- mypd %&gt;% # For instance, we can do the first step here, # taking the cumulative probability of costs i through j.... mutate(prob_cost_cumulative = cumsum(prob_cost_i) / sum(prob_cost_i)) %&gt;% # We can also then identify the segment that is extreme! mutate(prob_extreme = prob_cost_cumulative * prob_extreme_i) # Take a peek at the tail! mycd %&gt;% tail(3) ## # A tibble: 3 × 5 ## cost prob_cost_i prob_extreme_i prob_cost_cumulative prob_extreme ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4226 0.000144 TRUE 0.997 0.997 ## 2 4236 0.000142 TRUE 0.998 0.998 ## 3 4246 0.000140 TRUE 1 1 Lets visualize mycd, our cumulative probabilities! viz_cd &lt;- ggplot() + # Show the cumulative probability of each cost, # shaded by whether it is &quot;extreme&quot; (cost &gt;= stat) or not geom_area(data = mycd, mapping = aes(x = cost, y = prob_cost_cumulative, fill = prob_extreme_i)) + # Show cumulative probability of getting an extreme cost geom_area(data = mycd, mapping = aes(x = cost, y = prob_extreme, fill = prob_extreme_i)) + # Show the 16th patient&#39;s cost geom_vline(xintercept = stat, color = &quot;red&quot;, size = 3) + # Add formatting theme_classic() + labs(x = &quot;Cost of Hospital Stays (n = 15)&quot;, y = &quot;Cumulative Probability of Cost&quot;, fill = &quot;P(Extreme i)&quot;, title = &quot;Cumulative Distribution Function for Cost of Hospital Stays&quot;, subtitle = &quot;Probability of Cost more Extreme than $3000 = 0.36&quot;) # View it! viz_cd # (Note: I&#39;ve added some more annotation to mine # than your image will have - don&#39;t worry!) (#fig:plot_cdf)Figure 4. Visualizing a Cumulative Distribution Function! But wouldnt it be handy if we could just make a literal cumulative distribution function, just like we did for the probability density function dobs()? pobs &lt;- obs %&gt;% density() %&gt;% tidy() %&gt;% # Sort from smallest to largest arrange(x) %&gt;% # take cumulative sum, divided by total probability mutate(y = cumsum(y) / sum(y)) %&gt;% # Make cumulative distribution function pobs()! approxfun() # We&#39;ll test it out! 1 - pobs(3000) ## [1] 0.3726192 # Pretty close to our probability we calculated before! # Clear unnecessary data. remove(stat, mycd, p, viz_cd) 3.2.5 Using Calculus! Above we took a computational-approach to the CDF, using R to number-crunch the CDF. To summarize: We took a vector of empirical data obs, We estimated the probability density function (PDF) using density() We calculated the cumulative probability distribution ourselves We connected-the-dots of our CDF into a function with approxfun(). We did this because we started with empirical data, where where the density function is unknown! But sometimes, we do know the density function, perhaps because systems engineers have modeled it for decades! In these cases, we could alternatively use calculus in R to obtain the CDF and make probabilistic assessments. Heres how! For example, this equation does a pretty okay job of approximating the shape of our distribution in obs. \\[ f(x) = \\frac{-2}{10^7} + \\frac{25x}{10^8} - \\frac{45x^2}{10^{12}} \\] We can write that up in a function, which we will call pdf. For every x value we supply, it will compute that equation to predict that values relative refequency/probability. # Write out our nice polynomial function pdf = function(x){ -2/10^7 + 25/10^8*x + -45/10^12*x^2 } # Check it! c(2000, 3000) %&gt;% pdf() ## [1] 0.0003198 0.0003448 The figure below demonstrates that it approximates the true density relatively closely. # We&#39;re going to add another column to our mypd dataset, mypd &lt;- mypd %&gt;% # approximating the probability of each cost with our new pdf() mutate(prob_cost_i2 = pdf(cost)) ggplot() + geom_line(data = mypd, mapping = aes(x = cost, y = prob_cost_i, color = &quot;from raw data&quot;)) + geom_line(data = mypd, mapping = aes(x = cost, y = prob_cost_i2, color = &quot;from function&quot;)) + theme_classic() + labs(x = &quot;Cost&quot;, y = &quot;Probability&quot;, color = &quot;Type&quot;) So how do we generate the cumulative density function? The mosaicCalc package can help us with its functions D() and antiD(). D() computes the derivative of a function (Eg. CDF -&gt; PDF) antiD() computes its integral (Eg. PDF -&gt; CDF) # Compute the anti-derivative (integral) of the function pdf(x), solving for x. cdf &lt;- antiD(tilde = pdf(x) ~ x) # It works just the same as our other functions obs %&gt;% head() %&gt;% cdf() ## [1] 0.1368449 0.2284130 0.2380292 0.2910549 0.3517389 0.3989108 # (Note: Our function is not a perfect fit for the data, so probabilities exceed 1!) # Let&#39;s compare our cdf() function made with calculus with pobs(), our computationally-generated CDF function. obs %&gt;% head() %&gt;% pobs() ## [1] 0.07774089 0.16376985 0.17348497 0.22750868 0.28831038 0.33387171 # Pretty similar results. The differences are due the fact that our original function is just an approximation, rather than dobs(), which is a perfect fit for our densities. And we can also take the derivative of our cdf() function with D() to get back our pdf(), which well call pdf2(). pdf2 &lt;- D(tilde = cdf(x) ~ x) # Let&#39;s compare results.... # Our original pdf... obs %&gt;% head() %&gt;% pdf() ## [1] 0.0002242456 0.0002727428 0.0002767347 0.0002960034 0.0003132915 ## [6] 0.0003238380 # Our pdf dervied from cdf... obs %&gt;% head() %&gt;% pdf2() ## [1] 0.0002242456 0.0002727428 0.0002767347 0.0002960034 0.0003132915 ## [6] 0.0003238380 # They&#39;re the same! Tada! You can do calculus in R! remove(mypd, pdf, pdf2, cdf, obs) Learning Check 1 Question A month has gone by, and our hospital has now billed 30 patients. Youve heard that hospital bills at or above $3000 a day may somewhat deter people from seeking future medical care, while bills at or above $4000 may greatly deter people from seeking future care. (These numbers are hypothetical.) Using the vectors below, please calculate the following, using a PDF or CDF. Whats the probability that a bill might somewhat deter a patient from going to the hospital? Whats the probability that a bill might greatly deter a patient from going to the hospital? Whats the probability that a patient might be somewhat deterred but not greatly deterred from going to the hospital? Note: Assume that the PDF matches the range of observed patients. # Let&#39;s record our vector of 30 patients patients &lt;- c(1126, 1493, 1528, 1713, 1912, 2060, 2541, 2612, 2888, 2915, 3166, 3552, 3692, 3695, 4248, 3000, 3104, 3071, 2953, 2934, 2976, 2902, 2998, 3040, 3030, 3021, 3028, 2952, 3013, 3047) # And let&#39;s get our statistics to compare against somewhat &lt;- 3000 greatly &lt;- 4000 [View Answer!] # Get the probability density function for our new data dobs2 &lt;- patients %&gt;% density() %&gt;% tidy() %&gt;% approxfun() # Get the probability densities mypd2 &lt;- tibble( cost = seq(min(patients), max(patients), by = 10), prob_cost_i = cost %&gt;% dobs2()) %&gt;% # Calculate probability of being somewhat deterred mutate( prob_somewhat_i = cost &gt;= somewhat, prob_greatly_i = cost &gt;= greatly, prob_somewhat_not_greatly_i = cost &gt;= somewhat &amp; cost &lt; greatly) To calculate these probabilities straight from the probability densities, do like so: mypd2 %&gt;% summarize( # Calculate total probability of a cost somewhat deterring medical care prob_somewhat = sum(prob_cost_i * prob_somewhat_i) / sum(prob_cost_i), # Calculate total probability of a cost greatly deterring medical care prob_greatly = sum(prob_cost_i * prob_greatly_i) / sum(prob_cost_i), # Calculate total probability of a cost somewhat-but-not-greatly deterring medical care prob_somewhat_not_greatly = sum(prob_cost_i * prob_somewhat_not_greatly_i) / sum(prob_cost_i)) ## # A tibble: 1 × 3 ## prob_somewhat prob_greatly prob_somewhat_not_greatly ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.431 0.0172 0.414 To calculate these probabilities from the cumulative distribution functions, we can do the following: # Get cumulative probabilities of each mycd2 &lt;- mypd2 %&gt;% mutate( # Calculate total probability of a cost somewhat deterring medical care prob_somewhat = cumsum(prob_cost_i) * prob_somewhat_i / sum(prob_cost_i), # Calculate total probability of a cost greatly deterring medical care prob_greatly = cumsum(prob_cost_i) * prob_greatly_i / sum(prob_cost_i), # Calculate total probability of a cost somewhat-but-not-greatly deterring medical care prob_somewhat_not_greatly = cumsum(prob_cost_i) * prob_somewhat_not_greatly_i / sum(prob_cost_i)) # Check it! mycd2 %&gt;% tail(3) ## # A tibble: 3 × 8 ## cost prob_cost_i prob_somewhat_i prob_greatly_i prob_somewhat_not_greatly_i ## &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 4226 0.000106 TRUE TRUE FALSE ## 2 4236 0.000107 TRUE TRUE FALSE ## 3 4246 0.000107 TRUE TRUE FALSE ## #  3 more variables: prob_somewhat &lt;dbl&gt;, prob_greatly &lt;dbl&gt;, ## # prob_somewhat_not_greatly &lt;dbl&gt; Finally, if you want to visualize them, heres how you would do it! ggplot() + # Get cumulative probability generally geom_area(data = mycd2, mapping = aes(x = cost, y = cumsum(prob_cost_i) / sum(prob_cost_i), fill = &quot;Very Little&quot;)) + # Get cumulative probability if somewhat but not greatly geom_area(data = mycd2, mapping = aes(x = cost, y = prob_somewhat_not_greatly, fill = &quot;Somewhat-not-Greatly&quot;)) + # Get cumulative probability if greatly geom_area(data = mycd2, mapping = aes(x = cost, y = prob_greatly, fill = &quot;Greatly&quot;)) + theme_classic() + labs(x = &quot;Cost of Hospital Stay (n = 30)&quot;, y = &quot;Cumulative Probability&quot;, subtitle = &quot;Probability of Hospital Bill Deterring Future Hospital Visits&quot;, fill = &quot;Deter Future Visits&quot;) Hypothetical Probability Functions Often, though, our sample of data is just one of the many samples we could have possibly gotten; had we looked at a different hospital (by chance), or a different sample of patients come in by chance, we might have gotten slightly different hospital bills. The problem is, we almost never can see the distribution of the true population of all observations (eg. all hospital bills). But, if we can approximately guess what type of distribution that population has, we can very easily compute the probability density functions and cumulative distribution functions of several of the most well known distributions in R (eg. Normal, Poisson, Gamma, etc.) Example: Farmers Market The Ithaca Farmers Market is a vendor-owned cooperative that runs a massive Saturday-and-Sunday morning market for local produce, street food, and hand-made goods, on the waterfront of Cayuga Lake. In markets and street fairs, some stalls brands are often better known than others, so businesses new to the market might worry that people wont come to their stalls without specific prompting. This past Saturday, a volunteer tracked 500 customers and recorded how many stalls each customers visited during their stay. They calculated the following statistics. The average customer visited a mean of 5.5 stalls and a median of 5 stalls, with a standard deviation of 2.5 stalls. Figure 3.1: Ithaca Farmers Market! Market operators wants to know: Whats the probability that customers will stop by 5 stalls? Whats the probability that customers will stop by at max 5 stalls? Whats the probability that customers will stop by over 5 stalls? How many visits did people usually make? Estimate the interquartile range (25th-75th percentiles). Unfortunately, the wind blew the raw data away into Cayuga Lake before they could finish their analysis. How can we approximate the unobserved distribution of visits and compute these probabilities? Below, we will (1) use these statistics to guess which of several possible archetypal hypothetical distributions it most resembles, and then (2) compute probabilities based off of the shape of that hypothetical distribution. Guessing the Shape of an Unobserved Distribution We dont have the actual data, but we know several basic features of our distribution! Our variable is visits, a count ranging from 0 to infinity. (Cant have -5 visits, cant have 1.3 visits.) The median (5) is less than the mean (5.5), so our distribution is right-skewed. This sounds like a classic Poisson distribution! Lets simulate some poisson-distributed data to demonstrate. # Randomly sample 500 visits from a poisson distribution with a mean of 5.5 visits &lt;- rpois(n = 500, lambda = 5.5) # Check out the distribution! visits %&gt;% hist() Compute Probabilities using Hypothetical Distributions Much like rpois() randomly generates poisson distributed values, dpois() generates the density of any value on a poisson distribution centered on a given mean, while ppois() returns for any percentile in the distribution the cumulative probability (percentage of the area under the density curve) up until that point. area under the curve (probability) of. See the Table below for several examples. Table 3.1: Table 1: Probability Functions (r, d, p, and q) Meaning Purpose Main Input Normal Poisson Gamma Exponential Random Draws from Distribution Simulate a distribution n = # of simulations rnorm() rpois() rgamma() rexp() Probability Density Function Get Probability of Value in Distribution x = value in distribution dnorm() dpois() dgamma() dexp() Cumulative Distribution Function Get % of Distribution LESS than Value q = a cumulative probability pnorm() ppois() pgamma() pexp() Quantiles Function Get Value of any Percentile in Distribution p = percentile qnorm() qpois() qgamma() qexp() Density So, what percentage of customers stopped by 1 stall? Below, dpois() tells us the density() or frequency of your value, given a distribution where the mean = 5.5. # Get the frequency for 5 visits in the distribution pd5 &lt;- dpois(5, lambda = 5.5) # Check it! pd5 ## [1] 0.1714007 Looks like 17.1% of customers stopped by 5 stalls. We can validate this using our simulated visits from above. We can calculate the density() function, extract it using approxfun(), and then assign it to dsim(), our own exact probability density function for our data. It works just like dpois(), but you dont need to specify lambda, because it only works for this exact distribution! # Approximate the PDF of our simulated visits dsim &lt;- visits %&gt;% density() %&gt;% approxfun() # Try our density function for our simulated data! dsim(5) ## [1] 0.1660901 # Pretty close to our results from dpois()! Cumulative Probabilities What percentage of customers stopped by at max 5 stalls? # Get the cumulative frequency for a value (5) in the distribution cd5 &lt;- ppois(q = 5, lambda = 5.5) # Check it! cd5 ## [1] 0.5289187 Looks like just 52.9% of customers stopped by 1 stall or fewer. What percentage of customers stopped by over 5 stalls? # Get the probability they will NOT stop at 5 or fewer stalls 1 - cd5 ## [1] 0.4710813 We can validate our results against our simulated distribution. psim &lt;- visits %&gt;% density() %&gt;% tidy() %&gt;% # Get cumulative probability distribution arrange(x) %&gt;% # Get cumulative probabilities mutate(y = cumsum(y) / sum(y)) %&gt;% # Turn it into a literal function! approxfun() # Check it! psim(5) ## [1] 0.4549954 # Pretty close to cdf5! Quantiles How many visits did people usually make? Estimate the interquartile range (25th-75th percentiles) of the unobserved distribution. q5 &lt;- qpois(p = c(.25, .75), lambda = 5.5) # Check it! q5 ## [1] 4 7 Looks like 50% of folks visited between 4 and 7 stalls We can compare against our simulated data using quantile(). # Approximate the quantile function of this distribution qsim &lt;- tibble( # Get a vector of percentiles from 0 to 1, in units of 0.001 x = seq(0, 1, by = 0.001), # Using our simulated distribution, # get the quantiles (values) at those points from this distribution y = visits %&gt;% quantile(probs = x)) %&gt;% # Approximate function! approxfun() # Check it! qsim(c(.25, .75)) ## [1] 4 7 rm(visits, pd5, cd5, q5, dsim, psim, qsim) Learning Check 2 Question What if we are not certain whether our unobserved vector of visits has a Poisson distribution or not? To give you more practice, please calculate the probability that customers will stop at more than 5 stalls, using appropriate functions for the (1) Normal, (2) Gamma, and (3) Exponential distribution! (See our table above for the list of function names.) [View Answer!] We know there were n = 500 customers, with a mean of 5.5 visits, a median of 5 visits, and a standard deviation of 2.5 visits. For a Normal Distribution: We learned in Workshop 2 that rnorm() requires a mean and sd (standard deviation); we conveniently have both! 1 - pnorm(5, mean = 5.5, sd = 2.5) ## [1] 0.5792597 For a Gamma Distribution: We learned in Workshop 2 that rgamma() requires a shape and scale (or rate); we can calculate these from the mean and sd (standard deviation). # shape = mean^2 / variance = mean^2 / sd^2 shape &lt;- 5.5^2 / 2.5^2 # scale = variance / mean scale &lt;- 2.5^2 / 5.5 # AND # rate = 1 / scale rate &lt;- 1 / scale # So... # Get 1 - cumulative probability up to 5 1 - pgamma(5, shape = shape, scale = scale) ## [1] 0.5211581 # OR (same) 1 - pgamma(5, shape = shape, rate = rate) ## [1] 0.5211581 For an Exponential Distribution: We learned in Workshop 2 that rexp() requires a rate; we can calculate this from the mean. # For exponential distribution, # rate = 1 / mean rate &lt;- 1 / 5.5 # So... # Get 1 - cumulative probability up to 5 1 - pexp(5, rate = rate) ## [1] 0.4028903 Conclusion And thats a wrap! Nice work! You can now figure out a lot of things about the world if you (a) can guess their distribution and (b) have one or two statistics about that distribution. Here we go! "],["workshop-system-reliability.html", "4 Workshop: System Reliability Getting Started 4.1 Concepts Learning Check 1 4.2 Joint Probabilities 4.3 Types of Failure Rates 4.4 Units Learning Check 2 4.5 System Reliability 4.6 Renewal Rates via Bayes Rule", " 4 Workshop: System Reliability In this tutorial, were going to learn how to conduct Reliability Analysis (also known as Survival Analysis) in R. (#fig:img_epic_fail)Figure 1. Why We Do Reliability Analysis Getting Started Packages library(tidyverse) library(broom) library(DiagrammeR) 4.1 Concepts In Reliability/Survival Analysis, our quantity of interest is the amount of time it takes to reach a particular outcome (eg. time to failure, time to death, time to market saturation, etc.) Lets learn how to approximate them in R! 4.1.1 Life Distributions All technologies, operations, etc. have a lifetime distribution. If you took a sample of, say, cars in New York, you could measure how long each car functioned properly (its life-span), and build a Lifetime Distribution from that vector. # Let&#39;s imagine a normally distributed lifespan for these cars... lifespan &lt;- rnorm(100, mean = 5, sd = 1) The lifetime distribution is the probability density function telling us how frequently each potential lifespan is expected to occur. # We can build ourself the PDF of our lifetime distribution here dlife &lt;- lifespan %&gt;% density() %&gt;% tidy() %&gt;% approxfun() In contrast, the Cumulative Distribution Function (CDF) for a lifetime distribution tells us, for any time \\(t\\), the probability that a car will fail by time \\(t\\). # And we can build the CDF here plife &lt;- lifespan %&gt;% density() %&gt;% tidy() %&gt;% mutate(y = cumsum(y) / sum(y)) %&gt;% approxfun() Having built these functions for our cars, we can generate the probability (PDF) and cumulative probability (CDF) of failure across our observed vector of car lifespans, from ~1.84 to ~7.56. Reliability or Survival Analysis is concerned with the probability that a unit (our car) will still be operating by a specific time \\(t\\), representing the percentage of all cars that will survive to that point in time. So lets also calculate 1 - cumulative probability of failure. mycars &lt;- tibble( time = seq(min(lifespan), max(lifespan), by = 0.1), # Get probability of failing at time time prob = time %&gt;% dlife(), # Get probability of failing at or before time t prob_cumulative = time %&gt;% plife(), # Get probability of surving past time t # (NOT failing at or before time t) prob_survival = 1 - time %&gt;% plife()) Lets plot our three curves! ggplot() + # Make one area plot for Cumulative Probability (CDF) geom_area(data = mycars, mapping = aes(x = time, y = prob_cumulative, fill = &quot;Cumulative Probability&quot;), alpha = 0.5) + # Make one area plot for Relibability geom_area(data = mycars, mapping = aes(x = time, y = prob_survival, fill = &quot;Reliability (Survival)&quot;), alpha = 0.5) + # Make one area plot for Probability (PDF) geom_area(data = mycars, mapping = aes(x = time, y = prob, fill = &quot;Probability&quot;), alpha = 0.5) + theme_classic() + theme(legend.position = &quot;bottom&quot;) + labs(x = &quot;Lifespan of Car&quot;, y = &quot;Probability&quot;, subtitle = &quot;Example Life Distributions&quot;) Figure 4.1: Figure 2. Life Distributions of a Fleet of Cars This new reliability function allows us to calculate 2 quantities of interest: expected (average) number of cars that fail up to time \\(t\\). total cars expected to still operate after time \\(t\\). 4.1.2 Example: Airplane Propeller Failure! Suppose Lockheed Martin purchases 800 new airplane propellers. When asked about the failure rate, the seller reports that every 1500 days, 2 of these propellers are expected to break. Using this, we can calculate \\(m\\), the mean time to fail! \\[ \\lambda \\ t = t_{days} \\times \\frac{2 \\ units}{1500 \\ days} \\ \\ \\ and \\ \\ \\ m = \\frac{1500}{2} = 750 \\ days \\] This lets us generate the failure rate \\(F(t)\\), also known as the Cumulative Distribution Function, and we can write it up like this. \\[ CDF(t) = F(t) = 1 - e^{-(2t/1500)} = 1 - e^{-t/750} \\] Whats pretty cool is, we can tell R to make a matching function fplane(), using the function() command. # For any value `t` we supply, do the following to that `t` value. fplane = function(t){ 1 - exp( -1*(t / 750)) } Lets use our function fplane() to answer our Lockheed engineers questions. Whats the probability a propeller will fail by t = 600 days? By t = 5000 days? fplane(t = c(600, 5000)) ## [1] 0.5506710 0.9987274 Looks like 55% will fail by 600 days, and 99% fail by 5000 days. Whats the probability a propeller will fail between 600 and 5000 days? fplane(t = 5000) - fplane(t = 600) ## [1] 0.4480563 ~45% more will fail between this period. What percentage of new propellers will work more than 6000 days? 1 - fplane(t = 6000) ## [1] 0.0003354626 0.03% will survive past 6000 days. If Lockheed uses 300 propellers, how many will fail in 1 year? In 3 years? # Given a sample of 300 propellers, n &lt;- 300 # We project n * fplane(t = 362.25) will fail in 1 year (365.25 days) # that&#39;s ~115 propellers. n*fplane(t = 365.25) ## [1] 115.6599 # We also prject that n * fplane(t = 3 * 362.25) will fail in 3 years # that&#39;s ~230 propellers! n*fplane(t = 3*365.25) ## [1] 230.3988 Pretty powerful! Learning Check 1 Question (#fig:img_phone)Figure 3. Exploding Phones! (True Story) Hypothetical: Samsung is releasing a new Galaxy phone. But after the 2016 debacle of exploding phones, they want to estimate how likely it is a phone will explode (versus stay intact). Their pre-sale trials suggest that every 500 days, 5 phones are expected to explode. What percentage of phones are expected to work after more than 6 months? 1 year? [View Answer!] Using the information above, we can calculate the mean time to fail m, the rate of how many days it takes for an average unit to fail. days &lt;- 500 units &lt;- 5 m &lt;- days / units # Check it! m ## [1] 100 We can use m to make our explosion function fexplode(), which in this case, is our (catastrophic) failure function \\(f(t)\\)! \\[ CDF(days) = Explode(days) = 1 - e^{-(days \\times \\frac{1 \\ unit}{100 \\ days})} = 1 - e^{-0.01 \\times days} \\] fexplode = function(days){ 1 - exp(-1*days*0.01) } Then, we can calculate \\(r(t)\\), the cumulative probability that a phone will not explode after \\(t\\) days. Lets answer our questions! What percentage of phone are expected to survive 6 months? # What percent 1 - fexplode(365.25 / 2) ## [1] 0.1610162 What percentage of phone are expected to survive 1 year? 1 - fexplode(365.25) ## [1] 0.02592623 4.2 Joint Probabilities Two extra rules of probability can help us understand system reliability. 4.2.1 Multiplication Rule probability that \\(n\\) units with a reliability function \\(R(t)\\) survive past time \\(t\\) is multiplied, because of conditional probability, to equal \\(R(t)^{n}\\).. Figure 4.2: Figure 4. Oops. For example, theres a 50% chance that 1 coffee cup breaks at local coffeeshop Coffee Please! every 6 months (180 days). Thus, the mean number of days to cup failure is \\(m = \\frac{180 \\ days}{ 1 \\ cup \\times 0.50 \\ chance} = 360 \\ days\\), while the relative frequency (probability) that a cup will break is $ = $. fcup = function(days){ 1 - exp( -1*(days/360))} # So the probability that 1 breaks within 100 days is XX percent fcup(100) ## [1] 0.2425349 And lets write out a reliability function too, based on our function for the failure function. # Notice how we can reference an earlier function fcup in our later function? Always have to define functions in order. rcup = function(days){ 1 - fcup(days) } # So the probability that 1 *doesn&#39;t* break within 100 days is XX perecent rcup(100) ## [1] 0.7574651 But the probability that two break within 100 days is fcup(100) * fcup(100) ## [1] 0.05882316 And the probability that 5 break within 100 days is very small! fcup(100)^5 ## [1] 0.0008392106 4.2.2 Compliment Rule The probability that at least 1 of \\(n\\) units fails by time \\(t\\) is \\(1 - R(t)^{n}\\). So, if Coffee Please! buys 2 new cups for their store, the probability that at least 1 unit breaks within a year is 1 - rcup(days = 365.25)^2 ## [1] 0.868555 While if they buy 5 new cups for their store, the chance at least 1 cup breaks within a year is 1 - rcup(days = 365.25)^5 ## [1] 0.9937359 4.3 Types of Failure Rates 4.3.1 Hazard Rate Function But if a unit has survived up until now, shouldnt its odds of failing change? We can express this as: \\[ P(Fail \\ Tomorrow | Survive \\ until \\ Today) = \\frac{ F(days + \\Delta days) - F(days) }{ \\Delta days \\times R(days)} = \\frac{ F(days + 1 ) - F(days) }{ 1 \\times R(days)} \\] Local coffeeshop Coffee Please! also has a lucky mug, which has stayed intact for 5 years, despite being dropped numerous times by patrons. Coffee Please!s failure rate suggests they had a 99.3% chance of it breaking to date. # we call this the Hazard Rate Z zcup = function(days, plus = 1){ ( fcup(days + plus) - fcup(days) ) / (plus * rcup(days) ) } 4.3.2 Accumulative Hazard Function \\(H(t)\\): total accumulated risk of experiencing the event of interest that has been gained by progressing from 0 to time \\(t\\). the (instantaneous) hazard rate \\(h(t)\\) can grow or shrink over time, but the cumulative hazard rate only increases or stays constant. hcup = function(days){ -1*log( rcup(days) ) } # This captures the accumulative probability of a hazard (failure) occurring given the number of days past. hcup(100) ## [1] 0.2777778 4.3.3 Average Failure Rate The hazard rate \\(z(t)\\) varies over time, so lets generate a single statistic to summarize the distribution of hazard rates that \\(z(t)\\) can provide us between times \\(t_{a} \\to t_{z}\\). Well call this the Average Failure Rate \\(AFR(T)\\). afrcup = function(t1,t2){ (hcup(t2) - hcup(t1) ) / (t2 - t1)} afrcup(0, 5) ## [1] 0.002777778 # Or write it as.... afrcup = function(t1,t2){ (log(rcup(t1)) - log(rcup(t2)) ) / (t2 - t1)} afrcup(0, 5) ## [1] 0.002777778 # And if we&#39;re going from 0 to time t, # it simplifies to... afrcup = function(days){ hcup(days) / days } afrcup(5) ## [1] 0.002777778 # or to this afrcup = function(days){ -1*log(rcup(days)) / days } afrcup(5) ## [1] 0.002777778 When the probability for a time \\(t\\) is less than 0.10, \\(AFR = F(t) / T\\). This means that \\(F(t) = 1 - e^{-T \\times AFR(T)} \\approx T \\times AFR(T) \\ \\ when \\ F(T) &lt; 0.10\\). afrcup = function(days){ fcup(days) / days } afrcup(5) ## [1] 0.002758577 # and this is approximately.... 4.3.4 Table of Functions (#tab:tab_functions)Table 1. Failure and Reliability Functions Function Name Formula Equivalency Meaning \\(F(t)\\) Failure Function \\(1 - e^{-\\lambda t}\\) \\(1 - e^{-H(t)}\\) Cumulative Distribution Function (CDF) of Lifespans \\(R(t)\\) Reliability Distribution \\(e^{-\\lambda t}\\) \\(e^{-H(t)}\\) Remainder of CDF for Lifespans \\(f(t)\\) Change in Failure Function \\(\\frac{F(t + \\Delta t) - F(t)}{\\Delta t}\\) \\(-R&#39;(t)\\) Change in CDF at time \\(t_{2}\\) \\(-\\) at \\(t_{1}\\), per extra timestep \\(z(t)\\) Failure Rate (Hazard Rate) \\(\\frac{f(t)}{R(t)}\\) \\(\\lambda = \\frac{1}{m}\\) Mean Failure Rate \\(\\lambda\\); Inverse of Mean Time to Fail \\(m\\) \\(H(t)\\) Accumulative Hazard Rate \\(-log( R(t) )\\) \\(\\lambda t\\) Total Risk of Failure gained from time 0 to \\(t\\) \\(AFR(t_1,t_2)\\) Average Failure Rate \\(\\frac{H(t_{2}) - H(t_{1})}{t_{2} - t_{1}}\\) \\(\\frac{log(R(t_{1})) -log(R(t_{2})}{t_{2} - t_{1}}\\) Average failures per timestep between times \\(t_{1}\\) and \\(t_{2}\\) 4.4 Units Units can be tough with failure rates, because they get tiny really quickly. Here are some common units: Percent per thousand hours, where \\(\\% / K = 10^5 \\times z(t)\\) Failure in Time (FIT) per thousand hours, also known as Parts per Million per Thousand Hours, written \\(PPM/K = 10^9 \\times z(t)\\). This equals \\(10^4 \\times Failure \\ Rate \\ in \\ \\% / K\\). For this lifetime function \\(F(t) = 1 - e^{-(t/2000)^{0.5}}\\), whats the failure rate at t = 10, t = 1000, and t = 10,000 hours? Convert them into \\(\\%/K\\) and \\(PPM/K\\). 4.4.1 Failure Functions First, lets write the failure function f(t). # Write failure rate f = function(t){ 1 - exp(-(t/2000)^0.5) } Second, lets write the hazard rate z(t), for a 1 unit change in t. # Write hazard rate z = function(t, change = 1){ # Often I like to break up my functions into multiple lines; # it makes it much clearer to me. # To help, we can make &#39;temporary&#39; objects; # they only exist within the function. # Get change in failure function deltaf &lt;- (f(t+change) - f(t) ) / change # Get reliability function r &lt;- 1 - f(t) # Get hazard rate deltaf / r } Third, lets write the average hazard rate afr(t1,t2). afr = function(t1,t2){ # Let&#39;s get the survival rate r(t) r1 = 1 - f(t1) r2 = 1 - f(t2) # Let&#39;s get the accumulative hazard rate h(t) h1 = -log(r1) h2 = -log(r2) # And let&#39;s calculate the averge failure rate! afr = (h2 - h1) / (t2 - t1) # And return it! afr } 4.4.2 Conversion Functions Fourth, lets write some functions to convert our results into %/K and PPM/K, so we can be lazy! Well call our functions pk() and ppmk(). # % per 1000 hours pk = function(rate){ rate * 100 * 10^3 } # PPM/1000 hours ppmk = function(rate){ rate * 10^9 } 4.4.3 Converting Estimates Lets compare our hazard rates when t = 10, per hour, in % per 1000 hours, and in PPM per 1000 hours. # Per hour... Ew. Not very readable. z(t = 10) ## [1] 0.003445358 # % per 1000 hours.... Wheee! Much more legible z(t = 10) %&gt;% pk() ## [1] 344.5358 # PPM per 1000 hours.... Who! Big numbers! z(t = 10) %&gt;% ppmk() ## [1] 3445358 Finally, lets calculate the Average Failure Rate between 1000 and 10000 hours, in %/K. # Tada! Average Failure Rate from 1000 to 10000 hours, in % of units per 1000 hours afr(1000, 10000) %&gt;% pk() ## [1] 16.98846 # And in ppmk! afr(1000, 10000) %&gt;% ppmk() ## [1] 169884.6 Learning Check 2 Question (#fig:img_ramen)Figure 5. Does Instant Ramen ever go bad? A food safety inspector is investigating the average shelf life of instant ramen noodles. A company estimates the average shelf life of a package of ramen noodles at ~240 days per package. In a moment of poor judgement, she hires a team of hungry college students to taste-test old packages of that companys ramen noodles, randomly sampled from a warehouse. When a student comes down with food poisoning, she records that product as having gone bad after XX days. She treats the record of ramen food poisonings as a sample of the lifespan of ramen products. ramen &lt;- c(163, 309, 215, 211, 246, 198, 281, 180, 317, 291, 238, 281, 215, 208, 212, 300, 231, 240, 285, 232, 252, 261, 310, 226, 282, 140, 208, 280, 237, 270, 185, 409, 293, 164, 231, 237, 269, 233, 246, 287, 187, 232, 180, 227, 215, 260, 236, 229, 263, 220) Using this data, please calculate Whats the cumulative probability of a pack of ramen going bad within 8 months (240 days)? Are the companys predictions accurate? Whats the average failure rate (\\(\\lambda\\)) for the period between 8 months (240 days) to 1 year? Whats the mean time to fail (\\(m\\)) for the period between 8 months to 1 year? [View Answer!] First, we take her ramen lifespan data, estimate the PDF with density(), and make the failure function (CDF), which Ive called framen() below. # Get failure function f(t) = CDF of ramen failure framen &lt;- ramen %&gt;% density() %&gt;% tidy() %&gt;% # Now compute CDF mutate(y = cumsum(y) / sum(y)) %&gt;% approxfun() Second, we calculate the reliability function rramen(). # Get survival function r(t) = 1 - f(t) rramen &lt;- function(days){ 1 - framen(days) } Third, we can shortcut to the average failure rate, called afrramen() below, by using the reliability function rramen() to make our hazard rates at time 1 (h1) and time 2 (h2). # Get average failure rate from time 1 to time 2 afrramen &lt;- function(days1, days2){ h1 &lt;- -1*log(rramen(days1)) h2 &lt;- -1*log(rramen(days2)) (h2 - h1) / (days2 - days1) } Whats the cumulative probability of a pack of ramen going bad within 8 months (240 days)? Are the companys predictions accurate? framen(240) ## [1] 0.5082465 Yes! ~50% of packages will go bad within 8 months. Pretty accurate! Whats the average failure rate (\\(\\lambda\\)) for the period between 8 months (240 days) to 1 year? lambda &lt;- afrramen(240, 365) # Check it! lambda ## [1] 0.0256399 On average, between 8 months to 1 year, ramen packages go bad at a rate of ~0.026 units per day. Whats the mean time to fail (\\(m\\)) for the period between 8 months to 1 year? # Calculate the inverse of lambda! m &lt;- 1 / lambda # check it! m ## [1] 39.00172 39 days per package. In other words, during this period post-expiration, this data suggests 1 package will go bad every 39 days. 4.5 System Reliability Reliability rates become extremely useful when we look at an entire system! This is where system reliability analysis can really improve the lives of ordinary people, decision-makers, and day-to-day users, because we can give them the knowledge they need to make decisions. So what knowledge do users usually need? How likely is the system as a whole to survive (or fail) over time? 4.5.1 Series Systems (#fig:fig_series)Dominos: A Series System In a series system, we have a set of \\(n\\) components (sometimes called nodes in a network), which get utilized sequentially. A domino train, for example, is a series system. It only takes 1 component to fail to stop the entire system (causing system failure). The overall reliability of a series system is defined as the success of every individual component (A AND B AND C). We write it using the formula below. \\[ Series \\ System \\ Reliability = R_{S} = \\prod^{n}_{i = 1}R_{i} = R_1 \\times R_2 \\times ... \\times R_n \\] We can also visualize it below, where each labelled node is a component. (#fig:mermaid_series)Figure 6. Example Series System 4.5.2 Parallel Systems (#fig:fig_parallel)Spoons: A Parallel System In a parallel system (a.k.a. redundant system), we have a set of \\(n\\) components, but only 1 component needs to function in order for the system to function. The overall reliability of a series system is defined as the success of any individual component (A OR B OR [A AND B]). A silverware drawer is an simple example of a parallel system. You probably just need 1 spoon for yourself at any time, but you have a stock of several spoons available in case you need them. We write it using the formula below, where each component \\(i\\) has a reliability rate of \\(R_{i}\\) and a failure rate of \\(F_{i}\\). \\[ Parallel \\ System \\ Reliability = R_{P} = 1 - \\prod^{n}_{i = 1}(1 - R_{i}) = 1 - \\prod^{n}_{i = 1}F_{i} = 1 - (F_1 \\times F_2 \\times ... \\times F_n) \\] Or we can represent it visually, where each labelled node is a component. (Unlabelled nodes are just junctions for relationships, so they dont get a probability. Some diagrams will not have these; you need them in mermaid.) (#fig:mermaid_parallel)Figure 7. Example Parallel System 4.5.3 Combined Systems Most systems actually involve combining the probabilities of several subsystems. When combining configurations, we calculate the probabilities of each subsystem, then then calculate the overall probability of the final system. Series System with Nested Parallel System: Figure 4.3: Figure 8. Series System with Nested Parallel System In the Figure above, we calculate the reliability rate for the parallel system, then calculate the reliability rate for the whole series; the parallel systems rate becomes just one probability in the overall series system: \\(0.80 \\times (1 - (1 - 0.98) \\times (1 - 0.99) \\times (1 - 0.90)) \\times 0.95 \\approx 0.76\\). Parallel System with Nested Series Systems (Right Diagram): Figure 4.4: Figure 9. Parallel System with Nested Series Systems In the figure above, we calculate the reliability rate for each series system first, then calculate the reliability rate for the whole parallel system; each series rate becomes one probability in the overall parallel system. \\(1 - ((1 - 0.80 \\times 0.90) \\times (1 - 0.95 \\times 0.99)) \\approx 0.98\\) The key is identifying exactly which system is nested in which system! 4.5.4 Example: Business Reliability Local businesses deal heavily with series system reliability, even if they dont regularly model it. Youve been hired to analyze the system reliability of our local coffee shop Coffee Please! Our coffee shops ability to serve cold brew coffee relies on 5 components, each with its own constant chance of failure. Water: Access to clean tap water. (Water outages occur ~ 3 days a year.) Coffee Grounds: Sufficient coffee grounds supply. (Ran out of stock 5 days in the last year). Refrigerator: Refrigerate coffee for 12 hours. (1% fail in a year.) Dishwasher: Run dishwasher on used cups (failed 2 times in last 60 days). Register: Use Cash Register to process transaction and give change (Failed 5 times in the 3 months). We can represent this in a system diagram below. (#fig:coffee_series)Figure 10. Example Series System in a Coffeeshop We can extract the average daily failure rate \\(lambda\\) for each of these components. # Water outage occrred 3 days in last 365 days lambda_water &lt;- 3 / 365 # Ran out of stock 5 days in last 365 days lambda_grounds &lt;- 5 / 365 # 1% of refrigerators fail within a 365 days lambda_refrigerator &lt;- 0.01 / 365 # Failed 2 times in last 60 days lambda_dishwasher &lt;- 2 / 60 # Failed 5 times in last 90 days lambda_cash &lt;- 5 / 90 Assuming a constant chance of failure, we can write ourselves a quick failure function f and reliability function r for an exponential distribution. # Write a reliability function r = function(t, lambda){ exp(-1*t*lambda) } And we can calculate the overall reliability of this coffeeshops series system in 1 day by multiplying these reliability rates together. r(1, lambda_water) * r(1, lambda_grounds) * r(1, lambda_refrigerator) * r(1, lambda_dishwasher) * r(1, lambda_cash) ## [1] 0.8950872 This means theres an 89.5% chance of this system fully functioning on a given day! 4.6 Renewal Rates via Bayes Rule We would hope that failed parts often get replaced, so we might want to adjust our functions accordingly. Renewal Rate: \\(r(t)\\) reflects the mean number of failures per unit at time \\(t\\). Example: Lets say that For 10 units, we calculated how many days post production they lasted till failure (failure) as well as how many days post production till they were replaced (replace). Using this, we can calculate the lag-time, or the time taken for renewal. units &lt;- tibble( id = 1:15, failure = c(10, 200, 250, 300, 350, 375, 525, 525, 525, 525, 600, 650, 650, 675, 725), replace = c(100, 250, 350, 440, 550, 390, 600, 625, 660, 605, 700, 700, 700, 725, 750), renewal = replace - failure ) # Let&#39;s get the failure funciont... # by calculating the CDF of failure # first, we calculate the PDF of failure f &lt;- units$failure %&gt;% density() %&gt;% tidy() %&gt;% # Get CDF mutate(y = cumsum(y) / sum(y)) %&gt;% # turn into function approxfun() # Let&#39;s also calculate the CDF of replacement fr &lt;- units$replace %&gt;% density() %&gt;% tidy() %&gt;% # Get CDF mutate(y = cumsum(y) / sum(y)) %&gt;% # Get function approxfun() Above, we made the function fr(), which represents the cumulative probability of replacement, unrelated to failure. But what we really want to know is a conditional probability, specifically: how likely is a unit to get replaced at time \\(b\\), given that it failed at time \\(a\\)? Fortunately, we can use Bayes Rule to deduce this. First, lets make a function f_fr(), meaning the cumulative probability of failure given replacement. This should (probably) be the same probability of failure as usual, but we need to restart the clock after replacement, so well set the time as \\(time_{failure} - time_{replacement}\\). # Probability of failure given replacement f_fr = function(time, time_replacement){ f(time - time_replacement) } Next, well use Bayes Rule to get the cumulative probability of replacement given failure, estimated in a function fr_f(). # Probability of replacement given failure fr_f = function(time, time_replacement){ # Estimate conditional probability of Failure given Replacement times Replacement top &lt;- f_fr(time, time_replacement) * fr(time_replacement) # Estimate total probability of Failure bottom &lt;- f_fr(time, time_replacement) * fr(time_replacement) + (1 - f_fr(time, time_replacement)) * (1 - fr(time_replacement)) # Divide them, and voila! top/bottom } Finally, what do these functions actually look like? Lets simulate failure and replacement over time, in a dataset of fakeunits. fakeunits &lt;- tibble( # As time increases from 0 to 1100, time = seq(0, 1100, by = 1), # Let&#39;s get the probability of failure at that time, prob_f = time %&gt;% f(), # Let&#39;s get the probability of replacement at time t + 10 given failure at time t prob_fr_f_10 = fr_f(time = time, time_replacement = time + 10), # How about t + 50? prob_fr_f_50 = fr_f(time = time, time_replacement = time + 50) ) # Check it! fakeunits %&gt;% head(3) ## # A tibble: 3 × 4 ## time prob_f prob_fr_f_10 prob_fr_f_50 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.0341 0.000436 0.000484 ## 2 1 0.0344 0.000442 0.000490 ## 3 2 0.0348 0.000449 0.000496 And lets visualize it! (#fig:img_cumulative)Figure 11. Renewal Rates From this point on, the math for \\(z(t)\\), \\(H(t)\\), and \\(AFR(t)\\) for renewal rates gets a little tricky, so well save that for another day. But youre well on your way to working with life distributions! "],["workshop-statistical-process-control-in-r.html", "5 Workshop: Statistical Process Control in R Getting Started 5.1 Visualizing Quality Control Learning Check 1 5.2 Average and Standard Deviation Graphs Learning Check 2 5.3 Moving Range Charts 5.4 Constants Learning Check 3", " 5 Workshop: Statistical Process Control in R Figure 1.1: Statistical Process Control! In this workshop, we will learn how to perform statistical process control in R, using statistical tools and ggplot visualizations! Statistical Process Control refers to using statistics to (1) measure variation in product quality over time and (2) identify benchmarks to know when intervention is needed. Lets get started! Getting Started Packages Well be using the tidyverse package for visualization, viridis for color palletes, moments for descriptive statistics, plus ggpubr for some add-on functions in ggplot. library(tidyverse) library(viridis) # you&#39;ll probably need to install these packages! # install.packages(c(&quot;ggpubr&quot;, &quot;moments&quot;)) library(ggpubr) library(moments) Our Case Figure 1.4: Obanazawa City, Yamagata Prefecture - A Hot Springs Economy. Photo credit and more here. For todays workshop, were going to think about why quality control matters in a local economy, by examining the case of the Japanese Hot Springs bath economy! Hot springs, or onsen, are a major source of tourism and recreation for families in Japan, bringing residents from across the country every year to often rural communities where the right geological conditions have brought on naturally occurring hot springs. Restaurants, taxi and bus companies, and many service sector firms rely on their local onsen to bring in a steady stream (pun intended) of tourists to the local economy. So, its often in the best interest of onsen operators to keep an eye on the temperature, minerals, or other aspects of their hot springs baths to ensure quality control, to keep up their firm (and towns!) reputation for quality rest and relaxation! Onsen-goers often seek out specific types of hot springs, so its important for an onsen to actually provide what it advertises! Serbulea and Payyappallimana (2012) describe some of these benchmarks. Temperature: Onsen are divided into Extra Hot Springs (&gt;42 °C), Hot Springs (41~34°C), and Warm Springs (33~25°C). pH: Onsen are classified into Acidic (pH &lt; 3), Mildly Acidic (pH 3~6), Neutral (pH 6~7.5), Mildly alkaline (ph 7.5~8.5), and Aklaline (pH &gt; 8.5). Sulfur: Sulfur onsen typically have about 2mg of sulfur per 1kg of hot spring water; sulfur levels must exceed 1 mg to count as a Sulfur onsen. (It smells like rotten eggs!) These are decent examples of quality control metrics that onsen operators might want to keep tabs on! Figure 5.1: Monkeys are even fans of onsen! Read more here! Our Data Youve been hired to evaluate quality control at a local onsen in sunny Kagoshima prefecture! Every month, for 15 months, you systematically took 20 random samples of hot spring water and recorded its temperature, pH, and sulfur levels. How might you determine if this onsen is at risk of slipping out of one sector of the market (eg. Extra Hot!) and into another (just normal Hot Springs?). Lets read in our data from workshops/onsen.csv! # Let&#39;s import our samples of bathwater over time! water = read_csv(&quot;workshops/onsen.csv&quot;) # Take a peek! water %&gt;% glimpse() ## Rows: 160 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,  ## $ time &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3,  ## $ temp &lt;dbl&gt; 43.2, 45.3, 45.5, 43.9, 45.9, 45.0, 42.3, 44.2, 42.2, 43.4, 46. ## $ ph &lt;dbl&gt; 5.1, 4.8, 6.2, 6.4, 5.1, 5.6, 5.5, 5.3, 5.2, 5.9, 5.8, 5.3, 5.9 ## $ sulfur &lt;dbl&gt; 0.0, 0.4, 0.9, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.7, 1.1, 0.1 Our dataset contains: id: unique identifer for each sample of onsen water. time: categorical variable describing date of sample (month 1, month 3,  month 15). temp: temperature in celsius. ph: pH (0 to 14) sulfur: milligrams of sulfur ions. 5.1 Visualizing Quality Control Lets learn some key techniques for visualizing quality control! 5.1.1 theme_set() First, when youre about to make a bunch of ggplot visuals, it can help to set a common theme across them all with theme_set(). # By running theme_set() theme_set( # we tell ggplot to give EVERY plot this theme theme_classic(base_size = 14) + # With these theme traits, including theme( # Putting the legend on the bottom, if applicable legend.position = &quot;bottom&quot;, # horizontally justify plot subtitle and caption in center plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5), plot.caption = element_text(hjust = 0.5), # Getting rid of busy axis ticks axis.ticks = element_blank(), # Getting rid of busy axis lines axis.line = element_blank(), # Surrounding the plot in a nice grey border panel.border = element_rect(fill = NA, color = &quot;grey&quot;), # Remove the right margin, for easy joining of plots plot.margin = margin(r = 0) ) ) 5.1.2 Process Descriptive Statistics First, lets describe our process, using favorite description statistics. Were going to want to do this a bunch, so why dont we just write a function for it? Lets write describe(), which will take a vector x and calculate the mean(), sd(), skewness(), and kurtosis(), and then paste() together a nice caption describing them. (I encourage you to write your own functions like this to help expedite your coding! Start simple!) describe = function(x){ # Put our vector x in a tibble tibble(x) %&gt;% # Calculate summary statistics summarize( mean = mean(x, na.rm = TRUE), sd = sd(x, na.rm = TRUE), # We&#39;ll use the moments package for these two skew = skewness(x, na.rm = TRUE), kurtosis = kurtosis(x, na.rm = TRUE)) %&gt;% # Let&#39;s add a caption, that compiles all these statistics mutate( # We&#39;ll paste() the following together caption = paste( # Listing the name of each stat, then reporting its value and rounding it, then separating with &quot; | &quot; &quot;Process Mean: &quot;, mean %&gt;% round(2), &quot; | &quot;, &quot;SD: &quot;, sd %&gt;% round(2), &quot; | &quot;, &quot;Skewness: &quot;, skew %&gt;% round(2), &quot; | &quot;, &quot;Kurtosis: &quot;, kurtosis %&gt;% round(2), # Then make sure no extra spaces separate each item sep = &quot;&quot;)) %&gt;% return() } # Run descriptives! tab = water$temp %&gt;% describe() # Check it out! tab ## # A tibble: 1 × 5 ## mean sd skew kurtosis caption ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 44.8 1.99 0.0849 3.62 Process Mean: 44.85 | SD: 1.99 | Skewness: 0.08 | 5.1.3 Process Overview Visual Your first step should always be to look at the data overall! geom_jitter(), geom_boxplot(), and ggMarginal() can help you do this. geom_jitter() is a jittered scatterplot, jittering points a little to help with visibility. Since we want to be really precise on our quality control metrics, we could jitter the width a little (width = 0.25), but hold the y-axis (quality metric) constant at height = 0. These are in your x and y axis units, so decide based on your data each time. geom_boxplot() makes a boxplot for each group (time) in our data, showing the interquartile range (25th, 50th, and 75th percentiles) of our y variable for each group. Helpful way to view distributions. (Alternatively, you can try geom_violin(), which works the same way.) geom_hline() makes a horizontal line at the yintercept; we can tell it to show the mean() of y (in this case, temp). geom_histogram() is a histogram! We can use coord_flip() to turn it vertical to match the y-axis. ggarrange() from the ggpubr package binds two plots together into one, giving each a specific proportional width (eg. c(0.25, 0.75) percent, or c(5, 1) would be 5/6 and 1/6.) # Make the initial boxplot... g1 = water %&gt;% ggplot(mapping = aes(x = time, y = temp, group = time)) + # Plot grand mean geom_hline(mapping = aes(yintercept = mean(temp)), color = &quot;lightgrey&quot;, size = 3) + # Plot points and boxplots geom_jitter(height = 0, width = 0.25) + geom_boxplot() + labs(x = &quot;Time (Subgroup)&quot;, y = &quot;Temperature (Celsius)&quot;, subtitle = &quot;Process Overview&quot;, # Add our descriptive stats in the caption! caption = tab$caption) # Part 1 of plot g1 # Make the histogram, but tilt it on its side g2 = water %&gt;% ggplot(mapping = aes(x = temp)) + geom_histogram(bins = 15, color = &quot;white&quot;, fill = &quot;grey&quot;) + theme_void() + # Clear the them coord_flip() # tilt on its side # Part 2 of plot g2 # Then bind them together into 1 plot, &#39;h&#39;orizontally aligned. p1 = ggarrange(g1,g2, widths = c(5,1), align = &quot;h&quot;) # Check it out! p1 We can tell from this visual several things! Side Histogram: Our overall distribution is pretty centered. Descriptive Statistics: Our distribution has little skew (~0) and has slightly higher-than-average kurtosis (&lt;3) (very centered) (Review Skewness and Kurtosis here.) Line vs. Boxplots: Over time, our samples sure do seem to be getting slightly further from the mean! Learning Check 1 Question We analyzed temperature variation above, but our hot springs owner wants to know about variation in pH too! Write a function to produce a process overview plot given any 2 vectors (water$time and water$pH, in this case), and visualize the process overview for pH! (You can do it!) [View Answer!] ggprocess = function(x, y, xlab = &quot;Subgroup&quot;, ylab = &quot;Metric&quot;){ # Get descriptive statistics tab = describe(y) # Make the initial boxplot... g1 = ggplot(mapping = aes(x = x, y = y, group = x)) + # Plot grand mean geom_hline(mapping = aes(yintercept = mean(y)), color = &quot;lightgrey&quot;, size = 3) + # Plot points and boxplots geom_jitter(height = 0, width = 0.25) + geom_boxplot() + labs(x = xlab, y = ylab, subtitle = &quot;Process Overview&quot;, # Add our descriptive stats in the caption! caption = tab$caption) # Make the histogram, but tilt it on its side g2 = ggplot(mapping = aes(x = y)) + geom_histogram(bins = 15, color = &quot;white&quot;, fill = &quot;grey&quot;) + theme_void() + # Clear the them coord_flip() # tilt on its side # Then bind them together into 1 plot, &#39;h&#39;orizontally aligned. p1 = ggarrange(g1,g2, widths = c(5,1), align = &quot;h&quot;) return(p1) } # Visualize it! ggprocess(x = water$time, y = water$ph) In comparison to temp, pH is a much more controlled process. I encourage you to use this ggprocess() function you just created! 5.2 Average and Standard Deviation Graphs 5.2.1 Key Statistics Next, to analyze these processes more in depth, we need to assemble statistics at 2 levels: within-group statistics measure quantities of interest within each subgroup (eg. each monthly slice time in our onsen data). between-group statistics measure total quantities of interest for the overall process (eg. the overall grand mean, overall standard deviation in our onsen data). 5.2.2 Subgroup (Within-Group) Statistics Lets apply these to our onsen data to get statistics describing each subgroups distribution, a.k.a. short-term or within-group statistics. # Calculate short-term statistics within each group stat_s = water %&gt;% # For each timestpe group_by(time) %&gt;% # Calculate these statistics of interest! summarize( # within-group mean xbar = mean(temp), # within-group range r = max(temp) - min(temp), # within-group standard deviation sd = sd(temp), # within-group sample size nw = n(), # Degrees of freedom within groups df = nw - 1) %&gt;% # Last, we&#39;ll calculate sigma_short (within-group variance) # We&#39;re going to calculate the short-term variation parameter sigma_s (sigma_short) # by taking the square root of the average of the standard deviation # Essentially, we&#39;re weakening the impact of any special cause variation # so that our sigma is mostly representative of common cause (within-group) variation mutate( # these are equivalent sigma_s = sqrt( sum(df * sd^2) / sum(df) ), sigma_s = sqrt(mean(sd^2)), # And get standard error (in a way that retains each subgroup&#39;s sample size!) se = sigma_s / sqrt(nw), # Calculate 6-sigma control limits! upper = mean(xbar) + 3*se, lower = mean(xbar) - 3*se) # Check it! stat_s %&gt;% head(3) ## # A tibble: 3 × 10 ## time xbar r sd nw df sigma_s se upper lower ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 44.6 4.20 1.34 20 19 1.99 0.444 46.2 43.5 ## 2 3 45.3 7.9 2.00 20 19 1.99 0.444 46.2 43.5 ## 3 5 44.8 5.90 1.63 20 19 1.99 0.444 46.2 43.5 5.2.3 Total Statistics (Between Groups) # To get between-group estimates.... stat_t = stat_s %&gt;% summarize( xbbar = mean(xbar), rbar = mean(r), sdbar = mean(sd), # We can also recalculate sigma_short here too sigma_s = sqrt( mean(sd^2) ), # Or we can calculate overall standard deviation sigma_t = sd(water$temp) ) So, now that we have estimated within-group, common cause variation via \\(\\sigma_{short}\\) (sigma_s) and the standard error (se), what can we say about our process? 5.2.4 Average and Standard Deviation Charts The preferred method for measuring within-group variability is the standard deviation, rather than the range, so we generally recommend (a) Average (\\(\\bar{X}\\)) and Standard Deviation (\\(S\\)) charts over (b) Average (\\(\\bar{X}\\)) and Range (\\(R\\)) charts. # Let&#39;s extract some labels labels = stat_s %&gt;% summarize( time = max(time), type = c(&quot;xbbar&quot;, &quot;upper&quot;, &quot;lower&quot;), name = c(&quot;mean&quot;, &quot;+3 s&quot;, &quot;-3 s&quot;), value = c(mean(xbar), unique(upper), unique(lower)), value = round(value, 2), text = paste(name, value, sep = &quot; = &quot;)) stat_s %&gt;% ggplot(mapping = aes(x = time, y = xbar)) + geom_hline(mapping = aes(yintercept = mean(xbar)), color = &quot;lightgrey&quot;, size = 3) + geom_ribbon(mapping = aes(ymin = lower, ymax = upper), fill = &quot;steelblue&quot;, alpha = 0.2) + geom_line(size = 1) + geom_point(size = 5) + # Plot labels geom_label(data = labels, mapping = aes(x = time, y = value, label = text), hjust = 1) + labs(x = &quot;Time (Subgroups)&quot;, y = &quot;Average&quot;, subtitle = &quot;Average and Standard Deviation Chart&quot;) This tells us that excitingly, our onsen temperatures are quite firmly within range. While the average varies quite a bit, it remains comfortably within 3 standard deviations of the mean. Learning Check 2 Question Well, that was nifty, but can we do it all over again for pH? Make some rad upper and lower confidence intervals for \\(\\bar{X}\\) for pH! [View Answer!] # Get the within-group stats for ph! ph_s = water %&gt;% group_by(time) %&gt;% summarize( xbar = mean(ph), r = max(ph) - min(ph), sd = sd(ph), nw = n(), df = nw - 1) %&gt;% mutate( sigma_s = sqrt(mean(sd^2)), se = sigma_s / sqrt(nw), upper = mean(xbar) + 3*se, lower = mean(xbar) - 3*se) # Let&#39;s extract some labels labels = ph_s %&gt;% summarize( time = max(time), type = c(&quot;xbbar&quot;, &quot;upper&quot;, &quot;lower&quot;), name = c(&quot;mean&quot;, &quot;-3 s&quot;, &quot;+3 s&quot;), value = c(mean(xbar), unique(upper), unique(lower)), value = round(value, 2), text = paste(name, value, sep = &quot; = &quot;)) # and let&#39;s visualize it! ph_s %&gt;% ggplot(mapping = aes(x = time, y = xbar)) + geom_hline(mapping = aes(yintercept = mean(xbar)), color = &quot;lightgrey&quot;, size = 3) + geom_ribbon(mapping = aes(ymin = lower, ymax = upper), fill = &quot;steelblue&quot;, alpha = 0.2) + geom_line(size = 1) + geom_point(size = 5) + # Plot labels geom_label(data = labels, mapping = aes(x = time, y = value, label = text), hjust = 1) + labs(x = &quot;Time (Subgroups)&quot;, y = &quot;Average pH&quot;, subtitle = &quot;Average and Standard Deviation Chart&quot;) 5.3 Moving Range Charts 5.3.1 Individual and Moving Range Charts Suppose we only had 1 observation per subgroup! Theres no way to calculate standard deviation for that - after all, theres no variation within each subgroup! Instead, we can generate an individual and moving range chart. # Suppose we sample just the first out of each our months. indiv = water %&gt;% filter(id %in% c(1, 21, 41, 61, 81, 101, 121, 141)) The average moving range \\(m\\bar{R}\\) aptly refers to the average of the moving Range \\(mR\\), the difference in values over time. We can calculate the moving range using the diff() function on a vector like temp, shown below. abs() converts each value to be positive, since ranges are always 0 to infinity. # Let&#39;s see our original values indiv$temp ## [1] 43.2 46.0 46.6 42.1 44.4 46.8 43.7 45.9 # diff() gets range between second and first, third and second, and so on indiv$temp %&gt;% diff() %&gt;% abs() ## [1] 2.8 0.6 4.5 2.3 2.4 3.1 2.2 Just like all statistics, \\(m\\bar{R}\\) too has its own distribution, containing a range of slightly higher and lower \\(m\\bar{R}\\) statistics we might have gotten had our sample been just slightly different due to chance. As a result, we will want to estimate a confidence interval around \\(m\\bar{R}\\), but how are we to do that if we have no statistic like \\(\\sigma\\) to capture that variation? Well, good news: We can approximate \\(\\sigma_s\\) by taking the ratio of \\(m\\bar{R}\\) over a factor called \\(d_{2}\\). What is \\(d_{2}\\)? Im glad you asked! \\[\\sigma_{short} \\approx \\frac{m\\bar{R}}{d_{2}} \\] 5.3.2 Factors \\(d_{2}\\) and Friends As discussed above, any statistics has a latent distribution of other values you might have gotten for your statistic had your sample been just slightly different due to random error. Fun fact: we can actually see those distributions pretty easily thanks to simulation! Suppose we have a subgroup of size n = 1, so we calculate a moving range of length 1. This subgroup and its moving range is just one of the possible subgroups we could have encountered by chance, so we can think of it as a random draw from an archetypal normal distribution (mean = 0 and sd = 1). If we take enough samples of moving ranges from that distribution, we can plot the distribution. Below, we take n = 10000 samples with rnorm() and plot the vector with hist(). We find a beautiful distribution of moving range statistics for n=1 size subgroups. mrsim = rnorm(n = 10000, mean = 0, sd = 1) %&gt;% diff() %&gt;% abs() mrsim %&gt;% hist() Much like \\(k\\) factors in the exponential distribution, we can use this distribution of \\(mR\\) stats to produce a series of factors that can help us estimate any upper or lower confidence interval in a moving range distribution. For example, we can calculate: \\(d_{2}\\), the mean of this archetypal \\(mR\\) distribution. \\(d_{3}\\), the standard deviation of this \\(mR\\) distribution. Technically, \\(d_{2}\\) is a ratio, which says that in a distribution with a standard deviation of 1, the mean mR is \\(d_{2}\\). In other words, \\(d_{2} = \\frac{m\\bar{R}_{normal, n = 1} }{\\sigma_{normal,n=1} }\\). So, if we have observed a real life average moving range \\(m\\bar{R}_{observed,n=1}\\), we can use this \\(d_{2}\\) factor to convert out of units of \\(1 \\sigma_{normal,n=1}\\) into units the \\(\\sigma_{short}\\) of our observed data! # For example, the mean of our vector mrsim, for subgroup size n = 1, # says that d2 (mean of these mR stats) is... mrsim %&gt;% mean() ## [1] 1.126582 # While d3 (standard deviation is...) mrsim %&gt;% sd() ## [1] 0.8419088 But why stop there? We can calculate loads of other interesting statistics! # For example, these statistics # estimate the median, upper 90, and upper 95% of the distribution! mrsim %&gt;% quantile(probs = c(0.50, 0.90, .95)) %&gt;% round(3) ## 50% 90% 95% ## 0.945 2.306 2.734 5.3.3 Estimating \\(\\sigma_{short}\\) for Moving Range Statistics Lets apply our new knowledge about \\(d_{2}\\) to calculate some upper bounds (\\(+3 \\sigma\\)) for our average moving range estimates! istat_s = indiv %&gt;% summarize( time = time[-1], # get moving range mr = temp %&gt;% diff() %&gt;% abs(), # Get average moving range mrbar = mean(mr), # Get total sample size! d2 = rnorm(n = 10000, mean = 0, sd = 1) %&gt;% diff() %&gt;% abs() %&gt;% mean(), # If we approximate sigma_s.... # pretty good! sigma_s = mrbar / d2, # Our subgroup size was 1, right? n = 1, # so this means sigma_s just equals the standard error here se = sigma_s / sqrt(n), # compute upper 3-se bound upper = mrbar + 3 * se, # and lower ALWAYS equals 0 for moving range lower = 0) Why stop there? Lets visualize it! # Let&#39;s get our labels! labels = istat_s %&gt;% summarize( time = max(time), type = c(&quot;mean&quot;, &quot;+3 s&quot;, &quot;lower&quot;), value = c(mrbar[1], upper[1], lower[1]) %&gt;% round(2), name = paste(type, value, sep = &quot; = &quot;)) # Now make the plot! istat_s %&gt;% ggplot(mapping = aes(x = time, y = mr)) + # Plot the confidence intervals geom_ribbon(mapping = aes(x = time, ymin = lower, ymax = upper), fill = &quot;steelblue&quot;, alpha = 0.25) + # Plot mrbar geom_hline(mapping = aes(yintercept = mean(mr)), size = 3, color = &quot;darkgrey&quot;) + # Plot moving range geom_line(size = 1) + geom_point(size = 5) + geom_label(data = labels, mapping = aes(x = time, y = value, label = name), hjust = 1) + labs(x = &quot;Time (Subgroup)&quot;, y = &quot;Moving Range&quot;, subtitle = &quot;Moving Range Chart&quot;) Again, this process looks pretty sound, firmly within range. 5.4 Constants 5.4.1 Find any \\(d_x\\) Factor While our book writes extensively about \\(d_{2}\\) and other \\(d_{whatever}\\) factors, its not strictly necessary to calculate them unless you need them. Usually, we do this when we cant calculate the standard deviation normally (eg. when we have only moving range statistics or only a subgroup sample size of n=1). If youre working with full data from your process though, you can easily calculate \\(\\sigma_{short}\\) right from the empirical data, without ever needing to use \\(d_x\\) factors. But lets say you did need a \\(d_x\\) factor for a subgroup range of a given sample size n = 1, 2, 3.... n. Could we calculate some kind of function to give us it? Funny you should ask! Ive written a little helper function you can use. # Let&#39;s calculate our own d function dn = function(n, reps = 1e4){ # For 10,0000 reps tibble(rep = 1:reps) %&gt;% # For each rep, group_by(rep) %&gt;% # Simulate the ranges of n values summarize(r = rnorm(n = n, mean = 0, sd = 1) %&gt;% range() %&gt;% diff() %&gt;% abs()) %&gt;% ungroup() %&gt;% # And calculate... summarize( # Mean range d2 = mean(r), # standard deviation of ranges d3 = sd(r), # and constants for obtaining lower and upper ci for rbar D3 = 1 - 3*(d3/d2), # sometimes written D3 D4 = 1 + 3*(d3/d2), # sometimes written D4 # Sometimes D3 goes negative; we need to bound it at zero D3 = if_else(D3 &lt; 0, true = 0, false = D3) ) %&gt;% return() } # Let&#39;s try it, where subgroup size is n = 2 dn(n = 2) ## # A tibble: 1 × 4 ## d2 d3 D3 D4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.11 0.838 0 3.27 # Let&#39;s get the constants we need too. # Each of our samples has a sample size of 20 d = dn(n = 20) # Check it! d ## # A tibble: 1 × 4 ## d2 d3 D3 D4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.74 0.731 0.413 1.59 5.4.2 Using \\(d_x\\) factors Using d_n(), we can make a quick approximation for the upper and lower control limits for the range \\(\\bar{R}\\) as well (as opposed to \\(m\\bar{R}\\))! # Let&#39;s get within group range for temperature... stat_w = water %&gt;% group_by(time) %&gt;% summarize(r = temp %&gt;% range() %&gt;% diff() %&gt;% abs(), n_w = n()) # get subgroup size # Let&#39;s get average within group range for temperature... stat = stat_w %&gt;% summarize(rbar = mean(r), # get Rbar n_w = unique(n_w)) # assuming constant subgroup size... # Check it! stat ## # A tibble: 1 × 2 ## rbar n_w ## &lt;dbl&gt; &lt;int&gt; ## 1 7.26 20 # We find that dn() gives us constants D3 and D4... mydstat = dn(n = stat$n_w) mydstat ## # A tibble: 1 × 4 ## d2 d3 D3 D4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.74 0.735 0.411 1.59 And use these constants to estimate the upper and lower CI for \\(\\bar{r}\\)! stat %&gt;% mutate(rbar_lower = rbar * mydstat$D3, rbar_upper = rbar * mydstat$D4) %&gt;% select(rbar, rbar_lower, rbar_upper) ## # A tibble: 1 × 3 ## rbar rbar_lower rbar_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7.26 2.98 11.5 So quick! You could use these values to make a range chart now. remove(stat, stat_w) 5.4.3 Finding any \\(b_x\\) Factor. We might also want to know how much the standard deviation could possible vary due to sampling error. To figure this out, well simulate many many standard deviations from a normal distribution, like in dn(), for a given subgroup size n. Then, we can calculate some quantities of interest like \\(C_{4}\\) (the mean standard deviation from an archetypal normal distribution), \\(B_{3}\\) (a multiplier for getting the lower control limit for 3 sigmas), and \\(B_{4}\\) (a multiplier for getting the upper control limit for 3 sigmas.) # Let&#39;s write a function bn() to calculate our B3 and B4 statistics for any subgroup size n bn = function(n, reps = 1e4){ tibble(rep = 1:reps) %&gt;% group_by(rep) %&gt;% summarize(s = rnorm(n, mean = 0, sd = 1) %&gt;% sd()) %&gt;% summarize(b2 = mean(s), b3 = sd(s), C4 = b2, # this is sometimes called C4 A3 = 3 / (b2 * sqrt( n )), B3 = 1 - 3 * b3/b2, B4 = 1 + 3 * b3/b2, # bound B3 at 0, since we can&#39;t have a standard deviation below 0 B3 = if_else(B3 &lt; 0, true = 0, false = B3)) %&gt;% return() } Lets apply this to our temp vector. First, well calculate the standard deviation within each subgroup, saved in stat_w under s. # Let&#39;s get within group standard deviation for temperature... stat_w = water %&gt;% group_by(time) %&gt;% summarize(s = temp %&gt;% sd(), n_w = n()) # get subgroup size stat_w ## # A tibble: 8 × 3 ## time s n_w ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 1.34 20 ## 2 3 2.00 20 ## 3 5 1.63 20 ## 4 7 2.66 20 ## 5 9 2.57 20 ## 6 11 2.02 20 ## 7 13 1.65 20 ## 8 15 1.61 20 Second, well calculate the average standard deviation across subgroups, saved in stat under sbar. # Let&#39;s get average within group range for temperature... stat = stat_w %&gt;% summarize(sbar = mean(s), # get Rbar n_w = unique(n_w)) # assuming constant subgroup size... # Check it! stat ## # A tibble: 1 × 2 ## sbar n_w ## &lt;dbl&gt; &lt;int&gt; ## 1 1.94 20 Third, well get our constants \\(B_{3}\\) and \\(B_{4}\\)! # For a subgroup size of 20... stat$n_w ## [1] 20 # Let&#39;s get our B constants! mybstat = bn(n = stat$n_w) # Check it out! mybstat ## # A tibble: 1 × 6 ## b2 b3 C4 A3 B3 B4 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.988 0.162 0.988 0.679 0.508 1.49 Finally, lets calculate our control limits! stat = stat %&gt;% # Add our constants to the data.frame... mutate(mybstat) %&gt;% # Calculate 3 sigma control limits mutate(sbar_lower = sbar * B3, sbar_upper = sbar * B4) # Check it out! stat %&gt;% select(sbar, sbar_lower, sbar_upper) ## # A tibble: 1 × 3 ## sbar sbar_lower sbar_upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.94 0.984 2.89 Now youre all ready to make a control chart showing variation in the standard deviation! Learning Check 3 Question Using our dn() function above, compile for yourself a short table of the \\(d_{2}\\) and \\(d_{3}\\) factors for subgroups sized 2 to 10. [View Answer!] # Let&#39;s bind them together! dx = bind_rows( dn(2), dn(3), dn(4), dn(5), dn(6), dn(7), dn(8), dn(9), dn(10)) %&gt;% mutate(n = 2:10) %&gt;% select(n, d2, d3) # Look at that cool table! dx ## # A tibble: 9 × 3 ## n d2 d3 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 1.14 0.860 ## 2 3 1.69 0.887 ## 3 4 2.05 0.874 ## 4 5 2.32 0.867 ## 5 6 2.53 0.847 ## 6 7 2.71 0.835 ## 7 8 2.86 0.815 ## 8 9 2.96 0.810 ## 9 10 3.09 0.802 All done! Great work! "],["workshop-indices-and-confidence-intervals-for-statistical-process-control-in-r.html", "6 Workshop: Indices and Confidence Intervals for Statistical Process Control in R Getting Started 6.1 Process Capability vs. Stability 6.2 Index Functions Learning Check 1 6.3 Confidence Intervals 6.4 CIs for any Index! Conclusion", " 6 Workshop: Indices and Confidence Intervals for Statistical Process Control in R Figure 1.1: Bootstrapping Sampling Distributions for Statistics!! This workshop extends our toolkit developed in Workshop 9, discussing Process Capability and Stability Indices, and introducing means to calculate confidence intervals for these indices. Getting Started Packages Well be using the tidyverse package for visualization, viridis for color palletes, moments for descriptive statistics, plus ggpubr for some add-on functions in ggplot. library(tidyverse) library(viridis) # you&#39;ll probably need to install these packages! # install.packages(c(&quot;ggpubr&quot;, &quot;moments&quot;)) Our Data Well be continuing to analyze our quality control data from a local hot springs inn (onsen) in sunny Kagoshima Prefecture, Japan. Every month, for 15 months, you systematically took 20 random samples of hot spring water and recorded its temperature, pH, and sulfur levels. How might you determine if this onsen is at risk of slipping out of one sector of the market (eg. Extra Hot!) and into another (just normal Hot Springs?). Lets read in our data from workshops/onsen.csv! # Let&#39;s import our samples of bathwater over time! water = read_csv(&quot;workshops/onsen.csv&quot;) # Take a peek! water %&gt;% glimpse() ## Rows: 160 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,  ## $ time &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3,  ## $ temp &lt;dbl&gt; 43.2, 45.3, 45.5, 43.9, 45.9, 45.0, 42.3, 44.2, 42.2, 43.4, 46. ## $ ph &lt;dbl&gt; 5.1, 4.8, 6.2, 6.4, 5.1, 5.6, 5.5, 5.3, 5.2, 5.9, 5.8, 5.3, 5.9 ## $ sulfur &lt;dbl&gt; 0.0, 0.4, 0.9, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.7, 1.1, 0.1 Our dataset contains: id: unique identifer for each sample of onsen water. time: categorical variable describing date of sample (month 1, month 3,  month 15). temp: temperature in celsius. ph: pH (0 to 14) sulfur: milligrams of sulfur ions. 6.1 Process Capability vs. Stability 6.1.1 Definitions Production processes can be categorized in terms of Capability (does it meet required specifications?) and Stability (is production consistent and predictable?) Both are vital. A capable process delivers goods that can actually perform their function, like a 20-foot ladder that is actually 20 feet! A stable process delivers products with consistent and predictable traits (regardless of whether those traits are good). We need to maximize both process capability and stability to make an effective process, be it in manufacturing, health care, local businesses, or social life! Depending on the shape and stability of our data, we should choose one of the 4 statistics to evaluate our data. 6.1.2 Table of Indices These statistics rely on some combination of (1) the mean \\(\\mu\\), (2) the standard deviation \\(\\sigma\\), and (3) the upper and lower specification limits; the specification limits are our expected values \\(E_{upper}\\) and \\(E_{lower}\\), as compared to our actual observed values, summarized by \\(\\mu\\) and \\(\\sigma\\). Index Shape Sided Stability Formula Meaning Capability Indices (How \\(\\textit{could}\\) it perform, if stable?) \\(C_{p}\\) Centered 2-sided Stable \\(\\frac{E_{upper} - E_{lower}}{6 \\sigma_{short}}\\) How many times wider is the expected range than the observed range, assuming it is stable? \\(C_{pk}\\) Uncentered 1-sided Stable \\(\\frac{\\mid E_{limit} - \\mu \\mid}{3 \\sigma_{short}}\\) How many times wider is the expected vs. observed range for the left/right side, assuming it is stable? Process Performance Indices (How is it performing, stable or not?) \\(P_{p}\\) Centered 2-sided Unstable \\(\\frac{E_{upper} - E_{lower}}{6 \\sigma_{total}}\\) How many times wider is the expected vs. observed range, stable or not? \\(P_{pk}\\) Uncentered 1-sided Unstable \\(\\frac{\\mid E_{limit} - \\mu \\mid}{3 \\sigma_{total}}\\) How many times wider is the expected vs. observed range for the left/right side, stable or not? 6.2 Index Functions Lets do ourselves a favor and write up some simple functions for these. # Capability Index (for centered, normal data) cp = function(sigma_s, upper, lower){ abs(upper - lower) / (6*sigma_s) } # Process Performance Index (for centered, normal data) pp = function(sigma_t, upper, lower){ abs(upper - lower) / (6*sigma_t) } # Capability Index (for skewed, uncentered data) cpk = function(mu, sigma_s, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_s) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_s) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } # Process Performance Index (for skewed, uncentered data) ppk = function(mu, sigma_t, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_t) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_t) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } How might we use these indices to describe our process data? For example, recall that our onsen operator needs to be sure that their hot springs water is consistently falling into the temperature bins for Extra Hot Springs, which start at 42 degrees Celsius (and go as high as 80). Lets use those as our specification limits (pretty easy-going limits, I might add). Lets start by calculating our quantities of interest. stat = water %&gt;% group_by(time) %&gt;% summarize( xbar = mean(temp), # Get within group mean sd = sd(temp), # Get within group sigma n_w = n() # Get within subgroup size ) %&gt;% summarize( xbbar = mean(xbar), # Get grand mean sigma_s = sqrt(mean(sd^2)), # Get sigma_short sigma_t = sd(water$temp), # get sigma_total n = sum(n_w), # Get total observations n_w = unique(n_w), # get size of subgroups k = n()) # Get number of subgroups k # Check it! stat ## # A tibble: 1 × 6 ## xbbar sigma_s sigma_t n n_w k ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 44.8 1.99 1.99 160 20 8 6.2.1 Capacity Index \\(C_{p}\\) Our \\(C_{p}\\) Capacity index says, assuming that the distribution is centered and stable, how many times wider are our limits than our approximate observed distribution (\\(6 \\sigma_{short}\\))? mycp = cp(sigma_s = stat$sigma_s, lower = 42, upper = 80) # Check it! mycp ## [1] 3.18871 Great! This says, our observed variation is many times (3.1887098 times) narrower than the expected specification limits. 6.2.2 Process Performance Index \\(P_{p}\\) Our \\(P_{p}\\) Process Performance Index asks, even if the distribution is not stable (meaning it varies not just due to common causes), how many times wider are our specification limits than our approximate observed distribution (\\(6 \\sigma_{total}\\))? mypp = pp(sigma_t = stat$sigma_t, lower = 42, upper = 80) mypp ## [1] 3.183378 Much like before, the specification limit range remains quite bigger than the observed distribution. 6.2.3 Capacity Index \\(C_{pk}\\) Our \\(C_{pk}\\) Capacity index says, assuming the distribution is pretty stable across subgroups, how many times wider is (a) the distance from the tail of interest to the mean than (b) our approximate observed tail (3 sigma)? This always looks at the shorter tail. mycpk = cpk(sigma_s = stat$sigma_s, mu = stat$xbbar, lower = 42, upper = 80) mycpk ## [1] 0.4783065 If we only care about one of the tails, eg. the lower specification limit of 42, which is much closer than the upper limit of 80, we can just write the lower limit only. cpk(sigma_s = stat$sigma_s, mu = stat$xbbar, lower = 42) ## [1] 0.4783065 This says, our observed variation is much wider than the lower specification limit, since \\(C_{pk}\\) is far from 1, which which show equality. 6.2.4 Process Performance Index \\(P_{pk}\\) Our \\(P_{pk}\\) process performance index says, even if the distribution is neither stable nor centered, how much wider is the observed variation \\(3 \\sigma_{total}\\) than the distance from the tail of interest to the mean? We use \\(\\sigma_{total}\\) here to account for instability (considerable variation between subgroups) and one-tailed testing to account for the uncentered distribution. myppk = ppk(sigma_t = stat$sigma_t, mu = stat$xbbar, lower = 42, upper = 80) myppk ## [1] 0.4775067 6.2.5 Equality A final reason why these quantities are neat is that these 4 indices are related; if you know 3 of them, we can always calculate the 4th! See the identity below: \\[P_{p} \\times C_{pk} = P_{pk} \\times C_{p}\\] # whaaaaaat? They&#39;re equal!!!! mypp * mycpk == myppk * mycp ## [1] TRUE Learning Check 1 Question Lets apply this to some tasty examples! A manufacturer of granola bars is aiming for a weight of 2 ounces (oz), plus or minus 0.5 oz. Suppose the standard deviation of our granola bar machine is 0.02 oz, and the mean weight is 2.01 oz. Whats the process capability index? (I.e. How many times greater is the expected variation than the observed variation?) [View Answer!] lower = 2 + 0.05 upper = 2 - 0.05 sigma = 0.02 mu = 2.01 cp(sigma = 0.02, upper = 2.05, lower = 1.95) ## [1] 0.8333333 cpk(mu = 2.01, sigma = 0.02, lower = 1.95, upper = 2.05) ## [1] 0.6666667 6.3 Confidence Intervals Any statistic is really just 1 of the many possible values of statistics you could have gotten from your sample, had you taken just a slightly different random sample. So, when we calculate our indices, we should be prepared that our indices might vary due to chance (sampling error), so we should build in confidence intervals. This helps us benchmark how trustworthy any given index is. 6.3.1 Confidence Intervals show us Sampling Distributions Lets quickly go over what confidence intervals are trying to show us! Suppose we take a statistic like the mean \\(\\mu\\) to describe our vector temp. water %&gt;% summarize(mean = mean(temp)) ## # A tibble: 1 × 1 ## mean ## &lt;dbl&gt; ## 1 44.8 We might have gotten a slightly different statistic had we had a slightly different sample. We can approximate what slightly different sample might look like by using bootstrapped resamples. This means, randomly sampling a bunch of observations from our dataframe water, sometimes taking the same observation multiple times, sometimes leaving out some observations by chance. We can use the sample(x, size = ..., replace = TRUE) function to take a bootstrapped sample. water %&gt;% # Grab n() randomly sampled temperatures, with replacement, # we&#39;ll call those &#39;boot&#39;, since they were &#39;bootstrapped&#39; summarize(boot = sample(temp, size = n(), replace = TRUE)) %&gt;% # and take the mean! summarize(mean = mean(boot)) ## # A tibble: 1 × 1 ## mean ## &lt;dbl&gt; ## 1 44.7 Our bootstrapped mean is very, very close to the original mean - just slightly off due to sampling error! Bootstrapping is a very powerful tool, as it lets us circumvent many long formulas, as long as you take enough samples. Lets take 1000 resamples below: # Get a vector of ids from 1 to 1000 myboot = tibble(rep = 1:1000) %&gt;% # For each repetition, group_by(rep) %&gt;% # Get a random bootstrapped sample of temperatures summarize(boot = water$temp %&gt;% sample(size = n(), replace = TRUE)) %&gt;% # For that repetition, group_by(rep) %&gt;% # Calculate the mean of the bootstrapped samples! summarize(mean = mean(boot)) # Let&#39;s view them! myboot$mean %&gt;% hist() We can see above the latent distribution of 1000 statistics we could have gotten due to random sampling error. This is called a sampling distribution. Whenever we make confidence intervals, we are always drawing from a sampling distribution. 6.3.2 Bootstrapped or Theoretical Sampling Distributions? The question is, are you relying on a bootstrapped sampling distribution or a theoretical sampling distribution? If we assume a perfectly normal distribution, then were relying on a theoretical sampling distribution, and we need formulas to calculate our confidence intervals. This is a big assumption! If we are comfortable with computing 1000 or more replicates, then we can bootstrap those confidence intervals instead, gaining accuracy at the expense of computational power. Lets learn how to make confidence intervals for our indices from (1) a theoretical sampling distribution, and then well learn to make them from (2) a bootstrapped sampling distribution. 6.3.3 Confidence Intervals with Theoretical Sampling Distributions Suppose our lower and upper specification limits - the expectations of the market and/or regulators - are that our onsens temperature should be between 42 and 50 degrees Celsius if we advertise ourselves as an Extra Hot Onsen. For any index, youll need to get the ingredients needed to calculate the index and to calculate its standard error (the standard deviation of the sampling distribution youre trying to approximate). So, lets first get our ingredients stat = water %&gt;% group_by(time) %&gt;% summarize(xbar = mean(temp), s = sd(temp), n_w = n()) %&gt;% summarize( xbbar = mean(xbar), # grand mean x-double-bar sigma_s = sqrt(mean(s^2)), # sigma_short sigma_t = water$temp %&gt;% sd(), # sigma_total! n = sum(n_w), # or just n = n() # Total sample size n_w = unique(n_w), k = time %&gt;% unique() %&gt;% length()) # Get total subgroups # Check it! stat ## # A tibble: 1 × 6 ## xbbar sigma_s sigma_t n n_w k ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 44.8 1.99 1.99 160 20 8 Now, lets calculate our Capability Index \\(C_{p}\\), which assumes a process centered between the upper and lower specification limits and a stable process. # Capability Index (for centered, normal data) cp = function(sigma_s, upper, lower){ abs(upper - lower) / (6*sigma_s) } stat %&gt;% summarize( limit_lower = 42, limit_upper = 50, # index estimate = cp(sigma_s = sigma_s, lower = limit_lower, upper = limit_upper)) ## # A tibble: 1 × 3 ## limit_lower limit_upper estimate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 42 50 0.671 That was surprisingly painless! Now, lets estimate the two-sided, 95% confidence interval of our sampling distribution for the statistic cp, assuming that this sampling distribution has a normal shape. Were getting the interval that spans 95%, so its got to start at 2.5% and end at 97.5%, covering the 95% most frequently occurring statistics in the sampling distribution. bands = stat %&gt;% summarize( limit_lower = 42, limit_upper = 50, # index estimate = cp(sigma_s = sigma_s, lower = limit_lower, upper = limit_upper), # Get our extra quantities of interest v_short = k*(n_w - 1), # get degrees of freedom # Get standard error for estimate se = estimate * sqrt(1 / (2*v_short)), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Now if z, the 97.5th percentile, # is 1.96 standard deviations from the mean in the normal, # Then so too is the 2.5th percentile in the normal. # Give me 1.96 standard deviations above cp # in the sampling distribution of cp! # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) bands ## # A tibble: 1 × 8 ## limit_lower limit_upper estimate v_short se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 42 50 0.671 152 0.0385 1.96 0.596 0.747 6.3.4 Visualizing Confidence Intervals Were we to visualize this, it might look like bands %&gt;% ggplot(mapping = aes(x = &quot;Cp Index&quot;, y = estimate, ymin = lower, ymax = upper)) + # Get draw us some benchmarks to make our chart meaningful geom_hline(yintercept = c(0,1,2), color = c(&quot;grey&quot;, &quot;black&quot;, &quot;grey&quot;)) + # Draw the points! geom_point() + geom_linerange() + # Add theming theme_classic(base_size = 14) + coord_flip() + labs(y = &quot;Index Value&quot;, x = NULL) Its not the most exciting plot, but it does show very clearly that the value of \\(C_{p}\\) and its 95% confidence interval are nowhere even close to 1.0, the key threshold. This means we can say with 95% confidence that the true value of \\(C_{p}\\) is less than 1. 6.3.5 Bootstrapping \\(C_{p}\\) How might we estimate this using the bootstrap? Well, we could myboot = tibble(rep = 1:1000) %&gt;% # For each rep, group_by(rep) %&gt;% # Give me the data.frame water, summarize(water) %&gt;% # And give me a random sample of observations sample_n(size = n(), replace = TRUE) myboot %&gt;% glimpse() ## Rows: 160,000 ## Columns: 6 ## Groups: rep [1,000] ## $ rep &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  ## $ id &lt;dbl&gt; 146, 106, 15, 11, 5, 153, 100, 19, 19, 64, 103, 95, 30, 112, 11 ## $ time &lt;dbl&gt; 15, 11, 1, 1, 1, 15, 9, 1, 1, 7, 11, 9, 3, 11, 11, 13, 9, 15, 5 ## $ temp &lt;dbl&gt; 47.0, 48.4, 46.0, 46.4, 45.9, 44.1, 41.1, 45.9, 45.9, 40.9, 47. ## $ ph &lt;dbl&gt; 5.6, 5.3, 5.3, 5.8, 5.1, 5.0, 6.1, 5.5, 5.5, 5.5, 5.4, 5.7, 5.3 ## $ sulfur &lt;dbl&gt; 3.0, 0.2, 0.2, 0.7, 0.0, 2.5, 2.1, 1.5, 1.5, 0.4, 0.3, 0.0, 1.0 This produces a very, very big data.frame! Lets now, for each rep, calculate our statistics from before! mybootstat = myboot %&gt;% # For each rep, and each subgroup... group_by(rep, time) %&gt;% summarize( xbar = mean(temp), # Get within group mean sigma_w = sd(temp), # Get within group sigma n_w = n() # Get within subgroup size ) %&gt;% # For each rep... group_by(rep) %&gt;% summarize( limit_upper = 42, limit_lower = 50, xbbar = mean(xbar), # Get grand mean sigma_s = sqrt(mean(sigma_w^2)), # Get sigma_short n = sum(n_w), # Get total observations n_w = unique(n_w)[1], k = n(), # Get number of subgroups k limit_lower = 42, limit_upper = 50, estimate = cp(sigma_s, upper = limit_upper, lower = limit_lower)) So cool! Weve now generated the sampling distributions for xbbar, sigma_s, and cp! We can even visualize the raw distributions now! Look at those wicked cool bootstrapped sampling distributions!!! g = mybootstat %&gt;% # For each rep, group_by(rep) %&gt;% # Stack our values atop each other... summarize( # Get these names, and repeat them each n times type = c(&quot;xbbar&quot;, &quot;sigma_s&quot;, &quot;cp&quot;) %&gt;% rep(each = n()), value = c(xbbar, sigma_s, estimate)) %&gt;% ggplot(mapping = aes(x = value, fill = type)) + geom_histogram() + facet_wrap(~type, scales = &quot;free&quot;) + guides(fill = &quot;none&quot;) # View it! g So last, lets take our boostrapped \\(C_{p}\\) statistics in mybootstat$cp and estimate a confidence interval and standard error for this sampling distribution. Because we have the entire distribution, we can extract values at specific percentiles in the distribution using quantiles(), rather than qnorm() or such theoretical distributions. # We&#39;ll save it as &#39;myqi&#39;, for quantities of interest myqi = mybootstat %&gt;% summarize( # Let&#39;s grab the original cp statistic cp = bands$estimate, # Get the lower and upper 95% confidence intervals lower = quantile(estimate, probs = 0.025), upper = quantile(estimate, probs = 0.975), # We can even get the standard error, # which is *literally* the standard deviation of this sampling distribution se = sd(estimate)) # Check it out! myqi ## # A tibble: 1 × 4 ## cp lower upper se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.671 0.614 0.780 0.0423 This suggests a wider confidence interval that our normal distribution assumes by default - interesting! We can perform bootstrapping to estimate confidence intervals for any statistic, including \\(C_{p}\\), \\(C_{pk}\\), \\(P_{p}\\), or \\(P_{pk}\\). The only limit is your computational power! Wheee! Note: Whenever you bootstrap, its important that you clear out your R environment to keep things running quickly, because you tend to accumulate a lot of really big data.frames. You can use remove() to do this. remove(myboot, mybootstat) 6.4 CIs for any Index! Lets practice calculating confidence intervals (CIs) for each of these indices. 6.4.1 CIs for \\(P_p\\) Now that we have our ingredients, lets get our index and its confidence intervals! # Capability Index (for centered, normal data) cp = function(sigma_s, upper, lower){ abs(upper - lower) / (6*sigma_s) } stat %&gt;% summarize( # index estimate = cp(sigma_s = sigma_s, lower = 42, upper = 50), # Get our extra quantities of interest v_short = k*(n_w - 1), # get degrees of freedom # Get standard error for cpk se = estimate * sqrt(1 / (2*v_short)), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 × 6 ## estimate v_short se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.671 152 0.0385 1.96 0.596 0.747 6.4.2 CIs for \\(C_{pk}\\) Write the function and generate the confidence interval for \\(P_{pk}\\)! # Capability Index (for skewed, uncentered data) cpk = function(mu, sigma_s, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_s) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_s) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } stat %&gt;% summarize( # index estimate = cpk(mu = xbbar, sigma_s = sigma_s, lower = 42, upper = 50), # Get our extra quantities of interest v_short = k*(n_w - 1), # get degrees of freedom # Get standard error for ppk se = estimate * sqrt( 1 / (2*v_short) + 1 / (9 * n * estimate^2) ), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 × 6 ## estimate v_short se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.478 152 0.0380 1.96 0.404 0.553 6.4.3 CIs for \\(P_p\\) Now that we have our ingredients, lets get our index and its confidence intervals! # Suppose we&#39;re looking at the entire process! # Process Performance Index (for centered, normal data) pp = function(sigma_t, upper, lower){ abs(upper - lower) / (6*sigma_t) } stat %&gt;% summarize( # index estimate = pp(sigma_t = sigma_t, lower = 42, upper = 50), # Get our extra quantities of interest v_total = n_w*k - 1, # get degrees of freedom # Get standard error for cpk se = estimate * sqrt(1 / (2*v_total)), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 × 6 ## estimate v_total se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.670 159 0.0376 1.96 0.597 0.744 6.4.4 CIs for \\(P_{pk}\\) Write the function and generate the confidence interval for \\(P_{pk}\\)! # Process Performance Index (for skewed, uncentered data) ppk = function(mu, sigma_t, lower = NULL, upper = NULL){ if(!is.null(lower)){ a = abs(mu - lower) / (3 * sigma_t) } if(!is.null(upper)){ b = abs(upper - mu) / (3 * sigma_t) } # We can also write if else statements like this # If we got both stats, return the min! if(!is.null(lower) &amp; !is.null(upper)){ min(a,b) %&gt;% return() # If we got just the upper stat, return b (for upper) }else if(is.null(lower)){ return(b) # If we got just the lower stat, return a (for lower) }else if(is.null(upper)){ return(a) } } stat %&gt;% summarize( # index estimate = ppk(mu = xbbar, sigma_t = sigma_t, lower = 42, upper = 50), # Get our extra quantities of interest v_total = n_w*k - 1, # get degrees of freedom # Get standard error for cpk se = estimate * sqrt( 1 / (2*v_total) + 1 / (9 * n * estimate^2) ), # Get z score z = qnorm(0.975), # get position of 97.5th percentile in normal distribution # Get upper and lower confidence interval! lower = estimate - z * se, upper = estimate + z * se) ## # A tibble: 1 × 6 ## estimate v_total se z lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.478 159 0.0376 1.96 0.404 0.551 Conclusion Alright! You are now a confidence interval wizard! Go forth and make confidence intervals! "],["workshop-useful-life-distributions-exponential.html", "7 Workshop: Useful Life Distributions (Exponential) 7.1 Getting Started 7.2 Quantities of Interest Learning Check 1 Learning Check 2 7.3 Quantities of Interest (continued) Learning Check 3 7.4 System Failure with Independent Failure Rates 7.5 Statistical Techniques for Exponential Distributions Learning Check 4 7.6 Conclusion", " 7 Workshop: Useful Life Distributions (Exponential) In this workshop, were going to learn some R functions for working with common life distributions, namely the Exponential distributions. 7.1 Getting Started 7.1.1 Load Packages Lets start by loading the tidyverse package, which will let us mutate(), filter(), and summarize() data quickly. Well also load mosaicCalc, for taking derivatives and integrals (eg. D() and antiD()). # Load packages library(tidyverse) library(mosaicCalc) 7.1.2 Key Concepts In this lesson, well be building on several key concepts from prior lessons. Ive defined them below as a helpful review. life distribution: the distribution of a vector of \\(n\\) products, whose values recording the amount of time it took for each product to fail. In other words, its lifespan. probability density function (PDF): the function describing the probability (relative frequency) of any value in a distribution. cumulative distribution function (CDF): the function describing the cumulative probability of each successive value in a distribution. Spans from 0 to 1. Have questions? I strongly recommend you review Workshops 2, 3, and 4 before this one! It will help it all fit together! 7.1.3 Our Data In this workshop, were going to work with a data.frame called masks. An extremely annoying moment in the COVID-era is when a part of your mask snaps, requiring you to get a fresh mask. Lets examine a (hypothetical) sample of n = 50 masks produced by Company X to explore how often this happens! Please import the masks.csv data.frame below. Each row is a mask, with its own unique id. Columns describe how many hours it took for the left_earloop to snap, the right_earloop to snap, the nose wire to snap, and the fabric of the mask to tear. masks &lt;- read_csv(&quot;workshops/masks.csv&quot;) # Let&#39;s glimpse() its contents! masks %&gt;% glimpse() ## Rows: 50 ## Columns: 5 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1 ## $ left_earloop &lt;dbl&gt; 6, 16, 46, 4, 1, 5, 32, 35, 27, 3, 4, 7, 1, 20, 22, 17,  ## $ right_earloop &lt;dbl&gt; 12, 1, 17, 14, 19, 8, 18, 14, 5, 8, 8, 2, 12, 7, 20, 4,  ## $ wire &lt;dbl&gt; 4, 1, 8, 29, 23, 8, 10, 38, 11, 31, 7, 4, 3, 33, 13, 2,  ## $ fabric &lt;dbl&gt; 177, 462, 65, 405, 2483, 1064, 287, 2819, 1072, 288, 863 7.2 Quantities of Interest When we work with life distributions, we often want to find several useful quantities of interest (a.k.a. parameters) about them. Lets find out how to do that with an exponential distribution! 7.2.1 Lack of Memory Exponential distributions are famous for a key trait. The failure rate $ $ remains constant in an exponential distribution. The probability that a product fails in the next hour of use is the same at t = 0, t = 100, or t = infinity! It doesnt worsen with time. (Its the literal meaning of the saying, if it is not broke, dont fix it!) 7.2.2 Mean Time to Fail The Mean Time to Fail describes the mean of a lifespan distribution. For example, lets calculate the mean time to fail (in hours) for a masks left_earloop in our sample. stat &lt;- masks %&gt;% summarize( # We can take the mean of this vector of time to fail in hours mttf = mean(left_earloop), # Lambda is the reciprocal of the MTTF lambda = 1 / mttf) # Check out the contents! stat ## # A tibble: 1 × 2 ## mttf lambda ## &lt;dbl&gt; &lt;dbl&gt; ## 1 13.4 0.0749 In an exponential distribution, the MTTF always has a cumulative probability of 1 - 1 / e = 0.632. (This can be coded in R like so:) 1 - 1 / exp(1) ## [1] 0.6321206 Lets assume our samples left earloops have an exponential lifespan distribution, and use pexp() to calculate the cumulative probability of getting an MTTF of 13.36. Well need to supply pexp() the benchmark in the distribution in question (mttf), plus the the rate parameter \\(\\lambda\\), which we always need when simulating an exponential distribution. prob &lt;- pexp(stat$mttf, rate = stat$lambda) prob ## [1] 0.6321206 Indeed, the figure below compares the observed PDF function (made using density()) to the assumed exponential PDF (made using dexp()), and we can see that that 63% of the distribution has failed by the Mean Time to Fail. 7.2.3 Mean Time to Fail via Integration Mean Time to Fail (MTTF) can be number-crunched empirically as the mean of observed lifespans, assuming an exponential distribution. But it is also equal to the integral of the reliability function: $MTTF = _{0}^{}{R(t)dx} $. So, lets make ourselves a nice reliability function to help us calculate the MTTF. We know the reliability function can be stated as \\(R(t) = 1 - F(t)\\). Assuming an exponential distribution, the failure rate \\(F(t) = 1 - e^{-\\lambda t}\\). We know \\(\\lambda = \\frac{1}{MTTF}\\), and above, we found that lambda = 0.0748502994011976 for a left-earloop. # Reliability Function for exponential distribution r = function(t, lambda){ exp(-1*t*lambda)} We can calculate it below # Use mosaicCalc&#39;s antiD function # To get integral of r(t, lambda) as x goes from 0 to infinity mttf = antiD(tilde = r(t, lambda) ~ t) Great! We have developed our own mttf function for an exponential distribution! If we feed t a suitably large value, like 1000 (approaching infinity), we will reach the original observed/estimated mttf. mttf(t = 1000, lambda = stat$lambda) ## [1] 13.36 mttf(t = Inf, lambda = stat$lambda) ## [1] 13.36 This only helps you if you know lambda, the inverse of the MTTF, or have the reliability function but not the MTTF. 7.2.4 Median Time to Fail \\(T_{50}\\) We might also want to know the median time to failure (\\(T_{50}\\)), the value on the x axis that splits the area under the curve in half at 50% and 50%. We can calculate this as \\(F(T_{50}) = 50\\% = 0.5 = 1 - e^{-\\lambda T_{50}}\\) where: \\(T_{50} = \\frac{log(2)}{ \\lambda } = \\frac{0.693}{\\lambda}\\) # Let&#39;s update stat to include the observed &#39;median&#39; # and &#39;t50&#39;, the median assuming an exponential distribution stat &lt;- masks %&gt;% summarize( # The literal mean time to fail # in our observed distribution is this mttf = mean(left_earloop), # And lambda is this... lambda = 1 / mttf, # The observed median is this.... median = median(left_earloop), # But if we assume it&#39;s an exponential distribution # and calculate the median from lambda, # we get t50, which is very close. t50 = log(2) / lambda) # Check it out! stat ## # A tibble: 1 × 4 ## mttf lambda median t50 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 13.4 0.0749 8 9.26 7.2.5 Modal Time to Fail Finally, the modal time to fail is pretty easy to calculate. Its the most common time to fail, also known as the max probability in a PDF. masks %&gt;% summarize( # Let&#39;s get lambda, the reciprocal of the MTTF lambda = 1 / mean(left_earloop), # And let&#39;s estimate the PDF... t = 1:max(left_earloop), prob = dexp(t, rate = lambda)) %&gt;% # And let&#39;s sort the data.frame from highest to lowest arrange(desc(prob)) %&gt;% # Grab first 3 rows, for brevity head(3) ## # A tibble: 3 × 3 ## lambda t prob ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.0749 1 0.0695 ## 2 0.0749 2 0.0644 ## 3 0.0749 3 0.0598 # This reveals that t = 1 is our mode Learning Check 1 Question A competing mask manufacturer made a mask whose earloops fail at a constant failure rate of 0.08. What is the probability that 1 fails before 20 hours of use? What is the probability that 2 fail before 20 hours of use? After how long should we expect 1% failures? [View Answer!] What is the probability that 1 fails before 20 hours of use? # Let&#39;s generate an expondential failure function, # because **constant** rate of failure f = function(t, lambda){1 - exp(-1*t*lambda)} # Use failure function (CDF) to get area under curve BEFORE 20 hours. f(t = 20, lambda = 0.08) ## [1] 0.7981035 # There&#39;s a 79% chance 1 fails within 20 hours What is the probability that 2 fail before 20 hours of use? # For n failures, take F(t) to the nth power f(t = 20, lambda = 0.08)^2 ## [1] 0.6369692 # There&#39;s a 63% chance 2 fail within 20 hours. After how long should we expect 1% failures? # We can solve this using the failure function # f(t) = 1 - e^{-t*lambda} # But we need to invert it, # to solve for t # f(t) = 1 - e^{-t*lambda} # e^{-t*lambda} = 1 - f(t) # log(1 - f(t)) = -t*lambda # -log(1 - f(t)) / lambda = t # So if we set f(t) = 1%, and lambda = 0.08, # this will tell us at what time F(t) will equal 1% -log(1 - 0.01) / 0.08 ## [1] 0.1256292 # Looks like that time of interest is t = 0.1256 hours. Learning Check 2 Question Above, we examined a sample of surgical masks, checking how often their left_earloop snapped. How does that compare with the right_earloop? Calculate the mean time to fail for the right earloop, and \\(\\lamba\\), the mean failure rate. Is the right earloop more or less reliable than the left earloop? [View Answer!] MTTF and Lambda compare &lt;- masks %&gt;% summarize(mttf_right = mean(right_earloop), mttf_left = mean(left_earloop), lambda_right = 1 / mttf_right, lambda_left = 1 / mttf_left) # Check it compare ## # A tibble: 1 × 4 ## mttf_right mttf_left lambda_right lambda_left ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.6 13.4 0.0943 0.0749 # Looks like the left earloop fails less often. 7.3 Quantities of Interest (continued) 7.3.1 Conditional Reliability (Survival) Function We may also want to know, after age t, whats the probability that a product survives an additional x years to age t + x? We can restate this in terms of \\(T_{Fail}\\), the time at which the product finally fails. We want to know, whats the probability that \\(T_{Fail}\\) is greater than \\(t + x\\), given that we already know \\(T_{Fail}\\) must be greater than \\(t\\) (since it hasnt failed yet as of time \\(t\\))? Fortunately, this can be simplified in terms of the reliability functions. As long as we can calculate \\(R(x + t)\\) and \\(R(t)\\), we can find \\(R(x|t)\\), the conditional survival function. \\[ R(x | t) = \\frac{ R(x + t) }{ R(t) } = \\frac{ P(T_{Fail} &gt; x + t) }{P(T_{Fail} &gt; t)} \\] So, lets use our nice reliability function from before to help us calculate the conditional reliability function. # Reliability Function for exponential distribution r = function(t, lambda){ exp(-1*t*lambda)} So, whats the probability that a left-earloop that has lasted 10 hours will last another 5 hours? r(t = 10 + 5, lambda = stat$lambda) / r(t = 10, lambda = stat$lambda) ## [1] 0.6878039 Looks like theres a 69% chance it will last another 5 hours, given that it has already lasted 10 hours. Lets finish up by building ourselves a nice Conditional Reliability function cr, which which calculates the conditional probability of any item surviving x more hours given that it survived t hours and a mean failure rate of lambda. cr = function(t, x, lambda){ # We can actually nest functions inside each other, # to make them easier to write r = function(t, lambda){ exp(-1*t*lambda)} # Calculate R(x + t) / R(t) output &lt;- r(t = t + x, lambda) / r(t = t, lambda) # and return the result! return(output) } # Let&#39;s compare our result to above! It&#39;s the same! cr(t = 10, x = 5, lambda = stat$lambda) ## [1] 0.6878039 If we were to visualize our conditional reliability function cr below as x ranges from 1 to 50, it would produce the following curve. 7.3.2 \\(\\mu(t)\\): Mean Residual Life The Conditional Reliability Function \\(R(x|t)\\) also allows us to calculate the Mean Residual Life (MRL, a.k.a. \\(\\mu\\)) at time \\(t\\), in this case referring to the average \\(x\\) more years the product is expected to survive after time \\(t\\). We can calculate it using the distribution below. \\[MRL(t) = \\mu(t) = \\int_{t}^{\\infty}{ R(x|t)dx} = \\frac{1}{R(t)} \\int_{t}^{\\infty}{ R(x) dx} \\] It shows the mean expected remaining life years, for however \\(x\\) many more time steps come after \\(t\\). We can formalize this as function mu(t, lambda). (Since the Greek letter \\(\\mu\\) is pronounced mu.) # Conditional Reliability library(mosaicCalc) library(dplyr) # Calculate Mean Residual Life mu = function(t = 5, lambda = 0.001){ #t = 5 #lambda = 0.001 # Get the Reliability Function for exponential distribution r = function(t, lambda){ exp(-1*t*lambda)} # Get the integral of R(t), from t (time already spent) to infinity integral = antiD(tilde = r(t, lambda) ~ t, lower.bound = t) # Now calculate mu(), the Mean Residual Life function at time t output &lt;- integral(t = Inf, lambda = lambda) / r(t = t, lambda = lambda) integral(t = Inf, lambda = lambda) / r(t = t, lambda = lambda) return(output) } # Let&#39;s try it. mu(t = 500, lambda = 0.001) ## [1] 1000 mu(t = 501, lambda = 0.001) ## [1] 1000 # Interesting - the MRL remains constant, # perhaps because lambda is fixed in the exponential # [Note: mu is NOT vectorized] Its value will become clearer in later chapters, when using distributions like the Weibull, whose failure rate function \\(z(t)\\) is not constant but varies over time \\(t\\). For example, heres a exmaple using the Weibulls reliability function r(t, m, c). muw = function(t = 5, m = 2, c = 20000){ #t = 5 #lambda = 0.001 # Get the Reliability Function for exponential distribution r = function(t, m, c){ exp(-1*(t/c)^m) } # Get the integral of R(t), from t (time already spent) to infinity integral = antiD(tilde = r(t, m, c) ~ t, lower.bound = t) # Now calculate mu(), the Mean Residual Life function at time t output &lt;- integral(t = Inf, m = m, c = c) / r(t = t, m = m, c= c) integral(t = Inf, m = m, c= c) / r(t = t, m = m, c= c) return(output) } # Try it! muw(t = 500, m = 2, c = 20000) ## [1] 17235.41 muw(t = 501, m = 2, c = 20000) ## [1] 17234.45 # See how the MRL changes over time, now that the failure rate can change over time? Learning Check 3 Question A competing firm produced a mask with a wire that fails at a constant rate of 1 failure per 240 hours. The probability that the wire survives 1 week (t = 168 hours) in continuous use is ( You buy the mask, and it works without failure for 2 weeks (t = 336 hours) The probability the wire will snap during the next week (t = 504 hours) is [View Answer!] The probability that the wire survives 1 week (t = 168 hours) in continuous use is ( # Write the reliability function r = function(t, lambda){ exp(-1*t*lambda)} # Probability it survives 1 month (760 hours) is... r(t = 168, lambda = 1 / 240) ## [1] 0.4965853 # ~ 50% You buy the mask, and it works without failure for 2 weeks (t = 336 hours) The probability the wire will snap during the next week (t = 504 hours) is # We can calculate it 2 ways. # First, we can take # R(t + x) / R(t) r(t = 504, lambda = 1 / 240) / r(t = 336, lambda = 1 / 240) ## [1] 0.4965853 # Or, we can calculate the failure function f = function(t, lambda){1 - exp(-1*t*lambda)} # And just calculate the rate of F(t = x) # Because lambda is a constant failure rate f(t = 168, lambda = 1 / 240) ## [1] 0.5034147 7.4 System Failure with Independent Failure Rates Consider a mask with a left and right earloop, which have independent failure rates \\(\\lambda_{left}\\) and \\(\\lambda_{right}\\). Whats the probability that the left loop fails before the right loop? We can express this as: \\[ P(j \\ fails \\ first) = \\frac{ \\lambda_{j}}{ \\sum_{i=1}^{n}{ \\lambda{i}} }\\] In other words, the probability that component \\(j\\) fails first reflects how big \\(\\lambda_{j}\\) is relative to all the other failure rates in total. Lets test this out with our masks dataset. masks %&gt;% summarize( # Calculate failure rates of left and right loops lambda_left = 1 / mean(left_earloop), lambda_right = 1 / mean(right_earloop), # Calculate the total probability of either loop failing lambda_sum = lambda_left + lambda_right, # Calculate probability the left loop fails first prob_left_first = lambda_left / lambda_sum) ## # A tibble: 1 × 4 ## lambda_left lambda_right lambda_sum prob_left_first ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0749 0.0943 0.169 0.442 # Looks like a probability of about 44% that left loop fails first. 7.4.1 Reliability Functions with Multiple Inputs Suppose a fraction of our masks are shipped in from manufacturer 1, while some are shipped from manufacturer 2. A fraction \\(p\\) is coming from manufacturer 1, while \\(1-p\\) is coming from manufacturer 2. We can use the rules of total probability to calculate the reliability function and other quantities for this mask: \\[R(t) = \\sum_{i=1}^{n}{ ( \\ p_{i} \\times R_i(t) \\ ) } = \\sum_{i=1}^{n}{ ( \\ p_{i} \\times e^{-\\lambda_{i} t} ) }\\] \\[MTTF = \\sum_{i=1}^{n}{ \\frac{p_i}{\\lambda_{i}}}\\] \\[f(t) = \\frac{-d}{dt}R(t) = \\sum_{i=1}^{n}{p\\lambda_{i}e^{-\\lambda_{i}t}}\\] \\[z(t) = \\frac{ f(t) }{ R(t)} = \\frac{ \\sum_{i=1}^{n}{p\\lambda_{i}e^{-\\lambda_{i}t}} }{ \\sum_{i=1}^{n}{ ( \\ p_{i} \\times R_i(t) \\ ) } } \\] Its pretty messy, but powerful! Suppose we order 75% of our stock from a manufacturer with a failure rate of 1 fabric tear per 50 hours, but 25% from our stock from a manufacturer with a failure rate of 1 tear per 100 hours. What is the (1) overall mean time to failure and (2) overall failure rate at 100 hours for any random mask in your supply? We can tally this up in a stock data.frame. stock &lt;- data.frame( prob = c(0.75, 0.25), lambda = c(1 / 50, 1 / 100)) To calculate the MTTF, we just take the sum of the fraction of each proportion and each failure rate lambda. stock %&gt;% summarize(mttf = sum(prob / lambda)) ## mttf ## 1 62.5 To calculate the overall failure rate, we will generate the reliability function, take its derivative to get the PDF. # Let&#39;s write an exponential reliability function r = function(t, lambda){ exp(-1*t*lambda) } # Let&#39;s derive an exponential PDF f(t), using mosaicCalc&#39;s D() f = D(-1*r(t, lambda) ~ t) Then, we use the PDFs \\(f_i(t)\\) and the Reliability functions \\(R_i(t)\\) to get calculate the overall failure rate \\(z(t)\\). stock %&gt;% summarize( total_f = sum(prob * f(t = 100, lambda = lambda)), total_r = sum(prob * r(t = 100, lambda = lambda)), z = total_f / total_r) ## total_f total_r z ## 1 0.002949728 0.1934713 0.01524633 We could even write it as a function, where p and lambda are equal length vectors for plant 1, plant 2, plant 3,  plant \\(n\\). z = function(t){ # Set the input percentages of products from plants 1 and 2 p = c(0.75, 0.25) # Set the failure rates for each lambda = c(0.02, 0.01) # Let&#39;s write an exponential reliability function r = function(t, lambda){ exp(-1*t*lambda) } # Let&#39;s derive an exponential PDF f(t), f = D(-1*r(t, lambda) ~ t) # Calculate total probability total_f = sum( p * f(t, lambda) ) # Calculate total reliability total_r = sum( p * r(t, lambda) ) # Calculate overall failure rate z = total_f / total_r return(z) } # Try it out! z(t = 1) ## [1] 0.0174812 z(t = 10) ## [1] 0.01730786 z(t = 100) ## [1] 0.01524633 7.4.2 Phase-Type Distributions One way to more accurately model the lifespan of a product is to accept that its failure rate may remain constant, but it might change between failure rates as it passes through phases. Lets model that! We can write the time \\(T\\) to critical failure (via either overstress OR degraded failure) as \\(T = min(T_c, T_d+T_{dc})\\). The total probability of a product being in any phase \\(a_{i \\to n}\\) equals 1. \\(a_c = 25\\%\\) \\(a_d = 40\\%\\) \\(a_dc = 10\\%\\) We can represent it in a dataframe, like so: myphase &lt;- data.frame( id = 1:4, phase = c(&quot;c&quot;, &quot;d&quot;, &quot;c&quot;, &quot;dc&quot;), alpha = c(0.25, 0.40, 0.25, 0.10), lambda = c(0.50, 1, 0.50, 3) ) And, using the same tricks from the preceding section, we can calculate z(t), the overall hazard rate of a phase-type exponential distribution, by taking the sum of the weighted probabilities. r = function(t, lambda){ exp(-1*t*lambda) } # Let&#39;s derive an exponential PDF f(t), f = D(-1*r(t, lambda) ~ t) z = function(t, data){ output &lt;- data %&gt;% summarize( prob_f = sum(alpha * f(t = t, lambda)), prob_r = sum(alpha * r(t = t, lambda)), ratio_z = prob_f / prob_r) output$ratio_z %&gt;% return() } # Take a peek! z(t = 1, data = myphase) ## [1] 0.6888965 z(t = 2, data = myphase) ## [1] 0.6161738 z(t = 5, data = myphase) ## [1] 0.5308124 z(t = 10, data = myphase) ## [1] 0.5026807 7.5 Statistical Techniques for Exponential Distributions Finally, were going to examine several key tools that will help you (1) cross-tabulate failure data over time, (2) use statistics to determine whether an archetypal distribution (eg. exponential) fits your data sufficiently, (3) how to estimate the failure rate \\(\\lambda\\) from tabulated data, and (4) how to plan experiments for product testing. Here we go! 7.5.1 Factors in R For the next section, youll need to understand factors. Factors are ordered vectors. They are helpful in ggplot and elsewhere for telling R what order to interpret things in. You can make your own vector into a factor using factor(vector, levels = c(\"first\", \"second\", \"etc\")). # Make character vector myvector &lt;- c(&quot;surgical&quot;, &quot;KN-95&quot;, &quot;KN-95&quot;, &quot;N-95&quot;, &quot;surgical&quot;) # Turn it into a factor myfactor &lt;- factor(myvector, levels = c(&quot;N-95&quot;, &quot;KN-95&quot;, &quot;surgical&quot;)) # check it myfactor ## [1] surgical KN-95 KN-95 N-95 surgical ## Levels: N-95 KN-95 surgical factors can be reduced to numeric vectors using as.numeric(). This returns the level for each value in the factor. # Turn the factor numeric mynum &lt;- myfactor %&gt;% as.numeric() # Compare data.frame(myfactor, mynum) ## myfactor mynum ## 1 surgical 3 ## 2 KN-95 2 ## 3 KN-95 2 ## 4 N-95 1 ## 5 surgical 3 # for example, N-95, which was ranked first in the factor, receives a 1 anytime the value N-95 appears 7.5.2 Crosstabulation Sometimes, we might want to tabulate failures in terms of meaningful units of time, counting the total failures every 5 hours, every 24 hours, etc. Lets learn how! d1 &lt;- data.frame(t = masks$left_earloop) %&gt;% # classify each value into 5-point width bins (0 to 5, 6-10, 11-15, etc.) # then convert it to a numeric ranking of categories from 1 to n bins mutate(label = cut_interval(t, length = 5)) # Let&#39;s take a peek d1 %&gt;% glimpse() ## Rows: 50 ## Columns: 2 ## $ t &lt;dbl&gt; 6, 16, 46, 4, 1, 5, 32, 35, 27, 3, 4, 7, 1, 20, 22, 17, 8, 18, 1 ## $ label &lt;fct&gt; &quot;(5,10]&quot;, &quot;(15,20]&quot;, &quot;(45,50]&quot;, &quot;[0,5]&quot;, &quot;[0,5]&quot;, &quot;[0,5]&quot;, &quot;(30, Step 2: Tabulate Observations per Bin. d2 &lt;- d1 %&gt;% # For each bin label group_by(label, .drop = FALSE) %&gt;% # Get total observed rows in each bin # .drop = FALSE records factor levels in label that have 0 cases summarize(r_obs = n()) d2 %&gt;% glimpse() ## Rows: 11 ## Columns: 2 ## $ label &lt;fct&gt; &quot;[0,5]&quot;, &quot;(5,10]&quot;, &quot;(10,15]&quot;, &quot;(15,20]&quot;, &quot;(20,25]&quot;, &quot;(25,30]&quot;, &quot; ## $ r_obs &lt;int&gt; 17, 13, 3, 7, 2, 2, 3, 0, 0, 1, 2 Step 3: Get bounds and midpoint of Bins Last, we might need the bounds (upper and lower value), or the midpoint. Heres how! d3 &lt;- d2 %&gt;% # Get bin ranking, lower and upper bounds, and midpoint mutate( bin = as.numeric(label), lower = (bin - 1) * 5, upper = bin * 5, midpoint = (lower + upper) / 2) # Check it! d3 ## # A tibble: 11 × 6 ## label r_obs bin lower upper midpoint ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [0,5] 17 1 0 5 2.5 ## 2 (5,10] 13 2 5 10 7.5 ## 3 (10,15] 3 3 10 15 12.5 ## 4 (15,20] 7 4 15 20 17.5 ## 5 (20,25] 2 5 20 25 22.5 ## 6 (25,30] 2 6 25 30 27.5 ## 7 (30,35] 3 7 30 35 32.5 ## 8 (35,40] 0 8 35 40 37.5 ## 9 (40,45] 0 9 40 45 42.5 ## 10 (45,50] 1 10 45 50 47.5 ## 11 (50,55] 2 11 50 55 52.5 7.5.3 Chi-squared Chi-squared (\\(\\chi^{2}\\)) is a special statistic used frequently to evaluate the relationship between two categorical variables. In this case, the first variable is the type of data (observed vs. model), while the second variable is bin. Back in Workshop 2, we visually compared several distributions to an observed distribution, to determine which fits best. But can we do that statistically? One way to do this is to chop our observed data into bins of length equal width using cut_interval() from ggplot2. We must specify: Step 1 Crosstabulate Observed Values into Bins. # Let&#39;s repeat our process from before! c1 &lt;- data.frame(t = masks$left_earloop) %&gt;% # Part 1.1: Split into bins mutate(interval = cut_interval(t, length = 5)) %&gt;% # Part 1.2: Tally up observed failures &#39;r_obs&#39; by bin group_by(interval, .drop = FALSE) %&gt;% summarize(r_obs = n()) %&gt;% mutate( bin = 1:n(), # give each bin a numeric id from 1 to inf # bin = as.numeric(interval), # you could alternatively turn the factor numeric lower = (bin - 1) * 5, upper = bin * 5, midpoint = (lower + upper) / 2) Step 2: Calculate Observed and Expected Values per Bin. Sometimes you might only receive tabulated data, meaning a table of bins, not the original vector. In that case, start from Step 2! # Get any parameters you need (might need to be provided if only tabulated data) mystat = masks %&gt;% summarize( # failure rate lambda = 1 / mean(left_earloop), # total number of units under test n = n()) # Get your &#39;model&#39; function f = function(t, lambda){ 1 - exp(-1*lambda*t) } # Note: pexp(t, rate) is equivalent to f(t, lambda) for exponential distribution # Now calculate expected units to fail per interval, r_exp c2 = c1 %&gt;% mutate( # Get probability of failure by time t = upper bound p_upper = f(t = upper, lambda = mystat$lambda), # Get probability of failure by time t = lower bound p_lower = f(t = lower, lambda = mystat$lambda), # Get probability of failure during the interval, # i.e. between these thresholds p_fail = p_upper - p_lower, # Add in total units under test n_total = mystat$n, # Calculate expected units to fail in that interval r_exp = n_total * p_fail) # Check it! c2 ## # A tibble: 11 × 11 ## interval r_obs bin lower upper midpoint p_upper p_lower p_fail n_total ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 [0,5] 17 1 0 5 2.5 0.312 0 0.312 50 ## 2 (5,10] 13 2 5 10 7.5 0.527 0.312 0.215 50 ## 3 (10,15] 3 3 10 15 12.5 0.675 0.527 0.148 50 ## 4 (15,20] 7 4 15 20 17.5 0.776 0.675 0.102 50 ## 5 (20,25] 2 5 20 25 22.5 0.846 0.776 0.0699 50 ## 6 (25,30] 2 6 25 30 27.5 0.894 0.846 0.0481 50 ## 7 (30,35] 3 7 30 35 32.5 0.927 0.894 0.0331 50 ## 8 (35,40] 0 8 35 40 37.5 0.950 0.927 0.0227 50 ## 9 (40,45] 0 9 40 45 42.5 0.966 0.950 0.0156 50 ## 10 (45,50] 1 10 45 50 47.5 0.976 0.966 0.0108 50 ## 11 (50,55] 2 11 50 55 52.5 0.984 0.976 0.00740 50 ## #  1 more variable: r_exp &lt;dbl&gt; # We only need a few of these columns; let&#39;s look at them: c2 %&gt;% # interval: get time interval in that bin # r_obs: Get OBSERVED failures in that bin # r_exp: Get EXPECTED failures in that bin # n_total: Get TOTAL units, failed or not, overall select(interval, r_obs, r_exp, n_total) ## # A tibble: 11 × 4 ## interval r_obs r_exp n_total ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 [0,5] 17 15.6 50 ## 2 (5,10] 13 10.7 50 ## 3 (10,15] 3 7.38 50 ## 4 (15,20] 7 5.08 50 ## 5 (20,25] 2 3.49 50 ## 6 (25,30] 2 2.40 50 ## 7 (30,35] 3 1.65 50 ## 8 (35,40] 0 1.14 50 ## 9 (40,45] 0 0.782 50 ## 10 (45,50] 1 0.538 50 ## 11 (50,55] 2 0.370 50 Step 3: Calculate Chi-squared Statistic Chi-squared here represents the sum of ratios for each bin. Each ratio is the (1) squared difference between the observed and expected value over (2) the expected value. Ranges from 0 to infinity. The bigger (more positive) the statistic, the greater difference between the observed and expected data. Degrees of Freedom (df) is used as the standard deviation in the chi-squared distribution, to help evaluate how extreme is our chi-squared statistic? Chi-squared Distribution is a distribution of squared deviations from a normal distribution centered at 0. This means it only has positive values. c3 &lt;- c2 %&gt;% summarize( # Calculate Chi-squared statistic chisq = sum((r_obs - r_exp)^2 / r_exp), # Calculate number of bins (rows) nbin = n(), # Record number of parameters used (jjust lambda) np = 1, # Calculate degree of freedom df = nbin - np - 1) # Check it! c3 ## # A tibble: 1 × 4 ## chisq nbin np df ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.2 11 1 9 Step 4: Calculate p-values and confidence intervals Last, lets use the pchisq() function to evaluate the CDF of the Chi-squared distribution. Well find out: p_value: whats the probability of getting a value greater than or equal to (more extreme than) our observed chi-squared statistic? statistically significant: if our observed statistic is more extreme than most possible chi-squared statistics (eg. &gt;95% of the distribution), its probably not due to chance! We call it statistically significant. # Calculate area remaining under the curve c4 &lt;- c3 %&gt;% mutate(p_value = 1 - pchisq(q = chisq, df = df)) # Check c4 ## # A tibble: 1 × 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.2 11 1 9 0.0847 For a visual representation: 7.5.4 Building a Chi-squared function That was a lot of work! It might be more helpful for us to build our own function doing all those steps. Heres a function I built, which should work pretty flexibly. You can improve it, tweak it, or use it for your own purposes. Our function get_chisq() is going to need several kinds of information. Our exponential failure function f Our parameters, eg. failure rate lambda Total number of parameters (if exponential, 1) Our total number of units under test n Then, well need a observed vector of times to failure t, plus a constant binwidth (previously, 5). Or, if our data is pre-crosstabulated, we can ignore t and binwidth and just supply a data.frame data of crosstabulated vectors lower, upper, and r_obs. For example, these inputs might look like this: # Get your &#39;model&#39; function f = function(t, lambda){ 1 - exp(-1*lambda*t) } # Parameters # lambda = 1 / mean(masks$left_earloop) # number of parameters # np = 1 # total number of units (sometimes provided, if the data is time-censored) # n_total = length(masks$left_earloop) # AND # Raw observed data + binwidth # t = masks$left_earloop # binwidth = 5 # OR # Crosstabulated data (c1 is an example we made before) # data = c1 %&gt;% select(lower, upper, r_obs) Then, we could write out the function like this! Ive added some fancy @ tags below just for notation, but you can ditch them if you prefer. This is a fairly complex function! Ive shared it with you as an example to help you build your own for your projects. #&#39; @name get_chisq #&#39; @title Function to Get Chi-Squared! #&#39; @name Tim Fraser #&#39; If observed vector... #&#39; @param t a vector of times to failure #&#39; @param binwidth size of intervals (eg. 5 hours) (Only if t is provided) #&#39; If cross-tabulated data... #&#39; @param data a data.frame with the vectors `lower`, `upper`, and `r_obs` #&#39; Common Parameters: #&#39; @param n_total total number of units. #&#39; @param f specific failure function, such as `f = f(t, lambda)` #&#39; @param np total number of parameters in your function (eg. if exponential, 1 (lambda)) #&#39; @param ... fill in here any named parameters you need, like `lambda = 2.4` or `rate = 2.3` or `mean = 0, sd = 2` get_chisq = function(t = NULL, binwidth = 5, data = NULL, n_total, f, np = 1, ...){ # If vector `t` is NOT NULL # Do the raw data route if(!is.null(t)){ # Make a tibble called &#39;tab&#39; tab = tibble(t = t) %&gt;% # Part 1.1: Split into bins mutate(interval = cut_interval(t, length = binwidth)) %&gt;% # Part 1.2: Tally up observed failures &#39;r_obs&#39; by bin group_by(interval, .drop = FALSE) %&gt;% summarize(r_obs = n()) %&gt;% # Let&#39;s repeat our process from before! mutate( bin = 1:n(), lower = (bin - 1) * binwidth, upper = bin * binwidth, midpoint = (lower + upper) / 2) # Otherwise, if data.frame `data` is NOT NULL # Do the cross-tabulated data route }else if(!is.null(data)){ tab = data %&gt;% mutate(bin = 1:n(), midpoint = (lower + upper) / 2) } # Part 2. Calculate probabilities by interval output = tab %&gt;% mutate( p_upper = f(upper, ...), # supplied parameters p_lower = f(lower, ...), # supplied parameters p_fail = p_upper - p_lower, n_total = n_total, r_exp = n_total * p_fail) %&gt;% # Part 3-4: Calculate Chi-Squared statistic and p-value summarize( chisq = sum((r_obs - r_exp)^2 / r_exp), nbin = n(), np = np, df = nbin - np - 1, p_value = 1 - pchisq(q = chisq, df = df) ) return(output) } Finally, lets try using our function! Using a raw observed vector t: get_chisq( t = masks$left_earloop, binwidth = 5, n_total = 50, f = f, np = 1, lambda = mystat$lambda) ## # A tibble: 1 × 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.2 11 1 9 0.0847 Or using crosstabulated data: get_chisq( data = c1, n_total = 50, f = f, np = 1, lambda = mystat$lambda) ## # A tibble: 1 × 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.2 11 1 9 0.0847 Using our pexp function instead of our homemade f function: get_chisq( data = c1, n_total = 50, f = pexp, np = 1, rate = mystat$lambda) ## # A tibble: 1 × 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.2 11 1 9 0.0847 Or using a different function that is not exponential! get_chisq( data = c1, n_total = 50, f = pweibull, np = 2, shape = 0.2, scale = 0.5) ## # A tibble: 1 × 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 171. 11 2 8 0 7.5.5 Estimating Lambda Depending on whether we have complete data or not, we may need to estimate \\(\\lambda\\), rather than compute it directly. If we have a complete sample of data (eg. not censored), then we can just calculate: $ = $ # Eg. 1 / mean(masks$left_earloop) ## [1] 0.0748503 If our data is censored or pre-tabulated into groups, we may need to use the midpoint and count of failures in each bin to calculate \\(\\lambda\\). Tabulated: tallied up in equally sized bins. Tabulated, Type I Censoring: experiment stops when time \\(t\\) reaches \\(limit\\). Tabulated, Type II Censoring: experiment stops when number of units failed \\(n_{failed}\\) reaches \\(limit\\). For example, if we receive just the d3 data.frame we made above, how do we estimate \\(\\lambda\\) from it? We will need: \\(r\\): total failures (sum(r_obs)) \\(n\\): total observations (failed or not) (provided; otherwise, \\(n = r\\)) \\(\\sum_{i=1}^{r}{t_i}\\): total unit-hours (sum(midpoint*r_obs)) \\(T\\): timestep of (1) last failure observed (sometimes written \\(t_r\\)) or (2) last timestep recorded; usually obtained by max(midpoint). Unless some cases do not fail, \\((n - r)t_z\\) will cancel out as 0. \\[ \\hat{\\lambda} = \\frac{ r }{ \\sum_{i=1}^{r}{t_i} + (n - r)t_z} \\] # Let&#39;s calculate it! d4 &lt;- d3 %&gt;% summarize( r = sum(r_obs), # total failures hours = sum(midpoint*r_obs), # total failure-hours n = r, # in this case, total failures = total obs tz = max(midpoint), # end of study period # Calculate lambda hat! lambda_hat = r / (hours + (n - r)*tz)) 1 / mean(masks$left_earloop) ## [1] 0.0748503 Also, our estimate of lambda might be slightly off due to random sampling error. (Every time you take a random sample, theres a little change, right?) So, lets build a confidence interval around our estimate. We can do so by weighting \\(\\hat{\\lambda}\\) by a factor \\(k\\). This factor \\(k\\) may be greater depending on (1) the total number of failures \\(r\\) and (2) our level of acceptable error (\\(alpha\\)). Like any statistic, our estimate \\(\\hat{\\lambda}\\) has a full sampling distribution of \\(\\hat{\\lambda}\\) values we could get due to random sampling error, with some of them occurring more frequently than others. We want to find the upper and lower bounds around the 90%, 95%, or perhaps 99% most common (middle-most) values in that sampling distribution. So, if \\(alpha = 0.10\\), were going to get a 90% confidence interval (dubbed \\(interval\\)) spanning from the 5%~95% of that sampling distribution. # To calculate a &#39;one-tailed&#39; 90% CI (eg. we only care if above 90%) alpha = 0.10 ci = 1 - alpha # To adjust this to be &#39;two-tailed&#39; 90% CI (eg. we care if below 5% or above 95%)... 0.5 + (ci / 2) ## [1] 0.95 0.5 - ci / 2 ## [1] 0.05 No Censoring: If we have complete data (all observations failed!), using this formula to calculate factor \\(k\\): # For example, let&#39;s say r = 50 r = 50 k = qchisq(ci, df = 2*r) / (2*r) Time-Censoring: If we only record up to a specific time (eg. planning an experiment), use this formula to calculate factor \\(k\\), setting as the degrees of freedom \\(df = 2(r+1)\\). # For example, let&#39;s say r = 50 r = 50 k = qchisq(ci, df = 2*(r+1)) / (2*r) # Clear these remove(r, k) So, since we do not have time censoring in our d4 dataset, we can go ahead an calculate the df = 2*r and compute the 90% lower and upper confidence intervals. d4 %&gt;% summarize( lambda_hat = lambda_hat, r = r, k_upper = qchisq(0.95, df = 2*r) / (2*r), k_lower = qchisq(0.05, df = 2*r) / (2*r), lower = lambda_hat * k_lower, upper = lambda_hat * k_upper) ## # A tibble: 1 × 6 ## lambda_hat r k_upper k_lower lower upper ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0769 50 1.24 0.779 0.0599 0.0956 7.5.6 Planning Experiments When planning an experiment, we can use the following formula to determine the necessary values of \\(r\\), \\(t\\), \\(lambda\\), \\(k\\), or \\(n\\) for our experiment to function: \\[ \\frac{r}{n \\ t} \\times k_{r, 1 - \\alpha} = \\hat{\\lambda} \\] Note: Experiment planning by definition makes time-censored datasets, so youll need your adjusted \\(k\\) formula so that \\(df = 2(r+1)\\). Lets try an example. Imagine a company wants to test a new line of masks. Your company can budget 500 hours to test these masks elastic bands, and they accept that up to 10 of these masks may break in the process. Company policy anticipates a failure rate of just 0.00002 masks per hour, and we want to be 90% confidence that the failure rate will not exceed this level. (But its fine if it goes below that level.) If we aim to fit within these constraints, how many masks need to be tested in this product trial? # Max failures r = 10 # Number of hours t = 500 # Max acceptable failure rate lambda = 0.00002 # Calcuate a one-tailed confidence interval alpha = 0.10 ci = 1 - alpha # Use these ingredients to calculate the factor k, using time-censored rule df = 2*(r+1) k = qchisq(ci, df = 2*(r+1)) / (2*r) # Reformat # r / (t*n) * k = lambda n = k * r / (lambda * t) # Check it! n ## [1] 1540.664 Looks like youll need a sample of about 1541 masks to be able to fit those constraints. Learning Check 4 Question A small start-up is product testing a new super-effective mask. They product tested 25 masks over 60 days. They contract you to analyze the masks lifespan data, recorded below as the number of days to failure. # Lifespan in days supermasks &lt;- c(1, 2, 2, 2, 3, 3, 4, 4, 5, 9, 13, 15, 17, 19, 20, 21, 23, 24, 24, 24, 32, 33, 33, 34, 54) Cross-tabulate the lifespan distribution in intervals of 7 days. Estimate \\(\\hat{\\lambda}\\) from the cross-tabulated data. Estimate a 95% confidence interval for \\(\\hat{\\lambda}\\). Using the cross-tabulated data, do these masks lifespan distribution fit an exponential distribution, or does their distribution differ to a statistically significant degree from the exponential? How much? (eg. statistic and p-value). [View Answer!] Cross-tabulate the lifespan distribution in intervals of 7 days. ## # A tibble: 8 × 6 ## label r_obs bin lower upper midpoint ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [0,7] 9 1 0 7 3.5 ## 2 (7,14] 2 2 7 14 10.5 ## 3 (14,21] 5 3 14 21 17.5 ## 4 (21,28] 4 4 21 28 24.5 ## 5 (28,35] 4 5 28 35 31.5 ## 6 (35,42] 0 6 35 42 38.5 ## 7 (42,49] 0 7 42 49 45.5 ## 8 (49,56] 1 8 49 56 52.5 Estimate \\(\\hat{\\lambda}\\) from the cross-tabulated data. # Let&#39;s estimate lambda! b &lt;- a %&gt;% summarize( r = sum(r_obs), # total failures days = sum(midpoint*r_obs), # total failure-days n = r, # in this case, total failures = total obs tz = max(midpoint), # end of study period # Calculate lambda hat! lambda_hat = r / (days + (n - r)*tz)) # Check it b ## # A tibble: 1 × 5 ## r days n tz lambda_hat ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 416. 25 52.5 0.0600 Estimate a 95% confidence interval for \\(\\hat{\\lambda}\\). c &lt;- b %&gt;% summarize( lambda_hat = lambda_hat, r = r, k_upper = qchisq(0.975, df = 2*r) / (2*r), k_lower = qchisq(0.025, df = 2*r) / (2*r), lower = lambda_hat * k_lower, upper = lambda_hat * k_upper) # Check it c ## # A tibble: 1 × 6 ## lambda_hat r k_upper k_lower lower upper ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0600 25 1.43 0.647 0.0388 0.0857 Using the cross-tabulated data, do these masks lifespan distribution fit an exponential distribution, or does their distribution differ to a statistically significant degree from the exponential? How much? (eg. statistic and p-value). # Get your &#39;model&#39; function f = function(t, lambda){ 1 - exp(-1*lambda*t) } # Get lambda-hat from b$lambda_hat # Step 3a: Calculate Observed vs. Expected get_chisq(data = a, n_total = 25, np = 1, f = f, lambda = b$lambda_hat) ## # A tibble: 1 × 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.85 8 1 6 0.131 # Step 3b: Calculate Chi-squared if we had received the vector supermasks all along, assuming that were the whole population # Get lambda from directly from the data f = function(t, lambda){ 1 - exp(-1*lambda*t) } get_chisq(t = supermasks, binwidth = 7, n_total = 25, np = 1, f = f, lambda = 1 / mean(supermasks) ) ## # A tibble: 1 × 5 ## chisq nbin np df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.72 8 1 6 0.137 The slight difference in results is due to b$lambda_hat being slightly different from 1 / mean(supermasks), the true empirical failure rate. 7.6 Conclusion Congrats! You made it! Youve just picked up some of the key techniques for evaluating product lifespans in R! "],["workshop-useful-life-distributions-weibull-gamma-lognormal.html", "8 Workshop: Useful Life Distributions (Weibull, Gamma, &amp; Lognormal) Getting Started 8.1 Maximum Likelihood Estimation (MLE) Learning Check 1 8.2 Gamma Distribution Learning Check 2 8.3 Weibull Distribution Learning Check 3 Learning Check 4 8.4 Lognormal Distribution Learning Check 5 Learning Check 6 8.5 Conclusion", " 8 Workshop: Useful Life Distributions (Weibull, Gamma, &amp; Lognormal) In this workshop, were going to continue learning some R functions for working with common life distributions, specifically the Weibull, Gamma, and Log-Normal distributions. Getting Started Load Packages Lets start by loading the tidyverse package. Well also load mosaicCalc, for taking derivatives and integrals (eg. D() and antiD()). # Load packages library(tidyverse) library(mosaicCalc) Load Data In this workshop, were going to use a dataset of crop lifetimes! (Agriculture needs statistics too!) For each crop (id), we measured its lifespan in days from the day its seed is planted to the time it is ripe (usually about 90 days). # Import crop lifespan data! crops &lt;- read_csv(&quot;workshops/crops.csv&quot;) 8.1 Maximum Likelihood Estimation (MLE) In our last few workshops, we learned that the exponential distribution does a pretty good job of approximating many life distributions, and that we can evaluate the fit of a distribution using a chi-squared test. In this workshop, well learn several alternative, related distributions that might fit your data even better depending on the scenario. But modeling any data using a distribution requires that we know the parameters that best match the observed data. This can be really hard, as we found out with \\(\\hat{\\lambda}\\) in the exponential distribution. Below, well learn a computational approach called Maximum Likelihood Estimation (MLE), that will help us find the true (most likely) values for any parameter in any dataset. Its pretty robust if the number of failures is large enough, where large counts as &gt;10 (which is really small!). 8.1.1 Likelihood Function MLE involves 3 ingredients: sample: a vector of raw empirical data probability density function (PDF): tells us probability (relative frequency) of each value in a sample occurring. likelihood function: tells us probability of getting this EXACT sample, given the PDF values for each observed data point. We can get the probability of any sample by multiplying the density function f(t) at each data point \\(t_{i}\\), for r data points. Multiplication indicates joint probability, so this product really means how likely is is to get this specific sample. We call it the likelihood of that sample. \\[ LIK = \\prod_{i = 1}^{r}{ f(t_i) } = f(t_1) \\times f(t_2) \\times ... f(t_n) \\] # Let&#39;s write a basic pdf function d = function(t, lambda){lambda * exp(-t*lambda) } # Let&#39;s imagine we already knew lambda... mylambda = 0.01 crops %&gt;% # Calculate probability density function for observed data mutate(prob = d(days, lambda = mylambda)) %&gt;% summarize( # Let&#39;s calculate the likelihood likelihood = prob %&gt;% prod(), # Since that&#39;s a really tiny number, # let&#39;s take its log to get log-Likelihood loglik = prob %&gt;% prod() %&gt;% log()) ## # A tibble: 1 × 2 ## likelihood loglik ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7.55e-116 -265. Technically, our likelihood above shows the joint probability of each of these observed values occurring together supposing that: they have an exponential probability density function (PDF), and that the parameter lambda in that PDF equals 0.01. So we can get different (higher/lower) likelihoods of these values occuring together if we supply (1) different parameter values and therefore (2) a different hypothesized PDF. The most accurate parameters will be the ones that maximize the likelihood. In practice though, the likelihood is teeny-tiny, and multiplying tiny numbers is hard! Fortunately the log of a tiny value makes it bigger and easier to read and compute. the log of a product of values actually equals the sum of the log of those values (see equation below!). \\[ log ( \\ L(t) \\ ) = \\log\\prod_{i=1}^{n}{ f(t_i) = \\sum_{i=1}^{n}\\log( \\ f(t_i) \\ ) } \\] ll = function(data, lambda){ # Calculate Log-Likelihood, # by summing the log d(t = data, lambda) %&gt;% log() %&gt;% sum() } 8.1.2 Maximizing with optim() We can use the optim() function from base R to: run our function (fn = ) many times, varying the value of lambda (or any parameter). supply any other parameters like data = crops$days to each run. identify the log-likelihood that is greatest (if control = list(fnscale = -1)); by default, optim() actually minimizes otherwise. return the corresponding value for lambda (or any other parameter). optim() will output a $par (our parameter estimate(s)) and $value (the maximum log-likelihood). # Maximize the log-likelihood! optim(par = c(0.01), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.014375 ## ## $value ## [1] -262.167 ## ## $counts ## function gradient ## 18 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 8.1.3 Under the Hood Wondering whats happening inside optim()? Good news: its not too tough. We can actually code and visualize it ourselves! First, lets get all the log-likelihoods in that interval. # Let&#39;s manyll &lt;- data.frame(parameter = seq(from = 0.00001, to = 1, by = 0.001)) %&gt;% # For each parameter, get the loglikelihood group_by(parameter) %&gt;% summarize(loglik = ll(data = crops$days, lambda = parameter)) # Check a few manyll %&gt;% head(3) ## # A tibble: 3 × 2 ## parameter loglik ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00001 -576. ## 2 0.00101 -348. ## 3 0.00201 -317. Lets maximize the log-likelihood manually # Now find the parameter that has the greatest log-likelihood output &lt;- manyll %&gt;% filter(loglik == max(loglik)) #check it output ## # A tibble: 1 × 2 ## parameter loglik ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0140 -262. Now, lets visualize it! ggplot() + geom_line(data = manyll, mapping = aes(x = parameter, y = loglik), color = &quot;steelblue&quot;) + geom_vline(xintercept = output$parameter, linetype = &quot;dashed&quot;) + theme_classic(base_size = 14) + labs(x = &quot;parameter (lambda)&quot;, y = &quot;loglik (Log-Likelihood)&quot;, subtitle = &quot;Maximizing the Log-Likelihood (Visually)&quot;) + # We can actually adust the x-axis to work better with log-scales here scale_x_log10() + # We can also annnotate our visuals like so. annotate(&quot;text&quot;, x = 0.1, y = -2000, label = output$parameter) 8.1.4 Multi-parameter optimization Multi-parameter optimization works pretty similarly. We can use optim() for this too! The key here is that optim() only varies whatever values we supply to optim(par = ...). So if we have multiple parameters, like the mean and sd for the normal distribution, we need to put both parameters into one vector in our input par, like so: # Let&#39;s write a new function ll = function(data, par){ # Our parameters input is now going to be vector of 2 values # par[1] gives the first value, the mean # par[2] gives the second value, the standard deviation dnorm(data, mean = par[1], sd = par[2]) %&gt;% log() %&gt;% sum() } Now, lets maximize! # Let&#39;s optimize it! # put in some reasonable starting values for &#39;par&#39; and it optim() will do the rest. optim(par = c(90, 15), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 69.65196 47.56853 ## ## $value ## [1] -264.0527 ## ## $counts ## function gradient ## 51 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL That was ridiculously easy! Lets compare to their known values: crops %&gt;% summarize(mu = mean(days), sigma = sd(days)) ## # A tibble: 1 × 2 ## mu sigma ## &lt;dbl&gt; &lt;dbl&gt; ## 1 69.6 48.0 Almost spot on! Pretty good for an estimation technique! 8.1.5 MLE with Censored Data But what do we do if we need to estimate parameters with MLE, but our data is censored (eg. time censored, or cross-tabulated?) Well, fortunately, same general rules apply! \\[ LIK = k \\times ( \\Pi_{i = 1}^{r}{f(t_i) \\ } \\times [1 - F(T_{max})]^{n-r} ) \\approx \\prod_{i=1}^{r}{ f(t_i) } \\times R(T_{max})^{n-r} \\] We can re-write the likelihood function as the joint probability of each time to failure \\(f(t_i)\\) for \\(r\\) total failures that did occur, times the cumulative probability that they did not fail by the end of the study period \\(T_{max}\\) for \\(n - r\\) potential failures that did not occur. Just like when estimating lambda though, we know that theres room for error, so we can calculate a \\(k\\) factor like before to upweight or downweight our likelihood statistic, giving us upper and lower confidence intervals. Most often, we are looking for the precise estimate of parameters, not upper and lower confidence intervals for them, so in those cases, \\(k\\) can be omitted, simplifying down to the right-side equation above. Lets try a few applications of this formula below. 8.1.5.1 Time Censored Data Suppose we actually had \\(n = 75\\) crops, but we only ever saw \\(r = 50\\) of them fail, and we stopped recording after 200 days. How would we estimate the maximum likelihood to get our parameter of interest \\(\\lambda\\)? First, lets write our functions. # Let&#39;s write a basic pdf function &#39;d&#39; and cdf function &#39;f&#39; d = function(t, lambda){ lambda * exp(-t*lambda) } f = function(t, lambda){ 1 - exp(-t*lambda)} Now, using our observed (but incomplete data in crops), lets calculate the log-likelihood if \\(\\lambda = 0.01\\). crops %&gt;% summarize( # Get total failures observed r = n(), # Get total sample size, n = 75, # Get last timestep tmax = 200, # Take the product of the PDF at each timestep prob_d = d(t = days, lambda = 0.01) %&gt;% prod(), # Get probability of survival by the last time step, # for as many observations as did not fail prob_r = (1 - f(t = tmax, lambda = 0.01))^(n - r), # Get likelihood lik = prob_d * prob_r, # Get loglikelihood loglik = log(lik)) ## # A tibble: 1 × 7 ## r n tmax prob_d prob_r lik loglik ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 50 75 200 7.55e-116 1.93e-22 1.46e-137 -315. Great! Lets formalize that in a function, for any value of \\(\\lambda\\). # We&#39;ll write a log-likelihood function &#39;ll&#39; # that takes a data input and a single numeric parameter par ll = function(data, par){ output &lt;- data %&gt;% summarize( r = n(), n = 75, tmax = 200, prob_d = d(t = days, lambda = par) %&gt;% prod(), prob_r = (1 - f(t = tmax, lambda = par))^(n - r), loglik = log(prob_d * prob_r)) # Return the output output$loglik } # Last, we can run &#39;optim&#39; to get the MLE, with a starter guess of lambda at 0.05 optim(par = c(0.01), data = crops, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.005898438 ## ## $value ## [1] -306.6839 ## ## $counts ## function gradient ## 22 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL 8.1.5.2 Cross-Tabulated, Time-Censored Data Alternatively, what if our data were not only time-censored, but had been cross-tabulated!? The crosstab data.frame below encodes days to failure for our crops, tallied up in 40 day intervals, supposing a total sample of 75 sampled crops evaluated for 200 days. We can still estimate parameters with MLE using the formula above! crosstab &lt;- data.frame( label = c(&quot;[0,40]&quot;, &quot;(40,80]&quot;, &quot;(80,120]&quot;, &quot;(120,160]&quot;, &quot;(160,200]&quot;), t = c(20, 60, 100, 140, 180), count = c(18, 14, 10, 5, 3)) We can write our log-likelihood function very similarly to above. The main change is that we add up the counts to get r, the total observed failures. To estimate our probabilities, we just use the interval midpoints as our days to failure t. ll = function(data, par){ output &lt;- data %&gt;% summarize( # Get total failures observed (sum of all tallies) r = sum(count), # Get total sample size, n = 75, # Get last timestep tmax = 200, # Get PDF at each timestep; take product for each failure observed prob_d = d(t = t, lambda = par)^count %&gt;% prod(), # Get probability of survival by the last time step, # for as many n-r observations that did not fail prob_r = (1 - f(t = tmax, lambda = par))^(n - r), # Get log-likelihood loglik = log(prob_d * prob_r)) # Return the output output$loglik } # Last, we can run &#39;optim&#39; to get the MLE, with a starter guess of lambda at 0.05 optim(par = c(0.01), data = crosstab, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.005925781 ## ## $value ## [1] -306.4357 ## ## $counts ## function gradient ## 24 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL The estimate provided is a little different than the one above, but its still in a very similar ballpark. When working with cross-tabulations but estimating complex parameters, this estimation strategy can be a life-saver (literally). Learning Check 1 Question Weve learned several probability density functions for different distributions, including dexp(), dgamma(), dpois(), and dweibull(). Use optim() to maximize the likelihood of each of these PDFs likelihood, using our crops data. [View Answer!] For the exponential distribution ll = function(data, par){ dexp(data, rate = par) %&gt;% log() %&gt;% sum() } optim(par = c(0.01), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 0.014375 ## ## $value ## [1] -262.167 ## ## $counts ## function gradient ## 18 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL For the gamma distribution ll = function(data, par){ dgamma(data, shape = par[1], scale = par[2]) %&gt;% log() %&gt;% sum() } optim(par = c(1, 1), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 1.921441 36.239762 ## ## $value ## [1] -256.9295 ## ## $counts ## function gradient ## 83 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL For the Poisson distribution ll = function(data, lambda){ dpois(data, lambda) %&gt;% log() %&gt;% sum() } # Optimize! optim(par = 90, data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 69.63574 ## ## $value ## [1] -942.9518 ## ## $counts ## function gradient ## 26 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL For the Weibull distribution ll = function(data, par){ dweibull(data, shape = par[1], scale = par[2]) %&gt;% log() %&gt;% sum() } # Optimize! optim(par = c(1,1), data = crops$days, fn = ll, control = list(fnscale = -1)) ## $par ## [1] 1.492828 77.106302 ## ## $value ## [1] -256.775 ## ## $counts ## function gradient ## 107 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL remove(ll, d, output, mylambda, manyll) 8.2 Gamma Distribution Alternatively, the Gamma distribution is well suited to modeling products exposed to a series of shocks over time at a given rate over time. 8.2.1 \\(\\Gamma\\) function gamma() In the gamma distribution, events are exposed to \\(k\\) shocks. We often will use something called a gamma function \\(\\Gamma\\) of \\(k\\) to show the total ways a series of shocks could have occurred. Gamma functions are written \\((k - 1)!\\), where \\(!\\) means factorial, or taking the product of k by every prior integer. Since, \\((k - 1)! = \\Gamma(k)\\), then if \\(k = 4\\), then \\(\\Gamma(k = 4) = (4 - 1) \\times (3 - 1) \\times (2 - 1) = 3 \\ \\times 2 \\times 1 = 6\\). We can quickly code in this in r using the gamma() function. For example, in the prior example, gamma(4) renders 6. Pretty quick, right? We can also code this directly using the factorial() function. For example, if k = 4, we can write factorial(4 - 1). # Let&#39;s try out the gamma function. gamma(4) ## [1] 6 # Produces same output as factorial(k - 1) factorial(4 - 1) ## [1] 6 8.2.2 \\(f(t)\\) or d(t)(Probability Density Function, aka PDF) Excitingly, we can reapply most of the same rules we learned for exponential distributions; we just have to update our functions. In the gamma distribution, events are exposed to \\(k\\) shocks that occur at rate \\(\\lambda\\) over time \\(t_1, t_2, ... t_n\\). We can model the time to failure for such a function like so! \\[f(t) = \\frac{\\lambda}{(k - 1)!}(\\lambda t)^{k-1}e^{-\\lambda t}\\] We can also write \\((k - 1)!\\) above as \\(\\Gamma(k)\\). As mentioned above, this is called a \"Gamma function of k\", which is where the distributions name comes from. \\(k\\) effectively controls the shape of the function. # Let&#39;s write our new PDF function, d(t); written as f(t) above d = function(t, k, lambda){ lambda / factorial(k - 1) * (lambda*t)^(k-1) * exp(-t*lambda) } # Try it out! d(t = 1, k = 1, lambda = 1) ## [1] 0.3678794 # Compare with our dgamma() function! dgamma(x = 1, shape = 1, rate = 1) ## [1] 0.3678794 # They&#39;re the same! 8.2.3 \\(F(t)\\) Failure Function and \\(R(t)\\) Reliability Function The failure and reliability function are always closely related, no matter the distribution, where \\(R(t) = 1 - F(t)\\). We can write the failure function \\(F(t)\\) (a.k.a. the CDF) as: \\[ F(t) = 1 - \\sum_{n = 0}^{k - 1}{\\frac{(\\lambda t)^n }{n!}e^{-\\lambda t}}\\] Therefore, we can also write the reliability function \\(R(t)\\) as: \\[ R(t) = \\sum_{n = 0}^{k - 1}{\\frac{(\\lambda t)^n }{n!}e^{-\\lambda t}}\\] # Write the failure function for Gamma distribution f = function(t, k, lambda){ # Make a vector of values from 0 to k-1 n = seq(from = 0, to = k - 1) # Now compute the failure function 1 - sum( (lambda*t)^n / factorial(n) * exp(-lambda*t) ) } # We can also integrate the PDF to get the CDF, alternatively, using mosaicCalc fc = antiD(tilde = d(t, k, lambda) ~ t) # Let&#39;s compare! # Using pgamma() pgamma(q = 10,shape = 3,rate = 1) ## [1] 0.9972306 # Using our calculate-based fc() fc(t = 10, k = 3, lambda = 1) ## [1] 0.9972306 # Using our direct function f() f(t = 10, k = 3, lambda = 1) ## [1] 0.9972306 # All the same! Correspondingly, the reliability function \\(R(t)\\) would be # Write the reliability function r = function(t, k, lambda){ # Make a vector of values from 0 to k-1 n = seq(from = 0, to = k - 1) # Now compute the reliability function sum( (lambda*t)^n / factorial(n) * exp(-lambda*t) ) } # Get reliability function via calculus... rc = function(t,k,lambda){ # compute CDF fc = antiD(tilde = d(t, k, lambda) ~ t) # return 1 - CDF 1 - fc(t = t, k = k, lambda = lambda) } # Compare outputs! # with pgamma() 1 - pgamma(q = 10, shape = 3, rate = 1) ## [1] 0.002769396 # with rc() rc(t = 10, k = 3, lambda = 1) ## [1] 0.002769396 # with r() r(t = 10, k = 3, lambda = 1) ## [1] 0.002769396 # They&#39;re the same! \\(z(t)\\) Failure Rate function The failure rate remains \\(z(t) = \\frac{f(t)}{R(t)}\\). Further, in the gamma distribution, the rate remains constant, although the size parameter causes a result quite different from the exponential! # Let&#39;s write the failure rate function z = function(t,k,lambda){ # We&#39;ll break it up into parts to help readability # Compute the PDF d(t) (or f(t)) d_of_t = lambda / factorial(k - 1) * (lambda*t)^(k-1) * exp(-t*lambda) # Make a vector of values from 0 to k-1 n = seq(from = 0, to = k - 1) # Now compute the reliability function R(t) r_of_t = sum( (lambda*t)^n / factorial(n) * exp(-lambda*t) ) # Divide the two to get the failure rate! d_of_t / r_of_t } # Or, using calculus.... zc = function(t, k, lambda){ # Get CDF via integration fc = antiD(tilde = d(t, k, lambda) ~ t) # Get r(t) using 1 - f(t) # Take PDF / (1 - CDF) d(t = t, k = k, lambda = lambda) / (1 - fc(t = t, k = k, lambda = lambda) ) } # Try it out! # Using dgamma() and pgamma() dgamma(1,shape = 1, rate = 0.1) / (1 - pgamma(1,shape = 1, rate = 0.1)) ## [1] 0.1 # Using our calculus based function zc(t = 1, k = 1, lambda = 0.1) ## [1] 0.1 # Using our home cooked function z(t = 1, k = 1, lambda = 0.1) ## [1] 0.1 8.2.4 MTTF and Variance We can also compute the mean time to fail (MTTF) and variance using the following formulas. \\[ MTTF = \\int_{0}^{\\infty}{ t \\ f(t) dt} = \\frac{k}{\\lambda} \\] \\[ Var(t) = \\frac{k}{ \\lambda^2} \\] We can encode these as functions as follows. # Mean (aka mean time to fail) mttf = function(k, lambda){ k / lambda } # Check it! mttf(k = 1, lambda = 0.1) ## [1] 10 # Variance variance = function(k, lambda){ k / lambda^2 } # Try it! variance(k = 1, lambda = 0.1) ## [1] 100 Learning Check 2 Question A car bumper has demonstrated a gamma distribution with a failure rate of \\(\\lambda = 0.005\\) per day given \\(k = 3\\) potential shocks. Determine the reliability of this bumper over a 30, 300, and 600 day period. [View Answer!] # Compute a reliability function using pgamma() r = function(t,k,lambda){ 1 - pgamma(t,shape = k, rate = lambda) } # For 24 hours r(t = c(30, 300, 600), k = 3, lambda = 0.005) ## [1] 0.9994971 0.8088468 0.4231901 8.3 Weibull Distribution Another popular distribution is the Weibull distribution. The exponential is a special case of Weibull distribution where the failure rate \\(\\lambda\\) is held constant such that \\(m = 1\\). But in a usual Weibull distribution, the failure rate can change over time! This makes it very flexible, able to take on the shape of an exponential, gamma, or normal distribution depending on the parameters. We can show this easily using the Weibulls cumulative hazard function \\(H(t) = (\\lambda t)^m\\). If \\(m = 1\\), as in the exponential distribution, then \\(H(t) = \\lambda t\\), as in the exponential distribution. But if \\(m \\neq 1\\), then the accumulative hazard rate increases to the \\(m\\) power with ever passing hour. We can use this to derive the failure function \\(F(t)\\), reliability function \\(R(t)\\), and failure rate \\(z(t)\\). (Similarly, we can use all the same tricks from before to get the \\(AFR(t_1, t_2)\\), like using the accumulative hazard function \\(\\frac{H(t_2) - H(t_1)}{t_2 - t_1}\\).) 8.3.1 \\(F(t)\\) Failure Function and \\(R(t)\\) Reliability Function To derive \\(F(t)\\), we can sub in the new formula for the accumulative hazard function into the formula for the failure function. The Weibull failure function is also commonly written replacing \\(\\lambda\\) with \\(c = \\frac{1}{\\lambda}\\), the characteristic life. \\[ F(t) = 1 - e^{-H(t)} = 1 - e^{-(\\lambda t)^m} = 1 - e^{-(t/c)^m}\\] We can code it like so: f = function(t, m, c){ 1 - exp(-1*(t/c)^m) } # Compare! # Using pweibull() pweibull(1, shape = 1, scale = 1) ## [1] 0.6321206 # Using our home-made function! f(t = 1, m = 1, c = 1) ## [1] 0.6321206 Similarly, we can write the reliability function as r = function(t, m, c){ exp(-1*(t/c)^m) } # Try it with pweibull() 1 - pweibull(1, shape = 1, scale = 1) ## [1] 0.3678794 # Using our home-made function! r(t = 1, m = 1, c = 1) ## [1] 0.3678794 8.3.2 \\(f(t)\\) or d(t) (PDF) While not nearly as straightforward to derive, we can literally take the derivative of the Failure function to get the PDF. \\[ f(t) = \\frac{m}{t} \\times (\\frac{t}{c})^m \\times e^{-(t/c)^m}\\] d = function(t, m, c){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) # alternatively written using calculus: # D(f(t,m,c) ~ t) } # Compare results! dweibull(1, shape = 1, scale = 1) ## [1] 0.3678794 d(t = 1, m = 1, c = 1) ## [1] 0.3678794 8.3.3 \\(z(t)\\) Failure Rate We can similarly derive the failure rate \\(z(t)\\) by taking the derivative of the accumulative hazard rate, which gives us: \\[z(t) = m \\lambda (\\lambda t)^{m-1} = (m/c) \\times (t/c)^{m-1} \\]/ z = function(t, m, c){ (m/c) * (t/c)^(m-1) } # try it! z(1, m = 1, c = 0.1) ## [1] 10 # using dweibull() and pweibull() dweibull(1, shape = 1, scale = 0.1) / (1 - pweibull(1, shape = 1, scale = 0.1)) ## [1] 10 8.3.4 \\(m\\) and \\(c\\) Fortunately, if any 3 of the 4 parameters (\\(F(t), m, t, c\\) are known about a Weibull distribution, the remaining parameter can be calculated. Since \\(F(t) = 1 - e^{-(t/c)^m}\\), we can say: \\[ t = c \\times ( -log( 1 - F ))^{1/m} \\] \\[m = \\frac{log( -log( 1 - F) )}{log(t/c)}\\] \\[c = \\frac{t}{( -log(1 - F) )^{1/m}}\\] Characteristic life \\(c\\), in this case, really means the time at which 63.2% of units will consistently have failed. Shape parameter \\(m\\) is used to describe several different types of Weibull distributions. \\(m = 1\\) is an exponential; \\(m = 2\\) is a Reyleigh distribution, where the failure rate increases linearly. When \\(m &lt; 1\\), it looks like the left-end of a bath-tub. When \\(m &gt; 1\\), it looks like the right-end of a bath-tub. 8.3.5 Weibull Series Systems In a series system of independent components, which are each Weibull distributed with the same shape parameter \\(m\\), that system has a Weibull distribution, as follows: \\[ c_{series} = (\\sum_{i=1}^{n}{ \\frac{1}{c_i^m}})^{-\\frac{1}{m}} \\] 8.3.6 MTTF and Variance Finally, we could code the mean time to fail and variance as follows. \\[ MTTF = c \\Gamma(1 + \\frac{1}{m}) = c \\times (k - 1)! \\times (1 + \\frac{1}{m}) \\] \\[ Variance = c^2 \\Gamma(1 + \\frac{2}{m} ) - [ c \\Gamma(1 + \\frac{1}{m})]^{2} \\] # We could code these up simply like so mttf = function(c, m){ c * gamma( 1 + 1/m) } # Though we know that mttf = integral of R(t), so we could also write it like this mttf = antiD((1 - pweibull(t, shape = m, scale = c)) ~ t) # We could code these up simply like so variance = function(c, m){ c^2 * gamma( 1 + 2/m ) - ( c * gamma( 1 + 1/m) )^2 } Learning Check 3 Question Youve done this a bunch now - for this learning check, write your own function to find \\(t\\), \\(m\\), and \\(c\\) in a Weibull distribution. [View Answer!] # Write a function to get t get_t = function(f,m,c){ log(-log(1 - f) )/log(t/c) } # Write a function to get m get_m = function(t,f,c){ log(-log(1 - f) )/log(t/c) } # Write a function to get c get_c = function(t,f,m){ t / ( (-log(1 - f))^(1/m) ) } Learning Check 4 Question Find the characteristic life necessary for 10% of failures to occur by 168 hours, if the shape parameter \\(m = 2\\). Then, using that characteristic life, plot the probability of failure when \\(m = 1\\), \\(m = 2\\), and \\(m = 3\\). [View Answer!] # Write our function to find c get_c = function(t, f, m){ t / ( (-log(1 - f))^(1/m) ) } get_c(t = 168, f = 0.10, m = 2) ## [1] 517.5715 # Looks like we expect a characteristic life of &gt;500 hours. # Write the failure function f = function(t, c, m){ 1 - exp(-1*(t/c)^m) } m1 &lt;- data.frame(hours = 1:100) %&gt;% mutate(prob = f(t = hours, c = 517.5715, m = 1), m = &quot;m = 1&quot;) m2 &lt;- data.frame(hours = 1:100) %&gt;% mutate(prob = f(t = hours, c = 517.5715, m = 2), m = &quot;m = 2&quot;) m3 &lt;- data.frame(hours = 1:100) %&gt;% mutate(prob = f(t = hours, c = 517.5715, m = 3), m = &quot;m = 3&quot;) bind_rows(m1,m2,m3) %&gt;% ggplot(mapping = aes(x = hours, y = prob, color = m)) + geom_line() 8.4 Lognormal Distribution The log-normal distribution can be very useful, often in modeling semi-conductors, among other product. To understand the log-normal distribution, lets outline the normal distribution PDF, and compare it to the log-normal distribution PDF. 8.4.1 \\(f(t)\\) or d(t) (PDF) The normal distribution contains the parameters \\(\\mu\\) (mean) and \\(\\sigma\\) (standard deviation). \\[f(t) = \\frac{1}{\\sigma \\sqrt{2\\pi} } \\times e^{-(t - \\mu)^2 / 2\\sigma^2 } \\] The log-normal distribution is quite similar. Here, \\(t\\) becomes \\(e^t\\) and median \\(T_{50} = e^{\\mu}\\). \\(\\sigma\\) is really more of a shape parameter here than the standard deviation as we usually think of it. \\[ f(t) = \\frac{1}{\\sigma t \\sqrt{2\\pi}} \\times e^{-(1/2\\sigma^2) \\times (log(t) - log(T_{50} ))^2}\\] 8.4.2 \\(\\Phi\\) Interestingly, the CDF of the log-normal relies on the CDF of the normal distribution, sometimes written as \\(\\Phi\\). Both distributions CDFs are written below. \\[ Normal \\ F(t) = \\Phi ( \\frac{t - \\mu}{ \\sigma } ) \\] \\[ Log-Normal \\ \\ F(t) = \\Phi (\\frac{log(t / T_{50})}{\\sigma}) \\] We can write the normal distributions CDF, \\(\\Phi\\), as the cumulative probability of a lifespan \\(t\\) that is \\(\\frac{t - \\mu}{\\sigma}\\) standard deviations away from the mean. In the log-normal CDF, we transform our value \\(t\\) into a point measured in standard deviations from the mean. These points are called z-scores. We can feed that value to pnorm() to find out the cumulative probability of reaching that point on a normal distribution, with a mean of 0 and a standard deviation of 1. # Suppose... t = 50 t50 = 100 sigma = 2 # Get the cumulative probability of failure at time t p &lt;- pnorm( log(t/t50) / sigma ) # Check it! p ## [1] 0.3644558 Similarly, if we have a cumulative probability like p, we can solve for the z-score that made it using qnorm(). qnorm() is sometimes referred to as inverse-phi (\\(\\Phi^{-1}\\)). # Get the z-score for F(t) = 0.36! z &lt;- qnorm(p) # Check it! z ## [1] -0.3465736 Now that we know the z-score, we can easily solve for t, t50, or sigma with algebra if we know the other parameters, since \\(z = \\frac{log(t/T_{50})}{\\sigma}\\)! To get t # t = exp(z * sigma) * t50 exp(z * sigma) * t50 ## [1] 50 To get t50 # t50 = t / exp(z * sigma) t / exp(z * sigma) ## [1] 100 To get sigma # sigma = log(t/t50) / z log(t/t50) / z ## [1] 2 Pretty quick, right? 8.4.3 Key Functions This ones pretty messy. (One can use the dlnorm() function, but it contains 2 parameters, meanlog and sdlog, which are NOT \\(T_{50}\\) (median) and \\(\\sigma\\) (shape). For six-sigma, it tends to be more helpful for us to write the log-normal functions in terms of \\(T_{50}\\) and \\(\\sigma\\) instead.) # Write the pdf of the log-normal, f(t) or d(t)! d = function(t, t50, sigma){ 1 / (sigma * t * sqrt(2*pi)) * exp( -(1/ 2 * sigma^2 )*(log(t) - log(t50))^2) } # Write the failure function of the log-normal F(t)! f = function(t, t50, sigma){ pnorm( log(t / t50) / sigma ) } # Or with mosaicCalc via derivation... # dc = D(f(t,t50, sigma)~t) # Write the reliability function R(t)! r = function(t, t50, sigma){ 1 - pnorm( log(t / t50) / sigma ) } # Write the failure rate z(t)! z = function(t, t50, sigma){ d(t, t50, sigma) / r(t, t50, sigma) } # Write the accumulative hazard function H(t)! h = function(t, t50, sigma){ -log(r(t, t50, sigma)) } # Write the average failure rate function AFR(t) afr = function(t1, t2, t50, sigma){ (h(t2) - h(t1) ) / (t2 - t1) } Wow! That was surprisingly easy! In this way, as we pick up more and more distributions in this course, our tools for generating failure and reliability functions get easier and easier too! 8.4.4 MTTF and Variance Finally, we can quickly calculate these quantities of interest too. \\[ MTTF = T_{50} \\times e^{\\sigma^2 / 2} \\] \\[ Variance = (T_{50})^{2} \\times e^{\\sigma^2} \\times (e^{\\sigma^2} - 1)\\] # We could code these up simply like so mttf = function(median, sigma){ median * exp(sigma^2 / 2) } # We could code these up simply like so variance = function(median, sigma){ median^2 * exp(sigma^2) * (exp(sigma^2) - 1) } 8.4.5 Using Other Parameters Its important to note that there are other ways to specify the log-normal distribution in R too. For example, these ways of writing the mean time to failure (MTTF) are all equivalent. # Using the median... mttf = function(median, sigma){ median * exp(sigma^2 / 2) } mttf(median = 1, sigma = 3) ## [1] 90.01713 # Using the meanlog, which is equal to the log of the median mttf = function(meanlog, sigma){ exp(meanlog) * exp(sigma^2 / 2) } mttf(meanlog = log(1), sigma = 3) ## [1] 90.01713 # Using the meanlog, written a different way mttf = function(meanlog, sigma){ exp(meanlog + sigma^2 / 2) } mttf(meanlog = log(1), sigma = 3) ## [1] 90.01713 There are also multiple ways to calculate the variance(). # Requires the median, as well as sigma variance1 = function(median, sigma){ median^2 * exp( sigma^2 ) * (exp(sigma^2) - 1) } variance1(median = 2, sigma = 3) ## [1] 262607464 # Requires a statistic called the meanlog, as well as sigma variance2 = function(meanlog, sigma){ (exp(sigma^2) - 1) * exp(2*meanlog + sigma^2) } variance2(meanlog = log(2), sigma = 3) ## [1] 262607464 You could also calculate your own sigma() function if you chose. # Let&#39;s write our own sigma function... sigma = function(t, t50, prob){ # Let&#39;s derive it... #prob = pnorm(log(t/t50)/sigma) #qnorm(prob) = log(t/t50) / sigma sigma = log(t/t50) / qnorm(prob) return(sigma) } Learning Check 5 Question A crop tends to grow to a median height of 2 feet tall. We know from past data that this crop has a lognormal distribution, with a shape parameter \\(\\sigma\\) of about 0.2 feet. Market standards prefer crops between 1.5 and 8 feet tall. Given these standards, what percentage of crops are not eligible for market? [View Answer!] # Get percent under 1.5 feet below &lt;- f(1.5, t50 = 2, sigma = 0.2) # Get percentage over 8 feet above &lt;- 1 - f(8, t50 = 2, sigma = 0.2) # Get total percentage outside of these bounds below + above ## [1] 0.07515883 Learning Check 6 Question A log normal distribution has a median time to failure equal to 50,000 hours and shape parameter \\(\\sigma\\) equal to 0.8. What is the mean time to failure and true standard deviation in this distribution? [View Answer!] # Let&#39;s write a mean time to failure function mttf = function(median, sigma){ median * exp(sigma^2 / 2) } mttf(median = 50000, sigma = 0.8) ## [1] 68856.39 # Looks like the MTTF is ~69,000 hours # Let&#39;s write the variance function variance = function(median, sigma){ median^2 * exp(sigma^2 / 2) * (exp(sigma^2) - 1) } # Now calculate variance, and take square root to get sd variance(median = 50000, sigma = 0.8) %&gt;% sqrt() ## [1] 55555.57 # with a very wide standard deviation 8.5 Conclusion Great work! Youve learned to work with the Exponential, Gamma, Weibull, and Log-normal distribution. Youre ready to start coding some cool systems reliability analyses! Next week, well learn some new techniques to help! "],["workshop-fault-tree-analysis-in-r.html", "9 Workshop: Fault Tree Analysis in R Getting Started 9.1 Simulating Fault Trees Learning Check 1 Learning Check 2 Learning Check 3 9.2 Visualizing Fault Trees in R 9.3 Cutsets", " 9 Workshop: Fault Tree Analysis in R Fault trees are visual representation of boolean probability equations, typically depicting the minimal cut sets of necessary events leading to system failure. Were going to learn how to make calculations about fault trees in R! Getting Started Prerequisites As a prerequisite for this workshop, be sure to have read our class reading on Fault Trees! Youll need to install a few packages real quick below. Here they are. install.packages(c(&quot;devtools&quot;, &quot;admisc&quot;, &quot;tidygraph&quot;, &quot;ggraph&quot;, &quot;QCA&quot;)) # Install the tidyfault package! devtools::install_github(&quot;timothyfraser/tidyfault&quot;) # Select, &#39;install all packages&#39; # on the menu that will pop up in your console Load Packages library(tidyverse) library(tidyfault) 9.1 Simulating Fault Trees Fault Trees are a powerful way of modeling the probability of a top event, eg. a big prominent failure, like a nuclear disaster, a missile launch, etc. It supposes that a series of different chains of events can all lead to that top event. Each event in the chain has a probability, and these all can be represented graphically and numerically using boolean logic to create functions! All fault trees can be represented as a function - an equation to estimate the probability of the top event. Fortunately, fault trees are extremely flexible. You can apply all of our past simulation approaches, probability rules, lifespan distribution concepts, and reliability analysis tools to them. 9.1.1 Example: Superman Fault Tree A favorite simple fault tree example of mine is the Superman Turns Evil fault tree. It consists of a few events: T: Superman turns evil M: Superman movies do poorly at the box office C: Boring Childhood in Kansas D: Lois Lane Dumps Superman K: Steps on Kryptonite Lego in the middle of the night In this fault tree, it looks like Superman could turn evil if (A) Superman Movies do poorly at the box office OR (B) if all 3 other conditions happen (boring childhood, dumped by Lois Lane, steps on Kryptonite Lego). 9.1.2 Using Raw Probabilities We could represent that using boolean logic. So if we know the probability of events m, c, d, and k, we can calculate the probability of the top event top. f1 = function(m, c, d, k){ top = m + c * d * k return(top) } # For example... given these probabilities, the chance Superman turns evil is highly contigent on event m - the success of superman movies at the box office. probs1 = f1(m = 0.50, c = 0.99, d = 0.25, k = 0.01) # view it! probs1 ## [1] 0.502475 Learning Check 1 Question You are tasked with assessing the risk of a widespread outbreak of a contagious disease in a population. The top event is defined as the widespread outbreak of a contagious disease in a population. This top event T can occur (G1) if any of the 3 following crises occur (G2, G3, or G4). G2: Insufficient Vaccination Coverage: All of the following must occur for this crisis to happen: Causes and Probabilities: - A. Lack of public awareness: P(Lack of public awareness) = 0.2 (20%) - B. Limited access to vaccines: P(Limited access) = 0.15 (15%) - C. Vaccine hesitancy due to misinformation: P(Vaccine hesitancy) = 0.3 (30%) G3: Ineffective Quarantine Measures: Any of the following can lead to this crisis: Causes and Probabilities: - D. Inadequate quarantine protocols: P(Inadequate protocols) = 0.1 (10%) - E. Non-compliance with quarantine rules: P(Non-compliance) = 0.15 (15%) - F. Inefficient monitoring of quarantined individuals: P(Inefficient monitoring) = 0.2 (20%) G4: Mutation of the Pathogen: Both events must occur for this crisis to occur. Causes and Probabilities: - G. High mutation rate of the pathogen: P(High mutation rate) = 0.05 (5%) - H. Inadequate monitoring of the pathogen mutations: P(Inadequate monitoring) = 0.1 (10%) [View Answer!] Use these events probabilities to construct a boolean equation and its function in R! f = function(a = 0.2, b = 0.15, c = 0.3, d = 0.1, e = 0.15, f = 0.2, g = 0.05, h = 0.1){ # G2. Probability of Insufficient Vaccination Coverage: # All of the sub-events must occur for this intermediate event to happen: p_insufficient_vaccine_coverage = a * b * c # Probabilities for AND logic # G3. Probability of Ineffective Quarantine Measures: # Any of the sub-event event can lead to this intermediate event: p_quarantine_measures = d + e + f # Probabilities for OR logic # G4. Probability of Mutation of the Pathogen: # Both conditions need to be present in order for pathogen to mutate: p_pathogen_mutation &lt;- g * h # Probabilities for AND logic # If EITHER insufficient vaccine coverage # OR no quarantine measures # OR pathogen mutation occur, # the outbreak occurs. # T = OR Gate G1 = G2 + G3 + G4 p_top = p_insufficient_vaccine_coverage + p_quarantine_measures + p_pathogen_mutation return(p_top) } Lets see what the probability of widespread outbreak, in this hypothetical scenario, turns out to be: f(a = 0.2, b = 0.15, c = 0.3, d = 0.1, e = 0.15, f = 0.2, g = 0.05, h = 0.1) ## [1] 0.464 9.1.3 Using Failure Rates at Time t But if we knew the failure rate of each event, we could calculate the probability of the top event at any time t! f2 = function(t, lambda_m, lambda_c, lambda_d, lambda_k){ # Get probability at time t... prob_m = pexp(t, rate = lambda_m) prob_c = pexp(t, rate = lambda_c) prob_d = pexp(t, rate = lambda_d) prob_k = pexp(t, rate = lambda_k) # Use boolean equation to calculate top event prob_top = prob_m + (prob_c * prob_d * prob_k) return(prob_top) } # Then we could simulate the probability of the top event over time! probs2 = tibble(t = 1:100) %&gt;% mutate(prob = f2(t = t, lambda_m = 0.01, lambda_c = 0.001, lambda_d = 0.025, lambda_k = 0.00005)) # Check out the first few rows! probs2 %&gt;% head(3) ## # A tibble: 3 × 2 ## t prob ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.00995 ## 2 2 0.0198 ## 3 3 0.0296 # Let&#39;s visualize it! ggplot() + geom_area(data = probs2, mapping = aes(x = t, y = prob)) Learning Check 2 Question Suppose we have a small fault tree, where the top event T depends on either [A and B] OR [B and C] occuring. What is the probability of the top event after 1 hour vs. after 10 hours, given the following parameters? lambda_a = 0.05 lambda_b = 0.03 lambda_c = 0.02 [View Answer!] # Let&#39;s create a function calculate probability of the top event using failure rates and time period f = function( t = 1, # Time period we want to analyze the events for lambda_a = 0.05, # Failure rate for event A lambda_b = 0.03, # Failure rate for event B lambda_c = 0.02 # Failure rate for event C ){ # As these events follow exponential distribution, we need to calculate intermediate probabilities accoridingly prob_a = pexp(t, rate = lambda_a) prob_b = pexp(t, rate = lambda_b) prob_c = pexp(t, rate = lambda_c) # Get probability of top event prob_top = (prob_a * prob_b) + (prob_b * prob_c) return(prob_top) } # Probability of failure after 1 hour f(t = 1, lambda_a = 0.05, lambda_b = 0.03, lambda_c = 0.02) ## [1] 0.002026606 # Probability of failure after 10 hours f(t = 10, lambda_a = 0.05, lambda_b = 0.03, lambda_c = 0.02) ## [1] 0.1489618 9.1.4 Simulating Uncertainty in Probabilities We can use simulation to answer several important questions about variation. For example, we can simulate how the probability of the top event would change if the probability of each event varies ever so slightly across 1000 simulations. When simulating variation in probabilities, you can use random draws from the binomial distribution, randomly sampling 1s or 0s at a given probability prob a total of n times. probs3 = tibble( n = 1000, prob_m = rbinom(n = n, size = 1, prob = 0.50), prob_c = rbinom(n = n, size = 1, prob = 0.99), prob_d = rbinom(n = n, size = 1, prob = 0.25), prob_k = rbinom(n = n, size = 1, prob = 0.01), # Calculate probability of top event for each simulation prob_top = f1(m = prob_m, c = prob_c, d = prob_d, k = prob_k) ) # Let&#39;s get some descriptive statistics - the average will be particularly informative probs3 %&gt;% summarize( mu_top = mean(prob_top), sigma_top = sd(prob_top)) ## # A tibble: 1 × 2 ## mu_top sigma_top ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.49 0.504 9.1.5 Simulating Uncertainty in Failure Rates Usually, we go a few steps further, saying, suppose the probability of each failure \\(/lambda\\) might vary slightly, according to a normal distribution. What would be the average probability of failure for the top event? How much should we expect that average to vary, on average? Suppose each failure rate has a specific standard error: for m, 0.0001; for c, 0.00001, for d and k, 0.000002. f4 = function(t, lambda_m, lambda_c, lambda_d, lambda_k){ sim_lambda_m = rnorm(n = 1, mean = lambda_m, sd = 0.0001) sim_lambda_c = rnorm(n = 1, mean = lambda_c, sd = 0.00001) sim_lambda_d = rnorm(n = 1, mean = lambda_d, sd = 0.000002) sim_lambda_k = rnorm(n = 1, mean = lambda_k, sd = 0.000002) # Get probability at time t... sim_prob_m = pexp(t, rate = sim_lambda_m) sim_prob_c = pexp(t, rate = sim_lambda_c) sim_prob_d = pexp(t, rate = sim_lambda_d) sim_prob_k = pexp(t, rate = sim_lambda_k) # Use boolean equation to calculate top event prob_top = sim_prob_m + (sim_prob_c * sim_prob_d * sim_prob_k) return(prob_top) } # Then we could simulate the probability of the top event over time, probs4 = tibble(t = 1:100) %&gt;% # This would give us 1 random simulation per time period mutate(prob = f4(t = 1:100, lambda_m = 0.01, lambda_c = 0.001, lambda_d = 0.025, lambda_k = 0.00005)) # But we really probably want MANY random simulations per time period. probs5 = tibble(reps = 1:1000) %&gt;% group_by(reps) %&gt;% # We can use `reframe()`, a version of summarize() # used when you want to return MANY rows per group reframe( t = 1:100, prob = f4(t = t, lambda_m = 0.01, lambda_c = 0.001, lambda_d = 0.025, lambda_k = 0.00005)) probs5 %&gt;% head(3) ## # A tibble: 3 × 3 ## reps t prob ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0.0101 ## 2 1 2 0.0201 ## 3 1 3 0.0300 # And then we could get quantities of interest for each time period! probs6 = probs5 %&gt;% group_by(t) %&gt;% summarize( mu = mean(prob), sigma = sd(prob), # Exact lower and upper 95% simulated confidence intervals lower = quantile(prob, probs = 0.025), upper = quantile(prob, probs = 0.975), # Approximated lower and upper 95% confidence intervals lower_approx = mu - qnorm(0.025) * sigma, upper_approx = mu + qnorm(0.975) * sigma) probs6 %&gt;% head(3) ## # A tibble: 3 × 7 ## t mu sigma lower upper lower_approx upper_approx ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.00995 0.0000985 0.00975 0.0101 0.0101 0.0101 ## 2 2 0.0198 0.000195 0.0194 0.0202 0.0202 0.0202 ## 3 3 0.0296 0.000290 0.0290 0.0301 0.0301 0.0301 Lets visualize that confidence interval over time! ggplot() + geom_ribbon(data = probs6, mapping = aes(x = t, ymin = lower, ymax = upper)) The sky is the limit! Happy fault tree simulating! Learning Check 3 Suppose we have some fault tree involving 3 key events, A, B, and C. Suppose these events have the following failure rates: lambda_a = 0.05, lambda_b = 0.03, and lambda_c = 0.02. Simulate the 95% confidence interval for the probability of each event, for each hour from 0 to 100 hours, assuming a standard error for each of 0.001. Then visualize these confidence intervals as a ribbon plot. [View Answer!] # Firstly, let&#39;s load necessary libraries for plotting library(ggplot2) library(dplyr) stat = tibble( reps = 1:1000, # Simulate 1000 lambdas! lambda_a = rnorm(n = 1000, mean = 0.05, sd = 0.001), lambda_b = rnorm(n = 1000, mean = 0.03, sd = 0.001), lambda_c = rnorm(n = 1000, mean = 0.02, sd = 0.001) ) %&gt;% # For each simulation... group_by(reps) %&gt;% reframe( # Get a time frame from 1 to 100 hours t = 1:100, # Get the probabilities for that time frame given each simulated lambda prob_a = pexp(t, rate = lambda_a), prob_b = pexp(t, rate = lambda_b), prob_c = pexp(t, rate = lambda_c) ) %&gt;% # For each time period, get the lower and upper confidence interval group_by(t) %&gt;% summarize( # 95% CI for prob a lower_a = quantile(prob_a, probs = 0.025), upper_a = quantile(prob_a, probs = 0.975), # 95% CI for prob b lower_b = quantile(prob_b, probs = 0.025), upper_b = quantile(prob_b, probs = 0.975), # 95% CI for prob c lower_c = quantile(prob_c, probs = 0.025), upper_c = quantile(prob_c, probs = 0.975)) # Let&#39;s view it stat %&gt;% head(3) ## # A tibble: 3 × 7 ## t lower_a upper_a lower_b upper_b lower_c upper_c ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.0466 0.0506 0.0277 0.0314 0.0180 0.0217 ## 2 2 0.0911 0.0987 0.0546 0.0618 0.0357 0.0429 ## 3 3 0.133 0.144 0.0808 0.0913 0.0530 0.0637 # Let&#39;s make the plot! ggplot() + # Plot bands for event a geom_ribbon( data = stat, mapping = aes(x = t, ymin = lower_a, ymax = upper_a, fill = &quot;A&quot;), alpha = 0.25) + # Plot bands for event b geom_ribbon( data = stat, mapping = aes(x = t, ymin = lower_b, ymax = upper_b, fill = &quot;B&quot;), alpha = 0.25) + # Plot bands for event c geom_ribbon( data = stat, mapping = aes(x = t, ymin = lower_c, ymax = upper_c, fill = &quot;C&quot;), alpha = 0.25) + theme_classic() + labs(title = &quot;Probability of Events over 100 hours&quot;, x = &quot;Hours&quot;, y = &quot;Probability (95% CI)&quot;, fill = &quot;Event Type&quot;) 9.2 Visualizing Fault Trees in R We can draw fault trees easily enough by hand, but how do we record their data in a machine readable format? We can use conventions from network science to record these data in two lists: (1) an edgelist of edges connecting nodes, and (2) a nodelist of nodes, connected by edges. 9.2.1 Making a Nodelist We can use the tribble() function available when loading the tidyverse package to write out small data.frames row by row. In nodes, our nodelist: every node must have a unique ID, called name. some events reappear multiple times in the same tree; we must give these nodes different name but the same event name. each node should be classified by type as (1) an and gate, (2) an or gate, or (3) not a gate. We can use factor() to tell R to always remember \"and\", \"or\", and \"not\" in that order (useful for ggplot legends). classic fault trees sometimes write events and the gates that condition them as separate nodes, but as far as the graph is concerned, they are really the same node. So, remember to combine them for analysis. # Let&#39;s make a data.frame of nodes! tribble( # Make the headers, using a tilde ~id, ~event, ~type, # Add the value entries &quot;T&quot;, &quot;T&quot;, &quot;top&quot;, &quot;G1&quot;, &quot;G1&quot;, &quot;and&quot;, # Notice how event &#39;B&#39; appears twice, # so it has a logical but differentiated `name` &quot;B1&quot; and &quot;B2&quot;? &quot;B1&quot;, &quot;B&quot;, &quot;not&quot;, &quot;B2&quot;, &quot;B&quot;, &quot;not&quot;, &quot;C1&quot;, &quot;C&quot;, &quot;not&quot;) %&gt;% # Classify &#39;type&#39; as a factor, with specific levels mutate(type = factor(type, levels = c(&quot;top&quot;, &quot;and&quot;, &quot;or&quot;, &quot;not&quot;))) ## # A tibble: 5 × 3 ## id event type ## &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; ## 1 T T top ## 2 G1 G1 and ## 3 B1 B not ## 4 B2 B not ## 5 C1 C not 9.2.2 Making an Edgelist Next, we can use tribble() to make an edgelist of our fault tree (or use read_csv() to read in an edgelist). Edgelists should follow these conventions: each row shows a unique edge between nodes, recording their node ids from the top-down in the from and to columns. # For example.... tribble( ~from, ~to, &quot;T&quot;, &quot;G1&quot;, &quot;G1&quot;, &quot;G2&quot;, &quot;G2&quot;, &quot;B1&quot;, &quot;G2&quot;, &quot;G5&quot;) ## # A tibble: 4 × 2 ## from to ## &lt;chr&gt; &lt;chr&gt; ## 1 T G1 ## 2 G1 G2 ## 3 G2 B1 ## 4 G2 G5 9.2.3 Our Data In this example, were going to use the default data in tidyfault, called fakenodes and fakeedges. You can download it like this: data(&quot;fakenodes&quot;) data(&quot;fakeedges&quot;) fakenodes ## # A tibble: 12 × 3 ## id event type ## &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; ## 1 1 T top ## 2 2 G1 and ## 3 3 G2 and ## 4 4 G3 or ## 5 5 G4 and ## 6 6 G5 or ## 7 7 A not ## 8 8 B not ## 9 9 B not ## 10 10 C not ## 11 11 C not ## 12 12 D not fakeedges ## # A tibble: 11 × 2 ## from to ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 ## 2 2 3 ## 3 3 8 ## 4 3 6 ## 5 6 10 ## 6 6 12 ## 7 2 4 ## 8 4 7 ## 9 4 5 ## 10 5 9 ## 11 5 11 9.2.4 Getting a Fault Tree Layout Using tidyfaults illustrate() function, we can quickly assign x and y coordinates for a tree layout to to our fakenodes and fakeedges. We say type = \"both\" because we want to receive coordinates for both the nodes and edges. gg = illustrate(nodes = fakenodes, edges = fakeedges, type = &quot;both&quot;, node_key = &quot;id&quot;) This returns a list() object. list() objects are collections of data.frames - sort of like a vector whose values are entire data.frames. We can query the values inside lists by using the $, just like a data.frame. # see the nodes gg$nodes %&gt;% head(3) ## x y id event type ## 1 0 4 1 T top ## 2 0 3 2 G1 and ## 3 -1 2 3 G2 and # see the edges gg$edges %&gt;% head(3) ## # A tibble: 3 × 5 ## # Groups: edge_id [2] ## edge_id direction id x y ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 from 1 0 4 ## 2 1 to 2 0 3 ## 3 2 from 2 0 3 9.2.5 Visualize a Fault Tree ggplot() + # Plot each line corresponding to the unique edge id, used to group them geom_line(data = gg$edges, mapping = aes(x = x, y = y, group = edge_id)) + # Plot each point geom_point(data = gg$nodes, mapping = aes(x = x, y = y, shape = type, fill = type), size = 10) + # Plot labels! geom_text(data = gg$nodes, mapping = aes(x = x, y = y, label = event)) + # We can also assign shapes! # (shapes 21, 22, 23, 24, and 25 have fill and color) scale_shape_manual(values = c(22, 24, 25, 21)) + # And assign some poppy colors here scale_fill_manual(values = c(&quot;darkgrey&quot;, &quot;cyan&quot;, &quot;coral&quot;, &quot;lightgrey&quot;)) + # Finally, we can make a clean void theme, with the legend below. theme_void(base_size = 14) + theme(legend.position = &quot;bottom&quot;) 9.3 Cutsets Fault Trees are big - and some events are more critical than others. We want to identify two kinds of information. cutsets: all the possible sets of events which together trigger failure, called cutsets. minimal cutsets: the most reduced set of events which trigger failure. To do this, were going to use an algorithm, called the MOCUS top-down algorithm. I wrote it in R for you! Yay! 9.3.1 Gates # To get a data.frame of gates, use curate()! g = curate(fakenodes, fakeedges) # Check it g ## # A tibble: 6 × 6 ## gate type class n set items ## &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;list&gt; ## 1 T top top 1 &quot; (G1) &quot; &lt;chr [1]&gt; ## 2 G1 and gate 2 &quot; (G2 * G3) &quot; &lt;chr [2]&gt; ## 3 G2 and gate 2 &quot; (B * G5) &quot; &lt;chr [2]&gt; ## 4 G3 or gate 2 &quot; (A + G4) &quot; &lt;chr [2]&gt; ## 5 G4 and gate 2 &quot; (B * C) &quot; &lt;chr [2]&gt; ## 6 G5 or gate 2 &quot; (C + D) &quot; &lt;chr [2]&gt; 9.3.2 Equations To get the full boolean equation of the fault tree, use equate()! g %&gt;% equate() ## [1] &quot; ( ( (B * (C + D) ) * (A + (B * C) ) ) ) &quot; We can format it as a function too! f = g %&gt;% equate() %&gt;% formulate() # Suppose there is each of these probabilities that A, B, C, and D occur # what&#39;s the probability of the top event? f(A = 0.3, B = 0.5, C = 0.1, D = 0.5) ## [1] 0.105 9.3.3 Minimal Cutsets To simplify down to the minimal cutsets, use concentrate()! Its pretty quick for small graphs, but gets exponentially longer the more nodes you add. (Especially with OR statements.) Thats MOCUSs value as well as its tradeoff. m = g %&gt;% concentrate(method = &quot;mocus&quot;) # m ## [1] &quot;B*C&quot; &quot;A*B*D&quot; 9.3.4 Coverage We might want to know, how important are each of my minimal cutsets? We can compute the total number of cutsets that include the minimal cutset, the total cutsets leading to failure in general, and the percentage (coverage) of cutsets containing your minimal cutset out of all total cutsets leading to failure. Its basically a measure of the explanatory power of your cutsets! m %&gt;% tabulate(formula = f) ## # A tibble: 2 × 5 ## mincut query cutsets failures coverage ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 A*B*D filter(A == 1, B == 1, D == 1, outcome == 1) 2 5 0.4 ## 2 B*C filter(B == 1, C == 1, outcome == 1) 4 5 0.8 Happy Fault Tree analysis! "],["workshop-physical-acceleration-models.html", "10 Workshop: Physical Acceleration Models Getting Started 10.1 Acceleration Factor Learning Check 1 Learning Check 2 10.2 Modeling Normal Use Lifespan 10.3 Eyring Model (Multiple Stressors) 10.4 Degradation Model (Time Trends) 10.5 Burn-in Periods 10.6 Maximum Likelihood Estimation (MLE) for Physical Acceleration Models 10.7 Conclusion", " 10 Workshop: Physical Acceleration Models In this workshop, well learn how to use physical acceleration models to convert results from reliability tests done under laboratory conditions (which may be slightly unrealistic) into estimates that match real outcomes in the field! Figure 10.1: Crash Testing with LEGOs: a very safe prototype Getting Started Load Packages Lets start by loading the tidyverse and broom packages! Also, sometimes select gets overruled by other packages, so it can help to load it directly. # Load packages library(tidyverse) library(broom) # Load specific function to environment select = dplyr::select Helpful Functions Well be using the tibble() function; it works identically to the data.frame() function, but allows you to reference any vector that came before. For example: # This doesn&#39;t work... data.frame(x = c(1,2), y = x + 2) # But this does. tibble(x = c(1,2), y = x + 2) ## # A tibble: 2 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 3 ## 2 2 4 10.1 Acceleration Factor 10.1.1 Stress vs. Usage Conditions Imagine: You work in Dearborn, Michigan, testing airbag failure rates with car crash dummies every day. Your work helps people stay safe on the road! Unfortunately, lab conditions tend to be a little more extreme than the real world. Moisture levels, temperature, speed, friction, etc. are very hard (nay impossible) to perfectly match to real driving conditions. A product life distribution measured under lab stress conditions will always be slightly off from the real, use-case life distribution by a specific factor. What if we could measure and approximate that factor? Suppose researchers are stress testing airbags in the lab! Their empirical data reveals the airbag lifespans are Weibull distributed, with a characteristic life of c = 4100 hours and a shape parameter of m = 1.25. They write the Weibull density function (PDF) d() below, and use it to calculate and visualize the probability the airbags fail every hour from 1 to 8000 hours. They can visualize the blue density curve of the stress-tested airbags shown below (\\(s\\)), called \\(f_s(t)\\). But what they really want to know is the red dashed curve of the airbags under normal use conditions (\\(u\\)), called \\(f_u(t)\\)! But they dont know it! Fortunately, they know that in past on-road and off-road studies, the median airbags time to failure was 250% times greater under normal usage as when under stress condition. We write 250% here the Acceleration Factor \\(AF\\) (or \\(A\\)), because it describes how stress-testing accelerates the failure by a factor of 2.5. We can write it like: \\[f_u(t) = f_s(t) \\times AF = f_s(t) \\times 2.5 \\] # Let&#39;s write ourselves a speedy weibull density function &#39;d()&#39; d = function(t, m, c){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) # or dweibull(t, scale = c, shape = m) } airbags &lt;- data.frame(t = seq(1, 12000, by = 20)) %&gt;% mutate(d_stress = d(t, c = 4100, m = 1.25)) %&gt;% # Suppose the lifespans under normal usage are just off by a factor of ~2.5 # then we could project the PDF under normal usage like: mutate(d_usage = d(t = t / 2.5, c = 4100, m = 1.25) / 2.5) ggplot() + # Plot the PDF under stress conditions geom_line(data = airbags, mapping = aes(x = t, y = d_stress, color = &quot;Stress&quot;), size = 1.5) + # Plot the PDF under normal usage conditions geom_line(data = airbags, mapping = aes(x = t, y = d_usage, color = &quot;Normal Usage&quot;), size = 1, linetype = &quot;dashed&quot;) + # Add a nice theme and clear labels theme_classic(base_size = 14) + labs(x = &quot;Airbag Lifespan in hours (t)&quot;, y = &quot;Probability Density d(t)&quot;, color = &quot;Conditions&quot;, subtitle = &quot;We want Lifespan under Normal Usage,\\nbut can only get Lifespan under Stress&quot;) 10.1.2 Acceleration as a Function \\(\\frac{f_{s}(t)}{f_{u}(t)}\\) In reality, \\(f_{u}(t)\\) probably doesnt have exactly a constant relationship with \\(f_{s}(t)\\). In a perfect world, we could collect raw data for both lifetimes under normal conditions \\(t_u\\) and under stress-testing \\(t_s\\), and then estimate their density functions \\(f_u(t)\\) and \\(f_s(t)\\). We could then calculate \\(AF\\) exactly as a function af() that relates them. For example: # Let&#39;s write an acceleration factor function! af = function(t){ # Find density under stress ds &lt;- d(t, m = 1.25, c = 4100) # Find density function under normal conditions du &lt;- d(t, m = 1.5, c = 4500) # Since f_u(t) = f_s(t) x AF, # then AF = f_s(t) / f_u(t) = ds / du ds / du } Were we to plot it, we can see below AF is not constant here, but varies over time, because \\(f_u(t)\\) and \\(f_s(t)\\) vary over time. So, when we pick an AF, were usually picking the AF corresponding to a specific parameter, like the characteristic life or median of a distribution. data.frame( time = 1:1000, af = af(1:1000) ) %&gt;% # Visualize it! ggplot(mapping = aes(x = time, y = af)) + geom_line() + geom_point() + labs(subtitle = &quot;Acceleration Factor for Airbag Lifespan&quot;, y = &quot;Acceleration Factor function af()&quot;, x = &quot;Time in hours (t)&quot;) Supposing that \\(t_{u} = t_{s} \\times AF\\), we can say several more things: \\[ Failure \\ Function = F_{u}(t) = F_{s}(t/AF) \\] \\[ Density \\ Function = f_{u}(t) = \\frac{f_{s}(t/AF)}{AF} \\] \\[ Failure \\ Rate = z_{u}(t) = \\frac{z_{s}(t/AF)}{AF} \\] 10.1.3 Linear Acceleration However, its usually very difficult to obtain the density functions for both usage and stress conditions. Thats why we want acceleration factors (AF) - because theyll let use estimate \\(f_u(t)\\) when we only have \\(f_s(t)\\). So in practice, we usually assume \\(AF\\) shows a constant, linear relationship between \\(f_{u}(t)\\) and \\(f_{s}(t)\\), like when \\(AF = 2.5\\). This is called linear acceleration. Linear acceleration requires us to choose a constant value of \\(AF\\) from just 1 time-step from the plot above. How can we choose!? We should probably choose a fairly representative lifespan, based off a parameter like the median time to fail \\(T_{50}\\) (or the mean time to fail \\(m\\), characteristic life \\(c\\), etc.). Even if we dont have access to all the raw data, if we know the median lifespan under stress \\(T_{50_{s}}\\) and under normal conditions \\(T_{50_{u}}\\), we can estimate \\(AF\\) by taking \\(AF = \\frac{T_{50_{u}}}{T_{50_{s}}}\\). For example: # Let&#39;s write a weibull quantile function q = function(p, c, m){ qweibull(p, scale = c, shape = m) } # Get median under stress median_s &lt;- q(0.5, c = 4100, m = 1.25) # Get median under normal conditions median_u &lt;- q(0.5, c = 4500, m = 1.5) # Calculate it! af = median_u / median_s # Check the Acceleration Factor! af ## [1] 1.152529 Lets clear this data. remove(af, median_s, median_u, q, d) Learning Check 1 Question A kitchen mixer has an exponential distributed lifespan. Labs stress tested the mixer at 125 degrees Celsius, yielding a mean time to fail of 4500 hours. But, it is usually heats to 32 degrees Celsius, and has an acceleration factor of 35. What proportion of mixers do we expect will fail by 40,000 hours under normal use? [View Answer!] # Given an acceleration factor of 35 af &lt;- 35 # Write a failure function f = function(t, lambda){ 1 - exp(-t*lambda) } # Method 1: # Failure Function = fu(t) = fs(t/AF) f(t = 40000 / af, lambda = 1 / 4500) ## [1] 0.2242836 # Method 2: # We know that lambda = zu(t) = zs(t/AF) / AF # so... plus that in for lambda f(t = 40000, lambda = 1 / (4500 * af) ) ## [1] 0.2242836 We expect ~22% of mixers will fail after 40,000 hours when running at 32 degrees Celsius. remove(af, f) Learning Check 2 Question Suppose we want no more than 10% of components to fail after 40,000 hours at normal usage conditions. So, we redesign the mixer to operate at a lower temperature. What (linear) acceleration factor is needed to project from our 125 degree conditions in lab to the normal use temperature of 32 degrees? [View Answer!] # Write failure function f = function(t, lambda){ 1 - exp(-t*lambda) } # Failure Function = fu(t) = fs(t/AF) # 0.10 = fu(t = 40000) = 1 - exp(-40000*lambda) # log(1 - 0.10) = -40000*lambda lambda_u = -log(0.90) / 40000 lambda_s = 1/4500 # Get ratio between these parameters # lambda_u = 1 / af * lambda_s # so... af = lambda_s / lambda_u # Check acceleration factor! af ## [1] 84.36641 We would need an acceleration factor af of ~84.4. remove(lambda_u, lambda_s, af, f) 10.2 Modeling Normal Use Lifespan 10.2.1 Models and Prediction A model is an equation approximating the relationship between two or more vectors of data. Its not the real thing - its an approximation! Why use models? Usually we know quite a lot about the conditions under which a product was tested in lab, but we cant actually observe the characteristic lifespan of that product under normal use conditions. But if we can make a really good model of the relationship between the outcome and those conditions, then we can predict for any set of conditions what the outcome - the characteristic lifespan of a product - would be under those conditions. Fortunately, R was built to make statistical models! To make a model of lifespans, we need 4 ingredients. an outcome vector, saved in a data.frame. 1 (or more) vectors of condition/predictors, saved in the same data.frame. the lm() function in R, which makes a linear model drawing the line of best fit between each value of the predictor(s) and outcome. The model is the equation of that line! the outcome vector needs to be log-transformed, because (a) most useful life distributions involve exponential processes and (b) lifespans are by nature non-negative, right-skewed variables. Taking log(outcome) adjusts for that and lets us plot a straight line of best fit. Note: Well learn more in later weeks about how* lm() makes models, but for now, well assume its magic!* For example, lets think about a car alternator, the device that converts mechanical energy into electrical energy in a car. Well make a few tiny example models! Suppose we tested 5 samples of 100 alternators each, and recorded the results in our alt data.frame below. We calculated the characteristic life in each of our 5 samples in the c, but our samples were all subjected to different stress conditions: different temperatures in Celsius (temp), which were converted into units of temperature factor scores (tf); different voltage levels in volts; at different points in the devices lifespan (time, in hours); and with different average performance ratings (rating) in amps. alt &lt;- tibble( # Characteristic life in hours c = c(1400, 1450, 1500, 1550, 1650), # Temperature in Celsius temp = c(160, 155, 152, 147, 144), # Temperature Factor (a standardized unit) tf = 1 / (1 / 11605 * (temp + 273.15)), # Voltage, in volts volts = c(17, 16.5, 14.5, 14, 13), # Hours of life spent by time of test time = c(1200, 1000, 950, 600, 500), # Performance Rating, in Amps rating = c(60, 70, 80, 90, 100)) Lets use this data to explore some of the different classic physical acceleration models, including the Arrhenius Model, Eyring Model, and Degradation Model. 10.2.2 Arrhenius Model (Temperature) The Arrhenius Model is a simple equation that models the impact of temperature (tf) on the log() of lifespan (c). Lets explore it visually using ggplot(), and then estimate it using lm(). 10.2.3 Visualizing the Arrhenius Model We could visualize the relationship between each of these conditions and the log() of the characteristic lifespan c using geom_point() which makes a scatterplot between x and y coordinates in the aes(). geom_smooth(method = \"lm\"), which finds the line of best fit. g &lt;- alt %&gt;% ggplot(mapping = aes(x = tf, y = log(c) )) + geom_point(size = 5) + # Add scatterplot points geom_smooth(method = &quot;lm&quot;, se = FALSE) # Make line of best fit, using lm() - a linear model # we can write &#39;se = FALSE&#39; (standard error = FALSE) to get rid of the confidence interval We can also add in the equation of this line of best fit (which well calculate below), plus other labels. g + # Add theme theme_classic(base_size = 14) + # Add labels labs(title = &quot;Arrhenius Model, Visualized&quot;, subtitle = &quot;Model Equation: log(c) = 3.1701 + 0.1518 TF \\nModel Fit: 96%&quot;, # We can add a line-break in the subtitle by writing \\n x = &quot;Temperature Factor (TF)&quot;, y = &quot;Characteristic Lifespan log(c)&quot;) 10.2.4 Estimating the Arrhenius Model So how did R calculate that line of best fit? It used the lm() function to make a linear model - an equation, which fits the data with a specific accuracy rate (eg. 96%). Lets make a model m1 to predict the log() of characteristic lifespan c, based on our temperature factor vector tf in alt! First, lets make the model with lm(), piping our vectors from alt. m1 &lt;- alt %&gt;% lm(formula = log(c) ~ tf) Second, lets inspect the models fit with glance() from the broom package, and select() the r.squared statistic. 0% means terrible model fit. 100% means the model equation perfectly predicts every value of log(c) in our alt data.frame. We aim for excellent predictive power where possible. 96% is excellent! m1 %&gt;% glance() %&gt;% select(r.squared) ## # A tibble: 1 × 1 ## r.squared ## &lt;dbl&gt; ## 1 0.962 Third, we can now read the model equation for our line of best fit. m1 ## ## Call: ## lm(formula = log(c) ~ tf, data = .) ## ## Coefficients: ## (Intercept) tf ## 3.1701 0.1518 The lm() function estimated our model equation m1, including 2 constant coefficients named (Intercept) and tf. These coefficients show the y-intercept ((Intercept)), called \\(\\alpha\\) and the slope/effect/rate of change called \\(\\beta\\) for every 1 unit increase in the temperature factor tf. We can write this model equation formally as: \\[ \\begin{align*} log(c) =&amp; \\ Intercept + Slope \\ \\times TF \\\\ or:&amp; \\\\ log(c) =&amp; \\ \\alpha + \\beta \\times TF, \\\\ &amp; where \\ \\alpha = 3.1701 \\ and \\ \\beta = 0.1518, \\\\ so:&amp; \\\\ log(c) =&amp; \\ 3.1701 + 0.1518 \\ TF \\\\ so:&amp; \\\\ c =&amp; e^{\\alpha + \\beta \\times TF} \\\\ or:&amp; \\\\ c =&amp; \\ e^\\alpha \\times e^{\\beta \\times TF} \\\\ or:&amp; \\\\ c =&amp; e^{3.1701} + e^{0.1518 \\ TF} \\\\ or:&amp;\\\\ Arrhenius \\ Model:&amp; \\\\ c =&amp; A + e^{\\Delta H \\times TF} \\ \\\\ &amp;\\ where \\ A = e^{Intercept} = e^{\\beta} \\ \\ and \\ \\Delta H = Slope = \\beta \\\\ also:&amp; \\\\c =&amp; A + e^{\\Delta H \\times (1 / (k T))} \\\\ &amp;where \\ TF = 1 / (k \\times T_{Kelvin}) \\ and \\ k = 1 / 11605 \\end{align*} \\] As you can see above, there are many ways to write the Arrhenius model, but it boils down to this: any linear model of the log-characteristic life will involve: a y-intercept constant called \\(\\alpha\\) (\\(log(A)\\) in the Arrhenius model). \\(\\alpha\\) describes how much log(c) we get independent of any other factors (ie: if tf = 0). Interpreting our model: If the temperature factor \\(TF = 0\\), our model predicts that \\(log(c) = 3.1701\\). a slope constant called \\(\\beta\\) describing the effect of your variable (\\(TF\\) or tf), for every 1 unit increase in your variable. In the Arrhenius model, \\(\\beta\\) is written as \\(\\Delta H\\), the activation energy rate at which a temperature factor increase of 1 unit affects log(c). Interpreting our model: If the temperature factor \\(TF\\) increases by 1, our model predicts that \\(log(c)\\) will change by \\(\\beta = \\Delta H = 0.1518\\). And thats how we read any statistical model with two variables! &lt;br 10.2.5 Prediction Now that we have our model equation (and, importantly, a good fitting one), we can feed it values of our predictor \\(TF\\) (generically called \\(X\\)) to calculate our predicted log of the characteristic life \\(log(\\hat{c})\\) (generically called \\(log(\\hat{Y})\\)). We can do this two ways: (1) by writing a function or (2) using the predict() function in R. Writing our own function works the same way as writing our d(), f(), or r() function. Well call it c_hat(). # For any value of tf, we can now calculate c_hat. # We write the model equation for log(c), then exponentiate it with exp()! c_hat = function(tf){ exp( 3.1701 + 0.1518*tf) } # Test it! c_hat(tf = 28) ## [1] 1669.868 It works! The characteristic life for tf = 28 is ~1670. # Or better yet, let&#39;s calculate temperature factor &#39;tf&#39; too, # so we only have to supply a temperature in Celsius tf = function(temp){ k = 1 / 11605 # Get Boltzmann&#39;s constant 1 / (k * (temp + 273.15)) # Get TF! } # Now predict c_hat for 30, 60, and 90 degrees celsius! c_hat = function(temp){ exp( 3.1701 + 0.1518*tf(temp)) } c(30, 60, 90) %&gt;% c_hat() ## [1] 7952.275 4712.271 3044.511 So cool! Wouldnt it be nice though, if we could simplify that process? The predict() function in R can help! The predict function will run your model equation on any newdata that you feed it, calculating the predicted outcomes. newdata must be formatted as a data.frame, containing vectors named to match each predictor from your original data (which we named alt). Lets make a data.frame of fakedata with tibble(), varying temperature from 0 to 200 degrees Celsius, and then transform that into a temperature factor tf with our tf() function. fakedata &lt;- tibble( temp = seq(0, 200, by = 10), tf = tf(temp)) # Check the first 3 rows! fakedata %&gt;% head(3) ## # A tibble: 3 × 2 ## temp tf ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 42.5 ## 2 10 41.0 ## 3 20 39.6 Then, well feet our 21 rows of fakedata and our model m1 to the predict() function, which will output 21 predictions for log(c). m1 %&gt;% predict(newdata = fakedata) ## 1 2 3 4 5 6 7 8 ## 9.619376 9.391606 9.179376 8.981148 8.795580 8.621497 8.457864 8.303769 ## 9 10 11 12 13 14 15 16 ## 8.158401 8.021038 7.891038 7.767824 7.650877 7.539733 7.433969 7.333203 ## 17 18 19 20 21 ## 7.237091 7.145316 7.057591 6.973655 6.893267 But we can exponentiate it with exp() to get c_hat! m1 %&gt;% predict(newdata = fakedata) %&gt;% exp() ## 1 2 3 4 5 6 7 ## 15053.6495 11987.3387 9695.1035 7951.7546 6604.9840 5549.6862 4711.9837 ## 8 9 10 11 12 13 14 ## 4039.0668 3492.5958 3044.3360 2673.2172 2363.3224 2102.4897 1881.3274 ## 15 16 17 18 19 20 21 ## 1692.5112 1530.2759 1390.0440 1268.1516 1161.6437 1068.1196 985.6159 We could even write the whole thing inside a tibble() function: fakedata &lt;- tibble( temp = seq(0, 200, by = 10), tf = tf(temp), # Predict c_hat c_hat = predict(m1, newdata = tibble(tf)) %&gt;% exp()) # View the first 3 rows! fakedata %&gt;% head(3) ## # A tibble: 3 × 3 ## temp tf c_hat ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 42.5 15054. ## 2 10 41.0 11987. ## 3 20 39.6 9695. And now, we can plot the line of best fit between temp and c_hat, which are the quantities we actually care about. g2 &lt;- fakedata %&gt;% ggplot(mapping = aes(x = temp, y = c_hat)) + geom_line() + geom_point() # add some labels! g2 + labs(x = &quot;Temperature in Celsius (T)&quot;, y = &quot;Predicted Characteristic Life (c-hat)&quot;, title = &quot;Arrhenius Model of Car Alternator Characteristic Life&quot;, # Here&#39;s a little trick for cleanly adding linebreaks in subtitles: # Just write a paste() function subtitle = paste( &quot;Equation: c = e^(3.1701 + 0.1518 TF) = e^3.1701 * e^(0.1518 * (1 / kT))&quot;, &quot;Model: log(c) = 3.1701 + 0.1518 TF&quot;, sep = &quot;\\n&quot;)) 10.3 Eyring Model (Multiple Stressors) But in our alt data, multiple conditions varied, including tf but also volts and time. These might each have independent effects on log(c)! So lets estimate their effect by adding some more coefficients to our model! The Eyring Model provides an equation derived from chemical reaction theory and quantum mechanics, which supposes that we can predict lifespan parameters pretty well if we know temperature and any other stresses, such as voltage. The general form of the Eyring Model can also be distilled into a multivariate regression equation. This means instead of drawing a line of best fit approximating the relationship between 2 vectors (log(c) and tf), we can approximate the relationship between 3 vectors (log(c), tf, and volts), making a plane. \\[ \\begin{align*} c =&amp; AT^{\\alpha} \\times e^{\\frac{\\Delta H}{kT}} \\times e^{(B + C/T) \\times S} \\\\ &amp;where: \\ T = Temperature \\ in \\ Kelvin, \\\\ &amp;so: \\ if \\ \\alpha = 0, \\ T^{\\alpha} = 1 \\\\ &amp;also:\\\\ &amp;where: \\ S = Stressor \\ S, \\\\ &amp;and: \\ B + C/T = \\ temperature \\ contigent \\ effect \\ of \\ S\\\\ \\\\ &amp;simplifies \\ to: \\\\ c=&amp;e^{intercept} \\times T^{\\alpha} + e^{\\Delta H \\times (1 / (kT)) \\ + \\ (B + C/T)^S } \\\\ \\\\ &amp; so \\ assuming \\ \\ T^{\\alpha} = 1: \\\\ log(c) =&amp; intercept \\ + \\ \\Delta H \\times (1 / (kT)) + (B + C/T)\\times S \\\\ &amp; can \\ be \\ rewritten \\ as: \\\\ log(c) =&amp; \\alpha \\ + \\ \\beta_1 X_1 + B_2 X_2 + ... \\\\ &amp; where \\ \\alpha = intercept, \\\\ &amp;X_1 = Temperature \\ Factor = TF = 1 / (kT), \\\\ &amp;\\beta_1 = effect \\ of X_1 \\ [Temperature \\ Factor], \\\\ &amp;X_2 = Stressor \\ S, \\\\ &amp;B_2 = net \\ effect \\ of \\ X_2 \\ [Stressor \\ S] \\\\ &amp;... = any \\ other \\ stressors \\end{align*}\\] This is a monster of an equation, but we can still model it relatively simply using a linear model lm() of the log(c). Often, we care about voltage, which can be written as: \\[ \\beta_{2}X_{2} = B \\times (-log(V)), \\ where \\ \\beta_2 = B = effect \\ of \\ -log(Voltage) \\] We can write model this as m2, like so: m2 &lt;- alt %&gt;% lm(formula = log(c) ~ tf + log(volts) ) # See our model equation! m2 ## ## Call: ## lm(formula = log(c) ~ tf + log(volts), data = .) ## ## Coefficients: ## (Intercept) tf log(volts) ## 5.0086 0.1027 -0.1837 Then, we can just supply predict() with any values of tf and volts to predict log(c_hat), and exponeniate the result. fakedata &lt;- tibble( # Hold temperature constant temp = 30, tf = tf(temp), # But vary volts volts = seq(from = 1, to = 30, by = 1), # Predict c_hat c_hat = predict(m2, newdata = tibble(tf, volts)) %&gt;% exp()) And we can visualize our fakedata to see the impact of changing volts on c_hat as temp and tf were held constant. fakedata %&gt;% ggplot(mapping = aes(x = volts, y = c_hat)) + geom_line() + geom_point() + theme_classic(base_size = 14) + labs(title = &quot;Eyring Model of Effect of Voltage on Lifespan (30 Deg. Celsius)&quot;, subtitle = &quot;Equation: c = e^(5.0086 + 0.1027 TF - 0.1837 * log(volts))&quot;, x = &quot;Voltage (volts)&quot;, y = &quot;Predicted Characteristic Life (c-hat)&quot;) 10.4 Degradation Model (Time Trends) One final common impact on lifespan is simple degradation in functioning over time. With each passing day, the product is exposed to a stressor an additional time, bringing it a little closer to whatever we define as failure, and in turn, impacting the estimation of performance across our samples. We can model this as \\[ \\begin{align*} Q_t =&amp; \\ Q_0 \\times e^{-R(S)t} \\\\ &amp;where: \\\\&amp; Q_t = performance \\ metric \\ at \\ time \\ t, \\\\&amp; Q_0 = starting \\ value \\ at \\ t = 0, \\ and \\\\&amp; R(S) = effect \\ of \\ stressor \\ S \\ \\\\ \\\\&amp; modeled \\ as: \\\\ log(Y_t) =&amp; \\ \\alpha + \\beta X \\times t \\\\ &amp;where: \\\\&amp;log(Y_t) = log(Q_t), \\\\ &amp; \\alpha = intercept = log(Q_0), \\\\&amp; \\beta = -R = degradation \\ effect \\ of \\ S,\\\\&amp; X = stressor \\ S \\ (sometimes \\ excluded), \\ and \\\\ &amp;t = time \\ t \\\\&amp;or: \\\\ log(Y_t) =&amp; \\ \\alpha + \\beta \\times t \\\\&amp; where: \\beta = overall \\ degradation \\ effect \\ of \\ time \\ t \\end{align*} \\] Lets try this out using our alt data! We might expect the power rating of an alternator might decline over time. We could model the overall degradation effect on the log(rating) by regressing a single vector time against our model. This produces a great fit of ~94%. alt %&gt;% lm(formula = log(rating) ~ time) %&gt;% glance() %&gt;% select(r.squared) ## # A tibble: 1 × 1 ## r.squared ## &lt;dbl&gt; ## 1 0.940 Alternatively, if the degradation effect depends on another condition, like how much voltage was run on it, we could write an interaction effect using I(volts * time). This forces our model to be written as \\(log(Y) = \\alpha + \\beta X \\times t\\) instead of just \\(log(Y) = \\alpha + \\beta \\times t\\). alt %&gt;% lm(formula = log(rating) ~ I(volts * time) ) ## ## Call: ## lm(formula = log(rating) ~ I(volts * time), data = .) ## ## Coefficients: ## (Intercept) I(volts * time) ## 4.825e+00 -3.497e-05 We could even apply these effects right into our estimation of characteristic life \\(\\hat{c}\\) for our alternators! Perhaps we think the characteristic life would tend to be lower if the sample were measured later in time, after being exposed to higher volts. Lets estimate this model as m3! m3 &lt;- alt %&gt;% lm(formula = log(c) ~ tf + log(volts) + I(volts * time)) # Really good fit! m3 %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.981 0.924 0.0174 17.3 0.174 3 17.2 -24.4 -26.3 ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; As you can see, we can build nearly as complex models as you can imagine! A good rule of thumb is to seek to build the most parsimonious (simplest) model that explains the most variation (maximizes the r.squared) Figure 10.2: Wasnt that fun? 10.5 Burn-in Periods Sometimes, manufacturers will deliberately test-run their products for several hundred hours before selling them, just to burn-in to their lifespan distributions. The idea is, if they can force the defective products to fail early on, then the products that remain are less likely to fail. Acceleration Factors can help us identify the ideal burn in period. We can say: \\(z(t)\\) = failure rate curve with no burn-in. \\(z_b(t)\\) = failure rate curve after a burn-in period of \\(t_b\\) hours. \\(a\\) = acceleration factor between normal use vs. stress (burn-in), so: \\(z_b(t = 0) = z(a \\times t_b)\\): failure rate after 0 hours of burn-in \\(z_b(t = 0)\\) should equal the normal failure rate at a time \\(a\\) times greater than the burn-in period of \\(t_b\\) hours. In other words, we can use the acceleration factor \\(a\\) to project the conditional probability of failure after surviving burn-in. We can say, hey, whats the probability of failure under normal use \\(t\\) hours after burn-in \\(F_b(t)\\), given that we know it survived up through the burn-in period \\(t_b\\) and its use-to-stress relationship is characterized by an acceleration factor of \\(a\\)? # Let&#39;s write the Weibull density and failure function, as always... d = function(t, c, m){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) } f = function(t, c, m){ 1 - exp(-1*((t/c)^m)) } Since we can calculate \\(F(t)\\) as f(t,c,m) and \\(f(t)\\) as d(t,c,m), we can write the conditional probability of failure post-burn-in as: \\[ F_b(t) = \\frac{ F(t + a t_b) - F(at_b)}{ 1 - F(at_b) } \\] And we can code it as: fb = function(t, tb, a, c, m){ # Change in probability of failure delta_failure &lt;- f(t = t + a*tb, c, m) - f(t = a*tb, c, m) # Reliability after burn-in period reliability &lt;- 1 - f(t = a*tb, c, m) # conditional probability of failure delta_failure / reliability } Lets try it! # 1000 hours after burn-in # with a burn-in period of 100 hours # an acceleration factor of 20 # characteristic life c = 2000 hours # and # shape parameter m = 1.5 fb(t = 1000, tb = 100, a = 20, c = 2000, m = 1.5) ## [1] 0.5670432 Likewise, the conditional density function \\(f_b(t)\\) can be written as: \\[ f_b(t) = \\frac{ f(t + at_b)}{ 1 - F(at_b)} \\] # And we&#39;ll write the condition db = function(t, tb, a, c, m){ density &lt;- d(t = t + a*tb, c, m) reliability &lt;- 1 - f(t = a*tb, c, m) # get conditional density density / reliability } Lets try it! Whats the conditional probability of having a lifespan of 1000 hours, given that you had a burn-in period of 100 hours? Lets assume an acceleration factor of 20, characteristic life of 2000 hours, and a shape parameter of 1.5, like before. db(t = 1000, tb = 100, a = 20, c = 2000, m = 1.5) ## [1] 0.0003976962 10.6 Maximum Likelihood Estimation (MLE) for Physical Acceleration Models But how in the world did we get all these estimates of c in the first place? Often, we end up calculating it via maximum likelihood estimation (MLE) from cross-tabulated readout data. Suppose we have several cross-tabulations of readout data available to us about the occurrence of failure for wheels by temperature. We know these are Weibull distributed, but we dont know the distributions parameters for each temperature level, let alone the \\(\\Delta H\\) or the Acceleration Factor \\(AF\\)! The examples below will focus on samples of car wheels, each with a Weibull distribution, but they are equally applicable to other distributions. # Let&#39;s write Weibull density, failure, and reliability functions d = function(t, c, m){ (m / t) * (t / c)^m * exp(-1*(t/c)^m) } f = function(t, c, m){ 1 - exp(-1*((t/c)^m)) } r = function(t, c, m){ 1 - f(t,c,m) } 10.6.1 MLE for an Example Arrhenius Model # Let&#39;s load in our crosstable wheels &lt;- tibble( label = c(&quot;[0,1000]&quot;, &quot;(1000,2000]&quot;, &quot;(2000,3000]&quot;, &quot;(3000,4000]&quot;, &quot;(4000,5000]&quot;), t = c(500, 1500, 2500, 3500, 4500), temp_100 = c(106, 66, 22, 4, 2), # out of 200 wheels temp_150 = c(125, 25, 25, 15, 10), # out of 175 wheels temp_200 = c(140, 30, 15, 10, 15)) # out of 300 wheels # Check it! wheels ## # A tibble: 5 × 5 ## label t temp_100 temp_150 temp_200 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 [0,1000] 500 106 125 140 ## 2 (1000,2000] 1500 66 25 30 ## 3 (2000,3000] 2500 22 25 15 ## 4 (3000,4000] 3500 4 15 10 ## 5 (4000,5000] 4500 2 10 15 We can write a maximum likelihood estimation function to find the most likely parameters for our products stress tested at 100 degrees Celsius, using optim() to perform MLE. # Let&#39;s write our crosstable&#39;s likelihood function ll = function(t, x, par){ r = sum(x) # Get total failures n = 200 # Record total sampel size tmax = max(t) # Record last time step # Get the product of the log-densities at each time step, for all failures then prob_d = ((d(t, c = par[1], m = par[2]) %&gt;% log()) * x) %&gt;% sum() # For the last time step, get the probability of each remaining unit surviving prob_r = r(t = tmax, c = par[1], m = par[2])^(n - r) %&gt;% log() # Get joint log-likelihood prob_d + prob_r } # And let&#39;s run MLE! mle100 &lt;- optim(par = c(1000, 1), t = wheels$t, x = wheels$temp_100, fn = ll, control = list(fnscale = -1)) # Our characteristic life and shape parameter m! mle100$par ## [1] 1287.316800 1.511327 But doesnt it seem like a waste that we have all this data at multiple temperature readouts, but were just relying on one temperature to estimate parameters? We can do better! Lets write a maximum likelihood estimator that maximizes more parameters! Well assume that the shape parameter \\(m\\) is the same for each distribution, but the characteristic life varies (a common assumption in physical acceleration models). # Let&#39;s write our crosstable&#39;s likelihood function ll = function(t, x1, x2, x3, par){ # Get total failures r1 = sum(x1) r2 = sum(x2) r3 = sum(x3) # Record total sample size in each n1 = 200 n2 = 175 n3 = 300 tmax = max(t) # Record last time step # Get the product of the log-densities at each time step, for all failures then prob_d1 = ((d(t, c = par[1], m = par[4]) %&gt;% log()) * x1) %&gt;% sum() prob_d2 = ((d(t, c = par[2], m = par[4]) %&gt;% log()) * x2) %&gt;% sum() prob_d3 = ((d(t, c = par[3], m = par[4]) %&gt;% log()) * x3) %&gt;% sum() # For the last time step, get the probability of each remaining unit surviving prob_r1 = r(t = tmax, c = par[1], m = par[4])^(n1 - r1) %&gt;% log() prob_r2 = r(t = tmax, c = par[2], m = par[4])^(n2 - r2) %&gt;% log() prob_r3 = r(t = tmax, c = par[3], m = par[4])^(n3 - r3) %&gt;% log() # Get joint log-likelihood, across ALL vectors prob_d1 + prob_r1 + prob_d2 + prob_r2 + prob_d3 + prob_r3 } # And let&#39;s run MLE! mle &lt;- optim(par = c(1000, 1000, 1000, 1), t = wheels$t, x1 = wheels$temp_100, x2 = wheels$temp_150, x3 = wheels$temp_200, fn = ll, control = list(fnscale = -1)) # Check out our 3 characteristic life parameters, # for temp_100, temp_150, and temp_200, and our shared shape parameter! mle$par ## [1] 794.06372 1491.73881 1747.74451 1.02723 We can apply MLE to estimate as many parameters as our computers and patience for coding functions will allow! 10.6.2 Estimating \\(\\Delta H\\) with MLE Next, lets use our MLE values to estimate \\(\\Delta H\\), the impact of temperature on lifespan parameters. # Remember our function to calculate temperature factors tf = function(temp){ 1 / ((1 / 11605) * (temp + 273.15)) } # Let&#39;s collect our parameter estimates param &lt;- tibble( # For each temperature temp = c(100, 150, 200), # report the MLE c estimates c = mle$par[1:3], # and the shared MLE m estimate m = mle$par[4], # and Calculate TF (for each temperature...) # This will be our independent variable tf = tf(temp)) # Check it! param ## # A tibble: 3 × 4 ## temp c m tf ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 100 794. 1.03 31.1 ## 2 150 1492. 1.03 27.4 ## 3 200 1748. 1.03 24.5 Now, weve got three different \\(c\\) estimates. Wed really like to project, for any temperature temp, what would the characteristic life c be? Fortunately, we know we can estimate that with a line of best fit. m4 &lt;- param %&gt;% lm(formula = log(c) ~ tf) # Pretty good fit (93%) m4 %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.932 0.863 0.154 13.6 0.168 1 3.00 0.00617 -2.70 ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; Now that weve built a fairly good model, we can use it to predict the characteristic life \\(c\\) for any temperature temp. And if we have c_hat(), then all of a sudden, we can calculate the probability of failure at any time t! Suppose our wheel was used at 30 degrees Celsius. Our model projects a probability of failure of 32% by the 100th hour! (Dont buy that wheel!) tibble( t = 100, temp = 10, tf = tf(temp), # Predict c-hat for our specified temperature c_hat = predict(m4, newdata = tibble(tf)) %&gt;% exp(), # Grab our MLE estimate of m m = mle$par[4], # And calculate the probability of failure, given a use temperature of 30 prob_f = f(t, c = c_hat, m = m) ) 10.6.3 Visualizing Probabilities We can even then make some rad graphs! For example, we can vary time but hold constant temperature to calculate the probability of failure over time at a specific temperature. tibble( t = seq(0, 2000, by = 10), tf = tf(temp = 30), # Get parameters c_hat = predict(m4, newdata = tibble(tf)) %&gt;% exp(), m = mle$par[4], # Calculate Probability prob_f = f(t, c = c_hat, m = m) ) %&gt;% ggplot(mapping = aes(x = t, y = prob_f)) + geom_area() + labs(x = &quot;Time&quot;, y = &quot;Failure Function F(t)&quot;, subtitle = &quot;Probability of Failure at 30 Degrees Celsius&quot;) Or, we can hold constant time but vary temperature, to show the changing probability of failure given different stress levels by temperature. tibble( t = 1000, temp = seq(0, 200, by = 10), tf = tf(temp), # Get parameters c_hat = predict(m4, newdata = tibble(tf)) %&gt;% exp(), m = mle$par[4], # Calculate Probability prob_f = f(t, c = c_hat, m = m) ) %&gt;% ggplot(mapping = aes(x = temp, y = prob_f)) + geom_area() + labs(x = &quot;Temperature (Celsius)&quot;, y = &quot;Failure Function F(t)&quot;, subtitle = &quot;Probability of Failure after 1000 hours&quot;) 10.7 Conclusion All done! You have covered several major models for estimating change in lifespan parameters! Hooray! "],["workshop-bivariate-regression-modeling-diamond-pricing.html", "11 Workshop: Bivariate Regression: Modeling Diamond Pricing Getting Started 11.1 Review 11.2 Regression and the Line of Best Fit Learning Check 1 11.3 Statistical Significance Learning Check 2 11.4 Visualization Learning Check 3 11.5 Finding the Line of Best Fit Learning Check 4 11.6 F-statistic 11.7 summary() Learning Check 5", " 11 Workshop: Bivariate Regression: Modeling Diamond Pricing Social systems are full of numeric variables, like voter turnout, percentage of votes for party X, income, unemployment rates, and rates of policy implementation or people affected. So how do we analyze the association between two numeric variables? Today, were going to investigate a popular dataset on commerce. The ggplot2 packages diamonds dataset contains 53,940 diamond sales gathered from the Loose Diamonds Search Engine in 2017. Were going to examine a random sample of 1000 of these diamonds, saved as mydiamonds.csv. This dataset lets us investigate a popular question for consumers: Are diamonds size, measured by carat, actually related to their cost, measured by price? Lets investigate using the techniques below. Getting Started Load Data In this dataset, each row is a diamond! View Data price carat cut 4596 1.20 Ideal 1934 0.62 Ideal 4840 1.14 Very Good Codebook In this dataset, our variables mean: price: price of diamond in US dollars (from $326 to $18,823!) carat: weight of the diamond (0.2 to 5.01 carats) cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal) 11.1 Review We have several tools in our toolkit for measuring the association between two variables: (1) Scatterplots, (2) Correlation, and (3) Regression / Line of Best Fit (New!). Lets investigate! 11.1.1 Scatterplots First, we can visualize the relationship between 2 numeric variables using a scatterplot, putting one on the x-axis and one on the y-axis. In a scatterplot, each dot represents a row in our dataset. So, we can visualize just five randomly selected dots, like this: Or we can visualize all the dots, like this: We can see that theres a strong, positive relationship. As carat increases, price increases to! 11.1.2 Correlation We can measure the relationship between two numeric variables using Pearsons r, the correlation coefficient! This statistic ranges from -1 to 0 to +1. -1 indicates the strongest possible negative relationship, 0 indicates no relationship, and 1 indicates the strongest possible positive relationship. That could help us learn (1) how strongly associated are they, and (2) how positive or negative is that association. The animation below shows the full range of possible correlations we might get. 11.1.3 Example 11.1.4 cor.test() and tidy() We can use cor.test() to test correlation in R. Lets, for example, get the correlation between price and carat for each different cut of diamond. There are 5 cuts of diamonds, so we should get 5 correlations, using group_by(cut). cor.test() lets us test 3 things: direction (positive or negative?) strength of association (closer to +/-1 = stronger, closer to 0 = weaker) statistical sigificance (p-value), an indicator of how extreme our statistics are - how likely is it we got this statistic due to chance? (Likely due to chance = nearer to 1; not likely due to chance = near 0, eg. p &lt; 0.05.) To extract cor.test()s output, we can use the broom packages tidy() function. This takes the output of the cor.test() function and puts it in a nice tidy data.frame, which we can then give to summarize(), allowing us to still use group_by(). Then, the correlation is reported in the estimate column, a standardized t-statistic is calculated in the statistic column, significance is given in the p.value column, and the upper (97.5%) and lower (2.5%) 95% confidence intervals are reported in conf.low and conf.high. cut estimate statistic p.value parameter conf.low conf.high method alternative Fair 0.854 10.10 0 38 0.739 0.739 Pearsons two.sided Good 0.912 22.23 0 100 0.872 0.872 Pearsons two.sided Ideal 0.919 45.89 0 385 0.903 0.903 Pearsons two.sided Premium 0.927 37.21 0 228 0.906 0.906 Pearsons two.sided Very Good 0.924 37.28 0 239 0.903 0.903 Pearsons two.sided For correlation, most people just report the correlation coefficient, and dont go into significance, but its always an option. 11.2 Regression and the Line of Best Fit Wouldnt it be nice if we could say, how much does the price of a diamond tend to increase when the size of that diamond increases? Economists, consumers, and aspiring fiancees could use that information to determine what size of diamond they can afford. We learned to do this in physical acceleration models, but can we apply it to large datasets? 11.2.1 Model Equation We do this intuitively all the time, making projections based on data weve seen in the past. What we, and computers, are doing is building a model. Were taking lots of data weve observed and building a simplified version of it, a general trend line. Its not intended to a complete replica - its just a model! This is the meaning of a regression model. Regression models create a straight line that best approximates the shape of our data. The line of best fit is a model of the data. The line can be represented as: \\(Y_{observed} = Alpha + X_{observed} \\times Beta + Error\\) [Click Here to Review Definitions!] Lets break this down. \\(Y_{observed}\\): the raw, observed outcome for each observation (price for each diamond). \\(Y_{predicted}\\): the predicted outcome for each observation, based on the supplied data (`carat for each diamond). \\(Alpha\\): the predicted value of the outcome if all predictors equal zero. Also called the \\(Intercept\\), the point at which the line crosses the y-axis. \\(X_{Observed}\\) a vector of observed values of our predictor/explanatory variable. We feed each value into our model to generate a predicted outcome. \\(Beta\\): how much our outcome y (price) increases by when our predictor/explanatory variable x (carat) increases by 1. Also known as the slope of the line. \\(Error\\) or \\(Residuals\\): predicted outcome might deviate a little from the observed outcome. This deviation (\\(Y_{observed} - Y_{predicted}\\)) is call the \\(Residual\\) for each observation. Colloquially, we call it \\(Error\\). In other words, plus or minus a few \\(residuals\\), the \\(Alpha\\) (\\(Intercept\\)) plus the value of \\(X\\) times the \\(Beta\\) coefficient always gives you the value of \\(Y\\). 11.2.2 Coding Regression Models 11.2.3 lm() We can use the lm() function in R to estimate the model equation for our line of best fit. ## ## Call: ## lm(formula = price ~ carat, data = .) ## ## Coefficients: ## (Intercept) carat ## -2161 7559 This means \\[ Price_{predicted} = -2161 + Carats_{observed} \\times 7559 \\] Learning Check 1 Question Write out the meaning of the equation above in a sentence, replacing X, Y, ALPHA, and BETA, and UNIT OF Y below with their appropriate values and meanings. Use the format below: If the value of X is zero, the model projects that Y equals ALPHA. As X increases by 1, Y is projected to increase by BETA, plus or minus a few UNIT OF Y. [View Answer!] If a diamond weighed 0 carats, the model projects that the price of that diamond would be -2161 USD. But, as the weight of that diamond increases by 1 carat, that diamonds price is projected to increase by 7559 USD, plus or minus a few dollars. 11.3 Statistical Significance We can even assess statistical significance for our alpha and beta coefficients. We can use tidy() from the broom package. And if youre not satisfied with that layout, we can write our own function tidier() to get even tidier formatting! Copy and run the tidier() function, and compare your output to tidy() above. ## # A tibble: 2 × 8 ## term estimate se statistic p_value stars upper lower ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2161. -22.4 -22.4 0 *** -1972. -2351. ## 2 carat 7559. 71.5 71.5 0 *** 7766. 7351. A little cleaner, right? I hope this function can help you out. term estimate se statistic p_value stars upper lower (Intercept) -2161.429 -22.401 -22.401 0 *** -1972.090 -2350.768 carat 7558.676 71.527 71.527 0 *** 7766.047 7351.305 The table above outputs several columns of importance to us! Click Here to See Definitions term: the name of the intercept as well as the predictor whose be estimate: the value of the alpha coefficient (-2161) and beta coefficient (7559). statistic: a standardized t-statistic measuring how extreme each estimate is, based on sample size and variance in our data. se: standard error for each beta coefficient, describing the standard deviation of that statistics sampling distribution. p_value: the probability that our alpha or beta coefficients were that large just due to chance (eg. random sampling error). Our measure of statistical significance. When 4 or more decimal places, sometimes gets abbreviated to just 0 in R. stars: shorthand for significance. p &lt; 0.001 = ***; p &lt; 0.01 = **; p &lt; 0.05 = *; p &lt; 0.10 = .. lower: the lower bound for the range were 95% sure the true estimate lies in. upper: the upper bound for the range were 95% sure the true estimate lies in. Learning Check 2 Question Write out the meaning of the equation above in a sentence, replacing X, Y, ALPHA, and BETA, UNIT OF Y, and UNIT OF OBSERVATION below with their appropriate values and meanings. Use the format below: There is a less than [0.001, 0.01, 0.05, or 0.10] probability that our alpha coefficient of ALPHA occurred due to chance. We are 95% certain that the true Y for a UNIT OF OBSERVATION that weighs 0 UNIT OF X lies between LOWER CONFIDENCE INTERVAL and UPPER CONFIDENCE INTERVAL UNIT OF Y. There is a less than [0.001, 0.01, 0.05, or 0.10] probability that our beta coefficient of BETA occurred due to chance. We are 95% certain that as X increases by 1, the true Y for a UNIT OF OBSERVATION increases by between LOWER CONFIDENCE INTERVAL and UPPER CONFIDENCE INTERVAL UNIT OF Y. [View Answer!] There is a less than probability that our alpha coefficient of USD per carat occurred due to chance. We are 95% certain that the true price for a diamond that weighs 0 carats lies between and dollars. There is a less than 0.001 probability that our beta coefficient of 7559 USD per carat occurred due to chance. We are 95% certain that as the weight of our diamond increases by 1 carat, the true price for a diamond increases by between 7351 and 7766 dollars. Notice that the value of the intercept might be nonsensical sometimes, like a negative price. 11.4 Visualization Finally, visualizing the line of best fit is quite easy! We make a scatterplot in using the ggplot2 packages ggplot() function. Then, we add geom_smooth(method = \"lm\"). This uses the lm() function internally to make a line of best fit between our x and y variables in the aes() section of our plot. Learning Check 3 Question Add color = cut to the aes() in the plot above. What happens? What does this tell us about the relationship between price and carats for each cut of diamond? [View Answer!] ggplot generates 5 different lines of best fit, one for each level of cut. The slope of the line differs for each cut. As carats increase, price increases at a faster rate for \"Ideal\" cut diamonds than for \"Fair\" cut diamonds. 11.5 Finding the Line of Best Fit We know that lm() finds the line of best fit, but how exactly does it do it? It all has to do with predicted values, residuals, and R-squared. 11.5.1 Predicted Values All models have a lot of information stored inside them, which you can access using the $ sign. (Note: select() cant extract them, because model objects are not data.frames) price carat 4596 1.20 1934 0.62 4840 1.14 This code gives you the the predicted values for your outcome, dubbed price_hat (\\(Y_{Predicted}\\)), and your residuals (\\(Y_{Observed}  Y_{Predicted}\\)), given each row of data. ## # A tibble: 3 × 4 ## price carat price_hat residual ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4596 1.2 6909. -2313. ## 2 1934 0.62 2525. -591. ## 3 4840 1.14 6455. -1615. 11.5.2 Residuals Residuals represent the difference between the observed outcome and the outcome predicted by the model. A regression model finds the line which minimizes these residuals, thus getting the best model for the overall trend in the data. The animation below below visualizes residuals as lines stemming from the best fit line. The alpha and beta coefficients are varied to show changing residuals for each of these different lines, compared to the blue line, which is the actual line of best fit. Cases well predicted by the model are tiny, and close to the best fit line. Cases poorly predicted by the model are BIG, and far from the best fit line. Extra: code this as a static visual! (optional) 11.5.3 R-squared R-squared (\\(R^{2}\\)) is a simple statistic that ranges from 0 - 1. R2 measures the percentage of variation in the outcome that is explained by the model. Benchmarks for \\(R^{2}\\) \\(R^{2}\\) = 1.00 -&gt; perfect fit. All variation perfectly explained. \\(R^{2}\\) = 0.85 -&gt; pretty great fit; 85% of variation in outcome explained! \\(R^{2}\\) = 0.5 -&gt; pretty good; 50% explained! \\(R^{2}\\) = 0.2 -&gt; not good, but 20% explained is better than nothing! \\(R^{2}\\) = 0 -&gt; nothing. Just nothing. Calculating \\(R^{2}\\) We calculate \\(R^{2}\\) using this formula: \\(R^{2}\\) = 1 - (Residual Sum of Squares / Total Sum of Squares) Total Sum of Squares (TSS): the sum of the squared differences between observed outcomes and their overall mean outcome. A single number describing how much the outcome varies. sum( (y - mean(y))^2 ) Residual Sum of Squares (RSS): describes on average the difference between observed outcomes and predicted outcomes. A single number describing how much the model errs from the real data. sum( (y - ypredicted)^2 ) We can combine these to understand our model: RSS / TSS: percentage of variation that remains unexplained by the model. 1 - (RSS / TSS): percentage of variation that was explained by the model. \\(R^{2}\\): percentage of variation in the outcome that was explained by the model. We can manually code this in R2! tss rss R2 14222520095 2321500034 0.8367729 For example, the following animation shows how each of the possible lines plotted above produces a different residual sum of squares, leading to a different \\(R^{2}\\). The sweet spot, where \\(R^{2}\\) is highest (and therefore the residuals are minimized) is when the slope is closest to our actual observed beta value, $7559 per carat. Otherwise, both higher and lower slopes lead to a lower \\(R^{2}\\). Learning Check 4 Question How much of the variation in the outcome did the model explain? What statistic tells us this information? Does this mean our model fits well or poorly? [View Answer!] This model explained 84% of the variation in the outcome. The \\(R^{2}\\) statistic tells us this information. A higher \\(R^{2}\\) statistic means better model fit, and 0.84 is quite close to 1.00, the max possible model fit. 11.6 F-statistic Finally, we also want to measure how useful this model is, compared to the intercept. 11.6.1 Interpreting an F-statistic Models are imperfect approximations of trends in data. The simplest possible model of data is the intercept line, the amount of the outcome you would have if X were zero. (Imagine a flat line through your data at the intercept.) If we have a good model, it had better explain more variation than the amount explained by the intercept line, right? To do this, we can calculate an F-statistic for our model. F statistics (just like Chi-squared) range from 0 to infinity. If your F statistic is small, the model aint much better than the intercept. If your F statistic is large, the model explains much, much more variation than the intercept. We can compare your F statistic to a null distribution of scores wed get due to chance, and find the probability we got this statistic due to chance, our p-value. The broom packages glance() function lets us do this below, giving us a statistic and p.value. r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.8367729 0.8366094 1525.173 5116.183 0 1 -8747.801 17501.6 17516.32 2321500034 998 1000 ### How to calculate F statisic manually! (optional)** The F statistic requires five main ingredients: The residual sum of squares (variation NOT explained by the model) The total sum of squares (all variation IN the data) The explained sum of squares (variation EXPLAINED by the model), which is the difference between the total and residual sum of squares. The sample size (number of diamonds analyzed) Number of variables in the model (outcome + predictor = 2) ## Rows: 1 ## Columns: 5 ## $ residual_sum_of_squares &lt;dbl&gt; 2321500034 ## $ total_sum_of_squares &lt;dbl&gt; 14222520095 ## $ n &lt;int&gt; 1000 ## $ p &lt;int&gt; 2 ## $ explained_sum_of_squares &lt;dbl&gt; 11901020061 What do we do with our ingredients to make the F-statistic then? We need to calculate on average, how much variation was explained, relative to the number of predictors, compared to on average, how much error was not explained, relative to the number of cases analyzed and variables used. ## Rows: 1 ## Columns: 9 ## $ residual_sum_of_squares &lt;dbl&gt; 2321500034 ## $ total_sum_of_squares &lt;dbl&gt; 14222520095 ## $ n &lt;int&gt; 1000 ## $ p &lt;int&gt; 2 ## $ explained_sum_of_squares &lt;dbl&gt; 11901020061 ## $ mean_squares_due_to_regression &lt;dbl&gt; 11901020061 ## $ mean_squared_error &lt;dbl&gt; 2326152 ## $ f_statistic &lt;dbl&gt; 5116.183 ## $ p_value &lt;dbl&gt; 0 And there you have it! Thats how you calculate an F-statistic and its p-value manually. As you can see, its a lot faster to just use the broom packages glance() function. 11.7 summary() We learned in this workshop that every regression model generates several statistics: (1) intercept (alpha coefficient), (2) beta coefficient(s), (3) \\(R^{2}\\), and (4) F-statistic. 11.7.1 Get all stats, all at once with summary() Wouldnt it be handy if there were a convenient function that let us see all of this in one place? Try the summary() function. It can be overwhelming - it outputs lots of information. However, we only need to look in 4 places for key information. ## ## Call: ## lm(formula = price ~ carat, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8024.3 -760.3 -11.9 527.1 10368.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2161.43 96.49 -22.40 &lt;2e-16 *** ## carat 7558.68 105.68 71.53 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1525 on 998 degrees of freedom ## Multiple R-squared: 0.8368, Adjusted R-squared: 0.8366 ## F-statistic: 5116 on 1 and 998 DF, p-value: &lt; 2.2e-16 11.7.2 4 Key Statistics So what can you find in the summary() output? coefficients: Under Coefficients, we can see the intercept and predictor(s) in our model. Each row shows the name of a term in our model (eg. carat) and an Estimate. This is the Beta Coefficient, or in the case of the Intercept, the Alpha Coefficient. p-value: Then, at the end of each row is something called Pr(&gt;|t|). This is the weirdest way Ive ever seen it written, but in simple terms: its the p-value for the Alpha or Beta coefficient. Nothing else in the table matters much. R2: Below the coefficient table are the model information. Youll notice R-squared: ~0.84. This is where you can find R2 calculated for you. F-statistic: Beneath that is the F-statistic, 5116. At the end of the row is the p-value of the F-statistic, the more readable statistic. Learning Check 5 Question Using the filter() and lm() functions, test the effect of carat on diamond price, first looking at just \"Ideal\" diamonds. Use the summary() function, and report alpha, beta, R2, the F-statistic, and its p-value for this model. [View Answer!] ## ## Call: ## lm(formula = price ~ carat, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6325.2 -728.3 -12.4 480.1 9892.0 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2184.9 141.3 -15.47 &lt;2e-16 *** ## carat 7971.8 173.7 45.89 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1458 on 385 degrees of freedom ## Multiple R-squared: 0.8454, Adjusted R-squared: 0.845 ## F-statistic: 2106 on 1 and 385 DF, p-value: &lt; 2.2e-16 For ideal diamonds, the model projected that a diamond weighing 0 carats would cost -2185 USD, increasing by 7972 for every additional carat. Both coefficients were statistically significant at the p &lt; 0.001 level. The model explained 85% of the variation in ideal-cut diamond prices, and fit much better than the intercept, shown by our statistically significant f-statistic (F = 2106, p &lt; 0.001). "],["workshop-multivariate-regression-modeling-effects-of-disaster-on-social-capital.html", "12 Workshop: Multivariate Regression: Modeling Effects of Disaster on Social Capital Getting Started 12.1 Multiple Regression Learning Check 1 12.2 Effect Sizes Learning Check 2 12.3 Multiple Models Learning Check 3 12.4 Great Tables Learning Check 4", " 12 Workshop: Multivariate Regression: Modeling Effects of Disaster on Social Capital Why do some communities see stronger social capital than others? Social capital refers to the social ties that bind results, enabling trust and collective action among residents to tackle public issues. Recent studies suggest that after disasters, residents social capital actually increases, because people recognize the value of friends and family as they work to recover and rebuild. We can use regression analysis to test this hypothesis on new data! This workshop examines 151 Japanese municipalities over 7 years, from 2011 to 2017 (jp_matching_experiment.csv), totaling 1057 city-year observations. This includes 85 coastal municipalities hit by the 2011 tsunami and 66 municipalities as similar as possible, just next door, that were not hit. Lets load in our data and get started. Getting Started Load Data &amp; Packages In this dataset, each row is a city-year! # Load Packages library(tidyverse) # for data manipulation library(broom) # for each model summaries library(texreg) # for nice model tables # Load Data cities &lt;- read_csv(&quot;workshops/jp_matching_experiment.csv&quot;) %&gt;% # Tell R to treat year and pref as ordered categories mutate(year = factor(year), pref = factor(pref)) View Data # View first 3 rows of dataset cities %&gt;% head(3) muni_code muni pref year by_tsunami social_capital damage_rate pop_density exp_dis_relief_per_capita income_per_capita unemployment pop_women pop_over_age_65 02201 Aomori Aomori 2011 Hit 0.393 0.000 1119.2 0.01 1.11 5.9 53.6 27.9 02202 Hirosaki Aomori 2011 Not Hit 0.413 0.000 602.1 0.06 0.99 4.5 54.1 29.2 02203 Hachinohe Aomori 2011 Hit 0.408 0.001 1137.5 2.44 1.13 5.5 52.2 27.5 Codebook In this dataset, our variables mean: muni_code unique 5 digit idenfier for each municipality. muni: municipality where election took place pref: prefecture that municipality is in year: year of observation. by_tsunami: was that city struck by the tsunami (Hit), not hit but just next door (Not Hit), or some other municipality (Other)? Outcome Variable social_capital: index measuring overall social capital, the social ties between residents that build trust, using several dimensions. Measured on a scale from 0 (low) to 1 (high). Explanatory Variable damage_rate: rate of buildings damaged or destroyed by earthquake and tsunami, per million residents. Control Variables exp_dis_relief_per_capita: spending on disaster relief in 1000s of yen per capita. income_per_capita: income per capita in millions of yen per capita. unemployment: unemployment rate per 1,000 residents. pop_women: % residents who are women pop_over_age_65: % residents over age 65 pop_density: population in 1000s of residents per square kilometer 12.1 Multiple Regression 12.1.1 Beta coefficients We can use a regression model to test the association between our outcome variable social_capital and our explanatory variable by_tsunami. Using the lm() function, we can get a beta coefficient estimating how much higher a social capital index score they received for every additional building damaged per million residents. cities %&gt;% lm(formula = social_capital ~ damage_rate) ## ## Call: ## lm(formula = social_capital ~ damage_rate, data = .) ## ## Coefficients: ## (Intercept) damage_rate ## 0.39419 0.07485 12.1.2 Controls But many other things might affect social capital in a community, not just getting hit by the tsunami: For example, (1) population density, (2) wealth, (3) unemployment, (4) age, (5) government capacity, (6) disaster relief, (7) time, and even (8) regional differences. We need to add control variables to our model to control for these alternative explanations for variation in social capital. This will refine our beta coefficient for the effect of the tsunami, getting us closer to the truth. We can add extra control variables using + in the lm() function. For example, we test the effect of damage_rate below, controlling for income_per_capita. cities %&gt;% lm(formula = social_capital ~ damage_rate + income_per_capita) ## ## Call: ## lm(formula = social_capital ~ damage_rate + income_per_capita, ## data = .) ## ## Coefficients: ## (Intercept) damage_rate income_per_capita ## 0.46141 0.06303 -0.06148 Our model tells us that for every building damaged per million residents, the social capital index increased by 0.06. For every additional million yen per capita in income, the average citys social capital index increased by -0.06. 12.1.3 Planes Instead of a line of best fit, for 2 variables, this regression model now essentially predicts a plane of best fit for 3 variables. See below. And, given 4 or more variables, a regression model will predict a hyperplane of best fit. Not easy to visualize, but just think: 4 variables means 4 dimensions. 5 variables means 5 dimensions. Learning Check 1 Question Make a regression model testing the effect of a citys damage_rate on social_capital, controlling for pop_density. Whats the effect of damage_rate on social_capital? [View Answer!] cities %&gt;% lm(formula = social_capital ~ damage_rate + pop_density) ## ## Call: ## lm(formula = social_capital ~ damage_rate + pop_density, data = .) ## ## Coefficients: ## (Intercept) damage_rate pop_density ## 4.029e-01 6.812e-02 -9.173e-06 Controlling for population density, as damage rates increase by 1 building per million residents, the social capital index increases by 0.068 points. 12.2 Effect Sizes One big challenge with multiple regression is that its not really clear how to compare the size of our beta coefficients. Is 1 damaged building per million residents greater than or less than 1 square kilometer per 1000 residents, or 1 million yen per capita? To compare the size of our beta coefficients, our variables must have the same units. We can do this by turning our numeric variables into Z-scores. Remember that a Z-score is a measure of how many standard deviations from the mean a specific value is for a given variable. We can mutate() variables into Z-scores using the scale() function. rescaled &lt;- cities %&gt;% # For each numeric variable, rescale its values mutate(social_capital = scale(social_capital), damage_rate = scale(damage_rate), pop_density = scale(pop_density), exp_dis_relief_per_capita = scale(exp_dis_relief_per_capita), income_per_capita = scale(income_per_capita), unemployment = scale(unemployment), pop_women = scale(pop_women), pop_over_age_65 = scale(pop_over_age_65)) Check out our new rescaled variables. rescaled %&gt;% head(3) muni_code muni pref year by_tsunami social_capital damage_rate pop_density exp_dis_relief_per_capita income_per_capita unemployment pop_women pop_over_age_65 02201 Aomori Aomori 2011 Hit -0.1296000 -0.6351305 0.1290887 -0.3617464 0.09763001 1.0562282 1.2937535 -0.6429615 02202 Hirosaki Aomori 2011 Not Hit 0.3907693 -0.6351305 -0.1932630 -0.3611821 -0.34526666 -0.1842532 1.5495231 -0.4271015 02203 Hachinohe Aomori 2011 Hit 0.2606770 -0.6225776 0.1404966 -0.3343254 0.17144612 0.7018050 0.5775988 -0.7093800 Okay, lets repeat our model, this time using our new data.frame rescaled, and save the model as m0. m0 &lt;- rescaled %&gt;% lm(formula = social_capital ~ damage_rate + pop_density) # View model m0 ## ## Call: ## lm(formula = social_capital ~ damage_rate + pop_density, data = .) ## ## Coefficients: ## (Intercept) damage_rate pop_density ## -3.667e-16 1.412e-01 -3.829e-01 We can now interpret our results as: As the damage rate increases by 1 standard deviation (new unit of predictor), the social capital index increases by 0.14 standard deviations (new unit of outcome), controlling for population density. Learning Check 2 Question Make a second regression model called m1, that also controls for the effect of year. Because we made it a factor(), we control for each year. The beta coefficient tells us now how many more standard deviations of social capital we got in year X compared to the first year (2011), our baseline for comparison. The alpha coefficient tells us how many standard deviations we got during the our baseline year. Which year had the largest effect on social capital, and how much was that effect? [View Answer!] m1 &lt;- cities %&gt;% lm(formula = social_capital ~ damage_rate + pop_density + year) # View model m1 ## ## Call: ## lm(formula = social_capital ~ damage_rate + pop_density + year, ## data = .) ## ## Coefficients: ## (Intercept) damage_rate pop_density year2012 year2013 year2014 ## 3.989e-01 6.812e-02 -9.173e-06 4.987e-03 2.238e-03 3.172e-03 ## year2015 year2016 year2017 ## 5.391e-03 5.868e-03 6.351e-03 2017 had the largest effect on social capital, compared to 2011. In 2011, the average city saw 0.3989 standard deviations above the mean of social capital. In 2017, the average city saw 0.0064 standard deviation more social capital than in 2011 (totaling 0.4053 standard deviation above the mean). Notice that we didnt rescale categorical variables. In regression, categorical variables cant be rescaled or compared to numeric variables. 12.3 Multiple Models To find the best model, it helps to make several, in a logical, systematic way. Choose your explanatory variable whose effect you really want to test. For us, thats disaster damage (damage_rate). Add choose your absolutely most essential control variables, without which the model isnt very valid. For us, thats pop_density and year. (Already done and saved as m1!) # For your reference m1 &lt;- rescaled %&gt;% lm(formula = social_capital ~ damage_rate + pop_density + year) Add more controls, to wean out effects of other phenomena and get a more accurate beta coefficient for damage_rate. Lets add exp_dis_relief_per_capita, to control for city government spending on disaster relief. Save that as m2. m2 &lt;- rescaled %&gt;% lm(formula = social_capital ~ damage_rate + pop_density + year + exp_dis_relief_per_capita) Examine our two tables, using the texreg packages htmlreg() function. Were going to list() our models m1 and m2, and ask R to save a nice table in our files as \"table_1.html\". Try it out, then go to your files in the right-hand corner and click 'View in Web Browser'! htmlreg(list(m1,m2), bold = 0.05, include.fstat = TRUE, file = &quot;workshops/workshop_11_table_1.html&quot;) [Click to view table!] Statistical models &nbsp; Model 1 Model 2 (Intercept) -0.10 -0.09 &nbsp; (0.07) (0.07) damage_rate 0.14*** 0.02 &nbsp; (0.03) (0.03) pop_density -0.38*** -0.37*** &nbsp; (0.03) (0.03) year2012 0.13 0.08 &nbsp; (0.11) (0.10) year2013 0.06 0.02 &nbsp; (0.11) (0.10) year2014 0.08 0.09 &nbsp; (0.11) (0.10) year2015 0.14 0.15 &nbsp; (0.11) (0.10) year2016 0.15 0.15 &nbsp; (0.11) (0.10) year2017 0.17 0.16 &nbsp; (0.11) (0.10) exp_dis_relief_per_capita &nbsp; 0.22*** &nbsp; &nbsp; (0.03) R2 0.17 0.21 Adj. R2 0.17 0.20 Num. obs. 1057 1057 F statistic 27.50 30.63 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Pretty nice, right? The bold = 0.05 says, if your p-value is below p &lt; 0.05, make the estimate bold in the chart, so its easy to see. include.fstat = TRUE means, please include the F-statistic at the bottom of the chart. Learning Check 3 Question Make another model called m3, adding as controls income_per_capita, unemployment, pop_women, and pop_over_age_65. Then make a model called m4, which adds pref, the prefecture each city is in (like their state). Finally, put them together in a htmlreg() table that visualizes m1, m2, m3, and m4 side by side, called \"table_2.html\". Look at the R-squared statistic at the bottom. Which model fits best? [View Answer!] Adding controls income_per_capita, unemployment, pop_women, and pop_over_age_65 m3 &lt;- rescaled %&gt;% lm(formula = social_capital ~ damage_rate + pop_density + year + exp_dis_relief_per_capita + income_per_capita + unemployment + pop_women + pop_over_age_65) Adding prefectural controls m4 &lt;- rescaled %&gt;% lm(formula = social_capital ~ damage_rate + year + pop_density + exp_dis_relief_per_capita + income_per_capita + unemployment + pop_women + pop_over_age_65 + pref) Making a nice table! htmlreg(list(m1,m2,m3,m4), bold = 0.05, include.fstat = TRUE, file = &quot;workshops/workshop_11_table_2.html&quot;) Model 4 fits best, with an R2 of 0.89. It explains 89% of the variation in social capital! Thats wild! [Click to view Table from Answer!] Statistical models &nbsp; Model 1 Model 2 Model 3 Model 4 (Intercept) -0.10 -0.09 -0.18&#42;&#42; 1.41&#42;&#42;&#42; &nbsp; (0.07) (0.07) (0.06) (0.10) damage_rate 0.14&#42;&#42;&#42; 0.02 -0.03 -0.10&#42;&#42;&#42; &nbsp; (0.03) (0.03) (0.03) (0.02) pop_density -0.38&#42;&#42;&#42; -0.37&#42;&#42;&#42; -0.12&#42;&#42;&#42; -0.20&#42;&#42;&#42; &nbsp; (0.03) (0.03) (0.03) (0.02) year2012 0.13 0.08 0.01 0.13&#42;&#42;&#42; &nbsp; (0.11) (0.10) (0.09) (0.04) year2013 0.06 0.02 0.04 0.05 &nbsp; (0.11) (0.10) (0.09) (0.04) year2014 0.08 0.09 0.14 0.08&#42; &nbsp; (0.11) (0.10) (0.09) (0.04) year2015 0.14 0.15 0.24&#42;&#42; 0.13&#42;&#42;&#42; &nbsp; (0.11) (0.10) (0.09) (0.04) year2016 0.15 0.15 0.35&#42;&#42;&#42; 0.13&#42;&#42; &nbsp; (0.11) (0.10) (0.09) (0.04) year2017 0.17 0.16 0.46&#42;&#42;&#42; 0.13&#42;&#42; &nbsp; (0.11) (0.10) (0.09) (0.04) exp_dis_relief_per_capita &nbsp; 0.22&#42;&#42;&#42; 0.11&#42;&#42;&#42; 0.04&#42; &nbsp; &nbsp; (0.03) (0.03) (0.02) income_per_capita &nbsp; &nbsp; -0.77&#42;&#42;&#42; 0.10&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; (0.05) (0.03) unemployment &nbsp; &nbsp; -0.29&#42;&#42;&#42; -0.13&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; (0.03) (0.01) pop_women &nbsp; &nbsp; -0.02 0.03&#42; &nbsp; &nbsp; &nbsp; (0.03) (0.01) pop_over_age_65 &nbsp; &nbsp; -0.49&#42;&#42;&#42; -0.14&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; (0.03) (0.02) prefAomori &nbsp; &nbsp; &nbsp; -1.24&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.10) prefChiba &nbsp; &nbsp; &nbsp; -2.82&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.10) prefFukushima &nbsp; &nbsp; &nbsp; -0.24&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.10) prefIbaraki &nbsp; &nbsp; &nbsp; -1.52&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.11) prefIwate &nbsp; &nbsp; &nbsp; -0.25&#42;&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.10) prefMiyagi &nbsp; &nbsp; &nbsp; -1.30&#42;&#42;&#42; &nbsp; &nbsp; &nbsp; &nbsp; (0.10) R2 0.17 0.21 0.43 0.89 Adj. R2 0.17 0.20 0.43 0.89 Num. obs. 1057 1057 1057 1057 F statistic 27.50 30.63 61.45 446.20 &#42;&#42;&#42;p &lt; 0.001; &#42;&#42;p &lt; 0.01; &#42;p &lt; 0.05 12.4 Great Tables Finally, lets add a few bells and whistles to our model table, to make it look really nice. [Click here to learn about texreg() arguments!] custom.model.names lets you add names for each column. custom.coef.map lets you rename variables. It also lets you rearrange them in whatever order makes sense to you. Only variables you rename will stay in the table, so it also will let us exclude the year effects, which are a few too numerous to report. caption adds a nice title. caption.above = TRUE puts it on top of the table. custom.note adds a footnote. Always indicate levels of statistical significance. single.row = TRUE puts everything on one row, which is helpful. htmlreg( list(m1,m2,m3,m4), bold = 0.05, include.fstat = TRUE, file = &quot;workshops/workshop_11_table_3.html&quot;, # Add column labels custom.model.names = c( &quot;Basic Model&quot;, &quot;with Controls&quot;, # You can split lines in two with &lt;br&gt; &quot;With Extended&lt;br&gt;Controls&quot;, &quot;With Geographic&lt;br&gt;Controls&quot;), # Add labels custom.coef.map = list( &quot;damage_rate&quot; = &quot;Damage Rate&quot;, &quot;exp_dis_relief_per_capita&quot; = &quot;Disaster Spending Rate&quot;, &quot;income_per_capita&quot; = &quot;Income per capita&quot;, &quot;unemployment&quot; = &quot;Unemployment Rate&quot;, &quot;pop_women&quot; = &quot;% Women&quot;, &quot;pop_over_age_65&quot; = &quot;% Over Age 65&quot;, &quot;prefAomori&quot; = &quot;Aomori&quot;, &quot;prefChiba&quot; = &quot;Chiba&quot;, &quot;prefFukushima&quot; = &quot;Fukushima&quot;, &quot;prefIbaraki&quot; = &quot;Ibaraki&quot;, &quot;prefIwate&quot; = &quot;Iwate&quot;, &quot;prefMiyagi&quot; = &quot;Miyagi&quot;, &quot;(Intercept)&quot; = &quot;Intercept&quot;), # Add a table caption caption = &quot;OLS Model of Social Capital in Japanese Cities over 7 years&quot;, # You can still add a custom note too! custom.note = &quot;Statistical Significance: *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Akita is the baseline prefecture. All models also control for each year (2011-2017) as a categorical variable.&quot;) [Click to view table!] OLS Model of Social Capital in Japanese Cities over 7 years &nbsp; Basic Model with Controls With ExtendedControls With GeographicControls Damage Rate 0.14*** 0.02 -0.03 -0.10*** &nbsp; (0.03) (0.03) (0.03) (0.02) Disaster Spending Rate &nbsp; 0.22*** 0.11*** 0.04* &nbsp; &nbsp; (0.03) (0.03) (0.02) Income per capita &nbsp; &nbsp; -0.77*** 0.10*** &nbsp; &nbsp; &nbsp; (0.05) (0.03) Unemployment Rate &nbsp; &nbsp; -0.29*** -0.13*** &nbsp; &nbsp; &nbsp; (0.03) (0.01) % Women &nbsp; &nbsp; -0.02 0.03* &nbsp; &nbsp; &nbsp; (0.03) (0.01) % Over Age 65 &nbsp; &nbsp; -0.49*** -0.14*** &nbsp; &nbsp; &nbsp; (0.03) (0.02) Aomori &nbsp; &nbsp; &nbsp; -1.24*** &nbsp; &nbsp; &nbsp; &nbsp; (0.10) Chiba &nbsp; &nbsp; &nbsp; -2.82*** &nbsp; &nbsp; &nbsp; &nbsp; (0.10) Fukushima &nbsp; &nbsp; &nbsp; -0.24* &nbsp; &nbsp; &nbsp; &nbsp; (0.10) Ibaraki &nbsp; &nbsp; &nbsp; -1.52*** &nbsp; &nbsp; &nbsp; &nbsp; (0.11) Iwate &nbsp; &nbsp; &nbsp; -0.25** &nbsp; &nbsp; &nbsp; &nbsp; (0.10) Miyagi &nbsp; &nbsp; &nbsp; -1.30*** &nbsp; &nbsp; &nbsp; &nbsp; (0.10) Intercept -0.10 -0.09 -0.18** 1.41*** &nbsp; (0.07) (0.07) (0.06) (0.10) R2 0.17 0.21 0.43 0.89 Adj. R2 0.17 0.20 0.43 0.89 Num. obs. 1057 1057 1057 1057 F statistic 27.50 30.63 61.45 446.20 Statistical Significance: *** p Learning Check 4 Question Make a new texreg table called \"table_4.html\", but this time, remove the pref categorical effects from the table, and make a note in the custom note of in which model we controlled for prefecture. Finally, whats the effect of disaster damage in our final model? How significant is that effect? [View Answer!] htmlreg( list(m1,m2,m3,m4), bold = 0.05, include.fstat = TRUE, file = &quot;workshops/workshop_11_table_4.html&quot;, custom.model.names = c( &quot;Basic Model&quot;, &quot;with Controls&quot;, &quot;With Extended&lt;br&gt;Controls&quot;, &quot;With Geographic&lt;br&gt;Controls&quot;), custom.coef.map = list( &quot;damage_rate&quot; = &quot;Damage Rate&quot;, &quot;exp_dis_relief_per_capita&quot; = &quot;Disaster Spending Rate&quot;, &quot;income_per_capita&quot; = &quot;Income per capita&quot;, &quot;unemployment&quot; = &quot;Unemployment Rate&quot;, &quot;pop_women&quot; = &quot;% Women&quot;, &quot;pop_over_age_65&quot; = &quot;% Over Age 65&quot;, # Notice I removed the prefectures here &quot;(Intercept)&quot; = &quot;Intercept&quot;), caption = &quot;OLS Model of Social Capital in Japanese Cities over 7 years&quot;, # Notice I added more to the note. custom.note = &quot;Statistical Significance: *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. All models also control for each year (2011-2017) as a categorical variable. Final model also controls for prefectures. Akita is the baseline prefecture.&quot;) [Click to view table from Answer!] OLS Model of Social Capital in Japanese Cities over 7 years &nbsp; Basic Model with Controls With ExtendedControls With GeographicControls Damage Rate 0.14*** 0.02 -0.03 -0.10*** &nbsp; (0.03) (0.03) (0.03) (0.02) Disaster Spending Rate &nbsp; 0.22*** 0.11*** 0.04* &nbsp; &nbsp; (0.03) (0.03) (0.02) Income per capita &nbsp; &nbsp; -0.77*** 0.10*** &nbsp; &nbsp; &nbsp; (0.05) (0.03) Unemployment Rate &nbsp; &nbsp; -0.29*** -0.13*** &nbsp; &nbsp; &nbsp; (0.03) (0.01) % Women &nbsp; &nbsp; -0.02 0.03* &nbsp; &nbsp; &nbsp; (0.03) (0.01) % Over Age 65 &nbsp; &nbsp; -0.49*** -0.14*** &nbsp; &nbsp; &nbsp; (0.03) (0.02) Intercept -0.10 -0.09 -0.18** 1.41*** &nbsp; (0.07) (0.07) (0.06) (0.10) R2 0.17 0.21 0.43 0.89 Adj. R2 0.17 0.20 0.43 0.89 Num. obs. 1057 1057 1057 1057 F statistic 27.50 30.63 61.45 446.20 Statistical Significance: *** p "],["workshop-design-of-experiments-in-r.html", "13 Workshop: Design of Experiments in R Getting Started 13.1 Difference of Means (t-tests) 13.2 Definitions: Null vs. Sampling Distributions 13.3 Null Distributions 13.4 Sampling Distribution 13.5 Speedy t-tests 13.6 ANOVA Conclusion", " 13 Workshop: Design of Experiments in R Figure 1.1: Donuts! This workshop introduces basic statistical techniques for conducting experiments, including t-tests, anova, and F-tests, among others. Getting Started Suppose Kims Coffee is a growing coffee chain in upstate New York. For the past 10 years, they have been using old Krispy-Kreme donut-making machines to make their locally renowned donuts. Since the machines are quickly approaching the end of their lifespan, the owners of Kims Coffee want to try out an alternative donut-machine - the Monster-Donut Maker 5000 - to see if it can produce donuts of comparable quality! They decide to design an experiment! The Experiment At their main branch, they install a Monster-Donut Maker 5000, which well call type = \"b\", right next to their Krispy-Kreme Donut Machine, which well call type = \"a\". They plan to measure the weight (grams), lifespan at room temperature (hours), and tastiness (0-10) of each donut produced from both machines. Using the same ingredients, they bake the donuts for the same amount of time under the same level of heat. Each scoop they take from their dough, they randomly assign it to get baked in machine a or machine b. But they need help! Theyve hired us to measure the impacts of their potential new donut machine (type = \"b\"), compared to their existing machines (type = \"a\"). Lets get started! Import Data First, lets import our data, where each row is a donut, with produced in one type of machine, with a specific weight, lifespan, and tastiness, each made by a baker named \"Kim\", \"Melanie\", or \"Craig\". # Load packages, to get our dplyr, ggplot, tibble, and readr functions library(tidyverse) library(broom) # get our tidy() function # Read data! donuts = read_csv(&quot;workshops/donuts.csv&quot;) # Having trouble reading in your data? # You can also use this code: donuts = read_csv(&quot;https://raw.githubusercontent.com/timothyfraser/sysen/main/workshops/donuts.csv&quot;) # Check it out! donuts %&gt;% glimpse() ## Rows: 50 ## Columns: 5 ## $ type &lt;chr&gt; &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;,  ## $ weight &lt;dbl&gt; 28.2, 26.1, 27.2, 25.7, 29.4, 28.0, 27.4, 30.5, 29.8, 27.9,  ## $ lifespan &lt;dbl&gt; 52, 54, 38, 52, 49, 50, 45, 48, 53, 52, 52, 47, 54, 41, 49,  ## $ tastiness &lt;dbl&gt; 1, 4, 1, 2, 4, 2, 3, 3, 3, 4, 5, 2, 2, 4, 2, 3, 1, 1, 3, 3,  ## $ baker &lt;chr&gt; &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Kim&quot;, &quot;Mel 13.1 Difference of Means (t-tests) 13.1.1 Calculating dbar Whenever we set up an experiment, we need to select a control group and a treatment group. In this case, our control group is the donuts produced by the original machine (type = \"a\"), while our treatment group is the donuts produced by the new machine (type = b). We can then calculate a single number to summarize the relationship between these 2 groups - a statistic called the difference of means \\(\\bar{d}\\)! Its literally: \\(\\bar{d} = \\bar{x}_{treatment} - \\bar{x}_{control}\\), where \\(\\bar{x}_{treatment}\\) is the mean outcome for the treatment group and \\(\\bar{x}_{control}\\) is the mean outcome for the control group. We can use group_by() and summarize() from the dplyr package to calculate the difference diff. diff = donuts %&gt;% group_by(type) %&gt;% summarize(xbar = mean(weight)) Then, we can pivot the shape of this data.frame with summarize() again, so that our data.frame has just all its info in just 1 row: Well grab the xbar value from the row where type == \"a\" and putting it into the column xbar_a. Well repeat this with xbar_b, putting the xbar value from the row where type == \"b\" into the xbar_b column. Finally, we can calculate dbar, the difference of means, by subtracting xbar_b from xbar_a! stat = diff %&gt;% summarize(xbar_a = xbar[type == &quot;a&quot;], xbar_b = xbar[type == &quot;b&quot;], dbar = xbar_b - xbar_a) In summary, the mean weight of a donut from the Krispy Kreme Machine (type = a) was 28.364 grams, while the mean weight of a donut from the Monster Donut Maker 5000 (type = b) was 31.716 grams. This means that our treatment group of donuts (our type = b donuts) tended to be 3.352 grams heavier than our control group! 13.1.2 Visualizing the difference of means We can visualize the difference of means pretty quickly, using geom_jitter(). donuts %&gt;% ggplot(mapping = aes(x = type, y = weight)) + geom_jitter(height = 0, width = 0.1, size = 3, alpha = 0.5, # Let&#39;s make a a bunch of donuts, using shape = 21 with white fill shape = 21, fill = &quot;white&quot;, color = &quot;goldenrod&quot;, # and increase the width of the outline with &#39;stroke&#39; stroke = 5) + theme_classic(base_size = 14) We can see from above that donuts from machine b do sure seem to weight a little bit more. Lets estimate whether this difference is significant, below! 13.2 Definitions: Null vs. Sampling Distributions But, how much should we trust our statistic \\(\\bar{d}\\) (dbar)? We can use (1) null distributions and (2) sampling distributions to characterize our level of trust in our statistic using different statistics. Specifically: p-values are statistics that describe Null Distributions confidence intervals are statistics that describe Sampling Distributions. Lets build a working definition of these two terms. 13.3 Null Distributions Maybe our statistic dbar is not much different from the many possible dbars we might get if our choice of donut maker were not related to donut weight. To account for this, we can approximate the null distribution of all statistics we would get if our grouping variable were not related to our outcome, and generate a p-value for dbar. 13.3.1 Permutation Test (Computational Approach) How can we see the null distribution? We can use resampling without replacement on our donuts data, permuting or shuffling which treatment group our donut ingredients got assigned to. This breaks any association between the outcome weight and our grouping variable type, such that any statistic we get is purely due to chance. Well mutate() our type vector using the sample(replace = FALSE) function, for 1000 repetitions of our data, creating 1000 random datasets, then calculate 1000 differences of means. perm = tibble(rep = 1:1000) %&gt;% group_by(rep) %&gt;% summarize(donuts) %&gt;% group_by(rep) %&gt;% mutate(type = sample(type, size = n(), replace = FALSE)) %&gt;% group_by(rep, type) %&gt;% summarize(xbar = mean(weight)) %&gt;% group_by(rep) %&gt;% summarize(xbar_a = xbar[type == &quot;a&quot;], xbar_b = xbar[type == &quot;b&quot;], dbar = xbar_b - xbar_a) # Many different dbar statistics from many permuted datasets, where the statistic is ENTIRELY due to chance. perm %&gt;% head(3) ## # A tibble: 3 × 4 ## rep xbar_a xbar_b dbar ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 29.9 30.2 0.304 ## 2 2 30.1 30.0 -0.128 ## 3 3 29.8 30.3 0.472 Now, lets calculate - what percentage of random statistics were more extreme than than our observed statistic? perm %&gt;% summarize( # Get our observed statistic estimate = stat$dbar, # For a 2 tailed test... # Turn all dbars positive (this takes care of the 2-tails) # Now, what percentage of random stats were greater than the observed? # That&#39;s our p-value! p_value = ( sum(abs(dbar) &gt;= abs(estimate) ) / n() )) ## # A tibble: 1 × 2 ## estimate p_value ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.35 0.001 Wow! That is a super statistically significant difference of means! We can visualize it like this. Pretty extreme, huh? perm %&gt;% ggplot(mapping = aes(x = dbar)) + geom_density(fill = &quot;steelblue&quot;, color = &quot;white&quot;) + theme_classic(base_size = 14) + geom_vline(xintercept = stat$dbar, linetype = &quot;dashed&quot;) + labs(x = &quot;Difference of Means&quot;, y = &quot;Probability (%)&quot;) 13.3.2 t-statistics (Theoretical Approach) Alternatively, if we know the standard error of our statistic, we can calculate a t-statistic, a standardized measure telling us how extreme is our statistic in terms of a \\(t\\) distribution. We can calculate it as \\(t = \\bar{d} / \\sigma\\), where \\(\\sigma\\) is the standard error. Much like the \\(\\chi^{2}\\) distribution, a \\(t\\) distribution is a null distribution of random statistics we could get due to chance, except the \\(t\\) distribution is two-tailed. \\(t\\) ranges from -Infinity to +Infinity, with a mean of 0. Like the \\(\\chi^{2}\\) distribution, the \\(t\\) distribution gets stretched out wider for low sample sizes, but narrower for large sample sizes, to reflect our increasing precision as sample sizes increase. For any df (degrees of freedom), we can draw a t-distribution, where df = \\(n_{pairs} - 1\\). \\(t\\) statistics have their own dt(), pt(), and qt() functions in R, just like all other distributions. we can use pt() to calculate p-values, our chance of error. Suppose for a moment that we already found the standard error and we went and calculated our t-statistic, and we also have the degrees of freedom. We can approximate the null distribution as a t-distribution, for any number of degrees of freedom. See below for an example! tibble(df = c(2, 5, 100)) %&gt;% group_by(df) %&gt;% summarize(t = seq(from = -5, to = 5, by = 0.05), prob = dt(t, df)) %&gt;% ggplot(mapping = aes(x = t, y = prob, color = factor(df), group = df)) + geom_line() + theme_classic() + labs(color = &quot;Degrees of\\nFreedom (df)&quot;, x = &quot;t-statistic&quot;, y = &#39;Probability&#39;, subtitle = &quot;Student&#39;s T distribution by df&quot;) Well learn below how to calculate a t-statistic. The main value added behind a t-statistic is that it is computationally very easy to obtain, whereas a permutation test requires us to generate 1000s of random permutations - a more computationally expensive method. Why Do we Assume Normal Distributions?: The reason why we rely on these assumptions is because for decades, it was not possible to approximate and visualize the shape of the of these distributions, because it was so computationally expensive! Thats why we wrote up giant tables of \\(k\\), \\(\\chi^{2}\\), and \\(t\\) statistics instead, so that researchers could just calculate those statistics once and share the tables, rather than computing them over and over! But, today, its actually quite easy, thanks to faster computers and great statistical calculators like R, to compute \\(t\\), \\(k\\), and \\(\\chi^{2}\\) values in milliseconds, and we can even gather thousands of random samples to visualize the exact shape of our null and sampling distributions. 13.4 Sampling Distribution Maybe our statistic dbar might have been just a little higher or lower had we produced just a slightly different sample of donuts by chance. To account for this, we can approximate the sampling distribution for our statistic dbar and generate a confidence interval around dbar. 13.4.1 Bootstrapped CIs for dbar If we have lots of computational power available (we do now that its 2020), we could bootstrap our data, taking 1000 versions of our dataset and resampling with replacement to simulate sampling error, and calculate dbar 1000 times to get the sampling distribution. # The the donuts 1000 times stat_boot = tibble(rep = 1:1000) %&gt;% group_by(rep) %&gt;% summarize(donuts) %&gt;% # For each rep, randomly resample donuts group_by(rep) %&gt;% sample_n(size = n(), replace = TRUE) %&gt;% # For each rep and treatment group, get the mean group_by(rep, type) %&gt;% summarize(xbar = mean(weight)) %&gt;% # For each rep, get dbar group_by(rep) %&gt;% summarize(xbar_a = xbar[type == &quot;a&quot;], xbar_b = xbar[type == &quot;b&quot;], dbar = xbar_b - xbar_a) # Voila! Look at those beautiful `dbar` statistics! # We&#39;ve got a full 1000 slightly varying dbars! stat_boot %&gt;% head() ## # A tibble: 6 × 4 ## rep xbar_a xbar_b dbar ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 28.8 31.9 3.14 ## 2 2 28.2 31.3 3.07 ## 3 3 27.9 31.2 3.31 ## 4 4 27.9 32.1 4.18 ## 5 5 28.6 31.6 2.98 ## 6 6 28.5 32.6 4.14 We can then use summarize() to compute quantities of interest from our bootstrapped sampling distribution of dbar in stat_boot, like the standard deviation (which would be the literal standard error, in this case), and confidence intervals. stat_boot %&gt;% summarize( # Let&#39;s grab our observed dbar statistic from &#39;stat&#39; estimate = stat$dbar, # standard error of sampling distribution.... se = sd(dbar), # lower 2.5th percentile lower = quantile(dbar, prob = 0.025), # upper 97.5th percentile upper = quantile(dbar, prob = 0.975)) ## # A tibble: 1 × 4 ## estimate se lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.35 0.806 1.73 5.06 13.4.2 Assume Normal CIs for dbar Alternatively, if we dont have lots of computational power, we could assume a normally distributed sampling distribution. Though bootstrapping is usually more accurate, it is still more common to assume your sampling distribution is normally distributed and calculate confidence intervals using a standard error instead. Heres how you do it. 13.4.2.1 Paired-Sample/Dependent Sample T-Test (\\(n_1 = n_2\\)) Suppose our treatment and control group had the same sample size (they do in this case). Then, we can calculate the standard error of the difference of means \\(\\bar{d}\\) as: \\[ \\sigma = \\frac{ s_{\\bar{d}} }{ \\sqrt{n} } = \\sqrt{ \\frac{ \\sum{ (d - \\bar{d} )^{2} } }{ n - 1} } \\times \\frac{1}{\\sqrt{n}} \\] Here, \\(d\\) refers to the differences between group a and b. (This is pretty unrealistic.) You can only calculate it if the sample size of your treatment and control group is identical. For example: # In a paired sample t-test, our data might look like this.s paired = donuts %&gt;% summarize(weight_a = weight[type == &quot;a&quot;], weight_b = weight[type == &quot;b&quot;]) %&gt;% # and we could calculate their differences &#39;d&#39; like so: mutate(d = weight_b - weight_a) # Check it paired %&gt;% glimpse() ## Rows: 25 ## Columns: 3 ## $ weight_a &lt;dbl&gt; 28.2, 26.1, 27.2, 25.7, 29.4, 28.0, 27.4, 30.5, 29.8, 27.9, 2 ## $ weight_b &lt;dbl&gt; 28.1, 29.3, 28.4, 28.8, 31.7, 30.4, 27.5, 28.8, 30.0, 34.7, 3 ## $ d &lt;dbl&gt; -0.1, 3.2, 1.2, 3.1, 2.3, 2.4, 0.1, -1.7, 0.2, 6.8, 8.6, 1.5, And we could then calculate the standard error like so paired %&gt;% summarize( # Get the total pairs of donuts analyzed n = n(), # Get mean difference / difference of means &#39;d&#39; dbar = mean(d), # Calculate the variance of d, called &#39;variance&#39; variance = sum( (d - dbar)^2 ) / (n - 1), # Calculate the standard deviation of d, by taking the square root of the variance s_d = sqrt(variance), # Calculate the standard error sigma of d, # by dividing the standard deviation by the total sample size se = s_d / sqrt(n)) ## # A tibble: 1 × 5 ## n dbar variance s_d se ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 3.35 15.6 3.95 0.789 So, knowing what we do about the t-distribution, we can now run our paired sample t-test pretty quickly. Once we have a t-statistic, its pretty easy to then calculate a p-value, the share of statistics more extreme than ours. Lets try calculating it all in one step! paired %&gt;% summarize(n = n(), dbar = mean(d), se = sqrt( sum( (d - dbar)^2 ) / (n - 1) ) / sqrt(n), df = n - 1, # Let&#39;s calculate our t-statistic, called &#39;statistic&#39; statistic = dbar / se, # Get the **two-tailed** p-value # by looking at the right side of the null distribution # getting the percentage of stats greater than abs(statistic) # subtracting that from 1 to get the one-tailed p-value # then multiplying that percentage by 2, # to represent the combined error on BOTH tails p_value = 2*(1 - pt(abs(statistic), df)), # Let&#39;s also calculate our multiplier t on the t-distribution # which will tell us how many standard deviations from the mean is the 97.5th percentile t = qt(0.975, df), # Next, let&#39;s calculate the upper and lower confidence intervals! upper = dbar + t * se, lower = dbar - t * se) ## # A tibble: 1 × 9 ## n dbar se df statistic p_value t upper lower ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 3.35 0.789 24 4.25 0.000282 2.06 4.98 1.72 Wheeeeee! Statistics!!!!!! 13.4.2.2 Unpaired-Sample/Independent Sample T-Test On the other hand, suppose that our treatment and control group did not have the same sample size. (They usually dont.) We can alternatively calculate the standard error using the standard deviations (\\(s_1\\) and \\(s_2\\)) and sample sizes (\\(n_1\\) and \\(n_2\\)) of the treatment and control groups. \\[ \\sigma = \\sqrt{ \\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}} } \\] Lets try it! # For our unpaired sampled t-test... unpaired = donuts %&gt;% # For each type (donut maker) group_by(type) %&gt;% # Calculate our basic ingredients... summarize(xbar = mean(weight), # mean of each group s = sd(weight), # standard deviation of each group n = n()) %&gt;% # sample size of each group # then let&#39;s pivot the data into one row... summarize( # Get the means.... xbar_a = xbar[type == &quot;a&quot;], xbar_b = xbar[type == &quot;b&quot;], # Get the standard deviations... s_a = s[type == &quot;a&quot;], s_b = s[type == &quot;b&quot;], # Get the sample sizes... n_a = n[type == &quot;a&quot;], n_b = n[type == &quot;b&quot;]) unpaired ## # A tibble: 1 × 6 ## xbar_a xbar_b s_a s_b n_a n_b ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 28.4 31.7 1.63 3.60 25 25 We should also probably assume that our variances are not equal, meaning that we need to use a new formula to calculate our degrees of freedom. That formula is absolutely ridiculously large! \\[ df = \\frac{ ( \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2 }{ \\frac{ (s_1^2 / n_1)^2 }{ n_1 - 1 } + \\frac{(s_2^2 / n_2)^2 }{ n_2 - 1 }} \\] Now, thats pretty silly, so Ive written a short function for us here. In a moment, were about to learn a really-really short method for t-tests, but its important that you know how it works first. df = function(s1, n1, s2, n2){ # Calculate our ingredients... top = (s1^2 / n1 + s2^2 / n2)^2 bottom_left = (s1^2/n1)^2 / (n1 - 1) bottom_right = (s2^2/n2)^2 / (n2 - 1) # Return the degrees of freedom top / (bottom_left + bottom_right) } Now, lets calculate our quantities of interest! unpaired %&gt;% # Let&#39;s calculate... summarize( # difference of means dbar = xbar_b - xbar_a, # standard error se = sqrt(s_a^2 / n_a + s_b^2 / n_b), # t-statistic statistic = dbar / se, # degrees of freedom df = df(s1 = s_a, s2 = s_b, n1 = n_a, n2 = n_b), # p-value p_value = 2*(1 - pt(abs(statistic), df = df)), # multiplier t t = qt(0.975, df), # lower 2.5% CI lower = dbar - t * se, # upper 97.5% CI upper = dbar + t * se) ## # A tibble: 1 × 8 ## dbar se statistic df p_value t lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.35 0.790 4.24 33.5 0.000164 2.03 1.75 4.96 13.5 Speedy t-tests Wow, that took a bit! Wouldnt it be nice if some statisticians, social scientists, and natural scientists teamed up to write a single R function that did all that really quickly? They did! Its called t.test(). You can query it using the code below, then extract its results into a data.frame using tidy() from the broom package. It allows you to simply dictate whether you want a paired test or not (paired) and whether you assume the variances in each group to be equal or noth (var.equal). Theres just one catch. You must (!!!) turn your grouping variable type into a factor, where the treatment group comes first, followed by the control group. Otherwise, R will default to alphabetical order when calculating the difference of means. (Very important!) donuts2 = donuts %&gt;% # Convert type to factor, where treatment group b is first mutate(type = factor(type, levels = c(&quot;b&quot;, &quot;a&quot;))) paired t-test (equal variance) t.test(outcome_var ~ group_var, paired = TRUE, var.equal = TRUE) donuts2 %&gt;% # Run our t-test using the data from donuts, then tidy() it into a data.frame summarize( t.test(weight ~ type, paired = TRUE, var.equal = TRUE) %&gt;% tidy()) ## # A tibble: 1 × 8 ## estimate statistic p.value parameter conf.low conf.high method alternative ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3.35 4.25 0.000282 24 1.72 4.98 Paired t two.sided unpaired t-test (equal variance): t.test(outcome_var ~ group_var, paired = FALSE, var.equal = TRUE) donuts2 %&gt;% # Run our t-test using the data from donuts, then tidy() it into a data.frame summarize( t.test(weight ~ type, paired = FALSE, var.equal = TRUE) %&gt;% tidy() ) ## # A tibble: 1 × 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.35 31.7 28.4 4.24 0.000100 48 1.76 4.94 ## #  2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; unpaired t-test (unequal variance): t.test(outcome_var ~ group_var, paried = FALSE, var.equal = FALSE) donuts2 %&gt;% # Run our t-test using the data from donuts, then tidy() it into a data.frame summarize( t.test(weight ~ type, paired = FALSE, var.equal = FALSE) %&gt;% tidy() ) ## # A tibble: 1 × 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.35 31.7 28.4 4.24 0.000164 33.5 1.75 4.96 ## #  2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; 13.6 ANOVA Alternatively, what if were comparing across multiple groups? Suppose, for a moment, that 3 people were involved in the donut experiment, where each person handled a different number of donuts. What if any differences were not due to the donut machine, but actually due to the person who made the donut? We can test differences in a numeric outcome like weight between 3 or more groups (eg. baker) using Analysis of Variance (ANOVA). 13.6.1 Visualizing ANOVA Practically speaking, ANOVA tries to ascertain how far apart three distributions are from each other, and the F statistic (ranging from 0 to Inf) shows how extreme is the difference between these distributions. If minor, F is near 0. If large gaps, F grows. donuts %&gt;% ggplot(mapping = aes(x = baker, y = weight, color = baker)) + geom_jitter(height = 0, width = 0.1, size = 3, alpha = 0.5, # Let&#39;s make a a bunch of donuts, using shape = 21 with white fill shape = 21, fill = &quot;white&quot;, color = &quot;goldenrod&quot;, # and increase the width of the outline with &#39;stroke&#39; stroke = 5) + theme_classic(base_size = 14) I dunno what Craigs doing, but his donuts are definitely not like Melanie or Kims donuts. Craigs donuts are like lead bricks. So, can we put a number to just how bad Craig is at making donuts? 13.6.2 ANOVA Explained To do analysis of variance, were going to go through a few quick steps. First, lets get our values. Well need to know the grand mean \\(\\bar{\\bar{x}}\\), the mean \\(\\bar{x_t}\\) of each treatment group \\(t\\), as well as the values for the treatment group \\(t\\) and the outcome of interest \\(x\\) (in this case, weight). ## # A tibble: 6 × 4 ## baker weight xbbar xbar ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Kim 28.2 30.0 28.5 ## 2 Kim 26.1 30.0 28.5 ## 3 Kim 27.2 30.0 28.5 ## 4 Kim 25.7 30.0 28.5 ## 5 Kim 29.4 30.0 28.5 ## 6 Kim 28 30.0 28.5 Next, well take our ingredients to calculate our qi, our quantities of interest. These include the TSS (total sum of squares), RSS (residual sum of squares), and ESS (explained sum of squares). Total Sum of Squares measures the total variation in your data, using the grand mean \\(\\bar{\\bar{x}}\\) as a benchmark. Residual Sum of Squares measures how much deviation remains between your treatment group means and other values within each treatment group. This is the unexplained error in your model. Explained Sum of Squares measures how much deviation was explained by your treatment group means (your model). qi = values %&gt;% summarize( # Calculate the *total* sum of squares # compared to the grand mean tss = sum( (weight - xbbar)^2), # Calculate the *residual* sum of squares # compared to the your &#39;model&#39; prediction, xbar # in this case, your model is the idea that the # mean of each treatment group explains the difference in weight. rss = sum( (weight - xbar)^2 ), # calculate the ess - the explained sum of squares # a.k.a the variation explained by the &#39;model&#39; ess = tss - rss, # Get the total number of treatment groups, k k = baker %&gt;% unique() %&gt;% length(), # Get the total sample size n = n()) # View it! qi ## # A tibble: 1 × 5 ## tss rss ess k n ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 515. 407. 108. 3 50 Then, lets use our qi to calculate an F statistic! Though our ESS and RSS give us the sum of our squared deviations, we know that some data points are bound to vary more than others. So, we should really evaluate the relative distance between our distributions using an average of the ESS (called the mean_squares_explained) and an average of the RSS (called the mean_squares_explained - its squart root is the root-mean-squared-error, a.k.a. sigma!!!). qi %&gt;% mutate( # Get the average variation explained by dividing by k - 1 groups mean_squares_explained = ess / (k - 1), # Get the average variation unexplained by dividing by n - k mean_squared_error = rss / (n - k) ) %&gt;% # Calculate the f-statistic, # the ratio of the variation explained versus unexplained by your model! mutate(f = mean_squares_explained / mean_squared_error) ## # A tibble: 1 × 8 ## tss rss ess k n mean_squares_explained mean_squared_error f ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 515. 407. 108. 3 50 54.0 8.66 6.24 13.6.3 Functions for ANOVA There are a bunch of different ways to do ANVOA, depending on how much specificity you need. We could use the oneway.test(), aov() or lm() functions. Lets compare to show their results. 13.6.4 lm() - the easy way Here, sigma gives you the residual standard error (which is the square root of the root-mean-squared-error that anova gives you). statistic gives you the F-statistic, while p.value gives you the p-value for the F-statistic. m1 = donuts %&gt;% summarize(lm(weight ~ baker) %&gt;% glance()) %&gt;% select(sigma, statistic, p.value, df) m1 ## # A tibble: 1 × 4 ## sigma statistic p.value df ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.94 6.24 0.00395 2 Practically speaking, I tend to use the lm() function for my ANOVA needs, because I can use the predict() function to flexibly make predictions. 13.6.5 aov() - the normal way Here, we get the F statistic and its p.value, as well as several quantities used to calculate the f-statistic, including the root-mean-squared-error (row 2 Residuals, column meansq). donuts %&gt;% summarize(aov(weight ~ baker) %&gt;% tidy()) ## # A tibble: 2 × 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 baker 2 108. 54.0 6.24 0.00395 ## 2 Residuals 47 407. 8.66 NA NA 13.6.6 oneway.test() - if you know your group variances are unequal Alternatively, oneway.test() can let you use var.equal = TRUE or FALSE to lightly adjust your statistics to accommodate uneven variances among groups. Randomization should usually deal with unequal variances, but if youre working with non-random samples, be sure to check your variances and then use var.equal = FALSE. # If even... donuts %&gt;% summarize(oneway.test(data = ., formula = weight ~ baker, var.equal = TRUE) %&gt;% tidy()) # If uneven... donuts %&gt;% summarize(oneway.test(formula = weight ~ baker, var.equal = FALSE) %&gt;% tidy()) 13.6.7 Multiple Variables But what if we need to test the impact of multiple variables simultaneously? For example, what if we want to assess how much differences in average weight were due to the type of machine versus the baker? # We can add both type and baker to the model m2 = donuts %&gt;% summarize(lm(formula = weight ~ type + baker) %&gt;% glance()) %&gt;% # and compare our statistic and sigma values to before. select(sigma, statistic, p.value, df) Lets stack our earlier ANOVA model m1 on top of our new model m2. We can see from the sigma statistic that the residual standard error (average prediction error) has decreased in our new model, indicating better model fit. We can also see that adding baker had led to a greater F statistic with a lower, more significant p.value. bind_rows( m1 %&gt;% mutate(vars = &quot;baker&quot;), m2 %&gt;% mutate(vars = &quot;type + baker&quot;) ) ## # A tibble: 2 × 5 ## sigma statistic p.value df vars ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2.94 6.24 0.00395 2 baker ## 2 2.41 14.3 0.00000102 3 type + baker Alternatively, what if Craig is pretty okay at making donuts with the old machine, but is just particularly bad at making donuts on the new machine? For this, we can use interaction effects. m3 = donuts %&gt;% lm(formula = weight ~ type * baker) %&gt;% glance() %&gt;% select(sigma, statistic, p.value) m3 ## # A tibble: 1 × 3 ## sigma statistic p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.24 11.7 0.000000310 We see here that our statistic is greater than in the original model m1, and with a significance p.value, but m2 had a higher F statistic. However, if we had empirical reasons that supported this model - eg. we randomly assigned donuts by type and baker - then we would need to use an interaction model to estimate effects accurately. Conclusion Great work making it through this workshop. Well follow up with more DoE in our subsequent classes! For practice, you can go through any of the sections of this workshop again, this time, selecting your variable of interest - weight, lifespan, or tastiness - and compare your results. "],["workshop-factorial-design-and-interaction-effects-in-r.html", "14 Workshop: Factorial Design and Interaction Effects in R Getting Started 14.1 Estimating Direct Effects 14.2 Standard Errors for \\(\\bar{\\bar{d}}\\) 14.3 Pivoting Data with pivot_longer() &amp; pivot_wider() 14.4 Visualizing &amp; Reporting Treatment Effects 14.5 Interaction Effects 14.6 Estimating 2-way interactions 14.7 Estimating 3-way interactions 14.8 Estimating Interactions in lm() Conclusion", " 14 Workshop: Factorial Design and Interaction Effects in R Figure 1.1: Lattes! Photo Credit to Fahmi Fakhrudin This workshop introduces methods for computing interaction effects in experiments in R, through a very caffeinated case of commercial product testing - lattes! Getting Started Factorial Design Suppose a local coffee chain in upstate New York, Kims Coffee, faces repeated complaints from customers: while their coffeeshops are beloved, their lattes are just plain bad. With the holidays right around the corner, they need to produce better lattes ASAP, to attract cold holiday shoppers! Their local six-sigma expert recommends that one of their shops designs and implements a factorial-design experiment. When you have a limited amount of time and/or resources, factorial-design experiments can be very cost-efficient. In this case, Kims Coffee staff are not sure why their lattes are bad - is it due to the espresso machine, the milk, the coffee syrups, or the latte art? They create 240 lattes, and randomly assign each latte order to the following treatments: machine: Each latte order is randomly assigned to 1 of 2 espresso machines (machine \"a\" or \"b\"). milk: For each machine, they then randomly assign the lattes to 1 of 3 types of milk (\"whole\", \"skim\", and \"oat\" milk). syrup: For each type of milk, they randomly assign lattes with that milk to 1 of 2 brands of simple syrup (\"torani\" vs. \"monin\"). art: Finally, each is assigned to 1 of 2 types of latte art presentation (\"heart\" vs. \"foamy\"). Finally, they measure the tastiness on a scale from 0 to 100. Data &amp; Packages Figure 1.2: Coffee Shop Experiments? Photo by Photo by Nafinia Putra on Unsplash They produce the following data, saved in lattes.csv, and available on Github at this link. Use the code below to load in our data! # Load packages library(tidyverse) library(broom) # Load in data! # If you upload it to your workshops folder, read it in like this... lattes = read_csv(&quot;workshops/lattes.csv&quot;) # or # If you want to download it straight from github, read it in like this... lattes = read_csv(&quot;https://raw.githubusercontent.com/timothyfraser/sysen/main/workshops/lattes.csv&quot;) # Check it out! lattes %&gt;% glimpse() ## Rows: 240 ## Columns: 6 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1 ## $ tastiness &lt;dbl&gt; 42.79289, 34.51512, 46.33578, 44.57469, 44.40918, 56.27327,  ## $ machine &lt;chr&gt; &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;a&quot;,  ## $ milk &lt;chr&gt; &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oat&quot;, &quot;oat ## $ syrup &lt;chr&gt; &quot;monin&quot;, &quot;monin&quot;, &quot;monin&quot;, &quot;monin&quot;, &quot;monin&quot;, &quot;monin&quot;, &quot;monin ## $ art &lt;chr&gt; &quot;foamy&quot;, &quot;foamy&quot;, &quot;foamy&quot;, &quot;foamy&quot;, &quot;foamy&quot;, &quot;foamy&quot;, &quot;foamy 14.1 Estimating Direct Effects First, we can estimate the direct effects of one treatment versus another within the same variable (eg. lattes made by machine b versus machine a). 14.1.1 Difference of Grand Means \\(\\bar{\\bar{d}}\\) To start, we have our raw data, where each row is a latte (1 observation), and for each group, we produced 10 lattes/observations (sometimes called 10 replicates). We need to compute for each unique set of treatment conditions, what was the within-group mean (xbar), standard deviation sd, and sample size n? To get all unique combinations of machine, milk, syrup, and art that were tested, we can group_by() these variables, and then summarize() the mean(), sd(), and n() to get back a single row of statistics for each set of unique treatments. Well call this data.frame of summary statistics per set groups. # Let&#39;s make a data.frame &#39;groups&#39; groups = lattes %&gt;% # For each unique pairing of treatments tested, group_by(machine, milk, syrup, art) %&gt;% # Return one row containing these summary statistics summarize(xbar = mean(tastiness), s = sd(tastiness), n = n()) %&gt;% ungroup() # Check it out! groups %&gt;% head() ## # A tibble: 6 × 7 ## machine milk syrup art xbar s n ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 a oat monin foamy 43.3 6.60 10 ## 2 a oat monin heart 57.4 5.65 10 ## 3 a oat torani foamy 65.3 5.13 10 ## 4 a oat torani heart 83.2 4.85 10 ## 5 a skim monin foamy 45.0 4.65 10 ## 6 a skim monin heart 56.9 4.29 10 Next, we can compute the direct effect of a single variables treatment effect (eg. effect of machine b in the machine variable, compared to machine b), by taking (1) the mean tastiness of lattes produced by machine a (xbar[machine == \"a\"]), as well as (2) those produced by machine b (written as xbar[machine == \"b\"] below). We can do this several ways, some slower or faster, so Ill write out the slower-but-clearer method, followed by the faster-but-messier method. 14.1.2 Slower-but-Clearer Method of Getting the Difference of Grand Means \\(\\bar{\\bar{d}}\\) # Suppose you&#39;ve got a series of means `xbar` across multiple treatment groups. We can.... step1 = groups %&gt;% summarize( # put all means for treatment groups where machine == &quot;b&quot; into one column xbar_machine_b = xbar[machine == &quot;b&quot;], # put all means for treatment groups where machine == &quot;a&quot; into another column xbar_machine_a = xbar[machine == &quot;a&quot;]) # View it step1 %&gt;% head(3) ## # A tibble: 3 × 2 ## xbar_machine_b xbar_machine_a ## &lt;dbl&gt; &lt;dbl&gt; ## 1 22.4 43.3 ## 2 30.1 57.4 ## 3 31.8 65.3 Then step2 = step1 %&gt;% summarize( # Take the grand mean of each vector of means xbbar_machine_b = mean(xbar_machine_b), xbbar_machine_a = mean(xbar_machine_a), # Subtract the grand mean of a (control) from the grand mean of b (treatment) # It helps to record for yourself the variable and order of subtraction (eg. b_a = b - a) dbbar_machine_b_a = xbbar_machine_b - xbbar_machine_a) # Check it! step2 ## # A tibble: 1 × 3 ## xbbar_machine_b xbbar_machine_a dbbar_machine_b_a ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 32.2 62.6 -30.4 14.1.3 Faster-but-Messier Method of Getting the Difference of Grand Means \\(\\bar{\\bar{d}}\\) The method discussed above is slower but clean and clear. On the other hand, if we need to make a lot of dbbar statistics for different groups, it could be helpful to speed this process up. Heres a way to do that. Well use this below a bunch, so be sure to try both methods out yourself so you fully understand how they work. # Taking our group means xbar, groups %&gt;% # Take the grand mean of means from machine b and subtract the grand mean of means from machine a. summarize(dbar_machine_b_a = mean(xbar[machine == &quot;b&quot;]) - mean(xbar[machine == &quot;a&quot;])) ## # A tibble: 1 × 1 ## dbar_machine_b_a ## &lt;dbl&gt; ## 1 -30.4 # Boom! All in one line - Shazam! 14.1.4 Estimating Many Difference of Grand Means \\(\\bar{\\bar{d}}\\) Using the technique we learned above, lets estimate many difference of grand means \\(\\bar{\\bar{d}}\\) all at once! We can estimate direct effects by type of (1) machine, (2) milk, (3) syrup, and (4) art on the quality of our lattes (tastiness) by calculating overall difference of means statistics for each possible pairing of machine (a vs. b), milk (skim vs. oat, skim vs. whole, and whole vs. skim), syrup (torani vs. monin), and art (heart vs. foamy). Notice also that when the number of treatments in a variable is 3 or greater (eg. milk contains oat, skim, and whole), it is no longer sufficient to just get the difference of grand means between 2 categories; we now can compute not just 1 \\(\\bar{\\bar{d}}\\) statistic, but 3! Lets do it! dbbar = groups %&gt;% # Calculate a single line of summary statistics dbbar (d-double-bar, the difference of grand means) summarize( # How much tastier is a latte from machine &quot;b&quot; than machine &quot;a&quot;, on average? machine_b_a = mean(xbar[machine == &quot;b&quot;]) - mean(xbar[machine == &quot;a&quot;]), # How much tastier is an oat milk latte than a skim milk latte, on average? milk_oat_skim = mean(xbar[milk == &quot;oat&quot;]) - mean(xbar[milk == &quot;skim&quot;]), # How much tastier is an oat milk latte than a whole milk latte, on average? milk_oat_whole = mean(xbar[milk == &quot;oat&quot;]) - mean(xbar[milk == &quot;whole&quot;]), # How much tastier is a skim milk latte than a whole milk latte, on average? milk_skim_whole = mean(xbar[milk == &quot;skim&quot;]) - mean(xbar[milk == &quot;whole&quot;]), # How much tastier is a latte with Torani brand syrup than with Monin brand syrup, on average? syrup_torani_monin = mean(xbar[syrup == &quot;torani&quot;]) - mean(xbar[syrup == &quot;monin&quot;]), # How much tastier is a latte with a &#39;heart&#39; foam art, than just a normal foamy latte, on average? art_heart_foamy = mean(xbar[art == &quot;heart&quot;]) - mean(xbar[art == &quot;foamy&quot;]) ) # Glimpse the results! dbbar %&gt;% glimpse() ## Rows: 1 ## Columns: 6 ## $ machine_b_a &lt;dbl&gt; -30.37756 ## $ milk_oat_skim &lt;dbl&gt; -1.447917 ## $ milk_oat_whole &lt;dbl&gt; 0.2062967 ## $ milk_skim_whole &lt;dbl&gt; 1.654213 ## $ syrup_torani_monin &lt;dbl&gt; 18.81986 ## $ art_heart_foamy &lt;dbl&gt; 11.73731 Figure 14.1: Does it matter if it has a heart? Photo Credit to Olivia Anne Snyder 14.2 Standard Errors for \\(\\bar{\\bar{d}}\\) But how much would those effects \\(\\bar{\\bar{d}}\\) vary simply due to random sampling error, had they been drawn from a slightly different sample? We need to estimate a standard error around the sampling distribution of these statistics. For that, well need some overall estimate of variance and sample size shared across all these \\(\\bar{\\bar{d}}\\) statistics. 14.2.1 Estimating Standard Error when \\(s^2_i\\) is known If we suppose that the variances of each group are unequal (they probably are), then we can calculate a single standard error by taking the sum of the average variance within each group, then taking the square root of that total average variance. To do so, we must know the variance \\(s^2_i\\) for every treatment group \\(i\\) in our dataset. That can be written as: \\[ standard \\ error \\ \\sigma = \\sqrt{ \\sum_{i=1}^{n}{ \\frac{ s^{2}_i }{n_{i} } } } = \\sqrt{ \\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2} + ... \\frac{s^2_n}{n_n} } \\\\ where \\ i = group, \\ n = group \\ sample \\ size, \\\\ s^2_i = variance \\ of \\ group \\ i \\] We can code this extremely quickly as: error = groups %&gt;% # calculate a single row of statistics overall summarize( # If we know the standard deviation and sample size of each group # we can estimate the standard error directly se = sqrt( sum(s^2 / n) ) ) # Check it! error ## # A tibble: 1 × 1 ## se ## &lt;dbl&gt; ## 1 8.39 14.2.2 Using Pooled Variance to Estimate Standard Errors On the other hand, if we have reason to believe that the variances of each group are more or less equal, we can simplify this equation, calculating just a single pooled variance \\(s^2_i\\) that gets applied throughout, like so: \\[ pooled \\ variance = s^{2}_{p} = \\frac{ \\sum_{i=1}^{n}{ s^2_{i} \\times v_i } }{ \\sum_{i=1}^{n}{v_i} }, \\\\ where \\ i = group, \\\\ v_i = n_i - 1 = degrees \\ of \\ freedom \\ of \\ group \\ i, \\\\ s^2_i = variance \\ of \\ group \\ i, \\\\ assuming \\ equal \\ variance \\] Then, we could calculate the standard error with need for even fewer statistics, as: $ = $. With R, the calculations are so quick for unequal variances that it is no longer especially helpful to assume equal variances. But, if you ever find yourself without the specific subgroup variances and need to estimate a standard error, you can use the equal variance formula for pooled variances to find a standard error. We could code it like this: error = groups %&gt;% # Get degrees of freedom for each group... mutate(df = n - 1) %&gt;% # then calculate a single row of statistics overall summarize( # If we assume the variance is constant across groups (it probably isn&#39;t) # we can take the weighted average of variances, weighted by group degrees of freedom var_pooled = sum(s^2 * df) / sum(df), s_pooled = var_pooled %&gt;% sqrt(), # Finally, get the standard error! se = sqrt(s_pooled^2 * sum(1 / n)) ) # Take a peek! error ## # A tibble: 1 × 3 ## var_pooled s_pooled se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 29.3 5.41 8.39 Our standard errors assuming and not assuming equal variances end up pretty similar, so it looks like our variances must have been pretty equal across groups. 14.3 Pivoting Data with pivot_longer() &amp; pivot_wider() Lets be honest - that was a lot of typing, and though it was fast, it got a little old! Good news: The tidyr and dplyr packages, which get loaded automatically with the tidyverse package, includes several functions for pivoting your data - meaning rearranging it quickly. 14.3.1 contains() select(contains(\"string\")) in dplyr lets you grab all variables in a data.frame that contain the following \"string\". For example: dbbar %&gt;% select(contains(&quot;milk&quot;)) ## # A tibble: 1 × 3 ## milk_oat_skim milk_oat_whole milk_skim_whole ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.45 0.206 1.65 14.3.2 pivot_longer() pivot_longer() takes a set of column vectors cols = c(column1, column2, etc.), then stacks them on top of each other, sending the names of these columns to a names vector and the values of these columns to a values vector. It pairs well with contains(\"string\"). dbbar %&gt;% select(contains(&quot;milk&quot;)) %&gt;% # We can pivot these columns... pivot_longer(cols = c( milk_oat_skim, milk_oat_whole, milk_skim_whole) ) ## # A tibble: 3 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 milk_oat_skim -1.45 ## 2 milk_oat_whole 0.206 ## 3 milk_skim_whole 1.65 dbbar %&gt;% select(contains(&quot;milk&quot;)) %&gt;% # We can also just use contains to write a short hand pivot_longer(cols = c( contains(&quot;milk&quot;)) ) ## # A tibble: 3 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 milk_oat_skim -1.45 ## 2 milk_oat_whole 0.206 ## 3 milk_skim_whole 1.65 Generally, I like to specify a set of column vectors NOT to pivot, which sends all other vectors to the names and values columns. This is much faster! We can write cols = -c(whatever), which says send all variables except whatever. You could even write cols = -c(), which just sends all the columns. # For example... dbbar %&gt;% select(contains(&quot;milk&quot;)) %&gt;% # pivot all columns EXCEPT milk_oat_skim pivot_longer(cols = -c(milk_oat_skim)) ## # A tibble: 2 × 3 ## milk_oat_skim name value ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 -1.45 milk_oat_whole 0.206 ## 2 -1.45 milk_skim_whole 1.65 # My favorite: dbbar %&gt;% select(contains(&quot;milk&quot;)) %&gt;% pivot_longer(cols = -c()) ## # A tibble: 3 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 milk_oat_skim -1.45 ## 2 milk_oat_whole 0.206 ## 3 milk_skim_whole 1.65 Finally, if you dont select() anything, you can pivot all the variables at once! (as long as they are all numeric or all character vectors). # If you don&#39;t select, you can get **all** the variables too! dbbar %&gt;% pivot_longer(cols = -c()) ## # A tibble: 6 × 2 ## name value ## &lt;chr&gt; &lt;dbl&gt; ## 1 machine_b_a -30.4 ## 2 milk_oat_skim -1.45 ## 3 milk_oat_whole 0.206 ## 4 milk_skim_whole 1.65 ## 5 syrup_torani_monin 18.8 ## 6 art_heart_foamy 11.7 14.3.3 pivot_wider() Finally, sometimes we need to spread out values from a long, tidy format into a wider, matrix format! For example, we did this earlier when getting the xbar values for machine a and machine b in their own columns, by typing xbar[machine == \"a\"], etc. But this takes a while, whereas pivot_wider() can do it quickly, all in one line. Its up to you which to use, but pivot_wider() can make your life a lot easier! When you use pivot_wider(), we need to provide a set of id_cols, which is a vector of unique id columns we will use to make the rows in our data. For example, if we want to pivot the machine and xbar columns into an xbar_a and xbar_b column, we need to tell R that each row should refer to an xbar value for lattes of a specific milk, syrup, and art, by saying id_cols = c(milk, syrup, art). We then can tell pivot_wider(), grab the vector names_from the machine vector and the values_from the xbar vector. # Check it out! groups %&gt;% pivot_wider(id_cols = c(milk, syrup, art), names_from = machine, values_from = xbar) %&gt;% head() ## # A tibble: 6 × 5 ## milk syrup art a b ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 oat monin foamy 43.3 22.4 ## 2 oat monin heart 57.4 30.1 ## 3 oat torani foamy 65.3 31.8 ## 4 oat torani heart 83.2 42.4 ## 5 skim monin foamy 45.0 24.8 ## 6 skim monin heart 56.9 28.7 14.4 Visualizing &amp; Reporting Treatment Effects Using the dbbar data.frame from section 1 and error data.frame from section 2 above, we can report and visualize the overall treatment effects. 14.4.1 Building a Table of Direct Effects # Compile our direct effects of treatments direct = dbbar %&gt;% # let&#39;s pivot our data, pivot_longer( cols = -c(), # specifying names for our &#39;name&#39; and &#39;value&#39; columns names_to = &quot;term&quot;, values_to = &quot;dbbar&quot;) %&gt;% # Let&#39;s add in our singular standard error shared across experiments mutate(se = error$se) %&gt;% # And then estimate a 95% confidence interval! mutate(lower = dbbar - qnorm(0.975) * se, upper = dbbar + qnorm(0.975) * se) %&gt;% # And then write up a nice label mutate(label = paste(round(dbbar, 2), &quot;\\n(&quot;, round(se, 2), &quot;)&quot;, sep = &quot;&quot;)) # Let&#39;s view them! direct ## # A tibble: 6 × 6 ## term dbbar se lower upper label ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 machine_b_a -30.4 8.39 -46.8 -13.9 &quot;-30.38\\n(8.39)&quot; ## 2 milk_oat_skim -1.45 8.39 -17.9 15.0 &quot;-1.45\\n(8.39)&quot; ## 3 milk_oat_whole 0.206 8.39 -16.2 16.6 &quot;0.21\\n(8.39)&quot; ## 4 milk_skim_whole 1.65 8.39 -14.8 18.1 &quot;1.65\\n(8.39)&quot; ## 5 syrup_torani_monin 18.8 8.39 2.38 35.3 &quot;18.82\\n(8.39)&quot; ## 6 art_heart_foamy 11.7 8.39 -4.70 28.2 &quot;11.74\\n(8.39)&quot; 14.4.2 Visualization Strategies We can visualize these using geom_linerange(), geom_errorbar(), or geom_crossbar(). Lets take a peek. # You could use geom_crossbar, which uses ymin, ymax, and y direct %&gt;% ggplot(mapping = aes(x = term, y = dbbar, ymin = lower, ymax = upper)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_crossbar(fill = &quot;steelblue&quot;) # Or with geom_linerange(), which also uses ymin and ymax direct %&gt;% ggplot(mapping = aes(x = term, y = dbbar, ymin = lower, ymax = upper)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_linerange(size = 1) + geom_point(size = 3) Or. # You could use geom_errorbar, which uses ymin, ymax, plus an optional width direct %&gt;% ggplot(mapping = aes(x = term, y = dbbar, ymin = lower, ymax = upper)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_errorbar(width = 0.25) + geom_point() 14.4.3 An Excellent Visual But it we wanted to make a prettier chart, we could add color and labels, like so! direct %&gt;% ggplot(mapping = aes(x = term, y = dbbar, ymin = lower, ymax = upper, # specify a label and fill variable label = label, fill = dbbar)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_crossbar() + # Add labels geom_label(fill = &quot;white&quot;) + # Add a color pallet scale_fill_gradient2(low = &quot;red&quot;, mid = &quot;white&quot;, high = &quot;blue&quot;) + # Add x and y axis labels labs(y = &quot;Difference of Means across Groups&quot;, x = &quot;Treatment Groups&quot;, fill = &quot;Effect&quot;) + # Relabel our axis ticks scale_x_discrete( # Each label should take the original value and map it to a new, more readable version labels = c(&quot;syrup_torani_monin&quot; = &quot;Torani vs. Monin&quot;, &quot;milk_skim_whole&quot; = &quot;Skim Milk - Whole Milk&quot;, &quot;milk_oat_whole&quot; = &quot;Oatmilk - Whole Milk&quot;, &quot;milk_oat_skim&quot; = &quot;Oatmilk - Skim Milk&quot;, &quot;machine_b_a&quot; = &quot;Machine B - A&quot;, &quot;art_heart_foamy&quot; = &quot;Heart - Foam Art&quot;)) + # Flip the scale coord_flip() + # Add a theme theme_classic(base_size = 14) 14.5 Interaction Effects Alternatively, we might want to estimate treatment effects for combinations of treatments. 14.6 Estimating 2-way interactions For example, do these lattes tend to taste better specifically when both conditions are true? Eg. lets test the effect of having a latte from Machine B with Torani syrup, by comparing it against lattes made from Machine A with Monin syrup. This demonstrates the value added of both conditions. groups %&gt;% summarize( xbbar_b_torani = xbar[machine == &quot;b&quot; &amp; syrup == &quot;torani&quot;] %&gt;% mean(), xbbar_a_monin = xbar[machine == &quot;a&quot; &amp; syrup == &quot;monin&quot;] %&gt;% mean(), dbbar = xbbar_b_torani - xbbar_a_monin, # Finally, we would use the same shared standard error that we calculated before se = error$se) ## # A tibble: 1 × 4 ## xbbar_b_torani xbbar_a_monin dbbar se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 38.4 50.0 -11.6 8.39 14.7 Estimating 3-way interactions We could even calculate a three-way interaction between machine type, syrup brand, and latte art, like so: groups %&gt;% summarize( # This time, we just add `art == &quot;heart&quot;` as an extra condition, and vice versa below xbbar_b_torani_heart = xbar[machine == &quot;b&quot; &amp; syrup == &quot;torani&quot; &amp; art == &quot;heart&quot;] %&gt;% mean(), xbbar_a_monin_foamy = xbar[machine == &quot;a&quot; &amp; syrup == &quot;monin&quot; &amp; art == &quot;foamy&quot;] %&gt;% mean(), dbbar = xbbar_b_torani_heart - xbbar_a_monin_foamy, # Finally, we would use the same shared standard error that we calculated before se = error$se) ## # A tibble: 1 × 4 ## xbbar_b_torani_heart xbbar_a_monin_foamy dbbar se ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 43.1 43.2 -0.123 8.39 But we can see that these threeway effects tend can often be quite small relative to the standard error. It is difficult to find statistically significant three-way effects. 14.8 Estimating Interactions in lm() A quick strategy for estimating interaction effects in R can be using the lm() function. 14.8.1 Modeling Interactions While it uses a regression model, rather than the difference of means, to estimate these effects, it tends to be pretty brief and doesnt require making our groups, dbbar, or error data.frames. # Using our raw data of observed lattes m = lattes %&gt;% # Make a linear model, showing interactions with the &#39;*&#39; sign lm(formula = tastiness ~ machine * milk * syrup * art) # View our model! m %&gt;% tidy() ## # A tibble: 24 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 43.3 1.71 25.3 1.64e-66 ## 2 machineb -20.9 2.42 -8.63 1.37e-15 ## 3 milkskim 1.67 2.42 0.689 4.92e- 1 ## 4 milkwhole -1.90 2.42 -0.786 4.33e- 1 ## 5 syruptorani 22.0 2.42 9.09 6.48e-17 ## 6 artheart 14.1 2.42 5.84 1.88e- 8 ## 7 machineb:milkskim 0.734 3.42 0.214 8.30e- 1 ## 8 machineb:milkwhole 1.82 3.42 0.532 5.95e- 1 ## 9 machineb:syruptorani -12.6 3.42 -3.69 2.80e- 4 ## 10 milkskim:syruptorani 1.20 3.42 0.350 7.27e- 1 ## #  14 more rows We see potentially significant interaction effects for the joint impact of machine b and torani syrup, as well as several strong direct effects. Three way effects though, are minimal at best. 14.8.2 Using predict() to visualize interaction effects The big power of lm() in factorial experiments is getting to visualize these effects. # We can generate a set of all observed combinations of our treatments as &#39;newdata newdata = lattes %&gt;% group_by(machine, milk, syrup, art) %&gt;% summarize() newdata %&gt;% head() ## # A tibble: 6 × 4 ## # Groups: machine, milk, syrup [3] ## machine milk syrup art ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a oat monin foamy ## 2 a oat monin heart ## 3 a oat torani foamy ## 4 a oat torani heart ## 5 a skim monin foamy ## 6 a skim monin heart Then, we can generate either \"confidence\" or \"prediction\" intervals - confidence intervals are a little narrower, used in estimating effects youve already observed, while prediction intervals are a little wider, used for estimating predicted quantities that you naturally want a little more caution around. I like to save this as an object pred, then extract the results in a second step. # For confidence intervals pred = predict(m, newdata = newdata, se.fit = TRUE, interval = &quot;confidence&quot;, level = 0.95) %&gt;% # Extract just the fitted results and intervals with(fit) %&gt;% # And convert it to a tibble as_tibble() # or for prediction intervals... #predict(m, newdata = newdata, # se.fit = TRUE, interval = &quot;prediction&quot;, level = 0.95) # See? Not very tidy pred %&gt;% head() ## # A tibble: 6 × 3 ## fit lwr upr ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 43.3 39.9 46.7 ## 2 57.4 54.1 60.8 ## 3 65.3 61.9 68.7 ## 4 83.2 79.9 86.6 ## 5 45.0 41.6 48.3 ## 6 56.9 53.5 60.2 Now, we can bind the two sets of values together, either using tibble() or bind_col(), because they share the same rows. cis = tibble( newdata, yhat = pred$fit, lower = pred$lwr, upper = pred$upr ) To visual an interaction effect, we can now narrow into 4 predictions, showing all possible combinations of machine and syrup, while zooming into just one type of milk and art, to hold these other concepts constant. viz = cis %&gt;% filter(machine %in% c(&quot;a&quot;, &quot;b&quot;), syrup %in% c(&quot;torani&quot;, &quot;monin&quot;), milk == &quot;whole&quot;, art == &quot;heart&quot;) %&gt;% # add a label! mutate(label = round(yhat, 2) ) # view it! viz ## # A tibble: 4 × 8 ## machine milk syrup art yhat lower upper label ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a whole monin heart 55.9 52.5 59.2 55.9 ## 2 a whole torani heart 86.8 83.4 90.1 86.8 ## 3 b whole monin heart 27.9 24.5 31.2 27.9 ## 4 b whole torani heart 43.2 39.8 46.6 43.2 And last, we can visualize the effect with geom_ribbon() or any other technique that accepts ymin and ymax. We just have to remember to put one variable on the x axis (eg. machine) and one variable as the group and fill, to distinguish our two lines. viz %&gt;% ggplot(mapping = aes(x = machine, group = syrup, fill = syrup, y = yhat, ymin = lower, ymax = upper, label = label)) + geom_ribbon() + geom_line() + # Plot the labels, bumping the text upwards! geom_text(nudge_y = 5) + labs(x = &quot;Espresso Machine Type&quot;, y = &quot;Predicted Tastiness (0-100) with 95% CIs&quot;, fill = &quot;Syrup Brand&quot;) + theme_classic(base_size = 14) How should we interpret this visual? Well, lets just describe what we see! More descriptive language is always better than less, when it comes to explaining results for new viewers. Heres an example descriptive! The figure above visualizes our model ms predicted values for latte tastiness with 95% confidence intervals, assuming those lattes were made with whole milk and had heart foam art, but their only differences were the machine that made them and the brand of syrup they used. We see that lattes made from espresso machine B tend to be much less tasty than lattes made from espress machine A, given either type of syrup brand. Torani syrups see a sharper decrease than Monin syrups, but lattes made from Torani syrups remain much tastier. The lines do not cross in the given treatment categories, indicating that, at least as far as these dichotomous categories are concerned, the interaction effect between machine type and syrup brand is statistically significant at a 95% level of confidence - that is, we are at least 95% certain that lattes from torani syrups and machine B are tastier than lattes from monin syrups and machine A. 14.8.3 Finding the best model with anova() We can even use the anova() function (not to be confused with aov()) to find the best fitting model from a set of model objects. Here, we compare the second model against the first (as opposed to the model against an intercept, as in traditional anova). Then, we calculate the residual sum of squares in both, calculate the extra explained sum of squares in the second model, and estimate an F statistic. If the F statistic is statistically significant and the RSS has decreased, we would say that our second model fits better. # Let&#39;s make a second model for comparison against the first. m2 = lattes %&gt;% # Make a linear model, showing interactions with the &#39;*&#39; sign lm(formula = tastiness ~ machine * milk * syrup) # Here, for instance, there is a significant difference, but the RSS increased, # so we know that model 1 is better than model 2 anova(m, m2) ## Analysis of Variance Table ## ## Model 1: tastiness ~ machine * milk * syrup * art ## Model 2: tastiness ~ machine * milk * syrup ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 216 6330 ## 2 228 16182 -12 -9852 28.015 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Conclusion In summary, Kims Coffee can rather quickly estimate the treatment effects of specific changes in their latte brewing process, as long as they randomly assign their lattes to different treatments and estimate their treatment effects with appropriate standard errors. Happy experimenting! "],["workshop-response-surface-methodology-in-r.html", "15 Workshop: Response Surface Methodology in R Getting Started 15.1 Models for RSM Learning Check 1 15.2 Contour Plots Learning Check 2 15.3 Iterate! Learning Check 3 15.4 Quantities of Interest in RSM 15.5 Extra Concepts of Interest: Canonical Form", " 15 Workshop: Response Surface Methodology in R Figure 1.2: RSM Contour Plots! Getting Started Imagine: A team of enterprising Systems Engineering students have decided to start their own baking company, selling gingerbread cookies in the month of December in the greater Ithaca area! None of them are particularly good at baking, but theyre rad good at design of experiments, so they set out to discover the ultimate gingerbread cookie through factorial design and our new tool, response surface methodology! Follow along below to see a surprisingly accurate example of how you can apply RSM to find the optimal design of your product (in this case, gingerbread!) Packages # Load packages! library(tidyverse) # dplyr and ggplot! library(broom) # glance() and tidy()! library(viridis) # color palettes! install.packages(c(&quot;rsm&quot;, &quot;metR&quot;)) # you&#39;ll need to install these! library(rsm) # for RSM library(metR) # for contour plot labels in ggplot (#fig:img_intro)Gingerbread Cookies! Courtesy of Casey Chae @ Unsplash Our Data They know from past research that the amount of molasses and ginger in gingerbread cookies are likely significantly related to the overall tastiness (called the yum factor in our dataset). But, theyre not sure how much molasses and how much ginger are needed. Molasses can be somewhat expensive too, compared to other ingredients; so they want to optimize the amount of molasses necessary to produce the best cookies. So, holding all other conditions in the recipe constant, they ran a factorial experiment, making 16 batches of cookies (about 20 cookies per batch). In their experiment, they tested 4 different amounts of molasses, including \\(\\frac{1}{2}, \\ \\frac{3}{4}, \\ 1,\\ \\&amp; \\ 1 \\frac{1}{4}\\) cups of molasses. They also tested 4 different amounts of ginger, including \\(\\frac{1}{2}, \\ 1, \\ 1 \\frac{1}{2}, \\ \\&amp; \\ 2\\) tablespoons of ginger. Each batch was randomly assigned one of the 16 unique pairings of amounts of ginger and molasses; there are \\(4 \\times 4 = 16\\) unique ways to assign these ingredients, and they included them all to fully account for all the possibilities. Then, they randomly handed out cookies to folks on campus in exchange for them briefly ranking the yum factor of that cookie on a scale from 0 (disgusting!) to 100 (delicious!). Import Data They compiled their data in the following dataset. Read it in to help them analyze their data! This dataset includes the following variables: id: unique ID for each cookie (320 cookies!) batch: unique group ID for each batch of about 20 cookies. Outcome yum: numeric scale measuring deliciousness of cookie, from 0 (disgusting) to 100 (delicious) Predictors molasses: cups of molasses in batch: 0.75, 1, 1.25, or 1.5 cups. ginger: tablespoons of ginger in batch: 0.5, 1, 1.5, or 2 tablespoons. Fixed Conditions cinnamon: 1 tablespoon butter: 1 cup flour: 3 cups # Import our data cookies = read_csv(&quot;workshops/gingerbread_test1.csv&quot;) # Check it out! cookies %&gt;% glimpse() ## Rows: 320 ## Columns: 8 ## $ id &lt;dbl&gt; 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 9 ## $ batch &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4 ## $ yum &lt;dbl&gt; 5, 0, 13, 8, 9, 4, 1, 15, 10, 2, 3, 0, 5, 5, 8, 2, 0, 0, 8, 0 ## $ molasses &lt;dbl&gt; 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0 ## $ ginger &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2 ## $ cinnamon &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ## $ butter &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ## $ flour &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3 15.1 Models for RSM In our previous workshop, we learned to calculate the difference of means for any set of groups (in this case, batch of cookies). When we get many different levels in our predictors (eg. not just a treatment and control), we might prefer to use lm() to estimate a linear model of our outcome (yum), rather than computing the difference of means many times. 15.1.1 Specifying an Interaction Model with Polynomials However, we will see quickly that different specifications of our model may work better than others. Were going to try three different models that predict yum using cups of molasses and tablespoons of ginger, and well evaluate the \\(R^{2}\\) of each (% of variation in outcome yum explained by model). These include: Basic First-Order Polynomial Model, where: \\[ Yum = \\alpha + \\beta_{m} X_{m} + \\beta_{g} X_{g} \\] \\(B_{m}\\) is the effect of a 1 cup increase of Molasses. \\(B_{g}\\) is the effect of a 1 tablespoon increase of Ginger. m1 = cookies %&gt;% lm(formula = yum ~ molasses + ginger) Interaction Model, where: \\[ Yum = \\alpha + \\beta_{m} X_{m} + \\beta_{g} X_{g} + \\beta_{mg} X_{m} X_{g} \\] \\(B_{mg}\\) is the interaction effect as molasses increases by 1 cup AND ginger increases by 1 tablespoon. m2 = cookies %&gt;% lm(formula = yum ~ molasses * ginger) # Also written manually as: # yum ~ molasses + ginger + I(molasses * ginger) Second-Order Polynomial Model with Interaction, where: \\[ Yum = \\alpha + \\beta_{m} X_{m} + \\beta_{g} X_{g} + \\beta_{mg} X_{m} X_{g} + \\beta_{m^{2}} X_{m}^{2} + \\beta_{g^{2}} X_{g}^{2} \\] - \\(\\beta{m^2} X_{m}^{2}\\) is the effect as the square of molasses increases by 1. Together, \\(\\beta_{m} X_{m}\\) and \\(\\beta_{m^2} X_{m}^{2}\\) act as a polynomial term predicting yum. # Add a polynomial by to your existing interactions by using I(variable^2) m3 = cookies %&gt;% lm(formula = yum ~ molasses * ginger + I(molasses^2) + I(ginger^2)) To review, this model equation can be viewed by just checking m3$coefficients. We can also write it out below; Ive rounded to 2 decimal places for simplicity below. \\[ \\hat{yum} = \\hat{Y} = 35.18 + -50.05 X_{m} + 34.15 X_{m}^{2} + -5.06 X_{g} + 4.66 X_{g}^{2} + -7.13 X_{m} X_{g} \\] Lets evaluate the r.squared of our three models below using the glance() function from the broom package, and bind together those data.frames into one using bind_rows() from dplyr. We see that the polynomial terms dramatically improve the predictive power of our model, jumping from \\(R^{2}\\) = 0.03 to \\(R^{2}\\) = 0.14. bind_rows( glance(m1), glance(m2), glance(m3) ) ## # A tibble: 3 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.00167 -0.00463 7.30 0.265 0.767 2 -1089. 2185. 2200. ## 2 0.0252 0.0159 7.22 2.72 0.0447 3 -1085. 2180. 2198. ## 3 0.137 0.123 6.82 9.97 0.00000000719 5 -1065. 2145. 2171. ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; Not really amazing quality model fit here - and that does happen! We can tidy() our model m3 to confirm. The low p.value for many of our predictors tells us that our predictors do tend to have statistically significant relationships with the yum factor of our cookies. (Admittedly, gingers direct effect is not very significant - just ~75% confidence). But, it looks like other factors not currently in our model might also impact yum factor. m3 %&gt;% tidy() ## # A tibble: 6 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 35.2 5.53 6.36 7.24e-10 ## 2 molasses -50.1 11.2 -4.48 1.07e- 5 ## 3 ginger -5.06 4.42 -1.15 2.53e- 1 ## 4 I(molasses^2) 34.1 6.10 5.60 4.69e- 8 ## 5 I(ginger^2) 4.66 1.52 3.06 2.42e- 3 ## 6 molasses:ginger -7.13 2.44 -2.92 3.71e- 3 15.1.2 Modeling with rsm() Finally, we can also write this model using the rsm() function in the rsm package. It works very similarly to lm(), but has some shortcut tricks built it. Well use it more later in the workshop. For now, lets make some rsm model objects to match our lm model terms exactly. Lets make a simple First-Order polynomial model with FO(). That means just one term per predictor (eg. no \\(x^2\\), just \\(x\\)). Our r1 model will match our m1 model. r1 = cookies %&gt;% rsm(formula = yum ~ FO(molasses, ginger)) # Check it! r1$coefficients ## (Intercept) FO(molasses, ginger)molasses ## 13.67312 0.79500 ## FO(molasses, ginger)ginger ## 0.35250 # This is the same as m1 r1$coefficients == m1$coefficients ## (Intercept) FO(molasses, ginger)molasses ## TRUE TRUE ## FO(molasses, ginger)ginger ## TRUE Lets make a more complex Second-Order polynomial model with SO(). That means just two terms per predictor (eg. \\(x\\) and \\(x^2\\)), as well as an interaction effect (called TWI() for two-way interaction). # Let&#39;s make a &#39;Second-Order&#39; model with SO() r3 = cookies %&gt;% rsm(formula = yum ~ SO(molasses, ginger)) # Check it! r3$coefficients ## (Intercept) FO(molasses, ginger)molasses ## 35.17875 -50.05250 ## FO(molasses, ginger)ginger TWI(molasses, ginger) ## -5.06325 -7.13200 ## PQ(molasses, ginger)molasses^2 PQ(molasses, ginger)ginger^2 ## 34.15000 4.66250 # These coefficients match our m3$coefficients too. # FO(...)molasses = molasses # FO(...)ginger = ginger # PQ(...)molasses^2 = I(molasses^2) # PQ(...)ginger^2 = I(ginger^2) # TWI = molasses:ginger m3$coefficients ## (Intercept) molasses ginger I(molasses^2) I(ginger^2) ## 35.17875 -50.05250 -5.06325 34.15000 4.66250 ## molasses:ginger ## -7.13200 15.1.3 Transforming Variables By default, linear models estimate linear relationships between predictors and outcomes, but many relationship are indeed not linear! Here are 8 ways we might model associations! Figure 15.1: 8 Common x~y Modeling Strategies A logit function can sometimes help - that is designed for when a variable ranges between 0 and 1; we could write a classic logit as logit = function(p){ log(p / (1 - p) ) }. # Write a custom logit function for data from 0 to 100 logit = function(p){ log( p / (1 - p) ) } # Notice how it ONLY accepts our positive values greater than 0 and less than 1? c(-1, 0, 0.1, 0.2, 0.5, 1, 2) %&gt;% logit() ## [1] NaN -Inf -2.197225 -1.386294 0.000000 Inf NaN Lets try a few of these strategies for our x and y variables, and see if any of them improve our predictive power (\\(R^{2}\\)). Spoiler alert: In our data, they dont but in other datasets, they very well might! Always a good thing to check. For example, we can try transforming the outcome variable, using a standard linear trend (business as usual), a log transformation, or a square root transformation. # Linear (normal) cookies %&gt;% lm(formula = yum ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.137 0.123 6.82 9.97 0.00000000719 5 -1065. 2145. 2171. ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.137 # Logged (add 1 since yum contains 0s) cookies %&gt;% lm(formula = log(yum + 1) ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.110 0.0963 0.707 7.80 0.000000616 5 -340. 694. 721. ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.11 (Worse) # Square Root cookies %&gt;% lm(formula = sqrt(yum) ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.125 0.111 1.13 9.00 0.0000000522 5 -489. 992. 1018. ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.125 (Worse) Alternatively, we could try transforming the predictor variables, using a log-transformation. cookies %&gt;% lm(formula = yum ~ log(molasses) * log(ginger) + I(log(molasses)^2) + I(log(ginger)^2)) %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.173 0.160 6.67 13.2 1.17e-11 5 -1058. 2131. 2157. ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; It turns out that few of these transformations really dramatically change the predictive power of the model, so Ill stick with our original models m3/r3 for the time being. Figure 15.2: A poorly predicted Gingerbread Cookie Photo by Noelle Otto Learning Check 1 Question What happens when you (1) square \\(y\\), (2) cube \\(y\\), or (3) take the logit of \\((y + 1) / 100\\)? Find the r.squared for each of these models. [View Answer!] Looks like a linear, business-as-usual modeling strategy for our outcome variable \\(y\\) (yum) is best for this data. # Logit cookies %&gt;% lm(formula = log( (yum + 1)/100 / (1 - (yum + 1)/100) ) ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.115 0.101 0.779 8.19 0.000000274 5 -371. 756. 782. ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.115 (Worse) # Squared cookies %&gt;% lm(formula = I(yum^2) ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.119 0.105 208. 8.46 0.000000158 5 -2160. 4333. 4360. ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.118 (Worse) # Cubed cookies %&gt;% lm(formula = I(yum^3) ~ molasses * ginger + I(molasses^2) + I(ginger^2)) %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0913 0.0769 6147. 6.31 0.0000134 5 -3243. 6499. 6526. ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # R2 = 0.091 (Worse) 15.2 Contour Plots So now that we have this model, what do we do with it? Response Surface Methodology refers to using statistical models to predict an outcome (a.k.a. response variable) given a series of varying conditions. This lets us predict and visualize the full range/surface for that outcome. 15.2.1 Simple contour() plots The easiest way to think of this is in 3-dimensions, meaning 3 variables (1 outcome and 2 predictors). A regression model traditionally finds us the plane of best fit when looking at 3 dimensions, or the hyperplane of best fit when looking at +4 dimensions. However, when we use polynomial terms in our model equation, we can map that plane almost perfectly to our observed data, creating more of a contour or topographical surface than a simple plane. We can use our model object m3 or r3 from above to generate a contour plot, predicting the yum factor (shown by color and lines) while we varying ~molasses + ginger levels. We can add a heatmap by saying image = TRUE. Our model predicts that middling levels of ginger and molasses produce a kind of sad coldspot where the yum factor is about 11 (middle), but our model projects the yum factor will increase when you increase ginger and/or molasses from that center amount. contour(m3, ~molasses + ginger, image = TRUE) Thats beautiful - but a little unclear how it was produced! How could we make that plot ourselves in ggplot? 15.2.2 geom_tile() plots However, weve learned this term that ggplot can give us greater flexibility when designing and communicating information, so how would we make this in ggplot? Its really quick! We need to (1) make a grid of predictor values with expand_grid() to feed to predict(), (2) extract the predicted yum values (usually called yhat), and (3) then pipe the result to ggplot! # Let&#39;s check the range of our predictors... cookies$molasses %&gt;% range() ## [1] 0.50 1.25 cookies$ginger %&gt;% range() ## [1] 0.5 2.0 # Great! So we could vary our ingredient amounts from 0 to ~5 while still being realistic. Step 1: Well use expand_grid() to build a grid of molasses and ginger values, called myx, where molasses spans its observed range and ginger spans its own observed range. # Make the grid of conditions! myx = expand_grid( molasses = seq(from = min(cookies$molasses), to = max(cookies$molasses), length.out = 50), ginger = seq(from = min(cookies$ginger), to = max(cookies$ginger), length.out = 50) ) # Optionally, you could pick some arbitrary ranges, like 0 to 5 #myx = expand_grid( # molasses = seq(from = 0, to = 5, length.out = 50), # ginger = seq(from = 0, to = 3, length.out = 50) #) # Check it out! myx %&gt;% glimpse() ## Rows: 2,500 ## Columns: 2 ## $ molasses &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0 ## $ ginger &lt;dbl&gt; 0.5000000, 0.5306122, 0.5612245, 0.5918367, 0.6224490, 0.6530 Note: You have to pick these values!! (eg. 0 to 5, 0 to 3, etc.) contour() uses the min and max of molasses and ginger each, but often, we want to make predictions slightly beyond our observed data. Just remember, a grid of 20 by 20 items produces 400 cells; 100 by 100 produces 10,000 cells; etc. Once you get above a few 1000, ggplot starts to slow down quickly. Step 2: Next, well mutate() our myx data.frame to add a column yhat. In that column, we predict() the yum factor for those conditions based on our model m3 (or r3 - either work). As shown in previous workshops, we must give predict() a data.frame containing hypothetical values of each predictor in our model, called newdata. Well save the result in a data.frame called mypred. # Make predictions! mypred = myx %&gt;% mutate(yhat = predict(m3, newdata = tibble(molasses, ginger))) # Check it out! mypred %&gt;% glimpse() ## Rows: 2,500 ## Columns: 3 ## $ molasses &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0 ## $ ginger &lt;dbl&gt; 0.5000000, 0.5306122, 0.5612245, 0.5918367, 0.6224490, 0.6530 ## $ yhat &lt;dbl&gt; 15.54100, 15.42394, 15.31561, 15.21603, 15.12518, 15.04308, 1 Step 3: Finally, well visualize it using geom_tile(), which maps a fill (yhat) to every x (molasses) and y (ginger) coordinate. # Voila! g1 = ggplot() + geom_tile(data = mypred, mapping = aes(x = molasses, y = ginger, fill = yhat)) + scale_fill_viridis(option = &quot;plasma&quot;) # View it! g1 This matches the same pattern from our contour() plot, and was pretty painless! 15.2.3 Pretty geom_contour() plots! But can we make this prettier and clearer for our reader? ggplot includes a geom_contour() function that will plot the contour outlines on top. A few requirements: In the aes(), geom_contour() requires you to make x, y, and z aesthetics, where z is the predicted value yhat. You can set the number of bins (eg. bins = 10 intervals) OR the binwidth (eg. binwidth = 10, where each interval is 10 units of yhat wide). [Just like with cut_interval()!] # Using bins.... # Add contour lines, where each line is 1 unit apart on the `yum` factor scale! g1 + geom_contour(data = mypred, mapping = aes(x = molasses, y = ginger, z = yhat), color = &quot;white&quot;, bins = 10) # Using binwidth... # Add contour lines, where each line is 1 unit apart on the `yum` factor scale! g1 + geom_contour(data = mypred, mapping = aes(x = molasses, y = ginger, z = yhat), color = &quot;white&quot;, binwidth = 1) Alternatively, we could do this all in one fell swoop, using geom_contour_fill(), which combines geom_tile() and geom_contour() together. (Note: geom_contour_filled() is a different function. You want _fill(), not _filled().) g2 = ggplot() + # Make a filled contour plot, with a binwidth of 1 geom_contour_fill(data = mypred, mapping = aes(x = molasses, y = ginger, z = yhat), binwidth = 1, color = &quot;white&quot;) + scale_fill_viridis(option = &quot;plasma&quot;) # View it! g2 Finally, some coding wizards out there developed some ggplot add-on functions in the metR package that will let us add nice labels to our contour plots, using geom_label_contour(). We supply it the same information as geom_contour_filled(), including x, y, and z vectors Optional: Tell it to skip = 0 lines, labeling every contour line. Optional: Tell it to add a white border around our text with stroke.color = \"white\", where that border is stroke = 0.2 points wide. Optional: If you add label.placer = label_placer_n(1), it will label each contour line n = 1 time. All other rules of ggplot apply, eg. size, text color, and alpha transparency. Note: you must have loaded the metR package for this to work. g2 + geom_text_contour(data = mypred, mapping = aes(x = molasses, y= ginger, z = yhat), skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, label.placer = label_placer_n(1)) Beautiful! 15.2.4 One-step RSM in ggplot Finally, lets practice doing this all in one code chunk in ggplot. # Make our predictors... - this time let&#39;s expand the range mypred2 = expand_grid( molasses = seq(from = 0, to = 4, length.out = 50), ginger = seq(0, to = 4, length.out = 50) ) %&gt;% mutate(yhat = predict(m3, newdata = tibble(molasses, ginger))) mypred2 %&gt;% # Set aesthetics ggplot(mapping = aes(x = molasses, y = ginger, z = yhat)) + # Make a filled contour with 15 bins geom_contour_fill(bins = 15, color = &quot;white&quot;) + # Add labels geom_text_contour(skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, label.placer = label_placer_n(1)) + # Add a beautiful plasma viridis palette scale_fill_viridis(option = &quot;plasma&quot;) + # Add theming and labels theme_classic(base_size = 14) + theme(axis.line = element_blank()) + # get rid of axis line labs(x = &quot;Molasses (cups)&quot;, y = &quot;Ginger (tablespoons)&quot;, fill = &quot;Predicted\\nYum\\nFactor&quot;, subtitle = &quot;Contour of Predicted Gingerbread Cookie Tastiness&quot;) Excellent! Our plot can serve as a visual diagnostic. Tentatively, our model results suggest that increasing molasses may lead to considerable gains in our outcome, with ginger contributing some impact early on. Notably, we see that though our actual outcomes measurement ranged from 0 to 100, our predictions might exceed those limits. 15.2.5 More Realistic Plots Even though transformations dont improve our predictive accuracy, they might make our predictions more realistic. Lets try a few transformations. A logit() transformation could help with bounding yum to 0 and 1, if we scale down yum from 0-100 to 0-1. Well have to add +1 to the yum scale though, because some cookies got a score of zero, which can be logit-transformed. A log() transformation to molasses and ginger could help with bounding these conditions to only positive values, since we know we need at least a little of each, and we cant have negative ginger. # Write a quick adjusted logit function adj_logit = function(p){ p = (p + 1) / 100 # adjust p from 0 - 100 to 0 - 1 log(p / (1 - p)) # logit transformation } # Transform outcome and predictors m4 = cookies %&gt;% lm(formula = adj_logit(yum) ~ log(molasses) * log(ginger) + I(log(molasses)^2) + I(log(ginger)^2)) # Get conditions and predictions mypred3 = expand_grid( molasses = seq(from = 0.01, to = 4, length.out = 50), ginger = seq(0.01, to = 4, length.out = 50) ) %&gt;% mutate(yhat = predict(m4, newdata = tibble(molasses, ginger)), # Undo the logit transformation! yhat = exp(yhat) / (1 + exp(yhat)), # Undo the (y + 1) / 100 transformation yhat = 100*yhat - 1) # Visualize it! g3 = mypred3 %&gt;% ggplot(mapping = aes(x = molasses, y = ginger, z = yhat)) + geom_contour_fill(bins = 15, color = &quot;white&quot;) + geom_text_contour(skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, # Label each contour twice, but check_overlap deletes labels that overlap! label.placer = label_placer_n(2), check_overlap = TRUE) + scale_fill_viridis(option = &quot;plasma&quot;) + theme_classic(base_size = 14) + theme(axis.line = element_blank()) + # get rid of axis line labs(x = &quot;Molasses (cups)&quot;, y = &quot;Ginger (tablespoons)&quot;, fill = &quot;Predicted\\nYum\\nFactor&quot;, subtitle = &quot;Contour of Predicted Gingerbread Cookie Tastiness&quot;) g3 Ta-da! Now we have much more reasonable predictions, even though we lost 2% predictive power. Its always a trade-off between predictive power and our ability to generate reasonable, useful quantities of interest. Ideally, lets get a much better \\(R^{2}\\)! Learning Check 2 Suppose we expanded our factorial experiment based on this contour plot, adding more permutations of molasses and ginger, such that we now have 1280 cookies under test! Weve saved this data in workshops/gingerbread_test2.csv. Question Generate a second-order polynomial model like m3 and visualize the contour plot in ggplot. How do our predictions change? cookies2 = read_csv(&quot;workshops/gingerbread_test2.csv&quot;) [View Answer!] # Check the range cookies$ginger %&gt;% range() ## [1] 0.5 2.0 cookies$molasses %&gt;% range() ## [1] 0.50 1.25 # Write a quick adjusted logit function adj_logit = function(p){ p = (p + 1) / 100 # adjust p from 0 - 100 to 0 - 1 log(p / (1 - p)) # logit transformation } # Transform outcome and predictors m_lc = cookies2 %&gt;% lm(formula = adj_logit(yum) ~ log(molasses) * log(ginger) + I(log(molasses)^2) + I(log(ginger)^2)) # Check the R2 (still terrible! whoops!) m_lc %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.140 0.136 0.619 41.3 1.75e-39 5 -1199. 2411. 2447. ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # Get conditions and predictions mypred_lc = expand_grid( molasses = seq(from = 0.01, to = 4, length.out = 50), ginger = seq(0.01, to = 4, length.out = 50) ) %&gt;% mutate(yhat = predict(m_lc, newdata = tibble(molasses, ginger)), # Undo the logit transformation! yhat = exp(yhat) / (1 + exp(yhat)), # Undo the (y + 1) / 100 transformation yhat = 100*yhat - 1) # Visualize it! g_lc = mypred_lc %&gt;% ggplot(mapping = aes(x = molasses, y = ginger, z = yhat)) + geom_contour_fill(bins = 15, color = &quot;white&quot;) + geom_text_contour(skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, # Label each contour twice, but check_overlap deletes labels that overlap! label.placer = label_placer_n(2), check_overlap = TRUE) # Add good colors and theming! g_lc + scale_fill_viridis(option = &quot;plasma&quot;) + theme_classic(base_size = 14) + theme(axis.line = element_blank()) + # get rid of axis line labs(x = &quot;Molasses (cups)&quot;, y = &quot;Ginger (tablespoons)&quot;, fill = &quot;Predicted\\nYum\\nFactor&quot;, subtitle = &quot;Contour of Predicted Gingerbread Cookie Tastiness&quot;) Our predictive power is still not quite that good. Ironically, our model (based on fake data) suggests that the best gingerbread cookies you can make should either have very little molasses OR lots of molasses and ginger, but the payoff for using very little molasses will be higher! This plot demonstrates how even though your original model m4 predicted really high payoff for adding more molasses, when we compare those predictions to updated model predictions based on new experiments, we might find that the new empirical data tempers our earlier predictions. This is good news. This probably means that our earlier predictions were not very accurate, and our extra experiments paid off by helping clarify. New results can be suprising, but are never a bad thing - because they get you closer to truth. 15.3 Iterate! Suppose now that we expanded our factorial experiment to vary the amount of flour, butter, and cinnamon too! Weve saved this data in workshops/gingerbread_test3.csv. How would we model this data? cookies3 = read_csv(&quot;workshops/gingerbread_test3.csv&quot;) cookies3 %&gt;% glimpse() ## Rows: 46,080 ## Columns: 8 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18 ## $ batch &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2 ## $ yum &lt;dbl&gt; 0, 4, 9, 10, 4, 8, 7, 3, 1, 12, 0, 4, 6, 10, 0, 0, 10, 5, 1,  ## $ molasses &lt;dbl&gt; 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0 ## $ ginger &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ## $ cinnamon &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ## $ butter &lt;dbl&gt; 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0.75, 0 ## $ flour &lt;dbl&gt; 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2.75, 2 15.3.1 Modeling many Interactions We can make a second-order polynomial for these 5 variables with lm() or rsm(), like so: # model using rsm() r5 = cookies3 %&gt;% rsm(formula = yum ~ SO(molasses, ginger, cinnamon, butter, flour)) # model using lm() m5 = cookies3 %&gt;% lm(formula = yum ~ molasses * ginger * cinnamon * butter * flour + I(molasses^2) + I(ginger^2) + I(cinnamon^2) + I(butter^2) + I(flour^2)) # Check out our coefficients! Wow that&#39;s a long list! m5$coefficients %&gt;% round(3) ## (Intercept) molasses ## 6.628 11.816 ## ginger cinnamon ## 10.739 -60.324 ## butter flour ## -1.272 0.078 ## I(molasses^2) I(ginger^2) ## 2.585 0.302 ## I(cinnamon^2) I(butter^2) ## -0.034 -4.601 ## I(flour^2) molasses:ginger ## -0.868 -11.493 ## molasses:cinnamon ginger:cinnamon ## 32.053 5.694 ## molasses:butter ginger:butter ## -9.758 -5.691 ## cinnamon:butter molasses:flour ## 65.636 -1.146 ## ginger:flour cinnamon:flour ## -2.347 21.764 ## butter:flour molasses:ginger:cinnamon ## 7.047 -2.030 ## molasses:ginger:butter molasses:cinnamon:butter ## 6.798 -36.209 ## ginger:cinnamon:butter molasses:ginger:flour ## -11.122 2.366 ## molasses:cinnamon:flour ginger:cinnamon:flour ## -12.649 -2.576 ## molasses:butter:flour ginger:butter:flour ## 0.025 0.667 ## cinnamon:butter:flour molasses:ginger:cinnamon:butter ## -23.226 5.925 ## molasses:ginger:cinnamon:flour molasses:ginger:butter:flour ## 1.566 -0.987 ## molasses:cinnamon:butter:flour ginger:cinnamon:butter:flour ## 13.912 4.207 ## molasses:ginger:cinnamon:butter:flour ## -2.742 # Check predictive power (still pretty bad!) m5 %&gt;% glance() ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.168 0.167 7.06 258. 0 36 -155424. 310924. 311256. ## #  3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # Check which variables are significant # (if some were not, we might cut them if we wanted to make as parsimonious a model as possible) m5 %&gt;% tidy() %&gt;% filter(p.value &lt; 0.05) ## # A tibble: 10 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cinnamon -60.3 24.8 -2.43 1.51e- 2 ## 2 I(molasses^2) 2.58 0.115 22.5 1.35e-111 ## 3 I(ginger^2) 0.302 0.0287 10.5 6.74e- 26 ## 4 I(butter^2) -4.60 1.12 -4.12 3.77e- 5 ## 5 cinnamon:butter 65.6 24.3 2.70 6.99e- 3 ## 6 cinnamon:flour 21.8 8.26 2.64 8.41e- 3 ## 7 molasses:cinnamon:butter -36.2 16.3 -2.22 2.66e- 2 ## 8 molasses:cinnamon:flour -12.6 5.54 -2.28 2.25e- 2 ## 9 cinnamon:butter:flour -23.2 8.09 -2.87 4.10e- 3 ## 10 molasses:cinnamon:butter:flour 13.9 5.43 2.56 1.04e- 2 15.3.2 Contours with Multiple Variables Now, whenever we analyze contours, since we have more than 2 predictors, we need multiple plots. For example, lets examine variation in yum as 3 predictions change simultaneously. These include molasses, ginger, and cinnamon. We can just write in contour() the formula ~molasses + ginger + cinnamon. It will place molasses and ginger on the x and y axes, because they came first, and then report the values of cinnamon, butter, and flour for each panel. However, the mechanics of contour() can be tricky, and its tough to compare plots like these, since they are switching the x and y axis in every plot! But what if we could make our own in ggplot? # We can split it into 1 row and 3 columns using par(mfrow = c(1, 3)) par(mfrow = c(1,3)) # And plot the contous like so contour(m5, ~molasses + ginger + cinnamon, image = TRUE) 15.3.3 ggplot contour plots! This is the power of ggplot - since you have to work with the data yourself, you actually know what your plots mean and can design the plots most useful to your team. For example, I would love to see 2 panels showing the contours of molasses x ginger when cinnamon = 0, cinnamon = 1, and cinnamon = 2 tablespoons. All other conditions would be held constant, allowing us to see how the contour changes shape. If we hold constant the other values though, we should hold them at meaningful values, like the average or perhaps a value you know to be sufficient. cookies3$cinnamon %&gt;% range() ## [1] 0.5 2.0 cookies3$butter %&gt;% mean() ## [1] 1 # Get a grid... mygrid = expand_grid( molasses = seq(from = 0, to = 4, length.out = 30), ginger = seq(from = 0, to = 4, length.out = 30), # Now repeat that grid for each of these values of cinnamon! cinnamon = c(0, 1, 2), # Hold other constant at meaningful values flour = cookies3$flour %&gt;% mean(), butter = cookies3$butter %&gt;% mean()) %&gt;% # Then predict your outcome! mutate(yhat = predict(m5, newdata = tibble(molasses, ginger, cinnamon, flour, butter))) Next, lets use our grid to visualize the contours in ggplot! # Let&#39;s check it out! g4 = mygrid %&gt;% # Map aesthetics ggplot(mapping = aes(x = molasses, y = ginger, z = yhat)) + # SPLIT INTO PANELS by amount of cinnamon! facet_wrap(~cinnamon) + geom_contour_fill(binwidth = 5, color = &quot;white&quot;) + geom_text_contour(skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, label.placer = label_placer_n(1), check_overlap = TRUE) # View it! g4 Finally, lets improve the labels, colors, and theming for this plot. g5 = g4 + theme_classic(base_size = 14) + theme(axis.line = element_blank(), # clean up the lines axis.ticks = element_blank(), # clean up the ticks strip.background = element_blank()) + # clean up the facet labels scale_fill_viridis(option = &quot;plasma&quot;) + labs(x = &quot;Molasses (cups)&quot;, y = &quot;Ginger (tablespoons)&quot;, fill = &quot;Predicted\\nYum\\nFactor&quot;, subtitle = &quot;Predicted Gingerbread Cookie Tastiness\\nby Tablespoons of Cinnamon&quot;) # view it! g5 What can we learn from this plot? When we add more cinnamon (2 tbsp; right), the zone in which cookies are truly bad (&lt;15 points of yum) shrinks greatly (compared to left and center panels). Otherwise, cinnamon has fairly minimal interaction effects with ginger and molasses on yum scores. Learning Check 3 Suppose we want to examine other interactions! Design your own ggplot to test how butter shapes the yum factor as molasses and flour vary. Question Put molasses on the x-axis from 0 to 4 cups, flour on the y-axis from 0 to 4 cups, and vary the level of butter across panels from 0.75 to 1 to 1.25 cups. Hold other conditions at their mean values. [View Answer!] # Make the grid!s mygrid_lc3 = expand_grid( # Vary molasses and flour... molasses = seq(from = 0, to = 3, length.out = 30), flour = seq(from = 0, to = 3, length.out = 30), # Now repeat that grid for each of these values of butter! butter = c(0.75, 1, 1.25), # Hold other constant at meaningful values ginger = cookies3$ginger %&gt;% mean(), cinnamon = cookies3$cinnamon %&gt;% mean()) %&gt;% # Then predict your outcome! mutate(yhat = predict(m5, newdata = tibble(molasses, ginger, cinnamon, flour, butter))) # Visualize! mygrid_lc3 %&gt;% ggplot(mapping = aes(x = molasses, y = flour, z = yhat)) + # SPLIT INTO PANELS by amount of butter! facet_wrap(~butter) + geom_contour_fill(binwidth = 5, color = &quot;white&quot;) + geom_text_contour(skip = 0, stroke.color = &quot;white&quot;, stroke = 0.2, label.placer = label_placer_n(1), check_overlap = TRUE) + # Add theming! scale_fill_viridis(option = &quot;plasma&quot;) + theme_classic(base_size = 14) + theme(axis.line = element_blank(), # clean up the lines axis.ticks = element_blank(), # clean up the ticks strip.background = element_blank()) + labs(x = &quot;Molasses (cups)&quot;, y = &#39;Flour (cups)&#39;, fill = &quot;Predicted\\nYum\\nFactor&quot;, subtitle = &quot;Predicted Gingerbread Cookie Tastiness\\nby Cups of Butter&quot;) This plot tells us that adding more butter to the cookies tends to reduce the amount of the contour with low yum scores, and increases the relative share of the the response surface with scores of 15 or 20. 15.4 Quantities of Interest in RSM Finally, we might be interested in calculating (and annotating our charts) with some key quantities of interest! Lets use our model m5 from earlier and its rsm counterpart r5. 15.4.1 Percent Change in Bins First, when comparing change across panels, were essentially comparing change in area. So we can use our grid of conditions and predictions mygrid to calculate those percentages! area = mygrid %&gt;% # Cut the outcome into bins, 5 units wide on the yum scale mutate(bin = cut_interval(yhat, length = 5)) %&gt;% # For each panel and bin, count up the predictions in that interval group_by(cinnamon, bin) %&gt;% summarize(count = n()) %&gt;% ungroup() %&gt;% # Now, for each panel, calculate the percentage of predictions in that panel located in each bin group_by(cinnamon) %&gt;% mutate(percent = count / sum(count), percent = round(percent*100, 2)) # Zoom into the lowest bin. # What percentage of the area was in that bin given each level of cinnamon? qi1 = area %&gt;% filter(bin == &quot;[10,15]&quot;) qi1 ## # A tibble: 3 × 4 ## # Groups: cinnamon [3] ## cinnamon bin count percent ## &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 [10,15] 92 10.2 ## 2 1 [10,15] 66 7.33 ## 3 2 [10,15] 36 4 We computed that the area predicted to score lowest on the yum scale (10-15) decreased from 10.22 given 0 tablespoons of cinnamon to 7.33 given 1 tablespoon and then to 4 given 2 tablespoons of cinnamon. 15.4.2 Study Range We might want our reader to know what is the area that we actually had data on, versus what was the area we were generating predictions from. For this, we can just draw a box from our raw data, using summarize() and geom_rect(). geom_rect() requires an xmin, xmax, ymin, and ymax. For example, since molasses is our x variable and ginger has been our y variable in our ggplot analyses, we can do the following: box = cookies3 %&gt;% summarize(xmin = min(molasses), xmax = max(molasses), ymin = min(ginger), ymax = max(molasses)) # For example, we can start a new ggplot ggplot() + # Mapping the contour fill geom_contour_fill(data = mygrid, mapping = aes(x = molasses, y = ginger, z = yhat), color = &#39;white&#39;) + facet_wrap(~cinnamon) + # And then plotting a box overtop, with no fill geom_rect(data = box, mapping = aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), color = &quot;white&quot;, linetype = &#39;dashed&#39;, fill = NA) 15.5 Extra Concepts of Interest: Canonical Form The canonical form is an abbreviation of your long model equation into a much more understandable, short form. rsm will calculate the canonical form of your model equation for you, using canonical(). For example, if you have 2 predictors \\(X_{1}\\) and \\(X_{2}\\), the canonical form would look like: \\[ \\hat{Y} = Y_{s} + X_{1}^{2} + X_{2}^{2}\\] where \\(Y_{s}\\) is the value of \\(\\hat{Y}\\) at the stationary point, when \\(X_{1}\\) and \\(X_{2}\\) equal 0. The tricky thing is that the canonical form is not actually in units of our original predictors, say, cups of molasses and tablespoons of ginger. Instead, the canonical form is like a standardized format that maps every value of cups of molasses (written \\(x_{1}\\)) to a new value \\(X_{1}\\), which is a certain distance away from the stationary point. Why would we have a system like this? Its because certain shapes of contour plots can be recognized from their canonical form alone, with no other detail, so the canonical form can be useful for us. Its a little more than we can go into at this moment, but suffice it to say that the canonical form of a model is like a shortcut for interpreting the shape of a contour plot. 15.5.1 Finding the Canonical Form We can run the calculations for obtaining the canonical form using canonical() in rsm. canon = canonical(r5) Inside this object are several great quantities of interest. 15.5.2 Canonical Form Coefficients # These coefficients can form the canonical form model equation canon$eigen$values ## [1] 2.622365 0.000000 0.000000 -0.725773 -4.803057 15.5.3 Stationary Points # These are your stationary points. canon$xs ## molasses ginger cinnamon butter flour ## 0.5267221 -0.1922253 -0.5966090 1.3474297 3.2895704 15.5.4 Shift Points # These values can help you convert from x_1 (normal units) to X_1 (canonical form) canon$eigen$vectors ## [,1] [,2] [,3] [,4] [,5] ## molasses 0.99458167 -0.01095125 0.09352416 -0.004894275 -0.04377947 ## ginger -0.01596009 -0.99629842 0.02526971 -0.061114741 -0.05254671 ## cinnamon 0.08987515 -0.01363529 -0.97819754 -0.185204008 -0.02378214 ## butter -0.03957109 0.03864526 -0.02037369 0.210855287 -0.97573851 ## flour 0.03015222 -0.07476727 -0.18256277 0.957852054 0.20661795 In general, I find that the contour plots themselves tend to be the most effective tools for decision-making in most cases, but advanced applications can make great use of the canonical forms to identify key points after which increasing the amount of an ingredient will make no more difference. "],["conclusion-11.html", "Conclusion", " Conclusion Congratulations! You made it! Thank you for your hard work this term. You have built up considerable prowess making probabilistic predictions, making statistical models, and visualizing and communicating your findings in R. We hope that this course has given you a firm grounding in key tools for building safer, more reliable technologies for our communities! We wish you all the best in your future work! Figure 15.3: Courtesy of the Internet "]]
